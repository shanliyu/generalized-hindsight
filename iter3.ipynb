{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a167fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[15518]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa9778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a740). One of the two will be used. Which one is undefined.\n",
      "objc[15518]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa9700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a768). One of the two will be used. Which one is undefined.\n",
      "objc[15518]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa97a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a7b8). One of the two will be used. Which one is undefined.\n",
      "objc[15518]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa9818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a830). One of the two will be used. Which one is undefined.\n",
      "using sparse reward if specified\n",
      "2021-05-25 10:24:59.424982 PDT | Variant:\n",
      "2021-05-25 10:24:59.425577 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"pointreacherobs\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 20,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 20,\n",
      "    \"num_train_loops_per_epoch\": 5\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.97,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 2000,\n",
      "    \"latent_dim\": 6,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"sparse_reward\": null,\n",
      "    \"fixed_ratio\": null\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"horizon\": 20\n",
      "  }\n",
      "}\n",
      "pointreacher\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "2021-05-25 10:25:34.660135 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 0 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   1100\n",
      "trainer/QF1 Loss                                        0.576872\n",
      "trainer/QF2 Loss                                        0.57548\n",
      "trainer/Policy Loss                                    -1.37884\n",
      "trainer/Q1 Predictions Mean                            -0.00126527\n",
      "trainer/Q1 Predictions Std                              0.000605492\n",
      "trainer/Q1 Predictions Max                              0.000269961\n",
      "trainer/Q1 Predictions Min                             -0.00227255\n",
      "trainer/Q2 Predictions Mean                            -4.35855e-06\n",
      "trainer/Q2 Predictions Std                              0.000609121\n",
      "trainer/Q2 Predictions Max                              0.00133012\n",
      "trainer/Q2 Predictions Min                             -0.00163013\n",
      "trainer/Q Targets Mean                                  0.514032\n",
      "trainer/Q Targets Std                                   0.557929\n",
      "trainer/Q Targets Max                                   1.83678\n",
      "trainer/Q Targets Min                                  -1.35869\n",
      "trainer/Log Pis Mean                                   -1.38018\n",
      "trainer/Log Pis Std                                     0.289063\n",
      "trainer/Log Pis Max                                    -0.577882\n",
      "trainer/Log Pis Min                                    -2.60453\n",
      "trainer/Policy mu Mean                                  0.000988936\n",
      "trainer/Policy mu Std                                   0.00058491\n",
      "trainer/Policy mu Max                                   0.00221213\n",
      "trainer/Policy mu Min                                   1.31949e-05\n",
      "trainer/Policy log std Mean                             4.21378e-05\n",
      "trainer/Policy log std Std                              0.000645919\n",
      "trainer/Policy log std Max                              0.000916166\n",
      "trainer/Policy log std Min                             -0.00127692\n",
      "trainer/Alpha                                           0.997005\n",
      "trainer/Alpha Loss                                     -0\n",
      "exploration/num steps total                          1100\n",
      "exploration/num paths total                            55\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.674579\n",
      "exploration/Rewards Std                                 0.290019\n",
      "exploration/Rewards Max                                -0.105313\n",
      "exploration/Rewards Min                                -1.25229\n",
      "exploration/Returns Mean                              -13.4916\n",
      "exploration/Returns Std                                 1.35362\n",
      "exploration/Returns Max                               -11.0651\n",
      "exploration/Returns Min                               -14.8536\n",
      "exploration/Actions Mean                                0.0632062\n",
      "exploration/Actions Std                                 0.578647\n",
      "exploration/Actions Max                                 0.98148\n",
      "exploration/Actions Min                                -0.941187\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -13.4916\n",
      "exploration/env_infos/final/reward_dist Mean            3.94634e-51\n",
      "exploration/env_infos/final/reward_dist Std             7.89266e-51\n",
      "exploration/env_infos/final/reward_dist Max             1.97317e-50\n",
      "exploration/env_infos/final/reward_dist Min             4.19542e-102\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0344088\n",
      "exploration/env_infos/initial/reward_dist Std           0.0428659\n",
      "exploration/env_infos/initial/reward_dist Max           0.106806\n",
      "exploration/env_infos/initial/reward_dist Min           9.56386e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0187599\n",
      "exploration/env_infos/reward_dist Std                   0.0898929\n",
      "exploration/env_infos/reward_dist Max                   0.836662\n",
      "exploration/env_infos/reward_dist Min                   4.19542e-102\n",
      "exploration/env_infos/final/reward_energy Mean         -0.787813\n",
      "exploration/env_infos/final/reward_energy Std           0.299654\n",
      "exploration/env_infos/final/reward_energy Max          -0.19223\n",
      "exploration/env_infos/final/reward_energy Min          -1.00022\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.87809\n",
      "exploration/env_infos/initial/reward_energy Std         0.131384\n",
      "exploration/env_infos/initial/reward_energy Max        -0.719892\n",
      "exploration/env_infos/initial/reward_energy Min        -1.09336\n",
      "exploration/env_infos/reward_energy Mean               -0.775129\n",
      "exploration/env_infos/reward_energy Std                 0.277183\n",
      "exploration/env_infos/reward_energy Max                -0.19223\n",
      "exploration/env_infos/reward_energy Min                -1.25569\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.412974\n",
      "exploration/env_infos/final/end_effector_loc Std        0.646101\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.0216553\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.022725\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0471042\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0352084\n",
      "exploration/env_infos/end_effector_loc Mean             0.273985\n",
      "exploration/env_infos/end_effector_loc Std              0.485792\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           1000\n",
      "evaluation/num paths total                             50\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0780623\n",
      "evaluation/Rewards Std                                  0.0473164\n",
      "evaluation/Rewards Max                                  0.0425092\n",
      "evaluation/Rewards Min                                 -0.150422\n",
      "evaluation/Returns Mean                                -1.56125\n",
      "evaluation/Returns Std                                  0.945244\n",
      "evaluation/Returns Max                                  0.807234\n",
      "evaluation/Returns Min                                 -2.97967\n",
      "evaluation/Actions Mean                                 0.00098014\n",
      "evaluation/Actions Std                                  0.000587866\n",
      "evaluation/Actions Max                                  0.0020782\n",
      "evaluation/Actions Min                                  0.000173299\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.56125\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00593428\n",
      "evaluation/env_infos/final/reward_dist Std              0.00912983\n",
      "evaluation/env_infos/final/reward_dist Max              0.0441983\n",
      "evaluation/env_infos/final/reward_dist Min              5.15938e-07\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00533279\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00750724\n",
      "evaluation/env_infos/initial/reward_dist Max            0.02745\n",
      "evaluation/env_infos/initial/reward_dist Min            9.54696e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.00547818\n",
      "evaluation/env_infos/reward_dist Std                    0.0078623\n",
      "evaluation/env_infos/reward_dist Max                    0.0441983\n",
      "evaluation/env_infos/reward_dist Min                    5.15938e-07\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.00163802\n",
      "evaluation/env_infos/final/reward_energy Std            0.000251158\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00111856\n",
      "evaluation/env_infos/final/reward_energy Min           -0.00210778\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.00156036\n",
      "evaluation/env_infos/initial/reward_energy Std          0.00021699\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00111835\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.00196039\n",
      "evaluation/env_infos/reward_energy Mean                -0.00159921\n",
      "evaluation/env_infos/reward_energy Std                  0.000234643\n",
      "evaluation/env_infos/reward_energy Max                 -0.00111835\n",
      "evaluation/env_infos/reward_energy Min                 -0.00210778\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.0102276\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.00607673\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.0207555\n",
      "evaluation/env_infos/final/end_effector_loc Min         0.00191566\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      4.80703e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std       2.81336e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Max       9.61769e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Min       8.66496e-06\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00373819\n",
      "evaluation/env_infos/end_effector_loc Std               0.0043006\n",
      "evaluation/env_infos/end_effector_loc Max               0.0207555\n",
      "evaluation/env_infos/end_effector_loc Min               8.66496e-06\n",
      "time/data storing (s)                                   0.00299722\n",
      "time/evaluation sampling (s)                            1.39765\n",
      "time/exploration sampling (s)                           0.122225\n",
      "time/logging (s)                                        0.0222268\n",
      "time/saving (s)                                         0.0734678\n",
      "time/training (s)                                      32.3707\n",
      "time/epoch (s)                                         33.9893\n",
      "time/total (s)                                         38.19\n",
      "Epoch                                                   0\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:26:17.494277 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 1 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   1200\n",
      "trainer/QF1 Loss                                        0.0142788\n",
      "trainer/QF2 Loss                                        0.0179015\n",
      "trainer/Policy Loss                                    -0.759393\n",
      "trainer/Q1 Predictions Mean                            -0.46276\n",
      "trainer/Q1 Predictions Std                              0.624373\n",
      "trainer/Q1 Predictions Max                              0.586792\n",
      "trainer/Q1 Predictions Min                             -2.04044\n",
      "trainer/Q2 Predictions Mean                            -0.537618\n",
      "trainer/Q2 Predictions Std                              0.622405\n",
      "trainer/Q2 Predictions Max                              0.567538\n",
      "trainer/Q2 Predictions Min                             -2.17126\n",
      "trainer/Q Targets Mean                                 -0.486009\n",
      "trainer/Q Targets Std                                   0.654017\n",
      "trainer/Q Targets Max                                   0.673842\n",
      "trainer/Q Targets Min                                  -2.31864\n",
      "trainer/Log Pis Mean                                   -1.18884\n",
      "trainer/Log Pis Std                                     0.472486\n",
      "trainer/Log Pis Max                                    -0.544822\n",
      "trainer/Log Pis Min                                    -3.78867\n",
      "trainer/Policy mu Mean                                  0.0285259\n",
      "trainer/Policy mu Std                                   0.0346983\n",
      "trainer/Policy mu Max                                   0.134678\n",
      "trainer/Policy mu Min                                  -0.0757775\n",
      "trainer/Policy log std Mean                            -0.526448\n",
      "trainer/Policy log std Std                              0.0696464\n",
      "trainer/Policy log std Max                             -0.366739\n",
      "trainer/Policy log std Min                             -0.693374\n",
      "trainer/Alpha                                           0.225887\n",
      "trainer/Alpha Loss                                     -4.73488\n",
      "exploration/num steps total                          1200\n",
      "exploration/num paths total                            60\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.517996\n",
      "exploration/Rewards Std                                 0.284947\n",
      "exploration/Rewards Max                                 0.0111953\n",
      "exploration/Rewards Min                                -1.26828\n",
      "exploration/Returns Mean                              -10.3599\n",
      "exploration/Returns Std                                 2.90097\n",
      "exploration/Returns Max                                -6.89275\n",
      "exploration/Returns Min                               -15.7143\n",
      "exploration/Actions Mean                                0.046027\n",
      "exploration/Actions Std                                 0.385387\n",
      "exploration/Actions Max                                 0.915877\n",
      "exploration/Actions Min                                -0.852987\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -10.3599\n",
      "exploration/env_infos/final/reward_dist Mean            1.0713e-53\n",
      "exploration/env_infos/final/reward_dist Std             2.14259e-53\n",
      "exploration/env_infos/final/reward_dist Max             5.35648e-53\n",
      "exploration/env_infos/final/reward_dist Min             1.19818e-151\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0103099\n",
      "exploration/env_infos/initial/reward_dist Std           0.00620404\n",
      "exploration/env_infos/initial/reward_dist Max           0.017284\n",
      "exploration/env_infos/initial/reward_dist Min           2.62818e-07\n",
      "exploration/env_infos/reward_dist Mean                  0.0169753\n",
      "exploration/env_infos/reward_dist Std                   0.049046\n",
      "exploration/env_infos/reward_dist Max                   0.271489\n",
      "exploration/env_infos/reward_dist Min                   1.19818e-151\n",
      "exploration/env_infos/final/reward_energy Mean         -0.484143\n",
      "exploration/env_infos/final/reward_energy Std           0.217856\n",
      "exploration/env_infos/final/reward_energy Max          -0.152269\n",
      "exploration/env_infos/final/reward_energy Min          -0.809446\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.473428\n",
      "exploration/env_infos/initial/reward_energy Std         0.250453\n",
      "exploration/env_infos/initial/reward_energy Max        -0.136648\n",
      "exploration/env_infos/initial/reward_energy Min        -0.769096\n",
      "exploration/env_infos/reward_energy Mean               -0.491715\n",
      "exploration/env_infos/reward_energy Std                 0.243927\n",
      "exploration/env_infos/reward_energy Max                -0.0138491\n",
      "exploration/env_infos/reward_energy Min                -1.14363\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.281292\n",
      "exploration/env_infos/final/end_effector_loc Std        0.753392\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.000124701\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0189357\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0364533\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0353364\n",
      "exploration/env_infos/end_effector_loc Mean             0.131406\n",
      "exploration/env_infos/end_effector_loc Std              0.468006\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           2000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.129317\n",
      "evaluation/Rewards Std                                  0.0950763\n",
      "evaluation/Rewards Max                                  0.165276\n",
      "evaluation/Rewards Min                                 -0.637119\n",
      "evaluation/Returns Mean                                -2.58635\n",
      "evaluation/Returns Std                                  1.38028\n",
      "evaluation/Returns Max                                  1.35225\n",
      "evaluation/Returns Min                                 -5.58551\n",
      "evaluation/Actions Mean                                 0.0325454\n",
      "evaluation/Actions Std                                  0.0245784\n",
      "evaluation/Actions Max                                  0.123077\n",
      "evaluation/Actions Min                                 -0.057515\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.58635\n",
      "evaluation/env_infos/final/reward_dist Mean             2.69966e-07\n",
      "evaluation/env_infos/final/reward_dist Std              1.09715e-06\n",
      "evaluation/env_infos/final/reward_dist Max              5.76264e-06\n",
      "evaluation/env_infos/final/reward_dist Min              3.98986e-74\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00497559\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00811029\n",
      "evaluation/env_infos/initial/reward_dist Max            0.027829\n",
      "evaluation/env_infos/initial/reward_dist Min            1.12831e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0435771\n",
      "evaluation/env_infos/reward_dist Std                    0.143624\n",
      "evaluation/env_infos/reward_dist Max                    0.954145\n",
      "evaluation/env_infos/reward_dist Min                    3.98986e-74\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0490002\n",
      "evaluation/env_infos/final/reward_energy Std            0.00727559\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0370384\n",
      "evaluation/env_infos/final/reward_energy Min           -0.0729301\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.0621672\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0211793\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0401044\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.133877\n",
      "evaluation/env_infos/reward_energy Mean                -0.0556139\n",
      "evaluation/env_infos/reward_energy Std                  0.015287\n",
      "evaluation/env_infos/reward_energy Max                 -0.0370384\n",
      "evaluation/env_infos/reward_energy Min                 -0.134243\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.356476\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.266035\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.969306\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.581257\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00178086\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00149003\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.00614425\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00287575\n",
      "evaluation/env_infos/end_effector_loc Mean              0.133145\n",
      "evaluation/env_infos/end_effector_loc Std               0.172783\n",
      "evaluation/env_infos/end_effector_loc Max               0.969306\n",
      "evaluation/env_infos/end_effector_loc Min              -0.581257\n",
      "time/data storing (s)                                   0.00308579\n",
      "time/evaluation sampling (s)                            1.13693\n",
      "time/exploration sampling (s)                           0.127787\n",
      "time/logging (s)                                        0.0202216\n",
      "time/saving (s)                                         0.0366551\n",
      "time/training (s)                                      41.3416\n",
      "time/epoch (s)                                         42.6663\n",
      "time/total (s)                                         81.0213\n",
      "Epoch                                                   1\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:27:04.151054 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 2 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                   1300\r\n",
      "trainer/QF1 Loss                                        0.016673\r\n",
      "trainer/QF2 Loss                                        0.0196761\r\n",
      "trainer/Policy Loss                                     1.37269\r\n",
      "trainer/Q1 Predictions Mean                            -1.49769\r\n",
      "trainer/Q1 Predictions Std                              0.976716\r\n",
      "trainer/Q1 Predictions Max                              0.332782\r\n",
      "trainer/Q1 Predictions Min                             -4.23755\r\n",
      "trainer/Q2 Predictions Mean                            -1.54115\r\n",
      "trainer/Q2 Predictions Std                              0.980129\r\n",
      "trainer/Q2 Predictions Max                              0.280805\r\n",
      "trainer/Q2 Predictions Min                             -4.01748\r\n",
      "trainer/Q Targets Mean                                 -1.5193\r\n",
      "trainer/Q Targets Std                                   1.02966\r\n",
      "trainer/Q Targets Max                                   0.350677\r\n",
      "trainer/Q Targets Min                                  -4.41199\r\n",
      "trainer/Log Pis Mean                                    0.132319\r\n",
      "trainer/Log Pis Std                                     1.14895\r\n",
      "trainer/Log Pis Max                                     3.76002\r\n",
      "trainer/Log Pis Min                                    -6.51631\r\n",
      "trainer/Policy mu Mean                                 -0.0163182\r\n",
      "trainer/Policy mu Std                                   0.341392\r\n",
      "trainer/Policy mu Max                                   1.57785\r\n",
      "trainer/Policy mu Min                                  -1.65983\r\n",
      "trainer/Policy log std Mean                            -1.28305\r\n",
      "trainer/Policy log std Std                              0.384718\r\n",
      "trainer/Policy log std Max                             -0.30159\r\n",
      "trainer/Policy log std Min                             -2.16013\r\n",
      "trainer/Alpha                                           0.0639025\r\n",
      "trainer/Alpha Loss                                     -5.133\r\n",
      "exploration/num steps total                          1300\r\n",
      "exploration/num paths total                            65\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.317213\r\n",
      "exploration/Rewards Std                                 0.206701\r\n",
      "exploration/Rewards Max                                -0.0785534\r\n",
      "exploration/Rewards Min                                -1.08456\r\n",
      "exploration/Returns Mean                               -6.34426\r\n",
      "exploration/Returns Std                                 3.03259\r\n",
      "exploration/Returns Max                                -4.30967\r\n",
      "exploration/Returns Min                               -12.3355\r\n",
      "exploration/Actions Mean                                0.0176146\r\n",
      "exploration/Actions Std                                 0.249469\r\n",
      "exploration/Actions Max                                 0.793571\r\n",
      "exploration/Actions Min                                -0.562318\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -6.34426\r\n",
      "exploration/env_infos/final/reward_dist Mean            1.10525e-05\r\n",
      "exploration/env_infos/final/reward_dist Std             2.2105e-05\r\n",
      "exploration/env_infos/final/reward_dist Max             5.52624e-05\r\n",
      "exploration/env_infos/final/reward_dist Min             8.32703e-100\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00917166\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.0115859\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0301845\r\n",
      "exploration/env_infos/initial/reward_dist Min           1.11096e-05\r\n",
      "exploration/env_infos/reward_dist Mean                  0.0291417\r\n",
      "exploration/env_infos/reward_dist Std                   0.101679\r\n",
      "exploration/env_infos/reward_dist Max                   0.609998\r\n",
      "exploration/env_infos/reward_dist Min                   1.44801e-101\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.447668\r\n",
      "exploration/env_infos/final/reward_energy Std           0.173098\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.170972\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.6067\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.416838\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.144682\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.210439\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.655991\r\n",
      "exploration/env_infos/reward_energy Mean               -0.307407\r\n",
      "exploration/env_infos/reward_energy Std                 0.174901\r\n",
      "exploration/env_infos/reward_energy Max                -0.0300016\r\n",
      "exploration/env_infos/reward_energy Min                -0.815759\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.324347\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.475971\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.857327\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.902187\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00309135\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0152906\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0318791\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0214156\r\n",
      "exploration/env_infos/end_effector_loc Mean             0.143115\r\n",
      "exploration/env_infos/end_effector_loc Std              0.335973\r\n",
      "exploration/env_infos/end_effector_loc Max              0.857327\r\n",
      "exploration/env_infos/end_effector_loc Min             -1\r\n",
      "evaluation/num steps total                           3000\r\n",
      "evaluation/num paths total                            150\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.171235\r\n",
      "evaluation/Rewards Std                                  0.156176\r\n",
      "evaluation/Rewards Max                                  0.0822294\r\n",
      "evaluation/Rewards Min                                 -0.979425\r\n",
      "evaluation/Returns Mean                                -3.4247\r\n",
      "evaluation/Returns Std                                  2.37878\r\n",
      "evaluation/Returns Max                                 -1.03687\r\n",
      "evaluation/Returns Min                                -12.9465\r\n",
      "evaluation/Actions Mean                                -0.00332473\r\n",
      "evaluation/Actions Std                                  0.113027\r\n",
      "evaluation/Actions Max                                  0.655579\r\n",
      "evaluation/Actions Min                                 -0.545786\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -3.4247\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0091053\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.0369815\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.234501\r\n",
      "evaluation/env_infos/final/reward_dist Min              4.91896e-119\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00550878\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0120953\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0659473\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.36827e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0420782\r\n",
      "evaluation/env_infos/reward_dist Std                    0.136436\r\n",
      "evaluation/env_infos/reward_dist Max                    0.985674\r\n",
      "evaluation/env_infos/reward_dist Min                    1.56268e-124\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.189778\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.121088\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00870755\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.55283\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.157609\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.140839\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00895795\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.66551\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.121254\r\n",
      "evaluation/env_infos/reward_energy Std                  0.104257\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00162313\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.66551\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.123052\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.380354\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.51224\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00333402\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00668802\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.032779\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0145551\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0941151\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.253196\r\n",
      "evaluation/env_infos/end_effector_loc Max               1\r\n",
      "evaluation/env_infos/end_effector_loc Min              -0.71139\r\n",
      "time/data storing (s)                                   0.00349065\r\n",
      "time/evaluation sampling (s)                            1.06136\r\n",
      "time/exploration sampling (s)                           0.139693\r\n",
      "time/logging (s)                                        0.0188483\r\n",
      "time/saving (s)                                         0.0263298\r\n",
      "time/training (s)                                      45.341\r\n",
      "time/epoch (s)                                         46.5907\r\n",
      "time/total (s)                                        127.676\r\n",
      "Epoch                                                   2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 10:27:50.188074 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 3 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1400\n",
      "trainer/QF1 Loss                                        0.00992432\n",
      "trainer/QF2 Loss                                        0.0117483\n",
      "trainer/Policy Loss                                     3.07273\n",
      "trainer/Q1 Predictions Mean                            -2.17948\n",
      "trainer/Q1 Predictions Std                              1.31205\n",
      "trainer/Q1 Predictions Max                             -0.0172831\n",
      "trainer/Q1 Predictions Min                             -7.29756\n",
      "trainer/Q2 Predictions Mean                            -2.1591\n",
      "trainer/Q2 Predictions Std                              1.29779\n",
      "trainer/Q2 Predictions Max                             -0.00159588\n",
      "trainer/Q2 Predictions Min                             -7.20492\n",
      "trainer/Q Targets Mean                                 -2.18751\n",
      "trainer/Q Targets Std                                   1.31597\n",
      "trainer/Q Targets Max                                   0.0423553\n",
      "trainer/Q Targets Min                                  -7.1498\n",
      "trainer/Log Pis Mean                                    1.20588\n",
      "trainer/Log Pis Std                                     1.36652\n",
      "trainer/Log Pis Max                                     6.7834\n",
      "trainer/Log Pis Min                                    -3.3873\n",
      "trainer/Policy mu Mean                                 -0.0163686\n",
      "trainer/Policy mu Std                                   0.633887\n",
      "trainer/Policy mu Max                                   2.41305\n",
      "trainer/Policy mu Min                                  -2.83405\n",
      "trainer/Policy log std Mean                            -1.68811\n",
      "trainer/Policy log std Std                              0.555395\n",
      "trainer/Policy log std Max                              0.351206\n",
      "trainer/Policy log std Min                             -2.70888\n",
      "trainer/Alpha                                           0.0278039\n",
      "trainer/Alpha Loss                                     -2.84399\n",
      "exploration/num steps total                          1400\n",
      "exploration/num paths total                            70\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.218513\n",
      "exploration/Rewards Std                                 0.0870036\n",
      "exploration/Rewards Max                                -0.0625078\n",
      "exploration/Rewards Min                                -0.527785\n",
      "exploration/Returns Mean                               -4.37025\n",
      "exploration/Returns Std                                 0.745795\n",
      "exploration/Returns Max                                -3.18479\n",
      "exploration/Returns Min                                -5.5153\n",
      "exploration/Actions Mean                               -0.00675753\n",
      "exploration/Actions Std                                 0.191701\n",
      "exploration/Actions Max                                 0.585023\n",
      "exploration/Actions Min                                -0.698324\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -4.37025\n",
      "exploration/env_infos/final/reward_dist Mean            1.95884e-07\n",
      "exploration/env_infos/final/reward_dist Std             3.91767e-07\n",
      "exploration/env_infos/final/reward_dist Max             9.79418e-07\n",
      "exploration/env_infos/final/reward_dist Min             4.13074e-31\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0065823\n",
      "exploration/env_infos/initial/reward_dist Std           0.00953403\n",
      "exploration/env_infos/initial/reward_dist Max           0.0246377\n",
      "exploration/env_infos/initial/reward_dist Min           5.58389e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0741105\n",
      "exploration/env_infos/reward_dist Std                   0.213269\n",
      "exploration/env_infos/reward_dist Max                   0.996782\n",
      "exploration/env_infos/reward_dist Min                   4.13074e-31\n",
      "exploration/env_infos/final/reward_energy Mean         -0.34174\n",
      "exploration/env_infos/final/reward_energy Std           0.133296\n",
      "exploration/env_infos/final/reward_energy Max          -0.180776\n",
      "exploration/env_infos/final/reward_energy Min          -0.5564\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.194765\n",
      "exploration/env_infos/initial/reward_energy Std         0.0711915\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0959558\n",
      "exploration/env_infos/initial/reward_energy Min        -0.266792\n",
      "exploration/env_infos/reward_energy Mean               -0.227697\n",
      "exploration/env_infos/reward_energy Std                 0.147458\n",
      "exploration/env_infos/reward_energy Max                -0.0210123\n",
      "exploration/env_infos/reward_energy Min                -0.725702\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0748444\n",
      "exploration/env_infos/final/end_effector_loc Std        0.382551\n",
      "exploration/env_infos/final/end_effector_loc Max        0.524773\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.723146\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00169161\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00713376\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0111471\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0120533\n",
      "exploration/env_infos/end_effector_loc Mean             0.00363569\n",
      "exploration/env_infos/end_effector_loc Std              0.223757\n",
      "exploration/env_infos/end_effector_loc Max              0.524773\n",
      "exploration/env_infos/end_effector_loc Min             -0.723146\n",
      "evaluation/num steps total                           4000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.154526\n",
      "evaluation/Rewards Std                                  0.126546\n",
      "evaluation/Rewards Max                                  0.0852592\n",
      "evaluation/Rewards Min                                 -1.04159\n",
      "evaluation/Returns Mean                                -3.09053\n",
      "evaluation/Returns Std                                  1.68053\n",
      "evaluation/Returns Max                                 -0.385755\n",
      "evaluation/Returns Min                                 -9.44947\n",
      "evaluation/Actions Mean                                -0.0106644\n",
      "evaluation/Actions Std                                  0.115393\n",
      "evaluation/Actions Max                                  0.447187\n",
      "evaluation/Actions Min                                 -0.590926\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.09053\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0139149\n",
      "evaluation/env_infos/final/reward_dist Std              0.0439649\n",
      "evaluation/env_infos/final/reward_dist Max              0.234655\n",
      "evaluation/env_infos/final/reward_dist Min              1.7802e-77\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00607394\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00989201\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0349882\n",
      "evaluation/env_infos/initial/reward_dist Min            6.16823e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0783719\n",
      "evaluation/env_infos/reward_dist Std                    0.188876\n",
      "evaluation/env_infos/reward_dist Max                    0.983201\n",
      "evaluation/env_infos/reward_dist Min                    1.7802e-77\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.194703\n",
      "evaluation/env_infos/final/reward_energy Std            0.119711\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0241036\n",
      "evaluation/env_infos/final/reward_energy Min           -0.594926\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.175934\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0973403\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0299292\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.542958\n",
      "evaluation/env_infos/reward_energy Mean                -0.136909\n",
      "evaluation/env_infos/reward_energy Std                  0.0900816\n",
      "evaluation/env_infos/reward_energy Max                 -0.00539068\n",
      "evaluation/env_infos/reward_energy Min                 -0.594926\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.111028\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.366704\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.688137\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000338744\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00710071\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0223594\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.018419\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0352607\n",
      "evaluation/env_infos/end_effector_loc Std               0.245378\n",
      "evaluation/env_infos/end_effector_loc Max               0.688137\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00297755\n",
      "time/evaluation sampling (s)                            0.949526\n",
      "time/exploration sampling (s)                           0.12669\n",
      "time/logging (s)                                        0.0193505\n",
      "time/saving (s)                                         0.0291837\n",
      "time/training (s)                                      44.8391\n",
      "time/epoch (s)                                         45.9668\n",
      "time/total (s)                                        173.713\n",
      "Epoch                                                   3\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:28:37.803683 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 4 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1500\n",
      "trainer/QF1 Loss                                        0.00980622\n",
      "trainer/QF2 Loss                                        0.00970217\n",
      "trainer/Policy Loss                                     4.19382\n",
      "trainer/Q1 Predictions Mean                            -2.77101\n",
      "trainer/Q1 Predictions Std                              1.67244\n",
      "trainer/Q1 Predictions Max                             -0.181449\n",
      "trainer/Q1 Predictions Min                             -9.09773\n",
      "trainer/Q2 Predictions Mean                            -2.83605\n",
      "trainer/Q2 Predictions Std                              1.68902\n",
      "trainer/Q2 Predictions Max                             -0.220586\n",
      "trainer/Q2 Predictions Min                             -9.08909\n",
      "trainer/Q Targets Mean                                 -2.80172\n",
      "trainer/Q Targets Std                                   1.67376\n",
      "trainer/Q Targets Max                                  -0.304971\n",
      "trainer/Q Targets Min                                  -9.10335\n",
      "trainer/Log Pis Mean                                    1.73946\n",
      "trainer/Log Pis Std                                     1.47903\n",
      "trainer/Log Pis Max                                     8.6428\n",
      "trainer/Log Pis Min                                    -5.90367\n",
      "trainer/Policy mu Mean                                 -0.0116877\n",
      "trainer/Policy mu Std                                   0.768037\n",
      "trainer/Policy mu Max                                   3.12088\n",
      "trainer/Policy mu Min                                  -2.7154\n",
      "trainer/Policy log std Mean                            -1.85284\n",
      "trainer/Policy log std Std                              0.609531\n",
      "trainer/Policy log std Max                             -0.248283\n",
      "trainer/Policy log std Min                             -2.81579\n",
      "trainer/Alpha                                           0.0179338\n",
      "trainer/Alpha Loss                                     -1.04753\n",
      "exploration/num steps total                          1500\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.185039\n",
      "exploration/Rewards Std                                 0.0864499\n",
      "exploration/Rewards Max                                -0.00210195\n",
      "exploration/Rewards Min                                -0.502919\n",
      "exploration/Returns Mean                               -3.70077\n",
      "exploration/Returns Std                                 0.686651\n",
      "exploration/Returns Max                                -2.5347\n",
      "exploration/Returns Min                                -4.67839\n",
      "exploration/Actions Mean                               -0.0191028\n",
      "exploration/Actions Std                                 0.119402\n",
      "exploration/Actions Max                                 0.34221\n",
      "exploration/Actions Min                                -0.37382\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -3.70077\n",
      "exploration/env_infos/final/reward_dist Mean            2.69802e-06\n",
      "exploration/env_infos/final/reward_dist Std             3.4599e-06\n",
      "exploration/env_infos/final/reward_dist Max             8.42812e-06\n",
      "exploration/env_infos/final/reward_dist Min             4.61265e-28\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0120991\n",
      "exploration/env_infos/initial/reward_dist Std           0.0239685\n",
      "exploration/env_infos/initial/reward_dist Max           0.0600351\n",
      "exploration/env_infos/initial/reward_dist Min           9.77122e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0578942\n",
      "exploration/env_infos/reward_dist Std                   0.155278\n",
      "exploration/env_infos/reward_dist Max                   0.794904\n",
      "exploration/env_infos/reward_dist Min                   4.61265e-28\n",
      "exploration/env_infos/final/reward_energy Mean         -0.241985\n",
      "exploration/env_infos/final/reward_energy Std           0.0910221\n",
      "exploration/env_infos/final/reward_energy Max          -0.153321\n",
      "exploration/env_infos/final/reward_energy Min          -0.405928\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.192135\n",
      "exploration/env_infos/initial/reward_energy Std         0.0863032\n",
      "exploration/env_infos/initial/reward_energy Max        -0.124289\n",
      "exploration/env_infos/initial/reward_energy Min        -0.346944\n",
      "exploration/env_infos/reward_energy Mean               -0.150374\n",
      "exploration/env_infos/reward_energy Std                 0.0814307\n",
      "exploration/env_infos/reward_energy Max                -0.0108661\n",
      "exploration/env_infos/reward_energy Min                -0.405928\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0977437\n",
      "exploration/env_infos/final/end_effector_loc Std        0.328272\n",
      "exploration/env_infos/final/end_effector_loc Max        0.412813\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.550653\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00133301\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00732653\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0171105\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0098357\n",
      "exploration/env_infos/end_effector_loc Mean            -0.00177883\n",
      "exploration/env_infos/end_effector_loc Std              0.193416\n",
      "exploration/env_infos/end_effector_loc Max              0.412813\n",
      "exploration/env_infos/end_effector_loc Min             -0.550653\n",
      "evaluation/num steps total                           5000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.138405\n",
      "evaluation/Rewards Std                                  0.127079\n",
      "evaluation/Rewards Max                                  0.175818\n",
      "evaluation/Rewards Min                                 -0.875634\n",
      "evaluation/Returns Mean                                -2.7681\n",
      "evaluation/Returns Std                                  1.6903\n",
      "evaluation/Returns Max                                  0.848121\n",
      "evaluation/Returns Min                                 -7.44648\n",
      "evaluation/Actions Mean                                -0.0132224\n",
      "evaluation/Actions Std                                  0.123843\n",
      "evaluation/Actions Max                                  0.558661\n",
      "evaluation/Actions Min                                 -0.756\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.7681\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0327722\n",
      "evaluation/env_infos/final/reward_dist Std              0.0957473\n",
      "evaluation/env_infos/final/reward_dist Max              0.495844\n",
      "evaluation/env_infos/final/reward_dist Min              1.7421e-111\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.0056543\n",
      "evaluation/env_infos/initial/reward_dist Std            0.009642\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0369354\n",
      "evaluation/env_infos/initial/reward_dist Min            9.7795e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0708348\n",
      "evaluation/env_infos/reward_dist Std                    0.177649\n",
      "evaluation/env_infos/reward_dist Max                    0.998196\n",
      "evaluation/env_infos/reward_dist Min                    1.7421e-111\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.226136\n",
      "evaluation/env_infos/final/reward_energy Std            0.142231\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0337407\n",
      "evaluation/env_infos/final/reward_energy Min           -0.616076\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.24447\n",
      "evaluation/env_infos/initial/reward_energy Std          0.185361\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0357208\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.872488\n",
      "evaluation/env_infos/reward_energy Mean                -0.139616\n",
      "evaluation/env_infos/reward_energy Std                  0.107384\n",
      "evaluation/env_infos/reward_energy Max                 -0.00332395\n",
      "evaluation/env_infos/reward_energy Min                 -0.872488\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.120011\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.356042\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.965892\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000624074\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0108289\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.027933\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0378\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0319308\n",
      "evaluation/env_infos/end_effector_loc Std               0.22982\n",
      "evaluation/env_infos/end_effector_loc Max               0.965892\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00349711\n",
      "time/evaluation sampling (s)                            1.34777\n",
      "time/exploration sampling (s)                           0.132789\n",
      "time/logging (s)                                        0.026126\n",
      "time/saving (s)                                         0.0350646\n",
      "time/training (s)                                      45.9903\n",
      "time/epoch (s)                                         47.5356\n",
      "time/total (s)                                        221.335\n",
      "Epoch                                                   4\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:29:26.335015 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 5 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   1600\r\n",
      "trainer/QF1 Loss                                        0.00391405\r\n",
      "trainer/QF2 Loss                                        0.00828227\r\n",
      "trainer/Policy Loss                                     4.53958\r\n",
      "trainer/Q1 Predictions Mean                            -2.71977\r\n",
      "trainer/Q1 Predictions Std                              1.61255\r\n",
      "trainer/Q1 Predictions Max                             -0.24123\r\n",
      "trainer/Q1 Predictions Min                            -10.4015\r\n",
      "trainer/Q2 Predictions Mean                            -2.6909\r\n",
      "trainer/Q2 Predictions Std                              1.61907\r\n",
      "trainer/Q2 Predictions Max                             -0.162658\r\n",
      "trainer/Q2 Predictions Min                            -10.2839\r\n",
      "trainer/Q Targets Mean                                 -2.72431\r\n",
      "trainer/Q Targets Std                                   1.61548\r\n",
      "trainer/Q Targets Max                                  -0.134168\r\n",
      "trainer/Q Targets Min                                 -10.3591\r\n",
      "trainer/Log Pis Mean                                    2.15081\r\n",
      "trainer/Log Pis Std                                     1.18245\r\n",
      "trainer/Log Pis Max                                     5.55259\r\n",
      "trainer/Log Pis Min                                    -4.33486\r\n",
      "trainer/Policy mu Mean                                 -0.0691641\r\n",
      "trainer/Policy mu Std                                   0.567645\r\n",
      "trainer/Policy mu Max                                   2.31754\r\n",
      "trainer/Policy mu Min                                  -2.55865\r\n",
      "trainer/Policy log std Mean                            -2.14175\r\n",
      "trainer/Policy log std Std                              0.568775\r\n",
      "trainer/Policy log std Max                             -0.37912\r\n",
      "trainer/Policy log std Min                             -3.00872\r\n",
      "trainer/Alpha                                           0.0152722\r\n",
      "trainer/Alpha Loss                                      0.630623\r\n",
      "exploration/num steps total                          1600\r\n",
      "exploration/num paths total                            80\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.159711\r\n",
      "exploration/Rewards Std                                 0.077504\r\n",
      "exploration/Rewards Max                                -0.0124729\r\n",
      "exploration/Rewards Min                                -0.407509\r\n",
      "exploration/Returns Mean                               -3.19422\r\n",
      "exploration/Returns Std                                 1.14011\r\n",
      "exploration/Returns Max                                -1.35609\r\n",
      "exploration/Returns Min                                -4.27307\r\n",
      "exploration/Actions Mean                               -0.0130702\r\n",
      "exploration/Actions Std                                 0.182171\r\n",
      "exploration/Actions Max                                 0.616261\r\n",
      "exploration/Actions Min                                -0.672497\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -3.19422\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.129612\r\n",
      "exploration/env_infos/final/reward_dist Std             0.259203\r\n",
      "exploration/env_infos/final/reward_dist Max             0.648019\r\n",
      "exploration/env_infos/final/reward_dist Min             7.32844e-39\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00216965\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.00427531\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0107201\r\n",
      "exploration/env_infos/initial/reward_dist Min           4.88192e-06\r\n",
      "exploration/env_infos/reward_dist Mean                  0.128057\r\n",
      "exploration/env_infos/reward_dist Std                   0.281165\r\n",
      "exploration/env_infos/reward_dist Max                   0.961812\r\n",
      "exploration/env_infos/reward_dist Min                   7.32844e-39\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.245481\r\n",
      "exploration/env_infos/final/reward_energy Std           0.0895232\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.107677\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.347745\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.177843\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.156465\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0534861\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.480436\r\n",
      "exploration/env_infos/reward_energy Mean               -0.202719\r\n",
      "exploration/env_infos/reward_energy Std                 0.16006\r\n",
      "exploration/env_infos/reward_energy Max                -0.00457819\r\n",
      "exploration/env_infos/reward_energy Min                -0.768262\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0926429\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.236094\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.2314\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.48896\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00360691\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00755822\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00599825\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0230505\r\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0116781\r\n",
      "exploration/env_infos/end_effector_loc Std              0.137859\r\n",
      "exploration/env_infos/end_effector_loc Max              0.266246\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.48896\r\n",
      "evaluation/num steps total                           6000\r\n",
      "evaluation/num paths total                            300\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.140115\r\n",
      "evaluation/Rewards Std                                  0.0954639\r\n",
      "evaluation/Rewards Max                                  0.122094\r\n",
      "evaluation/Rewards Min                                 -0.676226\r\n",
      "evaluation/Returns Mean                                -2.8023\r\n",
      "evaluation/Returns Std                                  1.18033\r\n",
      "evaluation/Returns Max                                 -0.53575\r\n",
      "evaluation/Returns Min                                 -5.67409\r\n",
      "evaluation/Actions Mean                                -0.0217011\r\n",
      "evaluation/Actions Std                                  0.0957455\r\n",
      "evaluation/Actions Max                                  0.397837\r\n",
      "evaluation/Actions Min                                 -0.409855\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -2.8023\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0370594\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.138308\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.783803\r\n",
      "evaluation/env_infos/final/reward_dist Min              3.64493e-74\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00665165\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00947028\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0324627\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.2328e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0515727\r\n",
      "evaluation/env_infos/reward_dist Std                    0.134432\r\n",
      "evaluation/env_infos/reward_dist Max                    0.9591\r\n",
      "evaluation/env_infos/reward_dist Min                    3.64493e-74\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.187431\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.118135\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.043561\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.528155\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.153871\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0687537\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0442669\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.364019\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.114251\r\n",
      "evaluation/env_infos/reward_energy Std                  0.0788854\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00217171\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.528155\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.201088\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.29125\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.319776\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00126295\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00582315\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0111058\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0180466\r\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0514303\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.188284\r\n",
      "evaluation/env_infos/end_effector_loc Max               0.333243\r\n",
      "evaluation/env_infos/end_effector_loc Min              -1\r\n",
      "time/data storing (s)                                   0.00319928\r\n",
      "time/evaluation sampling (s)                            1.33085\r\n",
      "time/exploration sampling (s)                           0.144429\r\n",
      "time/logging (s)                                        0.020892\r\n",
      "time/saving (s)                                         0.0299084\r\n",
      "time/training (s)                                      46.882\r\n",
      "time/epoch (s)                                         48.4113\r\n",
      "time/total (s)                                        269.86\r\n",
      "Epoch                                                   5\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:11.294383 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 6 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1700\n",
      "trainer/QF1 Loss                                        0.00401628\n",
      "trainer/QF2 Loss                                        0.00301326\n",
      "trainer/Policy Loss                                     4.5679\n",
      "trainer/Q1 Predictions Mean                            -2.94904\n",
      "trainer/Q1 Predictions Std                              1.78239\n",
      "trainer/Q1 Predictions Max                             -0.145669\n",
      "trainer/Q1 Predictions Min                             -9.25414\n",
      "trainer/Q2 Predictions Mean                            -2.95907\n",
      "trainer/Q2 Predictions Std                              1.78011\n",
      "trainer/Q2 Predictions Max                             -0.138948\n",
      "trainer/Q2 Predictions Min                             -9.21313\n",
      "trainer/Q Targets Mean                                 -2.96176\n",
      "trainer/Q Targets Std                                   1.78064\n",
      "trainer/Q Targets Max                                  -0.134168\n",
      "trainer/Q Targets Min                                  -9.25124\n",
      "trainer/Log Pis Mean                                    1.98999\n",
      "trainer/Log Pis Std                                     1.43828\n",
      "trainer/Log Pis Max                                     7.88639\n",
      "trainer/Log Pis Min                                    -3.32601\n",
      "trainer/Policy mu Mean                                 -0.00859346\n",
      "trainer/Policy mu Std                                   0.609074\n",
      "trainer/Policy mu Max                                   2.67305\n",
      "trainer/Policy mu Min                                  -2.60096\n",
      "trainer/Policy log std Mean                            -2.12876\n",
      "trainer/Policy log std Std                              0.567733\n",
      "trainer/Policy log std Max                             -0.163518\n",
      "trainer/Policy log std Min                             -3.13718\n",
      "trainer/Alpha                                           0.0148503\n",
      "trainer/Alpha Loss                                     -0.0421255\n",
      "exploration/num steps total                          1700\n",
      "exploration/num paths total                            85\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.12011\n",
      "exploration/Rewards Std                                 0.0995193\n",
      "exploration/Rewards Max                                 0.0391944\n",
      "exploration/Rewards Min                                -0.515167\n",
      "exploration/Returns Mean                               -2.40221\n",
      "exploration/Returns Std                                 1.31784\n",
      "exploration/Returns Max                                -0.940018\n",
      "exploration/Returns Min                                -4.38468\n",
      "exploration/Actions Mean                               -0.0201889\n",
      "exploration/Actions Std                                 0.124215\n",
      "exploration/Actions Max                                 0.457947\n",
      "exploration/Actions Min                                -0.424974\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.40221\n",
      "exploration/env_infos/final/reward_dist Mean            0.0403122\n",
      "exploration/env_infos/final/reward_dist Std             0.0806241\n",
      "exploration/env_infos/final/reward_dist Max             0.20156\n",
      "exploration/env_infos/final/reward_dist Min             2.46859e-46\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0120158\n",
      "exploration/env_infos/initial/reward_dist Std           0.0158164\n",
      "exploration/env_infos/initial/reward_dist Max           0.0433042\n",
      "exploration/env_infos/initial/reward_dist Min           4.82377e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0555058\n",
      "exploration/env_infos/reward_dist Std                   0.0871416\n",
      "exploration/env_infos/reward_dist Max                   0.48784\n",
      "exploration/env_infos/reward_dist Min                   2.46859e-46\n",
      "exploration/env_infos/final/reward_energy Mean         -0.207155\n",
      "exploration/env_infos/final/reward_energy Std           0.0950714\n",
      "exploration/env_infos/final/reward_energy Max          -0.102552\n",
      "exploration/env_infos/final/reward_energy Min          -0.366865\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.229679\n",
      "exploration/env_infos/initial/reward_energy Std         0.161923\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0247831\n",
      "exploration/env_infos/initial/reward_energy Min        -0.483169\n",
      "exploration/env_infos/reward_energy Mean               -0.153621\n",
      "exploration/env_infos/reward_energy Std                 0.0898589\n",
      "exploration/env_infos/reward_energy Max                -0.0215799\n",
      "exploration/env_infos/reward_energy Min                -0.483169\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.185485\n",
      "exploration/env_infos/final/end_effector_loc Std        0.318867\n",
      "exploration/env_infos/final/end_effector_loc Max        0.246597\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.711116\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00150753\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00982047\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.012737\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0212487\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0659243\n",
      "exploration/env_infos/end_effector_loc Std              0.195697\n",
      "exploration/env_infos/end_effector_loc Max              0.252024\n",
      "exploration/env_infos/end_effector_loc Min             -0.711116\n",
      "evaluation/num steps total                           7000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.126556\n",
      "evaluation/Rewards Std                                  0.122697\n",
      "evaluation/Rewards Max                                  0.107346\n",
      "evaluation/Rewards Min                                 -0.939904\n",
      "evaluation/Returns Mean                                -2.53112\n",
      "evaluation/Returns Std                                  1.3597\n",
      "evaluation/Returns Max                                 -0.361605\n",
      "evaluation/Returns Min                                 -8.06351\n",
      "evaluation/Actions Mean                                -0.0139378\n",
      "evaluation/Actions Std                                  0.102267\n",
      "evaluation/Actions Max                                  0.43149\n",
      "evaluation/Actions Min                                 -0.801056\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.53112\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0116841\n",
      "evaluation/env_infos/final/reward_dist Std              0.0428651\n",
      "evaluation/env_infos/final/reward_dist Max              0.245459\n",
      "evaluation/env_infos/final/reward_dist Min              1.55703e-63\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00737524\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0124404\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0593836\n",
      "evaluation/env_infos/initial/reward_dist Min            3.99796e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0540835\n",
      "evaluation/env_infos/reward_dist Std                    0.146721\n",
      "evaluation/env_infos/reward_dist Max                    0.960647\n",
      "evaluation/env_infos/reward_dist Min                    2.778e-70\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.217906\n",
      "evaluation/env_infos/final/reward_energy Std            0.184873\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0116104\n",
      "evaluation/env_infos/final/reward_energy Min           -0.878679\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.154152\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0884744\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0229355\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.448848\n",
      "evaluation/env_infos/reward_energy Mean                -0.107257\n",
      "evaluation/env_infos/reward_energy Std                  0.0990024\n",
      "evaluation/env_infos/reward_energy Max                 -0.00113453\n",
      "evaluation/env_infos/reward_energy Min                 -0.878679\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.023272\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.328776\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.518937\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.0025474\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00574448\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0215745\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0149535\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0251593\n",
      "evaluation/env_infos/end_effector_loc Std               0.194369\n",
      "evaluation/env_infos/end_effector_loc Max               0.715969\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00296628\n",
      "time/evaluation sampling (s)                            1.01306\n",
      "time/exploration sampling (s)                           0.118467\n",
      "time/logging (s)                                        0.0265779\n",
      "time/saving (s)                                         0.0293528\n",
      "time/training (s)                                      43.6627\n",
      "time/epoch (s)                                         44.8531\n",
      "time/total (s)                                        314.825\n",
      "Epoch                                                   6\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:58.238699 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 7 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1800\n",
      "trainer/QF1 Loss                                        0.00532094\n",
      "trainer/QF2 Loss                                        0.00913572\n",
      "trainer/Policy Loss                                     4.66646\n",
      "trainer/Q1 Predictions Mean                            -3.1223\n",
      "trainer/Q1 Predictions Std                              1.9577\n",
      "trainer/Q1 Predictions Max                             -0.148908\n",
      "trainer/Q1 Predictions Min                            -10.6049\n",
      "trainer/Q2 Predictions Mean                            -3.04868\n",
      "trainer/Q2 Predictions Std                              1.9379\n",
      "trainer/Q2 Predictions Max                             -0.103933\n",
      "trainer/Q2 Predictions Min                            -10.6075\n",
      "trainer/Q Targets Mean                                 -3.09267\n",
      "trainer/Q Targets Std                                   1.95351\n",
      "trainer/Q Targets Max                                  -0.114265\n",
      "trainer/Q Targets Min                                 -10.6191\n",
      "trainer/Log Pis Mean                                    1.91905\n",
      "trainer/Log Pis Std                                     1.22466\n",
      "trainer/Log Pis Max                                     7.41546\n",
      "trainer/Log Pis Min                                    -3.28219\n",
      "trainer/Policy mu Mean                                 -0.0383501\n",
      "trainer/Policy mu Std                                   0.657596\n",
      "trainer/Policy mu Max                                   2.85599\n",
      "trainer/Policy mu Min                                  -2.87545\n",
      "trainer/Policy log std Mean                            -2.07488\n",
      "trainer/Policy log std Std                              0.596162\n",
      "trainer/Policy log std Max                             -0.0168958\n",
      "trainer/Policy log std Min                             -2.9021\n",
      "trainer/Alpha                                           0.014825\n",
      "trainer/Alpha Loss                                     -0.340906\n",
      "exploration/num steps total                          1800\n",
      "exploration/num paths total                            90\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.109248\n",
      "exploration/Rewards Std                                 0.078963\n",
      "exploration/Rewards Max                                 0.00457018\n",
      "exploration/Rewards Min                                -0.349884\n",
      "exploration/Returns Mean                               -2.18496\n",
      "exploration/Returns Std                                 1.01039\n",
      "exploration/Returns Max                                -1.00419\n",
      "exploration/Returns Min                                -4.03704\n",
      "exploration/Actions Mean                                0.00175343\n",
      "exploration/Actions Std                                 0.136674\n",
      "exploration/Actions Max                                 0.361814\n",
      "exploration/Actions Min                                -0.312836\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.18496\n",
      "exploration/env_infos/final/reward_dist Mean            0.0328673\n",
      "exploration/env_infos/final/reward_dist Std             0.054723\n",
      "exploration/env_infos/final/reward_dist Max             0.140989\n",
      "exploration/env_infos/final/reward_dist Min             4.04595e-10\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00521036\n",
      "exploration/env_infos/initial/reward_dist Std           0.00504233\n",
      "exploration/env_infos/initial/reward_dist Max           0.0125878\n",
      "exploration/env_infos/initial/reward_dist Min           5.22559e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0718827\n",
      "exploration/env_infos/reward_dist Std                   0.116213\n",
      "exploration/env_infos/reward_dist Max                   0.51901\n",
      "exploration/env_infos/reward_dist Min                   4.04595e-10\n",
      "exploration/env_infos/final/reward_energy Mean         -0.192104\n",
      "exploration/env_infos/final/reward_energy Std           0.117009\n",
      "exploration/env_infos/final/reward_energy Max          -0.0606609\n",
      "exploration/env_infos/final/reward_energy Min          -0.355326\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.184392\n",
      "exploration/env_infos/initial/reward_energy Std         0.109897\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0346794\n",
      "exploration/env_infos/initial/reward_energy Min        -0.367841\n",
      "exploration/env_infos/reward_energy Mean               -0.165589\n",
      "exploration/env_infos/reward_energy Std                 0.0997282\n",
      "exploration/env_infos/reward_energy Max                -0.0136825\n",
      "exploration/env_infos/reward_energy Min                -0.401459\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.00647823\n",
      "exploration/env_infos/final/end_effector_loc Std        0.250561\n",
      "exploration/env_infos/final/end_effector_loc Max        0.417808\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.369255\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.000362005\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00758064\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00967479\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0156418\n",
      "exploration/env_infos/end_effector_loc Mean             0.00333751\n",
      "exploration/env_infos/end_effector_loc Std              0.154675\n",
      "exploration/env_infos/end_effector_loc Max              0.417808\n",
      "exploration/env_infos/end_effector_loc Min             -0.369255\n",
      "evaluation/num steps total                           8000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.120225\n",
      "evaluation/Rewards Std                                  0.114706\n",
      "evaluation/Rewards Max                                  0.10838\n",
      "evaluation/Rewards Min                                 -0.824555\n",
      "evaluation/Returns Mean                                -2.40449\n",
      "evaluation/Returns Std                                  1.31461\n",
      "evaluation/Returns Max                                 -0.224426\n",
      "evaluation/Returns Min                                 -5.54946\n",
      "evaluation/Actions Mean                                -0.012673\n",
      "evaluation/Actions Std                                  0.0939277\n",
      "evaluation/Actions Max                                  0.474986\n",
      "evaluation/Actions Min                                 -0.631368\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.40449\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0359109\n",
      "evaluation/env_infos/final/reward_dist Std              0.126213\n",
      "evaluation/env_infos/final/reward_dist Max              0.759091\n",
      "evaluation/env_infos/final/reward_dist Min              4.1122e-70\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00473938\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00780024\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0308037\n",
      "evaluation/env_infos/initial/reward_dist Min            8.80153e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0778644\n",
      "evaluation/env_infos/reward_dist Std                    0.183512\n",
      "evaluation/env_infos/reward_dist Max                    0.993649\n",
      "evaluation/env_infos/reward_dist Min                    4.1122e-70\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.194139\n",
      "evaluation/env_infos/final/reward_energy Std            0.166278\n",
      "evaluation/env_infos/final/reward_energy Max           -0.011186\n",
      "evaluation/env_infos/final/reward_energy Min           -0.631476\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.138763\n",
      "evaluation/env_infos/initial/reward_energy Std          0.073323\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0173869\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.401362\n",
      "evaluation/env_infos/reward_energy Mean                -0.100189\n",
      "evaluation/env_infos/reward_energy Std                  0.0890407\n",
      "evaluation/env_infos/reward_energy Max                 -0.0037521\n",
      "evaluation/env_infos/reward_energy Min                 -0.631476\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0413599\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.33914\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.483354\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00209752\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00513709\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0132112\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0181754\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00788367\n",
      "evaluation/env_infos/end_effector_loc Std               0.191619\n",
      "evaluation/env_infos/end_effector_loc Max               0.483354\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00345626\n",
      "time/evaluation sampling (s)                            1.41541\n",
      "time/exploration sampling (s)                           0.128368\n",
      "time/logging (s)                                        0.0210347\n",
      "time/saving (s)                                         0.0292165\n",
      "time/training (s)                                      45.2144\n",
      "time/epoch (s)                                         46.8119\n",
      "time/total (s)                                        361.761\n",
      "Epoch                                                   7\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:31:44.198167 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 8 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1900\n",
      "trainer/QF1 Loss                                        0.00315309\n",
      "trainer/QF2 Loss                                        0.0186633\n",
      "trainer/Policy Loss                                     4.69278\n",
      "trainer/Q1 Predictions Mean                            -3.04336\n",
      "trainer/Q1 Predictions Std                              1.76928\n",
      "trainer/Q1 Predictions Max                             -0.101967\n",
      "trainer/Q1 Predictions Min                            -10.13\n",
      "trainer/Q2 Predictions Mean                            -3.15417\n",
      "trainer/Q2 Predictions Std                              1.78896\n",
      "trainer/Q2 Predictions Max                             -0.116019\n",
      "trainer/Q2 Predictions Min                            -10.1045\n",
      "trainer/Q Targets Mean                                 -3.05003\n",
      "trainer/Q Targets Std                                   1.77127\n",
      "trainer/Q Targets Max                                  -0.119584\n",
      "trainer/Q Targets Min                                 -10.2059\n",
      "trainer/Log Pis Mean                                    1.89864\n",
      "trainer/Log Pis Std                                     1.52933\n",
      "trainer/Log Pis Max                                     7.42613\n",
      "trainer/Log Pis Min                                    -2.84766\n",
      "trainer/Policy mu Mean                                  0.00674611\n",
      "trainer/Policy mu Std                                   0.796232\n",
      "trainer/Policy mu Max                                   2.85768\n",
      "trainer/Policy mu Min                                  -2.94379\n",
      "trainer/Policy log std Mean                            -1.96356\n",
      "trainer/Policy log std Std                              0.660782\n",
      "trainer/Policy log std Max                              0.0122131\n",
      "trainer/Policy log std Min                             -3.01445\n",
      "trainer/Alpha                                           0.0155872\n",
      "trainer/Alpha Loss                                     -0.421793\n",
      "exploration/num steps total                          1900\n",
      "exploration/num paths total                            95\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.120469\n",
      "exploration/Rewards Std                                 0.0981408\n",
      "exploration/Rewards Max                                 0.028481\n",
      "exploration/Rewards Min                                -0.542977\n",
      "exploration/Returns Mean                               -2.40938\n",
      "exploration/Returns Std                                 0.874631\n",
      "exploration/Returns Max                                -1.33071\n",
      "exploration/Returns Min                                -3.75844\n",
      "exploration/Actions Mean                               -0.00631211\n",
      "exploration/Actions Std                                 0.187973\n",
      "exploration/Actions Max                                 0.727745\n",
      "exploration/Actions Min                                -0.506436\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.40938\n",
      "exploration/env_infos/final/reward_dist Mean            0.000490911\n",
      "exploration/env_infos/final/reward_dist Std             0.0009812\n",
      "exploration/env_infos/final/reward_dist Max             0.00245331\n",
      "exploration/env_infos/final/reward_dist Min             1.22721e-35\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000109025\n",
      "exploration/env_infos/initial/reward_dist Std           0.000112665\n",
      "exploration/env_infos/initial/reward_dist Max           0.000274696\n",
      "exploration/env_infos/initial/reward_dist Min           1.33551e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0641289\n",
      "exploration/env_infos/reward_dist Std                   0.181745\n",
      "exploration/env_infos/reward_dist Max                   0.867845\n",
      "exploration/env_infos/reward_dist Min                   1.22721e-35\n",
      "exploration/env_infos/final/reward_energy Mean         -0.244852\n",
      "exploration/env_infos/final/reward_energy Std           0.0872895\n",
      "exploration/env_infos/final/reward_energy Max          -0.0963312\n",
      "exploration/env_infos/final/reward_energy Min          -0.369723\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.454919\n",
      "exploration/env_infos/initial/reward_energy Std         0.176031\n",
      "exploration/env_infos/initial/reward_energy Max        -0.249878\n",
      "exploration/env_infos/initial/reward_energy Min        -0.728648\n",
      "exploration/env_infos/reward_energy Mean               -0.224167\n",
      "exploration/env_infos/reward_energy Std                 0.143166\n",
      "exploration/env_infos/reward_energy Max                -0.0209205\n",
      "exploration/env_infos/reward_energy Min                -0.728648\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0597196\n",
      "exploration/env_infos/final/end_effector_loc Std        0.401409\n",
      "exploration/env_infos/final/end_effector_loc Max        0.6004\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.768194\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00411278\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0167484\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0363872\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0164456\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0123202\n",
      "exploration/env_infos/end_effector_loc Std              0.265136\n",
      "exploration/env_infos/end_effector_loc Max              0.6004\n",
      "exploration/env_infos/end_effector_loc Min             -0.768194\n",
      "evaluation/num steps total                           9000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.121668\n",
      "evaluation/Rewards Std                                  0.117023\n",
      "evaluation/Rewards Max                                  0.104391\n",
      "evaluation/Rewards Min                                 -0.792242\n",
      "evaluation/Returns Mean                                -2.43336\n",
      "evaluation/Returns Std                                  1.28797\n",
      "evaluation/Returns Max                                 -0.0298396\n",
      "evaluation/Returns Min                                 -5.82313\n",
      "evaluation/Actions Mean                                -0.0164039\n",
      "evaluation/Actions Std                                  0.0997085\n",
      "evaluation/Actions Max                                  0.528124\n",
      "evaluation/Actions Min                                 -0.608067\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.43336\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0783531\n",
      "evaluation/env_infos/final/reward_dist Std              0.210993\n",
      "evaluation/env_infos/final/reward_dist Max              0.959972\n",
      "evaluation/env_infos/final/reward_dist Min              5.13075e-81\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00538389\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00752556\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0352487\n",
      "evaluation/env_infos/initial/reward_dist Min            1.07641e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0828767\n",
      "evaluation/env_infos/reward_dist Std                    0.177923\n",
      "evaluation/env_infos/reward_dist Max                    0.98926\n",
      "evaluation/env_infos/reward_dist Min                    5.13075e-81\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.144679\n",
      "evaluation/env_infos/final/reward_energy Std            0.0979144\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00811962\n",
      "evaluation/env_infos/final/reward_energy Min           -0.464694\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.233305\n",
      "evaluation/env_infos/initial/reward_energy Std          0.141318\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0240861\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.632523\n",
      "evaluation/env_infos/reward_energy Mean                -0.112431\n",
      "evaluation/env_infos/reward_energy Std                  0.08821\n",
      "evaluation/env_infos/reward_energy Max                 -0.00260803\n",
      "evaluation/env_infos/reward_energy Min                 -0.632523\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0991857\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.393903\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.771833\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00200743\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00943255\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0264062\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0304034\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0129488\n",
      "evaluation/env_infos/end_effector_loc Std               0.236235\n",
      "evaluation/env_infos/end_effector_loc Max               0.771833\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00310569\n",
      "time/evaluation sampling (s)                            0.985649\n",
      "time/exploration sampling (s)                           0.136818\n",
      "time/logging (s)                                        0.0185161\n",
      "time/saving (s)                                         0.0318406\n",
      "time/training (s)                                      44.6417\n",
      "time/epoch (s)                                         45.8176\n",
      "time/total (s)                                        407.717\n",
      "Epoch                                                   8\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:32:30.837165 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 9 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00445208\n",
      "trainer/QF2 Loss                                         0.00415612\n",
      "trainer/Policy Loss                                      4.46808\n",
      "trainer/Q1 Predictions Mean                             -2.91392\n",
      "trainer/Q1 Predictions Std                               1.97332\n",
      "trainer/Q1 Predictions Max                              -0.0731378\n",
      "trainer/Q1 Predictions Min                             -11.0758\n",
      "trainer/Q2 Predictions Mean                             -2.88326\n",
      "trainer/Q2 Predictions Std                               1.95605\n",
      "trainer/Q2 Predictions Max                              -0.10773\n",
      "trainer/Q2 Predictions Min                             -10.884\n",
      "trainer/Q Targets Mean                                  -2.88772\n",
      "trainer/Q Targets Std                                    1.95442\n",
      "trainer/Q Targets Max                                   -0.111562\n",
      "trainer/Q Targets Min                                  -11.063\n",
      "trainer/Log Pis Mean                                     1.88253\n",
      "trainer/Log Pis Std                                      1.37722\n",
      "trainer/Log Pis Max                                      7.06818\n",
      "trainer/Log Pis Min                                     -5.68848\n",
      "trainer/Policy mu Mean                                  -0.0142511\n",
      "trainer/Policy mu Std                                    0.634605\n",
      "trainer/Policy mu Max                                    2.53393\n",
      "trainer/Policy mu Min                                   -2.73537\n",
      "trainer/Policy log std Mean                             -2.06168\n",
      "trainer/Policy log std Std                               0.583922\n",
      "trainer/Policy log std Max                              -0.228722\n",
      "trainer/Policy log std Min                              -3.07149\n",
      "trainer/Alpha                                            0.015224\n",
      "trainer/Alpha Loss                                      -0.491583\n",
      "exploration/num steps total                           2000\n",
      "exploration/num paths total                            100\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117089\n",
      "exploration/Rewards Std                                  0.0956642\n",
      "exploration/Rewards Max                                  0.0709816\n",
      "exploration/Rewards Min                                 -0.41412\n",
      "exploration/Returns Mean                                -2.34178\n",
      "exploration/Returns Std                                  0.93993\n",
      "exploration/Returns Max                                 -1.16824\n",
      "exploration/Returns Min                                 -3.80301\n",
      "exploration/Actions Mean                                -0.000832796\n",
      "exploration/Actions Std                                  0.16664\n",
      "exploration/Actions Max                                  0.473452\n",
      "exploration/Actions Min                                 -0.678519\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.34178\n",
      "exploration/env_infos/final/reward_dist Mean             0.0788667\n",
      "exploration/env_infos/final/reward_dist Std              0.157733\n",
      "exploration/env_infos/final/reward_dist Max              0.394332\n",
      "exploration/env_infos/final/reward_dist Min              3.88658e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0134992\n",
      "exploration/env_infos/initial/reward_dist Std            0.0267896\n",
      "exploration/env_infos/initial/reward_dist Max            0.0670783\n",
      "exploration/env_infos/initial/reward_dist Min            2.26571e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.132425\n",
      "exploration/env_infos/reward_dist Std                    0.205354\n",
      "exploration/env_infos/reward_dist Max                    0.8765\n",
      "exploration/env_infos/reward_dist Min                    9.70504e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.299579\n",
      "exploration/env_infos/final/reward_energy Std            0.21723\n",
      "exploration/env_infos/final/reward_energy Max           -0.0449317\n",
      "exploration/env_infos/final/reward_energy Min           -0.682126\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.41637\n",
      "exploration/env_infos/initial/reward_energy Std          0.21694\n",
      "exploration/env_infos/initial/reward_energy Max         -0.141798\n",
      "exploration/env_infos/initial/reward_energy Min         -0.799736\n",
      "exploration/env_infos/reward_energy Mean                -0.196532\n",
      "exploration/env_infos/reward_energy Std                  0.130056\n",
      "exploration/env_infos/reward_energy Max                 -0.0124541\n",
      "exploration/env_infos/reward_energy Min                 -0.799736\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0561878\n",
      "exploration/env_infos/final/end_effector_loc Std         0.39595\n",
      "exploration/env_infos/final/end_effector_loc Max         0.499235\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.617098\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00108787\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0165635\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0228931\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0327848\n",
      "exploration/env_infos/end_effector_loc Mean              0.0335449\n",
      "exploration/env_infos/end_effector_loc Std               0.25367\n",
      "exploration/env_infos/end_effector_loc Max               0.499235\n",
      "exploration/env_infos/end_effector_loc Min              -0.617098\n",
      "evaluation/num steps total                           10000\n",
      "evaluation/num paths total                             500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.109354\n",
      "evaluation/Rewards Std                                   0.080017\n",
      "evaluation/Rewards Max                                   0.0688032\n",
      "evaluation/Rewards Min                                  -0.717519\n",
      "evaluation/Returns Mean                                 -2.18708\n",
      "evaluation/Returns Std                                   0.944662\n",
      "evaluation/Returns Max                                  -0.131174\n",
      "evaluation/Returns Min                                  -5.55097\n",
      "evaluation/Actions Mean                                 -0.0136973\n",
      "evaluation/Actions Std                                   0.0880017\n",
      "evaluation/Actions Max                                   0.735795\n",
      "evaluation/Actions Min                                  -0.797453\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.18708\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0363625\n",
      "evaluation/env_infos/final/reward_dist Std               0.118915\n",
      "evaluation/env_infos/final/reward_dist Max               0.610875\n",
      "evaluation/env_infos/final/reward_dist Min               4.08377e-65\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00587902\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0188641\n",
      "evaluation/env_infos/initial/reward_dist Max             0.131242\n",
      "evaluation/env_infos/initial/reward_dist Min             1.76679e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0692563\n",
      "evaluation/env_infos/reward_dist Std                     0.171646\n",
      "evaluation/env_infos/reward_dist Max                     0.9802\n",
      "evaluation/env_infos/reward_dist Min                     4.08377e-65\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.139345\n",
      "evaluation/env_infos/final/reward_energy Std             0.122126\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0109747\n",
      "evaluation/env_infos/final/reward_energy Min            -0.575587\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.212727\n",
      "evaluation/env_infos/initial/reward_energy Std           0.185945\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.030087\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.04139\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0912323\n",
      "evaluation/env_infos/reward_energy Std                   0.086836\n",
      "evaluation/env_infos/reward_energy Max                  -0.00263502\n",
      "evaluation/env_infos/reward_energy Min                  -1.04139\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0561189\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.333355\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.740545\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00146743\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00988088\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0367897\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0398726\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0039207\n",
      "evaluation/env_infos/end_effector_loc Std                0.190851\n",
      "evaluation/env_infos/end_effector_loc Max                0.740545\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00299985\n",
      "time/evaluation sampling (s)                             0.976468\n",
      "time/exploration sampling (s)                            0.118854\n",
      "time/logging (s)                                         0.0203661\n",
      "time/saving (s)                                          0.0399637\n",
      "time/training (s)                                       45.3413\n",
      "time/epoch (s)                                          46.4999\n",
      "time/total (s)                                         454.357\n",
      "Epoch                                                    9\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:33:16.836145 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 10 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00612566\n",
      "trainer/QF2 Loss                                         0.00450927\n",
      "trainer/Policy Loss                                      4.73966\n",
      "trainer/Q1 Predictions Mean                             -2.95715\n",
      "trainer/Q1 Predictions Std                               1.90926\n",
      "trainer/Q1 Predictions Max                              -0.161132\n",
      "trainer/Q1 Predictions Min                              -9.46225\n",
      "trainer/Q2 Predictions Mean                             -2.95376\n",
      "trainer/Q2 Predictions Std                               1.9247\n",
      "trainer/Q2 Predictions Max                              -0.174487\n",
      "trainer/Q2 Predictions Min                              -9.79856\n",
      "trainer/Q Targets Mean                                  -2.94762\n",
      "trainer/Q Targets Std                                    1.92437\n",
      "trainer/Q Targets Max                                   -0.153345\n",
      "trainer/Q Targets Min                                   -9.82373\n",
      "trainer/Log Pis Mean                                     2.04306\n",
      "trainer/Log Pis Std                                      1.46884\n",
      "trainer/Log Pis Max                                      9.24999\n",
      "trainer/Log Pis Min                                     -2.4602\n",
      "trainer/Policy mu Mean                                  -0.0136412\n",
      "trainer/Policy mu Std                                    0.706992\n",
      "trainer/Policy mu Max                                    2.57877\n",
      "trainer/Policy mu Min                                   -3.37981\n",
      "trainer/Policy log std Mean                             -2.08289\n",
      "trainer/Policy log std Std                               0.592796\n",
      "trainer/Policy log std Max                              -0.181075\n",
      "trainer/Policy log std Min                              -3.24415\n",
      "trainer/Alpha                                            0.0158679\n",
      "trainer/Alpha Loss                                       0.178438\n",
      "exploration/num steps total                           2100\n",
      "exploration/num paths total                            105\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.211701\n",
      "exploration/Rewards Std                                  0.0934085\n",
      "exploration/Rewards Max                                 -0.0372659\n",
      "exploration/Rewards Min                                 -0.556664\n",
      "exploration/Returns Mean                                -4.23401\n",
      "exploration/Returns Std                                  1.21427\n",
      "exploration/Returns Max                                 -2.7763\n",
      "exploration/Returns Min                                 -6.18214\n",
      "exploration/Actions Mean                                 0.00843026\n",
      "exploration/Actions Std                                  0.175229\n",
      "exploration/Actions Max                                  0.747791\n",
      "exploration/Actions Min                                 -0.840261\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -4.23401\n",
      "exploration/env_infos/final/reward_dist Mean             5.10512e-08\n",
      "exploration/env_infos/final/reward_dist Std              1.0208e-07\n",
      "exploration/env_infos/final/reward_dist Max              2.55211e-07\n",
      "exploration/env_infos/final/reward_dist Min              5.44369e-44\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000241264\n",
      "exploration/env_infos/initial/reward_dist Std            0.000357503\n",
      "exploration/env_infos/initial/reward_dist Max            0.000950018\n",
      "exploration/env_infos/initial/reward_dist Min            7.80915e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0147121\n",
      "exploration/env_infos/reward_dist Std                    0.0792127\n",
      "exploration/env_infos/reward_dist Max                    0.642368\n",
      "exploration/env_infos/reward_dist Min                    2.06964e-45\n",
      "exploration/env_infos/final/reward_energy Mean          -0.233899\n",
      "exploration/env_infos/final/reward_energy Std            0.046255\n",
      "exploration/env_infos/final/reward_energy Max           -0.189089\n",
      "exploration/env_infos/final/reward_energy Min           -0.29248\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.180868\n",
      "exploration/env_infos/initial/reward_energy Std          0.0902192\n",
      "exploration/env_infos/initial/reward_energy Max         -0.04204\n",
      "exploration/env_infos/initial/reward_energy Min         -0.322384\n",
      "exploration/env_infos/reward_energy Mean                -0.193262\n",
      "exploration/env_infos/reward_energy Std                  0.15557\n",
      "exploration/env_infos/reward_energy Max                 -0.0153834\n",
      "exploration/env_infos/reward_energy Min                 -0.944226\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00325038\n",
      "exploration/env_infos/final/end_effector_loc Std         0.420076\n",
      "exploration/env_infos/final/end_effector_loc Max         0.494579\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.991038\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000785262\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00710275\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00849682\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0159193\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00370282\n",
      "exploration/env_infos/end_effector_loc Std               0.287445\n",
      "exploration/env_infos/end_effector_loc Max               0.503013\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           11000\n",
      "evaluation/num paths total                             550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.120884\n",
      "evaluation/Rewards Std                                   0.0901638\n",
      "evaluation/Rewards Max                                   0.108132\n",
      "evaluation/Rewards Min                                  -0.718647\n",
      "evaluation/Returns Mean                                 -2.41767\n",
      "evaluation/Returns Std                                   1.2018\n",
      "evaluation/Returns Max                                   0.582752\n",
      "evaluation/Returns Min                                  -5.47631\n",
      "evaluation/Actions Mean                                 -0.00884906\n",
      "evaluation/Actions Std                                   0.0849369\n",
      "evaluation/Actions Max                                   0.60225\n",
      "evaluation/Actions Min                                  -0.58994\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.41767\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0513876\n",
      "evaluation/env_infos/final/reward_dist Std               0.162926\n",
      "evaluation/env_infos/final/reward_dist Max               0.988065\n",
      "evaluation/env_infos/final/reward_dist Min               6.14719e-68\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00744284\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0156932\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0817627\n",
      "evaluation/env_infos/initial/reward_dist Min             1.13791e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0557876\n",
      "evaluation/env_infos/reward_dist Std                     0.159761\n",
      "evaluation/env_infos/reward_dist Max                     0.988065\n",
      "evaluation/env_infos/reward_dist Min                     6.14719e-68\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.119466\n",
      "evaluation/env_infos/final/reward_energy Std             0.0836574\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0171798\n",
      "evaluation/env_infos/final/reward_energy Min            -0.433978\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.233227\n",
      "evaluation/env_infos/initial/reward_energy Std           0.165023\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0168539\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.768105\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0881556\n",
      "evaluation/env_infos/reward_energy Std                   0.0825455\n",
      "evaluation/env_infos/reward_energy Max                  -0.000673391\n",
      "evaluation/env_infos/reward_energy Min                  -0.768105\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0366146\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.332479\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.641184\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000604364\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0100831\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0301125\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.029497\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00377817\n",
      "evaluation/env_infos/end_effector_loc Std                0.206309\n",
      "evaluation/env_infos/end_effector_loc Max                0.641184\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00298404\n",
      "time/evaluation sampling (s)                             0.999527\n",
      "time/exploration sampling (s)                            0.120004\n",
      "time/logging (s)                                         0.0196417\n",
      "time/saving (s)                                          0.0301854\n",
      "time/training (s)                                       44.6641\n",
      "time/epoch (s)                                          45.8364\n",
      "time/total (s)                                         500.355\n",
      "Epoch                                                   10\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:05.145095 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 11 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00576653\n",
      "trainer/QF2 Loss                                         0.00529006\n",
      "trainer/Policy Loss                                      4.54779\n",
      "trainer/Q1 Predictions Mean                             -2.77117\n",
      "trainer/Q1 Predictions Std                               1.86587\n",
      "trainer/Q1 Predictions Max                              -0.130918\n",
      "trainer/Q1 Predictions Min                             -11.2886\n",
      "trainer/Q2 Predictions Mean                             -2.74842\n",
      "trainer/Q2 Predictions Std                               1.85531\n",
      "trainer/Q2 Predictions Max                              -0.108681\n",
      "trainer/Q2 Predictions Min                             -11.2334\n",
      "trainer/Q Targets Mean                                  -2.73754\n",
      "trainer/Q Targets Std                                    1.84606\n",
      "trainer/Q Targets Max                                   -0.0946199\n",
      "trainer/Q Targets Min                                  -11.3446\n",
      "trainer/Log Pis Mean                                     2.07449\n",
      "trainer/Log Pis Std                                      1.58767\n",
      "trainer/Log Pis Max                                      9.11468\n",
      "trainer/Log Pis Min                                     -2.71209\n",
      "trainer/Policy mu Mean                                  -0.0443209\n",
      "trainer/Policy mu Std                                    0.793992\n",
      "trainer/Policy mu Max                                    3.44733\n",
      "trainer/Policy mu Min                                   -3.32761\n",
      "trainer/Policy log std Mean                             -2.00017\n",
      "trainer/Policy log std Std                               0.599633\n",
      "trainer/Policy log std Max                              -0.0475062\n",
      "trainer/Policy log std Min                              -2.97489\n",
      "trainer/Alpha                                            0.016235\n",
      "trainer/Alpha Loss                                       0.306914\n",
      "exploration/num steps total                           2200\n",
      "exploration/num paths total                            110\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.183434\n",
      "exploration/Rewards Std                                  0.109815\n",
      "exploration/Rewards Max                                  0.0568666\n",
      "exploration/Rewards Min                                 -0.583449\n",
      "exploration/Returns Mean                                -3.66867\n",
      "exploration/Returns Std                                  0.671211\n",
      "exploration/Returns Max                                 -2.63071\n",
      "exploration/Returns Min                                 -4.27238\n",
      "exploration/Actions Mean                                -0.00890265\n",
      "exploration/Actions Std                                  0.113645\n",
      "exploration/Actions Max                                  0.288116\n",
      "exploration/Actions Min                                 -0.30274\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.66867\n",
      "exploration/env_infos/final/reward_dist Mean             0.033112\n",
      "exploration/env_infos/final/reward_dist Std              0.0546735\n",
      "exploration/env_infos/final/reward_dist Max              0.140758\n",
      "exploration/env_infos/final/reward_dist Min              1.42662e-40\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00919615\n",
      "exploration/env_infos/initial/reward_dist Std            0.0110347\n",
      "exploration/env_infos/initial/reward_dist Max            0.027808\n",
      "exploration/env_infos/initial/reward_dist Min            6.24952e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0686175\n",
      "exploration/env_infos/reward_dist Std                    0.114971\n",
      "exploration/env_infos/reward_dist Max                    0.435792\n",
      "exploration/env_infos/reward_dist Min                    1.42662e-40\n",
      "exploration/env_infos/final/reward_energy Mean          -0.178713\n",
      "exploration/env_infos/final/reward_energy Std            0.0543983\n",
      "exploration/env_infos/final/reward_energy Max           -0.0905265\n",
      "exploration/env_infos/final/reward_energy Min           -0.243733\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.215577\n",
      "exploration/env_infos/initial/reward_energy Std          0.0491593\n",
      "exploration/env_infos/initial/reward_energy Max         -0.155156\n",
      "exploration/env_infos/initial/reward_energy Min         -0.30568\n",
      "exploration/env_infos/reward_energy Mean                -0.141997\n",
      "exploration/env_infos/reward_energy Std                  0.0763271\n",
      "exploration/env_infos/reward_energy Max                 -0.0265819\n",
      "exploration/env_infos/reward_energy Min                 -0.360992\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.11665\n",
      "exploration/env_infos/final/end_effector_loc Std         0.385239\n",
      "exploration/env_infos/final/end_effector_loc Max         0.362007\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.842267\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00209792\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00753068\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0104526\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0149354\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0550128\n",
      "exploration/env_infos/end_effector_loc Std               0.206879\n",
      "exploration/env_infos/end_effector_loc Max               0.362007\n",
      "exploration/env_infos/end_effector_loc Min              -0.842267\n",
      "evaluation/num steps total                           12000\n",
      "evaluation/num paths total                             600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.107939\n",
      "evaluation/Rewards Std                                   0.0729217\n",
      "evaluation/Rewards Max                                   0.0645711\n",
      "evaluation/Rewards Min                                  -0.62767\n",
      "evaluation/Returns Mean                                 -2.15878\n",
      "evaluation/Returns Std                                   0.967839\n",
      "evaluation/Returns Max                                  -0.598675\n",
      "evaluation/Returns Min                                  -5.50918\n",
      "evaluation/Actions Mean                                  0.000196849\n",
      "evaluation/Actions Std                                   0.0938852\n",
      "evaluation/Actions Max                                   0.474783\n",
      "evaluation/Actions Min                                  -0.975392\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.15878\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0126502\n",
      "evaluation/env_infos/final/reward_dist Std               0.0580807\n",
      "evaluation/env_infos/final/reward_dist Max               0.401314\n",
      "evaluation/env_infos/final/reward_dist Min               9.40453e-47\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00976908\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0329858\n",
      "evaluation/env_infos/initial/reward_dist Max             0.22407\n",
      "evaluation/env_infos/initial/reward_dist Min             2.34212e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0557189\n",
      "evaluation/env_infos/reward_dist Std                     0.142974\n",
      "evaluation/env_infos/reward_dist Max                     0.972434\n",
      "evaluation/env_infos/reward_dist Min                     9.40453e-47\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.118539\n",
      "evaluation/env_infos/final/reward_energy Std             0.0847105\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0212853\n",
      "evaluation/env_infos/final/reward_energy Min            -0.412104\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.248573\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255942\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0150159\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.28634\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0874845\n",
      "evaluation/env_infos/reward_energy Std                   0.099877\n",
      "evaluation/env_infos/reward_energy Max                  -0.00165591\n",
      "evaluation/env_infos/reward_energy Min                  -1.28634\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0446976\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27984\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.662738\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.964706\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00106255\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0125694\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0237391\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0487696\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0179977\n",
      "evaluation/env_infos/end_effector_loc Std                0.175489\n",
      "evaluation/env_infos/end_effector_loc Max                0.662738\n",
      "evaluation/env_infos/end_effector_loc Min               -0.964706\n",
      "time/data storing (s)                                    0.0029602\n",
      "time/evaluation sampling (s)                             0.995818\n",
      "time/exploration sampling (s)                            0.132013\n",
      "time/logging (s)                                         0.0240012\n",
      "time/saving (s)                                          0.0349898\n",
      "time/training (s)                                       46.9489\n",
      "time/epoch (s)                                          48.1387\n",
      "time/total (s)                                         548.668\n",
      "Epoch                                                   11\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:52.614512 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 12 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00317505\n",
      "trainer/QF2 Loss                                         0.00356741\n",
      "trainer/Policy Loss                                      4.224\n",
      "trainer/Q1 Predictions Mean                             -2.44865\n",
      "trainer/Q1 Predictions Std                               1.56015\n",
      "trainer/Q1 Predictions Max                              -0.0263941\n",
      "trainer/Q1 Predictions Min                              -9.22536\n",
      "trainer/Q2 Predictions Mean                             -2.45026\n",
      "trainer/Q2 Predictions Std                               1.55578\n",
      "trainer/Q2 Predictions Max                              -0.0634723\n",
      "trainer/Q2 Predictions Min                              -9.20636\n",
      "trainer/Q Targets Mean                                  -2.46091\n",
      "trainer/Q Targets Std                                    1.56521\n",
      "trainer/Q Targets Max                                   -0.0572312\n",
      "trainer/Q Targets Min                                   -9.26282\n",
      "trainer/Log Pis Mean                                     2.00327\n",
      "trainer/Log Pis Std                                      1.21642\n",
      "trainer/Log Pis Max                                      7.88299\n",
      "trainer/Log Pis Min                                     -2.20284\n",
      "trainer/Policy mu Mean                                  -0.00195495\n",
      "trainer/Policy mu Std                                    0.537883\n",
      "trainer/Policy mu Max                                    2.93236\n",
      "trainer/Policy mu Min                                   -2.59237\n",
      "trainer/Policy log std Mean                             -2.15926\n",
      "trainer/Policy log std Std                               0.529236\n",
      "trainer/Policy log std Max                              -0.392954\n",
      "trainer/Policy log std Min                              -3.06677\n",
      "trainer/Alpha                                            0.0164994\n",
      "trainer/Alpha Loss                                       0.0134078\n",
      "exploration/num steps total                           2300\n",
      "exploration/num paths total                            115\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.131759\n",
      "exploration/Rewards Std                                  0.0715171\n",
      "exploration/Rewards Max                                  0.0031933\n",
      "exploration/Rewards Min                                 -0.4202\n",
      "exploration/Returns Mean                                -2.63519\n",
      "exploration/Returns Std                                  0.544396\n",
      "exploration/Returns Max                                 -1.78086\n",
      "exploration/Returns Min                                 -3.45244\n",
      "exploration/Actions Mean                                -0.00353127\n",
      "exploration/Actions Std                                  0.122522\n",
      "exploration/Actions Max                                  0.581829\n",
      "exploration/Actions Min                                 -0.349097\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.63519\n",
      "exploration/env_infos/final/reward_dist Mean             0.004765\n",
      "exploration/env_infos/final/reward_dist Std              0.00936162\n",
      "exploration/env_infos/final/reward_dist Max              0.0234869\n",
      "exploration/env_infos/final/reward_dist Min              8.48965e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00470963\n",
      "exploration/env_infos/initial/reward_dist Std            0.00768251\n",
      "exploration/env_infos/initial/reward_dist Max            0.0199204\n",
      "exploration/env_infos/initial/reward_dist Min            6.01152e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0420192\n",
      "exploration/env_infos/reward_dist Std                    0.113035\n",
      "exploration/env_infos/reward_dist Max                    0.570829\n",
      "exploration/env_infos/reward_dist Min                    8.48965e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.129404\n",
      "exploration/env_infos/final/reward_energy Std            0.0663869\n",
      "exploration/env_infos/final/reward_energy Max           -0.0778513\n",
      "exploration/env_infos/final/reward_energy Min           -0.259631\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.214067\n",
      "exploration/env_infos/initial/reward_energy Std          0.120652\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0838295\n",
      "exploration/env_infos/initial/reward_energy Min         -0.411506\n",
      "exploration/env_infos/reward_energy Mean                -0.143218\n",
      "exploration/env_infos/reward_energy Std                  0.097657\n",
      "exploration/env_infos/reward_energy Max                 -0.00381842\n",
      "exploration/env_infos/reward_energy Min                 -0.589576\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0781184\n",
      "exploration/env_infos/final/end_effector_loc Std         0.267225\n",
      "exploration/env_infos/final/end_effector_loc Max         0.335246\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.405207\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000212404\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00868514\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.014911\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0141776\n",
      "exploration/env_infos/end_effector_loc Mean             -0.047834\n",
      "exploration/env_infos/end_effector_loc Std               0.179718\n",
      "exploration/env_infos/end_effector_loc Max               0.335246\n",
      "exploration/env_infos/end_effector_loc Min              -0.493902\n",
      "evaluation/num steps total                           13000\n",
      "evaluation/num paths total                             650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.11331\n",
      "evaluation/Rewards Std                                   0.0805047\n",
      "evaluation/Rewards Max                                   0.089257\n",
      "evaluation/Rewards Min                                  -0.596522\n",
      "evaluation/Returns Mean                                 -2.26621\n",
      "evaluation/Returns Std                                   1.08585\n",
      "evaluation/Returns Max                                   0.051371\n",
      "evaluation/Returns Min                                  -4.6357\n",
      "evaluation/Actions Mean                                 -0.00543072\n",
      "evaluation/Actions Std                                   0.080599\n",
      "evaluation/Actions Max                                   0.58027\n",
      "evaluation/Actions Min                                  -0.531828\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.26621\n",
      "evaluation/env_infos/final/reward_dist Mean              0.016623\n",
      "evaluation/env_infos/final/reward_dist Std               0.0590321\n",
      "evaluation/env_infos/final/reward_dist Max               0.394556\n",
      "evaluation/env_infos/final/reward_dist Min               3.77082e-97\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0056598\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00928809\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0354464\n",
      "evaluation/env_infos/initial/reward_dist Min             1.17022e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.054205\n",
      "evaluation/env_infos/reward_dist Std                     0.142844\n",
      "evaluation/env_infos/reward_dist Max                     0.994045\n",
      "evaluation/env_infos/reward_dist Min                     3.77082e-97\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.11998\n",
      "evaluation/env_infos/final/reward_energy Std             0.0784046\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0183616\n",
      "evaluation/env_infos/final/reward_energy Min            -0.349532\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.217207\n",
      "evaluation/env_infos/initial/reward_energy Std           0.132593\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0133179\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.616938\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0876509\n",
      "evaluation/env_infos/reward_energy Std                   0.0732715\n",
      "evaluation/env_infos/reward_energy Max                  -0.00223285\n",
      "evaluation/env_infos/reward_energy Min                  -0.616938\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0213813\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.364162\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.750014\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000851428\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00895683\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0290135\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0265914\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0184845\n",
      "evaluation/env_infos/end_effector_loc Std                0.211441\n",
      "evaluation/env_infos/end_effector_loc Max                0.750014\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00287573\n",
      "time/evaluation sampling (s)                             1.51933\n",
      "time/exploration sampling (s)                            0.120694\n",
      "time/logging (s)                                         0.0205859\n",
      "time/saving (s)                                          0.0283214\n",
      "time/training (s)                                       45.4317\n",
      "time/epoch (s)                                          47.1235\n",
      "time/total (s)                                         596.132\n",
      "Epoch                                                   12\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:35:40.069844 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 13 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00537421\n",
      "trainer/QF2 Loss                                         0.00195803\n",
      "trainer/Policy Loss                                      4.03001\n",
      "trainer/Q1 Predictions Mean                             -2.32261\n",
      "trainer/Q1 Predictions Std                               1.358\n",
      "trainer/Q1 Predictions Max                              -0.0723802\n",
      "trainer/Q1 Predictions Min                              -8.56189\n",
      "trainer/Q2 Predictions Mean                             -2.35368\n",
      "trainer/Q2 Predictions Std                               1.36607\n",
      "trainer/Q2 Predictions Max                              -0.094653\n",
      "trainer/Q2 Predictions Min                              -8.55066\n",
      "trainer/Q Targets Mean                                  -2.36893\n",
      "trainer/Q Targets Std                                    1.37124\n",
      "trainer/Q Targets Max                                   -0.119584\n",
      "trainer/Q Targets Min                                   -8.51335\n",
      "trainer/Log Pis Mean                                     1.88783\n",
      "trainer/Log Pis Std                                      1.27279\n",
      "trainer/Log Pis Max                                      5.90759\n",
      "trainer/Log Pis Min                                     -4.09385\n",
      "trainer/Policy mu Mean                                   0.00487049\n",
      "trainer/Policy mu Std                                    0.467628\n",
      "trainer/Policy mu Max                                    2.49156\n",
      "trainer/Policy mu Min                                   -2.47192\n",
      "trainer/Policy log std Mean                             -2.18588\n",
      "trainer/Policy log std Std                               0.553875\n",
      "trainer/Policy log std Max                              -0.303618\n",
      "trainer/Policy log std Min                              -3.17569\n",
      "trainer/Alpha                                            0.0167315\n",
      "trainer/Alpha Loss                                      -0.458823\n",
      "exploration/num steps total                           2400\n",
      "exploration/num paths total                            120\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.113472\n",
      "exploration/Rewards Std                                  0.132479\n",
      "exploration/Rewards Max                                  0.10711\n",
      "exploration/Rewards Min                                 -0.519395\n",
      "exploration/Returns Mean                                -2.26944\n",
      "exploration/Returns Std                                  2.00007\n",
      "exploration/Returns Max                                 -0.609371\n",
      "exploration/Returns Min                                 -6.13734\n",
      "exploration/Actions Mean                                -0.0153735\n",
      "exploration/Actions Std                                  0.158891\n",
      "exploration/Actions Max                                  0.35151\n",
      "exploration/Actions Min                                 -0.490659\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.26944\n",
      "exploration/env_infos/final/reward_dist Mean             0.0238925\n",
      "exploration/env_infos/final/reward_dist Std              0.044568\n",
      "exploration/env_infos/final/reward_dist Max              0.112911\n",
      "exploration/env_infos/final/reward_dist Min              2.22024e-44\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00283851\n",
      "exploration/env_infos/initial/reward_dist Std            0.00434978\n",
      "exploration/env_infos/initial/reward_dist Max            0.0113688\n",
      "exploration/env_infos/initial/reward_dist Min            7.41113e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.237012\n",
      "exploration/env_infos/reward_dist Std                    0.321921\n",
      "exploration/env_infos/reward_dist Max                    0.931536\n",
      "exploration/env_infos/reward_dist Min                    2.22024e-44\n",
      "exploration/env_infos/final/reward_energy Mean          -0.31262\n",
      "exploration/env_infos/final/reward_energy Std            0.167229\n",
      "exploration/env_infos/final/reward_energy Max           -0.123241\n",
      "exploration/env_infos/final/reward_energy Min           -0.589717\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.34263\n",
      "exploration/env_infos/initial/reward_energy Std          0.104192\n",
      "exploration/env_infos/initial/reward_energy Max         -0.190269\n",
      "exploration/env_infos/initial/reward_energy Min         -0.488087\n",
      "exploration/env_infos/reward_energy Mean                -0.196466\n",
      "exploration/env_infos/reward_energy Std                  0.111207\n",
      "exploration/env_infos/reward_energy Max                 -0.0182278\n",
      "exploration/env_infos/reward_energy Min                 -0.589717\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0682677\n",
      "exploration/env_infos/final/end_effector_loc Std         0.278402\n",
      "exploration/env_infos/final/end_effector_loc Max         0.354944\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.60749\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0023988\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0124322\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0175755\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0227185\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0125012\n",
      "exploration/env_infos/end_effector_loc Std               0.197948\n",
      "exploration/env_infos/end_effector_loc Max               0.395989\n",
      "exploration/env_infos/end_effector_loc Min              -0.60749\n",
      "evaluation/num steps total                           14000\n",
      "evaluation/num paths total                             700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.141833\n",
      "evaluation/Rewards Std                                   0.139444\n",
      "evaluation/Rewards Max                                   0.113222\n",
      "evaluation/Rewards Min                                  -0.852967\n",
      "evaluation/Returns Mean                                 -2.83665\n",
      "evaluation/Returns Std                                   1.7884\n",
      "evaluation/Returns Max                                  -0.407308\n",
      "evaluation/Returns Min                                  -7.11624\n",
      "evaluation/Actions Mean                                 -0.0176875\n",
      "evaluation/Actions Std                                   0.112689\n",
      "evaluation/Actions Max                                   0.460985\n",
      "evaluation/Actions Min                                  -0.965456\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.83665\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0410933\n",
      "evaluation/env_infos/final/reward_dist Std               0.120438\n",
      "evaluation/env_infos/final/reward_dist Max               0.64469\n",
      "evaluation/env_infos/final/reward_dist Min               3.17996e-71\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00902278\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0176072\n",
      "evaluation/env_infos/initial/reward_dist Max             0.106044\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0219e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0799971\n",
      "evaluation/env_infos/reward_dist Std                     0.17262\n",
      "evaluation/env_infos/reward_dist Max                     0.993073\n",
      "evaluation/env_infos/reward_dist Min                     3.17996e-71\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.172793\n",
      "evaluation/env_infos/final/reward_energy Std             0.176303\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00893808\n",
      "evaluation/env_infos/final/reward_energy Min            -0.716867\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.291787\n",
      "evaluation/env_infos/initial/reward_energy Std           0.252017\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0272597\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.23423\n",
      "evaluation/env_infos/reward_energy Mean                 -0.104271\n",
      "evaluation/env_infos/reward_energy Std                   0.123089\n",
      "evaluation/env_infos/reward_energy Max                  -0.00196708\n",
      "evaluation/env_infos/reward_energy Min                  -1.23423\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.113795\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.381246\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.872424\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.921956\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0018764\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0135016\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0230492\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0482728\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.040054\n",
      "evaluation/env_infos/end_effector_loc Std                0.246911\n",
      "evaluation/env_infos/end_effector_loc Max                0.872424\n",
      "evaluation/env_infos/end_effector_loc Min               -0.921956\n",
      "time/data storing (s)                                    0.00295392\n",
      "time/evaluation sampling (s)                             0.972253\n",
      "time/exploration sampling (s)                            0.13327\n",
      "time/logging (s)                                         0.0212546\n",
      "time/saving (s)                                          0.0287246\n",
      "time/training (s)                                       46.1097\n",
      "time/epoch (s)                                          47.2682\n",
      "time/total (s)                                         643.588\n",
      "Epoch                                                   13\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:36:26.274199 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 14 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00354493\n",
      "trainer/QF2 Loss                                         0.00852515\n",
      "trainer/Policy Loss                                      4.14236\n",
      "trainer/Q1 Predictions Mean                             -2.31629\n",
      "trainer/Q1 Predictions Std                               1.62204\n",
      "trainer/Q1 Predictions Max                              -0.102295\n",
      "trainer/Q1 Predictions Min                             -11.0259\n",
      "trainer/Q2 Predictions Mean                             -2.28972\n",
      "trainer/Q2 Predictions Std                               1.62082\n",
      "trainer/Q2 Predictions Max                              -0.095188\n",
      "trainer/Q2 Predictions Min                             -10.8642\n",
      "trainer/Q Targets Mean                                  -2.35058\n",
      "trainer/Q Targets Std                                    1.62666\n",
      "trainer/Q Targets Max                                   -0.0946199\n",
      "trainer/Q Targets Min                                  -11.1702\n",
      "trainer/Log Pis Mean                                     2.04076\n",
      "trainer/Log Pis Std                                      1.41477\n",
      "trainer/Log Pis Max                                      9.44729\n",
      "trainer/Log Pis Min                                     -4.11472\n",
      "trainer/Policy mu Mean                                  -0.000519301\n",
      "trainer/Policy mu Std                                    0.591826\n",
      "trainer/Policy mu Max                                    3.58411\n",
      "trainer/Policy mu Min                                   -3.15038\n",
      "trainer/Policy log std Mean                             -2.19905\n",
      "trainer/Policy log std Std                               0.571105\n",
      "trainer/Policy log std Max                               0.267343\n",
      "trainer/Policy log std Min                              -3.24373\n",
      "trainer/Alpha                                            0.0169097\n",
      "trainer/Alpha Loss                                       0.166235\n",
      "exploration/num steps total                           2500\n",
      "exploration/num paths total                            125\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0992958\n",
      "exploration/Rewards Std                                  0.0523542\n",
      "exploration/Rewards Max                                  6.9293e-05\n",
      "exploration/Rewards Min                                 -0.265651\n",
      "exploration/Returns Mean                                -1.98592\n",
      "exploration/Returns Std                                  0.168447\n",
      "exploration/Returns Max                                 -1.75153\n",
      "exploration/Returns Min                                 -2.26618\n",
      "exploration/Actions Mean                                 0.0094125\n",
      "exploration/Actions Std                                  0.110942\n",
      "exploration/Actions Max                                  0.34979\n",
      "exploration/Actions Min                                 -0.30868\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.98592\n",
      "exploration/env_infos/final/reward_dist Mean             0.0149397\n",
      "exploration/env_infos/final/reward_dist Std              0.0293737\n",
      "exploration/env_infos/final/reward_dist Max              0.0736822\n",
      "exploration/env_infos/final/reward_dist Min              1.03509e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0124829\n",
      "exploration/env_infos/initial/reward_dist Std            0.0138044\n",
      "exploration/env_infos/initial/reward_dist Max            0.0353458\n",
      "exploration/env_infos/initial/reward_dist Min            1.89945e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.13013\n",
      "exploration/env_infos/reward_dist Std                    0.200411\n",
      "exploration/env_infos/reward_dist Max                    0.952369\n",
      "exploration/env_infos/reward_dist Min                    1.03509e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0966569\n",
      "exploration/env_infos/final/reward_energy Std            0.0371921\n",
      "exploration/env_infos/final/reward_energy Max           -0.0510761\n",
      "exploration/env_infos/final/reward_energy Min           -0.158774\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.261763\n",
      "exploration/env_infos/initial/reward_energy Std          0.0588845\n",
      "exploration/env_infos/initial/reward_energy Max         -0.207782\n",
      "exploration/env_infos/initial/reward_energy Min         -0.370234\n",
      "exploration/env_infos/reward_energy Mean                -0.137738\n",
      "exploration/env_infos/reward_energy Std                  0.0762989\n",
      "exploration/env_infos/reward_energy Max                 -0.0123593\n",
      "exploration/env_infos/reward_energy Min                 -0.370234\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.131153\n",
      "exploration/env_infos/final/end_effector_loc Std         0.222824\n",
      "exploration/env_infos/final/end_effector_loc Max         0.421921\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.253324\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00254492\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00913823\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0174895\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0117221\n",
      "exploration/env_infos/end_effector_loc Mean              0.0779194\n",
      "exploration/env_infos/end_effector_loc Std               0.159239\n",
      "exploration/env_infos/end_effector_loc Max               0.421921\n",
      "exploration/env_infos/end_effector_loc Min              -0.272167\n",
      "evaluation/num steps total                           15000\n",
      "evaluation/num paths total                             750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0651951\n",
      "evaluation/Rewards Std                                   0.0671266\n",
      "evaluation/Rewards Max                                   0.120919\n",
      "evaluation/Rewards Min                                  -0.457957\n",
      "evaluation/Returns Mean                                 -1.3039\n",
      "evaluation/Returns Std                                   0.828431\n",
      "evaluation/Returns Max                                   0.492935\n",
      "evaluation/Returns Min                                  -3.027\n",
      "evaluation/Actions Mean                                  0.00805109\n",
      "evaluation/Actions Std                                   0.069193\n",
      "evaluation/Actions Max                                   0.447872\n",
      "evaluation/Actions Min                                  -0.888165\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.3039\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0835301\n",
      "evaluation/env_infos/final/reward_dist Std               0.201805\n",
      "evaluation/env_infos/final/reward_dist Max               0.894993\n",
      "evaluation/env_infos/final/reward_dist Min               2.30216e-28\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00669398\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0148527\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0835262\n",
      "evaluation/env_infos/initial/reward_dist Min             2.16964e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.146945\n",
      "evaluation/env_infos/reward_dist Std                     0.236651\n",
      "evaluation/env_infos/reward_dist Max                     0.997651\n",
      "evaluation/env_infos/reward_dist Min                     2.30216e-28\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.105141\n",
      "evaluation/env_infos/final/reward_energy Std             0.0485803\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0234537\n",
      "evaluation/env_infos/final/reward_energy Min            -0.251173\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.19599\n",
      "evaluation/env_infos/initial/reward_energy Std           0.149576\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0491944\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.05276\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0745109\n",
      "evaluation/env_infos/reward_energy Std                   0.0644446\n",
      "evaluation/env_infos/reward_energy Max                  -0.00219685\n",
      "evaluation/env_infos/reward_energy Min                  -1.05276\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0751944\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.298578\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.744223\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.800687\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000479177\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00870354\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0197703\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0444083\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0306339\n",
      "evaluation/env_infos/end_effector_loc Std                0.169931\n",
      "evaluation/env_infos/end_effector_loc Max                0.744223\n",
      "evaluation/env_infos/end_effector_loc Min               -0.800687\n",
      "time/data storing (s)                                    0.00300905\n",
      "time/evaluation sampling (s)                             1.08299\n",
      "time/exploration sampling (s)                            0.12979\n",
      "time/logging (s)                                         0.0197886\n",
      "time/saving (s)                                          0.0278049\n",
      "time/training (s)                                       44.7187\n",
      "time/epoch (s)                                          45.982\n",
      "time/total (s)                                         689.79\n",
      "Epoch                                                   14\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:37:15.380528 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 15 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00947299\r\n",
      "trainer/QF2 Loss                                         0.00308359\r\n",
      "trainer/Policy Loss                                      3.80736\r\n",
      "trainer/Q1 Predictions Mean                             -1.98889\r\n",
      "trainer/Q1 Predictions Std                               1.50386\r\n",
      "trainer/Q1 Predictions Max                               0.040041\r\n",
      "trainer/Q1 Predictions Min                              -8.6679\r\n",
      "trainer/Q2 Predictions Mean                             -2.00543\r\n",
      "trainer/Q2 Predictions Std                               1.50119\r\n",
      "trainer/Q2 Predictions Max                               0.00657093\r\n",
      "trainer/Q2 Predictions Min                              -8.69265\r\n",
      "trainer/Q Targets Mean                                  -2.02206\r\n",
      "trainer/Q Targets Std                                    1.51506\r\n",
      "trainer/Q Targets Max                                   -0.00576274\r\n",
      "trainer/Q Targets Min                                   -8.81367\r\n",
      "trainer/Log Pis Mean                                     1.94306\r\n",
      "trainer/Log Pis Std                                      1.46155\r\n",
      "trainer/Log Pis Max                                      7.6618\r\n",
      "trainer/Log Pis Min                                     -4.12302\r\n",
      "trainer/Policy mu Mean                                  -0.0299617\r\n",
      "trainer/Policy mu Std                                    0.573395\r\n",
      "trainer/Policy mu Max                                    2.85293\r\n",
      "trainer/Policy mu Min                                   -3.0388\r\n",
      "trainer/Policy log std Mean                             -2.18362\r\n",
      "trainer/Policy log std Std                               0.603666\r\n",
      "trainer/Policy log std Max                              -0.196057\r\n",
      "trainer/Policy log std Min                              -3.25162\r\n",
      "trainer/Alpha                                            0.0178759\r\n",
      "trainer/Alpha Loss                                      -0.229084\r\n",
      "exploration/num steps total                           2600\r\n",
      "exploration/num paths total                            130\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.145228\r\n",
      "exploration/Rewards Std                                  0.0450999\r\n",
      "exploration/Rewards Max                                 -0.0601713\r\n",
      "exploration/Rewards Min                                 -0.285053\r\n",
      "exploration/Returns Mean                                -2.90456\r\n",
      "exploration/Returns Std                                  0.219997\r\n",
      "exploration/Returns Max                                 -2.55286\r\n",
      "exploration/Returns Min                                 -3.16032\r\n",
      "exploration/Actions Mean                                 0.0106482\r\n",
      "exploration/Actions Std                                  0.0956965\r\n",
      "exploration/Actions Max                                  0.232541\r\n",
      "exploration/Actions Min                                 -0.26368\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.90456\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.015703\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0251714\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0649381\r\n",
      "exploration/env_infos/final/reward_dist Min              1.11619e-21\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00101802\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00145204\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00383731\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.74231e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.065356\r\n",
      "exploration/env_infos/reward_dist Std                    0.142251\r\n",
      "exploration/env_infos/reward_dist Max                    0.758304\r\n",
      "exploration/env_infos/reward_dist Min                    1.11619e-21\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.169584\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0858512\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0441417\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.286725\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.155508\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0762767\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0747444\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.279184\r\n",
      "exploration/env_infos/reward_energy Mean                -0.121667\r\n",
      "exploration/env_infos/reward_energy Std                  0.0611518\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0200404\r\n",
      "exploration/env_infos/reward_energy Min                 -0.286725\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.214847\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.234917\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.538943\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.12317\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000820742\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00606855\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0100372\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00959922\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0977765\r\n",
      "exploration/env_infos/end_effector_loc Std               0.165075\r\n",
      "exploration/env_infos/end_effector_loc Max               0.538943\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.163314\r\n",
      "evaluation/num steps total                           16000\r\n",
      "evaluation/num paths total                             800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0744945\r\n",
      "evaluation/Rewards Std                                   0.0822976\r\n",
      "evaluation/Rewards Max                                   0.183006\r\n",
      "evaluation/Rewards Min                                  -0.81635\r\n",
      "evaluation/Returns Mean                                 -1.48989\r\n",
      "evaluation/Returns Std                                   1.22768\r\n",
      "evaluation/Returns Max                                   1.21746\r\n",
      "evaluation/Returns Min                                  -4.36623\r\n",
      "evaluation/Actions Mean                                 -0.00288425\r\n",
      "evaluation/Actions Std                                   0.0896244\r\n",
      "evaluation/Actions Max                                   0.681598\r\n",
      "evaluation/Actions Min                                  -0.963146\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.48989\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0982139\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.179896\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.620801\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.11665e-90\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00543677\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111258\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0505848\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.27344e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.133422\r\n",
      "evaluation/env_infos/reward_dist Std                     0.233219\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996337\r\n",
      "evaluation/env_infos/reward_dist Min                     3.11665e-90\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0856777\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.076248\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00787643\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.337661\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.268818\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.239762\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0497387\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.13845\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0826603\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0961717\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00343799\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.13845\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0137987\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.301057\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.605947\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000130705\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127346\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0340799\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0481573\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00591288\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.188143\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.605947\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00313563\r\n",
      "time/evaluation sampling (s)                             0.958558\r\n",
      "time/exploration sampling (s)                            0.1377\r\n",
      "time/logging (s)                                         0.0196128\r\n",
      "time/saving (s)                                          0.0297937\r\n",
      "time/training (s)                                       47.7398\r\n",
      "time/epoch (s)                                          48.8886\r\n",
      "time/total (s)                                         738.894\r\n",
      "Epoch                                                   15\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:38:03.742903 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 16 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00364263\n",
      "trainer/QF2 Loss                                         0.00332528\n",
      "trainer/Policy Loss                                      3.88451\n",
      "trainer/Q1 Predictions Mean                             -1.92774\n",
      "trainer/Q1 Predictions Std                               1.35645\n",
      "trainer/Q1 Predictions Max                              -0.0201662\n",
      "trainer/Q1 Predictions Min                              -8.8972\n",
      "trainer/Q2 Predictions Mean                             -1.94057\n",
      "trainer/Q2 Predictions Std                               1.36417\n",
      "trainer/Q2 Predictions Max                              -0.0133821\n",
      "trainer/Q2 Predictions Min                              -9.0084\n",
      "trainer/Q Targets Mean                                  -1.95292\n",
      "trainer/Q Targets Std                                    1.3628\n",
      "trainer/Q Targets Max                                   -0.0231312\n",
      "trainer/Q Targets Min                                   -8.98498\n",
      "trainer/Log Pis Mean                                     2.06366\n",
      "trainer/Log Pis Std                                      1.39188\n",
      "trainer/Log Pis Max                                      7.54606\n",
      "trainer/Log Pis Min                                     -3.20364\n",
      "trainer/Policy mu Mean                                  -0.0310648\n",
      "trainer/Policy mu Std                                    0.532819\n",
      "trainer/Policy mu Max                                    2.96365\n",
      "trainer/Policy mu Min                                   -3.29314\n",
      "trainer/Policy log std Mean                             -2.26724\n",
      "trainer/Policy log std Std                               0.592817\n",
      "trainer/Policy log std Max                              -0.101008\n",
      "trainer/Policy log std Min                              -3.32593\n",
      "trainer/Alpha                                            0.0182089\n",
      "trainer/Alpha Loss                                       0.254999\n",
      "exploration/num steps total                           2700\n",
      "exploration/num paths total                            135\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.155666\n",
      "exploration/Rewards Std                                  0.0732162\n",
      "exploration/Rewards Max                                 -0.0239469\n",
      "exploration/Rewards Min                                 -0.354925\n",
      "exploration/Returns Mean                                -3.11332\n",
      "exploration/Returns Std                                  0.878222\n",
      "exploration/Returns Max                                 -1.71933\n",
      "exploration/Returns Min                                 -4.29106\n",
      "exploration/Actions Mean                                 0.00247802\n",
      "exploration/Actions Std                                  0.11933\n",
      "exploration/Actions Max                                  0.575437\n",
      "exploration/Actions Min                                 -0.495197\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.11332\n",
      "exploration/env_infos/final/reward_dist Mean             0.170989\n",
      "exploration/env_infos/final/reward_dist Std              0.336779\n",
      "exploration/env_infos/final/reward_dist Max              0.844499\n",
      "exploration/env_infos/final/reward_dist Min              6.75078e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00745628\n",
      "exploration/env_infos/initial/reward_dist Std            0.0123423\n",
      "exploration/env_infos/initial/reward_dist Max            0.0319307\n",
      "exploration/env_infos/initial/reward_dist Min            0.000181903\n",
      "exploration/env_infos/reward_dist Mean                   0.11577\n",
      "exploration/env_infos/reward_dist Std                    0.262172\n",
      "exploration/env_infos/reward_dist Max                    0.998944\n",
      "exploration/env_infos/reward_dist Min                    6.75078e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.10089\n",
      "exploration/env_infos/final/reward_energy Std            0.0689294\n",
      "exploration/env_infos/final/reward_energy Max           -0.0185651\n",
      "exploration/env_infos/final/reward_energy Min           -0.200434\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.265584\n",
      "exploration/env_infos/initial/reward_energy Std          0.226792\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0786856\n",
      "exploration/env_infos/initial/reward_energy Min         -0.712\n",
      "exploration/env_infos/reward_energy Mean                -0.137754\n",
      "exploration/env_infos/reward_energy Std                  0.097547\n",
      "exploration/env_infos/reward_energy Max                 -0.015592\n",
      "exploration/env_infos/reward_energy Min                 -0.712\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.110411\n",
      "exploration/env_infos/final/end_effector_loc Std         0.238506\n",
      "exploration/env_infos/final/end_effector_loc Max         0.480511\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.257732\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00729579\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00996162\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0287718\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00331793\n",
      "exploration/env_infos/end_effector_loc Mean              0.0827868\n",
      "exploration/env_infos/end_effector_loc Std               0.123895\n",
      "exploration/env_infos/end_effector_loc Max               0.480511\n",
      "exploration/env_infos/end_effector_loc Min              -0.257732\n",
      "evaluation/num steps total                           17000\n",
      "evaluation/num paths total                             850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0904323\n",
      "evaluation/Rewards Std                                   0.10273\n",
      "evaluation/Rewards Max                                   0.151495\n",
      "evaluation/Rewards Min                                  -0.893192\n",
      "evaluation/Returns Mean                                 -1.80865\n",
      "evaluation/Returns Std                                   1.5718\n",
      "evaluation/Returns Max                                   1.25263\n",
      "evaluation/Returns Min                                  -7.44473\n",
      "evaluation/Actions Mean                                  0.0042963\n",
      "evaluation/Actions Std                                   0.0750943\n",
      "evaluation/Actions Max                                   0.590102\n",
      "evaluation/Actions Min                                  -0.825016\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.80865\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0720422\n",
      "evaluation/env_infos/final/reward_dist Std               0.229787\n",
      "evaluation/env_infos/final/reward_dist Max               0.978556\n",
      "evaluation/env_infos/final/reward_dist Min               2.24021e-104\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00864416\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0158566\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0767524\n",
      "evaluation/env_infos/initial/reward_dist Min             1.94032e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.118339\n",
      "evaluation/env_infos/reward_dist Std                     0.230631\n",
      "evaluation/env_infos/reward_dist Max                     0.992252\n",
      "evaluation/env_infos/reward_dist Min                     2.24021e-104\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0882389\n",
      "evaluation/env_infos/final/reward_energy Std             0.0664704\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00674174\n",
      "evaluation/env_infos/final/reward_energy Min            -0.36752\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.213732\n",
      "evaluation/env_infos/initial/reward_energy Std           0.18317\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0316562\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.03267\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0761563\n",
      "evaluation/env_infos/reward_energy Std                   0.0742661\n",
      "evaluation/env_infos/reward_energy Max                  -0.000825669\n",
      "evaluation/env_infos/reward_energy Min                  -1.03267\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0811854\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.341554\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.839028\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.001867\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00977523\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0295051\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0412508\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0367432\n",
      "evaluation/env_infos/end_effector_loc Std                0.20107\n",
      "evaluation/env_infos/end_effector_loc Max                0.839028\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00295026\n",
      "time/evaluation sampling (s)                             0.995473\n",
      "time/exploration sampling (s)                            0.127965\n",
      "time/logging (s)                                         0.0228965\n",
      "time/saving (s)                                          0.0285141\n",
      "time/training (s)                                       46.9488\n",
      "time/epoch (s)                                          48.1266\n",
      "time/total (s)                                         787.259\n",
      "Epoch                                                   16\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:38:51.603925 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 17 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00134535\n",
      "trainer/QF2 Loss                                         0.00185492\n",
      "trainer/Policy Loss                                      3.79255\n",
      "trainer/Q1 Predictions Mean                             -1.92636\n",
      "trainer/Q1 Predictions Std                               1.23702\n",
      "trainer/Q1 Predictions Max                              -0.143558\n",
      "trainer/Q1 Predictions Min                              -6.7644\n",
      "trainer/Q2 Predictions Mean                             -1.92472\n",
      "trainer/Q2 Predictions Std                               1.23606\n",
      "trainer/Q2 Predictions Max                              -0.12905\n",
      "trainer/Q2 Predictions Min                              -6.71593\n",
      "trainer/Q Targets Mean                                  -1.91933\n",
      "trainer/Q Targets Std                                    1.23681\n",
      "trainer/Q Targets Max                                   -0.125169\n",
      "trainer/Q Targets Min                                   -6.75863\n",
      "trainer/Log Pis Mean                                     1.97943\n",
      "trainer/Log Pis Std                                      1.23391\n",
      "trainer/Log Pis Max                                      6.04687\n",
      "trainer/Log Pis Min                                     -2.26705\n",
      "trainer/Policy mu Mean                                  -0.0348583\n",
      "trainer/Policy mu Std                                    0.422634\n",
      "trainer/Policy mu Max                                    2.74933\n",
      "trainer/Policy mu Min                                   -2.5356\n",
      "trainer/Policy log std Mean                             -2.25391\n",
      "trainer/Policy log std Std                               0.517431\n",
      "trainer/Policy log std Max                              -0.132871\n",
      "trainer/Policy log std Min                              -3.09875\n",
      "trainer/Alpha                                            0.0184581\n",
      "trainer/Alpha Loss                                      -0.0821298\n",
      "exploration/num steps total                           2800\n",
      "exploration/num paths total                            140\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.105289\n",
      "exploration/Rewards Std                                  0.0546284\n",
      "exploration/Rewards Max                                 -0.0203557\n",
      "exploration/Rewards Min                                 -0.3092\n",
      "exploration/Returns Mean                                -2.10578\n",
      "exploration/Returns Std                                  0.543837\n",
      "exploration/Returns Max                                 -1.49298\n",
      "exploration/Returns Min                                 -2.94156\n",
      "exploration/Actions Mean                                -0.00807327\n",
      "exploration/Actions Std                                  0.140173\n",
      "exploration/Actions Max                                  0.43857\n",
      "exploration/Actions Min                                 -0.401733\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.10578\n",
      "exploration/env_infos/final/reward_dist Mean             0.0364483\n",
      "exploration/env_infos/final/reward_dist Std              0.0463598\n",
      "exploration/env_infos/final/reward_dist Max              0.119194\n",
      "exploration/env_infos/final/reward_dist Min              9.84639e-22\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103968\n",
      "exploration/env_infos/initial/reward_dist Std            0.0127951\n",
      "exploration/env_infos/initial/reward_dist Max            0.0280293\n",
      "exploration/env_infos/initial/reward_dist Min            1.82944e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0785548\n",
      "exploration/env_infos/reward_dist Std                    0.17422\n",
      "exploration/env_infos/reward_dist Max                    0.825281\n",
      "exploration/env_infos/reward_dist Min                    9.84639e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.283046\n",
      "exploration/env_infos/final/reward_energy Std            0.137616\n",
      "exploration/env_infos/final/reward_energy Max           -0.0864816\n",
      "exploration/env_infos/final/reward_energy Min           -0.464948\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.247942\n",
      "exploration/env_infos/initial/reward_energy Std          0.116682\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0871623\n",
      "exploration/env_infos/initial/reward_energy Min         -0.428858\n",
      "exploration/env_infos/reward_energy Mean                -0.171317\n",
      "exploration/env_infos/reward_energy Std                  0.100389\n",
      "exploration/env_infos/reward_energy Max                 -0.0246617\n",
      "exploration/env_infos/reward_energy Min                 -0.464948\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0579131\n",
      "exploration/env_infos/final/end_effector_loc Std         0.246385\n",
      "exploration/env_infos/final/end_effector_loc Max         0.282337\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.687232\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00223741\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00942636\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0188638\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0133595\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00539084\n",
      "exploration/env_infos/end_effector_loc Std               0.138298\n",
      "exploration/env_infos/end_effector_loc Max               0.298093\n",
      "exploration/env_infos/end_effector_loc Min              -0.687232\n",
      "evaluation/num steps total                           18000\n",
      "evaluation/num paths total                             900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.090208\n",
      "evaluation/Rewards Std                                   0.0776281\n",
      "evaluation/Rewards Max                                   0.0977076\n",
      "evaluation/Rewards Min                                  -0.589263\n",
      "evaluation/Returns Mean                                 -1.80416\n",
      "evaluation/Returns Std                                   1.14835\n",
      "evaluation/Returns Max                                   0.840507\n",
      "evaluation/Returns Min                                  -4.51516\n",
      "evaluation/Actions Mean                                 -0.00156521\n",
      "evaluation/Actions Std                                   0.0943222\n",
      "evaluation/Actions Max                                   0.818999\n",
      "evaluation/Actions Min                                  -0.914876\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.80416\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0836764\n",
      "evaluation/env_infos/final/reward_dist Std               0.200721\n",
      "evaluation/env_infos/final/reward_dist Max               0.903356\n",
      "evaluation/env_infos/final/reward_dist Min               1.15563e-64\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705409\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165747\n",
      "evaluation/env_infos/initial/reward_dist Max             0.104594\n",
      "evaluation/env_infos/initial/reward_dist Min             1.84652e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0898322\n",
      "evaluation/env_infos/reward_dist Std                     0.183387\n",
      "evaluation/env_infos/reward_dist Max                     0.987575\n",
      "evaluation/env_infos/reward_dist Min                     1.15563e-64\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0884292\n",
      "evaluation/env_infos/final/reward_energy Std             0.0764876\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0108143\n",
      "evaluation/env_infos/final/reward_energy Min            -0.351331\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.268741\n",
      "evaluation/env_infos/initial/reward_energy Std           0.305338\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0132435\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.10874\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0823987\n",
      "evaluation/env_infos/reward_energy Std                   0.104922\n",
      "evaluation/env_infos/reward_energy Max                  -0.00172516\n",
      "evaluation/env_infos/reward_energy Min                  -1.10874\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0180693\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.287308\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.590674\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.91359\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000114944\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0143806\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0409499\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0457438\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0144328\n",
      "evaluation/env_infos/end_effector_loc Std                0.188083\n",
      "evaluation/env_infos/end_effector_loc Max                0.590674\n",
      "evaluation/env_infos/end_effector_loc Min               -0.91359\n",
      "time/data storing (s)                                    0.00331784\n",
      "time/evaluation sampling (s)                             1.06589\n",
      "time/exploration sampling (s)                            0.151207\n",
      "time/logging (s)                                         0.0216172\n",
      "time/saving (s)                                          0.0285518\n",
      "time/training (s)                                       46.3353\n",
      "time/epoch (s)                                          47.6059\n",
      "time/total (s)                                         835.118\n",
      "Epoch                                                   17\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:39:40.020896 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 18 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00234868\n",
      "trainer/QF2 Loss                                         0.00141051\n",
      "trainer/Policy Loss                                      3.85093\n",
      "trainer/Q1 Predictions Mean                             -1.87584\n",
      "trainer/Q1 Predictions Std                               1.35617\n",
      "trainer/Q1 Predictions Max                              -0.0242751\n",
      "trainer/Q1 Predictions Min                              -8.4857\n",
      "trainer/Q2 Predictions Mean                             -1.88257\n",
      "trainer/Q2 Predictions Std                               1.35924\n",
      "trainer/Q2 Predictions Max                              -0.0286584\n",
      "trainer/Q2 Predictions Min                              -8.47534\n",
      "trainer/Q Targets Mean                                  -1.88788\n",
      "trainer/Q Targets Std                                    1.36053\n",
      "trainer/Q Targets Max                                   -0.00576274\n",
      "trainer/Q Targets Min                                   -8.54988\n",
      "trainer/Log Pis Mean                                     2.06299\n",
      "trainer/Log Pis Std                                      1.18116\n",
      "trainer/Log Pis Max                                      9.20379\n",
      "trainer/Log Pis Min                                     -2.44228\n",
      "trainer/Policy mu Mean                                  -0.041886\n",
      "trainer/Policy mu Std                                    0.365306\n",
      "trainer/Policy mu Max                                    3.03972\n",
      "trainer/Policy mu Min                                   -2.89623\n",
      "trainer/Policy log std Mean                             -2.34255\n",
      "trainer/Policy log std Std                               0.473543\n",
      "trainer/Policy log std Max                              -0.212367\n",
      "trainer/Policy log std Min                              -3.16668\n",
      "trainer/Alpha                                            0.0190544\n",
      "trainer/Alpha Loss                                       0.249439\n",
      "exploration/num steps total                           2900\n",
      "exploration/num paths total                            145\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.123431\n",
      "exploration/Rewards Std                                  0.0563293\n",
      "exploration/Rewards Max                                  0.0334956\n",
      "exploration/Rewards Min                                 -0.298208\n",
      "exploration/Returns Mean                                -2.46862\n",
      "exploration/Returns Std                                  0.755072\n",
      "exploration/Returns Max                                 -0.978008\n",
      "exploration/Returns Min                                 -2.96509\n",
      "exploration/Actions Mean                                -0.0048313\n",
      "exploration/Actions Std                                  0.107572\n",
      "exploration/Actions Max                                  0.336576\n",
      "exploration/Actions Min                                 -0.26348\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.46862\n",
      "exploration/env_infos/final/reward_dist Mean             0.00497595\n",
      "exploration/env_infos/final/reward_dist Std              0.00981316\n",
      "exploration/env_infos/final/reward_dist Max              0.0246017\n",
      "exploration/env_infos/final/reward_dist Min              1.31031e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00791899\n",
      "exploration/env_infos/initial/reward_dist Std            0.0132966\n",
      "exploration/env_infos/initial/reward_dist Max            0.0344729\n",
      "exploration/env_infos/initial/reward_dist Min            1.63266e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0789486\n",
      "exploration/env_infos/reward_dist Std                    0.194042\n",
      "exploration/env_infos/reward_dist Max                    0.892121\n",
      "exploration/env_infos/reward_dist Min                    4.17754e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.128683\n",
      "exploration/env_infos/final/reward_energy Std            0.0465194\n",
      "exploration/env_infos/final/reward_energy Max           -0.0750233\n",
      "exploration/env_infos/final/reward_energy Min           -0.18857\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.241831\n",
      "exploration/env_infos/initial/reward_energy Std          0.0964385\n",
      "exploration/env_infos/initial/reward_energy Max         -0.097135\n",
      "exploration/env_infos/initial/reward_energy Min         -0.369216\n",
      "exploration/env_infos/reward_energy Mean                -0.133601\n",
      "exploration/env_infos/reward_energy Std                  0.0730825\n",
      "exploration/env_infos/reward_energy Max                 -0.00744318\n",
      "exploration/env_infos/reward_energy Min                 -0.369216\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0642819\n",
      "exploration/env_infos/final/end_effector_loc Std         0.193274\n",
      "exploration/env_infos/final/end_effector_loc Max         0.331154\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.296752\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00299784\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00870294\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0168288\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00991487\n",
      "exploration/env_infos/end_effector_loc Mean              0.0537275\n",
      "exploration/env_infos/end_effector_loc Std               0.141439\n",
      "exploration/env_infos/end_effector_loc Max               0.331154\n",
      "exploration/env_infos/end_effector_loc Min              -0.296752\n",
      "evaluation/num steps total                           19000\n",
      "evaluation/num paths total                             950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0947729\n",
      "evaluation/Rewards Std                                   0.0785104\n",
      "evaluation/Rewards Max                                   0.0890514\n",
      "evaluation/Rewards Min                                  -0.496971\n",
      "evaluation/Returns Mean                                 -1.89546\n",
      "evaluation/Returns Std                                   1.04921\n",
      "evaluation/Returns Max                                  -0.056208\n",
      "evaluation/Returns Min                                  -4.94838\n",
      "evaluation/Actions Mean                                 -0.00483825\n",
      "evaluation/Actions Std                                   0.0755955\n",
      "evaluation/Actions Max                                   0.477069\n",
      "evaluation/Actions Min                                  -0.831519\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.89546\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0397\n",
      "evaluation/env_infos/final/reward_dist Std               0.122278\n",
      "evaluation/env_infos/final/reward_dist Max               0.518291\n",
      "evaluation/env_infos/final/reward_dist Min               1.04003e-39\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00687103\n",
      "evaluation/env_infos/initial/reward_dist Std             0.011469\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0442391\n",
      "evaluation/env_infos/initial/reward_dist Min             8.31955e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.078358\n",
      "evaluation/env_infos/reward_dist Std                     0.153832\n",
      "evaluation/env_infos/reward_dist Max                     0.939358\n",
      "evaluation/env_infos/reward_dist Min                     1.04003e-39\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0955743\n",
      "evaluation/env_infos/final/reward_energy Std             0.0728919\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0140328\n",
      "evaluation/env_infos/final/reward_energy Min            -0.275318\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.192148\n",
      "evaluation/env_infos/initial/reward_energy Std           0.197052\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00276471\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.02152\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0741009\n",
      "evaluation/env_infos/reward_energy Std                   0.0773643\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178216\n",
      "evaluation/env_infos/reward_energy Min                  -1.02152\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0190025\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.354451\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.752581\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.820331\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00100173\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00967907\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0238535\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0415759\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00587165\n",
      "evaluation/env_infos/end_effector_loc Std                0.205025\n",
      "evaluation/env_infos/end_effector_loc Max                0.752581\n",
      "evaluation/env_infos/end_effector_loc Min               -0.820331\n",
      "time/data storing (s)                                    0.00300187\n",
      "time/evaluation sampling (s)                             1.01949\n",
      "time/exploration sampling (s)                            0.136509\n",
      "time/logging (s)                                         0.0193517\n",
      "time/saving (s)                                          0.0295015\n",
      "time/training (s)                                       46.9456\n",
      "time/epoch (s)                                          48.1535\n",
      "time/total (s)                                         883.532\n",
      "Epoch                                                   18\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:40:28.051037 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 19 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00251724\n",
      "trainer/QF2 Loss                                         0.00149648\n",
      "trainer/Policy Loss                                      3.6917\n",
      "trainer/Q1 Predictions Mean                             -1.82294\n",
      "trainer/Q1 Predictions Std                               1.23015\n",
      "trainer/Q1 Predictions Max                              -0.00687152\n",
      "trainer/Q1 Predictions Min                              -8.12665\n",
      "trainer/Q2 Predictions Mean                             -1.83408\n",
      "trainer/Q2 Predictions Std                               1.23274\n",
      "trainer/Q2 Predictions Max                              -0.0346256\n",
      "trainer/Q2 Predictions Min                              -8.09983\n",
      "trainer/Q Targets Mean                                  -1.84844\n",
      "trainer/Q Targets Std                                    1.24047\n",
      "trainer/Q Targets Max                                   -0.0657975\n",
      "trainer/Q Targets Min                                   -8.158\n",
      "trainer/Log Pis Mean                                     1.9549\n",
      "trainer/Log Pis Std                                      1.23686\n",
      "trainer/Log Pis Max                                      4.65368\n",
      "trainer/Log Pis Min                                     -4.7722\n",
      "trainer/Policy mu Mean                                  -0.0485406\n",
      "trainer/Policy mu Std                                    0.397637\n",
      "trainer/Policy mu Max                                    2.35706\n",
      "trainer/Policy mu Min                                   -2.70976\n",
      "trainer/Policy log std Mean                             -2.28258\n",
      "trainer/Policy log std Std                               0.519785\n",
      "trainer/Policy log std Max                               0.39747\n",
      "trainer/Policy log std Min                              -3.1943\n",
      "trainer/Alpha                                            0.0191408\n",
      "trainer/Alpha Loss                                      -0.178329\n",
      "exploration/num steps total                           3000\n",
      "exploration/num paths total                            150\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.161702\n",
      "exploration/Rewards Std                                  0.0863981\n",
      "exploration/Rewards Max                                  0.0152004\n",
      "exploration/Rewards Min                                 -0.390631\n",
      "exploration/Returns Mean                                -3.23404\n",
      "exploration/Returns Std                                  1.12267\n",
      "exploration/Returns Max                                 -2.06462\n",
      "exploration/Returns Min                                 -5.19172\n",
      "exploration/Actions Mean                                -0.00669403\n",
      "exploration/Actions Std                                  0.177658\n",
      "exploration/Actions Max                                  0.743394\n",
      "exploration/Actions Min                                 -0.631316\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.23404\n",
      "exploration/env_infos/final/reward_dist Mean             0.0313882\n",
      "exploration/env_infos/final/reward_dist Std              0.0378835\n",
      "exploration/env_infos/final/reward_dist Max              0.0838394\n",
      "exploration/env_infos/final/reward_dist Min              1.48617e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00101911\n",
      "exploration/env_infos/initial/reward_dist Std            0.00130908\n",
      "exploration/env_infos/initial/reward_dist Max            0.0032901\n",
      "exploration/env_infos/initial/reward_dist Min            3.75583e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.102914\n",
      "exploration/env_infos/reward_dist Std                    0.219232\n",
      "exploration/env_infos/reward_dist Max                    0.895757\n",
      "exploration/env_infos/reward_dist Min                    1.48617e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.256606\n",
      "exploration/env_infos/final/reward_energy Std            0.148254\n",
      "exploration/env_infos/final/reward_energy Max           -0.102855\n",
      "exploration/env_infos/final/reward_energy Min           -0.458756\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.401437\n",
      "exploration/env_infos/initial/reward_energy Std          0.331354\n",
      "exploration/env_infos/initial/reward_energy Max         -0.125458\n",
      "exploration/env_infos/initial/reward_energy Min         -1.03444\n",
      "exploration/env_infos/reward_energy Mean                -0.19052\n",
      "exploration/env_infos/reward_energy Std                  0.164064\n",
      "exploration/env_infos/reward_energy Max                 -0.0191299\n",
      "exploration/env_infos/reward_energy Min                 -1.03444\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0317379\n",
      "exploration/env_infos/final/end_effector_loc Std         0.278544\n",
      "exploration/env_infos/final/end_effector_loc Max         0.307163\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.482666\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0049213\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0177331\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0371697\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0202071\n",
      "exploration/env_infos/end_effector_loc Mean              0.00809683\n",
      "exploration/env_infos/end_effector_loc Std               0.179612\n",
      "exploration/env_infos/end_effector_loc Max               0.311814\n",
      "exploration/env_infos/end_effector_loc Min              -0.482666\n",
      "evaluation/num steps total                           20000\n",
      "evaluation/num paths total                            1000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0873154\n",
      "evaluation/Rewards Std                                   0.075299\n",
      "evaluation/Rewards Max                                   0.153521\n",
      "evaluation/Rewards Min                                  -0.351075\n",
      "evaluation/Returns Mean                                 -1.74631\n",
      "evaluation/Returns Std                                   1.12527\n",
      "evaluation/Returns Max                                   1.65978\n",
      "evaluation/Returns Min                                  -4.49939\n",
      "evaluation/Actions Mean                                 -0.00544523\n",
      "evaluation/Actions Std                                   0.0647317\n",
      "evaluation/Actions Max                                   0.615275\n",
      "evaluation/Actions Min                                  -0.573434\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.74631\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0339472\n",
      "evaluation/env_infos/final/reward_dist Std               0.106366\n",
      "evaluation/env_infos/final/reward_dist Max               0.618989\n",
      "evaluation/env_infos/final/reward_dist Min               1.33749e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00379805\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00699191\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0306269\n",
      "evaluation/env_infos/initial/reward_dist Min             2.39838e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0969367\n",
      "evaluation/env_infos/reward_dist Std                     0.19606\n",
      "evaluation/env_infos/reward_dist Max                     0.998722\n",
      "evaluation/env_infos/reward_dist Min                     1.33749e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.07739\n",
      "evaluation/env_infos/final/reward_energy Std             0.0547344\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00413626\n",
      "evaluation/env_infos/final/reward_energy Min            -0.233816\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.181832\n",
      "evaluation/env_infos/initial/reward_energy Std           0.146164\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.019499\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.802407\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0664638\n",
      "evaluation/env_infos/reward_energy Std                   0.0634212\n",
      "evaluation/env_infos/reward_energy Max                  -0.000204311\n",
      "evaluation/env_infos/reward_energy Min                  -0.802407\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0184727\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.316183\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.621443\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.666319\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00175271\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00805987\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0307637\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0286717\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0247948\n",
      "evaluation/env_infos/end_effector_loc Std                0.184259\n",
      "evaluation/env_infos/end_effector_loc Max                0.621443\n",
      "evaluation/env_infos/end_effector_loc Min               -0.666319\n",
      "time/data storing (s)                                    0.00299726\n",
      "time/evaluation sampling (s)                             0.974664\n",
      "time/exploration sampling (s)                            0.128159\n",
      "time/logging (s)                                         0.0286003\n",
      "time/saving (s)                                          0.0303682\n",
      "time/training (s)                                       46.6086\n",
      "time/epoch (s)                                          47.7733\n",
      "time/total (s)                                         931.57\n",
      "Epoch                                                   19\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:41:16.799378 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 20 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00248698\r\n",
      "trainer/QF2 Loss                                         0.00261965\r\n",
      "trainer/Policy Loss                                      3.59663\r\n",
      "trainer/Q1 Predictions Mean                             -1.69227\r\n",
      "trainer/Q1 Predictions Std                               1.01348\r\n",
      "trainer/Q1 Predictions Max                              -0.0219329\r\n",
      "trainer/Q1 Predictions Min                              -5.05338\r\n",
      "trainer/Q2 Predictions Mean                             -1.66368\r\n",
      "trainer/Q2 Predictions Std                               1.01852\r\n",
      "trainer/Q2 Predictions Max                              -0.00909691\r\n",
      "trainer/Q2 Predictions Min                              -5.00154\r\n",
      "trainer/Q Targets Mean                                  -1.69052\r\n",
      "trainer/Q Targets Std                                    1.02921\r\n",
      "trainer/Q Targets Max                                   -0.0287947\r\n",
      "trainer/Q Targets Min                                   -5.08097\r\n",
      "trainer/Log Pis Mean                                     1.95785\r\n",
      "trainer/Log Pis Std                                      1.32828\r\n",
      "trainer/Log Pis Max                                      4.61581\r\n",
      "trainer/Log Pis Min                                     -3.44465\r\n",
      "trainer/Policy mu Mean                                  -0.0179185\r\n",
      "trainer/Policy mu Std                                    0.210716\r\n",
      "trainer/Policy mu Max                                    1.41571\r\n",
      "trainer/Policy mu Min                                   -1.55029\r\n",
      "trainer/Policy log std Mean                             -2.36776\r\n",
      "trainer/Policy log std Std                               0.439399\r\n",
      "trainer/Policy log std Max                              -0.652599\r\n",
      "trainer/Policy log std Min                              -3.33175\r\n",
      "trainer/Alpha                                            0.0201313\r\n",
      "trainer/Alpha Loss                                      -0.16461\r\n",
      "exploration/num steps total                           3100\r\n",
      "exploration/num paths total                            155\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.134735\r\n",
      "exploration/Rewards Std                                  0.0527813\r\n",
      "exploration/Rewards Max                                 -0.0101755\r\n",
      "exploration/Rewards Min                                 -0.259321\r\n",
      "exploration/Returns Mean                                -2.69469\r\n",
      "exploration/Returns Std                                  0.6064\r\n",
      "exploration/Returns Max                                 -1.88303\r\n",
      "exploration/Returns Min                                 -3.51046\r\n",
      "exploration/Actions Mean                                -0.00356104\r\n",
      "exploration/Actions Std                                  0.103819\r\n",
      "exploration/Actions Max                                  0.332155\r\n",
      "exploration/Actions Min                                 -0.3876\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.69469\r\n",
      "exploration/env_infos/final/reward_dist Mean             7.12171e-06\r\n",
      "exploration/env_infos/final/reward_dist Std              1.38087e-05\r\n",
      "exploration/env_infos/final/reward_dist Max              3.47335e-05\r\n",
      "exploration/env_infos/final/reward_dist Min              1.18478e-42\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0010101\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00125727\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00342967\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.7014e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0190489\r\n",
      "exploration/env_infos/reward_dist Std                    0.0428231\r\n",
      "exploration/env_infos/reward_dist Max                    0.220602\r\n",
      "exploration/env_infos/reward_dist Min                    1.18478e-42\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.176152\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0620387\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0785922\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.231599\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.117049\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0317396\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0673138\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.144941\r\n",
      "exploration/env_infos/reward_energy Mean                -0.123626\r\n",
      "exploration/env_infos/reward_energy Std                  0.0793651\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0132256\r\n",
      "exploration/env_infos/reward_energy Min                 -0.407985\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.000963422\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.371752\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.373629\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.760319\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00238737\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00356166\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00641923\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00344389\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0199854\r\n",
      "exploration/env_infos/end_effector_loc Std               0.201887\r\n",
      "exploration/env_infos/end_effector_loc Max               0.373629\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.760319\r\n",
      "evaluation/num steps total                           21000\r\n",
      "evaluation/num paths total                            1050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0841271\r\n",
      "evaluation/Rewards Std                                   0.0677043\r\n",
      "evaluation/Rewards Max                                   0.147359\r\n",
      "evaluation/Rewards Min                                  -0.582624\r\n",
      "evaluation/Returns Mean                                 -1.68254\r\n",
      "evaluation/Returns Std                                   0.865894\r\n",
      "evaluation/Returns Max                                   0.48494\r\n",
      "evaluation/Returns Min                                  -3.58603\r\n",
      "evaluation/Actions Mean                                 -0.00691527\r\n",
      "evaluation/Actions Std                                   0.0484629\r\n",
      "evaluation/Actions Max                                   0.399415\r\n",
      "evaluation/Actions Min                                  -0.284991\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.68254\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0640186\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.160954\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.939318\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.20436e-44\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0045537\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00767161\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0303989\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.15282e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0884866\r\n",
      "evaluation/env_infos/reward_dist Std                     0.179034\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996489\r\n",
      "evaluation/env_infos/reward_dist Min                     3.20436e-44\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.081909\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0760277\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00972662\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.404483\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.110375\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.0686613\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.023516\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.318258\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0533209\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0441569\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00188164\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.404483\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0451065\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.301534\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.454465\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.848303\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00026901\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00458791\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0158921\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0142496\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0102572\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.164542\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.454465\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.848303\r\n",
      "time/data storing (s)                                    0.00309027\r\n",
      "time/evaluation sampling (s)                             1.18336\r\n",
      "time/exploration sampling (s)                            0.12645\r\n",
      "time/logging (s)                                         0.0244806\r\n",
      "time/saving (s)                                          0.0319922\r\n",
      "time/training (s)                                       47.0561\r\n",
      "time/epoch (s)                                          48.4255\r\n",
      "time/total (s)                                         980.313\r\n",
      "Epoch                                                   20\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:42:07.689044 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 21 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00210275\n",
      "trainer/QF2 Loss                                         0.00304153\n",
      "trainer/Policy Loss                                      3.9314\n",
      "trainer/Q1 Predictions Mean                             -1.69986\n",
      "trainer/Q1 Predictions Std                               1.01105\n",
      "trainer/Q1 Predictions Max                              -0.0352082\n",
      "trainer/Q1 Predictions Min                              -6.33899\n",
      "trainer/Q2 Predictions Mean                             -1.68502\n",
      "trainer/Q2 Predictions Std                               1.01375\n",
      "trainer/Q2 Predictions Max                              -0.0391836\n",
      "trainer/Q2 Predictions Min                              -6.33102\n",
      "trainer/Q Targets Mean                                  -1.69475\n",
      "trainer/Q Targets Std                                    1.01455\n",
      "trainer/Q Targets Max                                   -0.0376383\n",
      "trainer/Q Targets Min                                   -6.50395\n",
      "trainer/Log Pis Mean                                     2.26102\n",
      "trainer/Log Pis Std                                      1.31855\n",
      "trainer/Log Pis Max                                      5.40832\n",
      "trainer/Log Pis Min                                     -3.51674\n",
      "trainer/Policy mu Mean                                  -0.0103715\n",
      "trainer/Policy mu Std                                    0.202554\n",
      "trainer/Policy mu Max                                    1.59305\n",
      "trainer/Policy mu Min                                   -2.39728\n",
      "trainer/Policy log std Mean                             -2.51204\n",
      "trainer/Policy log std Std                               0.392728\n",
      "trainer/Policy log std Max                              -0.461271\n",
      "trainer/Policy log std Min                              -3.3921\n",
      "trainer/Alpha                                            0.0202628\n",
      "trainer/Alpha Loss                                       1.01795\n",
      "exploration/num steps total                           3200\n",
      "exploration/num paths total                            160\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0569692\n",
      "exploration/Rewards Std                                  0.106616\n",
      "exploration/Rewards Max                                  0.153634\n",
      "exploration/Rewards Min                                 -0.339855\n",
      "exploration/Returns Mean                                -1.13938\n",
      "exploration/Returns Std                                  1.49792\n",
      "exploration/Returns Max                                  1.61441\n",
      "exploration/Returns Min                                 -2.55416\n",
      "exploration/Actions Mean                                 0.00352316\n",
      "exploration/Actions Std                                  0.111657\n",
      "exploration/Actions Max                                  0.355426\n",
      "exploration/Actions Min                                 -0.324545\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.13938\n",
      "exploration/env_infos/final/reward_dist Mean             0.0727321\n",
      "exploration/env_infos/final/reward_dist Std              0.127377\n",
      "exploration/env_infos/final/reward_dist Max              0.326748\n",
      "exploration/env_infos/final/reward_dist Min              1.0774e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00814386\n",
      "exploration/env_infos/initial/reward_dist Std            0.0145627\n",
      "exploration/env_infos/initial/reward_dist Max            0.0372108\n",
      "exploration/env_infos/initial/reward_dist Min            2.39896e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.159732\n",
      "exploration/env_infos/reward_dist Std                    0.23699\n",
      "exploration/env_infos/reward_dist Max                    0.894881\n",
      "exploration/env_infos/reward_dist Min                    1.0774e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.124808\n",
      "exploration/env_infos/final/reward_energy Std            0.0751601\n",
      "exploration/env_infos/final/reward_energy Max           -0.0393795\n",
      "exploration/env_infos/final/reward_energy Min           -0.230109\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.222566\n",
      "exploration/env_infos/initial/reward_energy Std          0.104817\n",
      "exploration/env_infos/initial/reward_energy Max         -0.117339\n",
      "exploration/env_infos/initial/reward_energy Min         -0.416374\n",
      "exploration/env_infos/reward_energy Mean                -0.13365\n",
      "exploration/env_infos/reward_energy Std                  0.0842442\n",
      "exploration/env_infos/reward_energy Max                 -0.00431652\n",
      "exploration/env_infos/reward_energy Min                 -0.416374\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00459556\n",
      "exploration/env_infos/final/end_effector_loc Std         0.29192\n",
      "exploration/env_infos/final/end_effector_loc Max         0.278122\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.636882\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00243703\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00834947\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00893417\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0162272\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0148786\n",
      "exploration/env_infos/end_effector_loc Std               0.189972\n",
      "exploration/env_infos/end_effector_loc Max               0.278122\n",
      "exploration/env_infos/end_effector_loc Min              -0.636882\n",
      "evaluation/num steps total                           22000\n",
      "evaluation/num paths total                            1100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0626847\n",
      "evaluation/Rewards Std                                   0.0745355\n",
      "evaluation/Rewards Max                                   0.111089\n",
      "evaluation/Rewards Min                                  -0.545762\n",
      "evaluation/Returns Mean                                 -1.25369\n",
      "evaluation/Returns Std                                   1.12259\n",
      "evaluation/Returns Max                                   1.22594\n",
      "evaluation/Returns Min                                  -3.49241\n",
      "evaluation/Actions Mean                                 -0.0077482\n",
      "evaluation/Actions Std                                   0.0558542\n",
      "evaluation/Actions Max                                   0.365591\n",
      "evaluation/Actions Min                                  -0.74091\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.25369\n",
      "evaluation/env_infos/final/reward_dist Mean              0.167529\n",
      "evaluation/env_infos/final/reward_dist Std               0.224108\n",
      "evaluation/env_infos/final/reward_dist Max               0.994827\n",
      "evaluation/env_infos/final/reward_dist Min               5.36533e-57\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00521255\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00933152\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0409575\n",
      "evaluation/env_infos/initial/reward_dist Min             1.33828e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.137158\n",
      "evaluation/env_infos/reward_dist Std                     0.212616\n",
      "evaluation/env_infos/reward_dist Max                     0.998779\n",
      "evaluation/env_infos/reward_dist Min                     5.36533e-57\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0706683\n",
      "evaluation/env_infos/final/reward_energy Std             0.0542872\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00334885\n",
      "evaluation/env_infos/final/reward_energy Min            -0.229215\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.157231\n",
      "evaluation/env_infos/initial/reward_energy Std           0.15346\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0164047\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.7706\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0547942\n",
      "evaluation/env_infos/reward_energy Std                   0.05794\n",
      "evaluation/env_infos/reward_energy Max                  -0.000380304\n",
      "evaluation/env_infos/reward_energy Min                  -0.7706\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0698358\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.266896\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.5753\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.919514\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0011473\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00768266\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0182796\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0370455\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0240066\n",
      "evaluation/env_infos/end_effector_loc Std                0.156785\n",
      "evaluation/env_infos/end_effector_loc Max                0.5753\n",
      "evaluation/env_infos/end_effector_loc Min               -0.919514\n",
      "time/data storing (s)                                    0.00304472\n",
      "time/evaluation sampling (s)                             1.19425\n",
      "time/exploration sampling (s)                            0.140406\n",
      "time/logging (s)                                         0.0201398\n",
      "time/saving (s)                                          0.0280863\n",
      "time/training (s)                                       49.1754\n",
      "time/epoch (s)                                          50.5614\n",
      "time/total (s)                                        1031.2\n",
      "Epoch                                                   21\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:42:55.654567 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 22 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00208177\n",
      "trainer/QF2 Loss                                         0.00075504\n",
      "trainer/Policy Loss                                      3.67203\n",
      "trainer/Q1 Predictions Mean                             -1.64912\n",
      "trainer/Q1 Predictions Std                               1.02217\n",
      "trainer/Q1 Predictions Max                              -0.0100421\n",
      "trainer/Q1 Predictions Min                              -6.28763\n",
      "trainer/Q2 Predictions Mean                             -1.63301\n",
      "trainer/Q2 Predictions Std                               1.00303\n",
      "trainer/Q2 Predictions Max                              -0.0400837\n",
      "trainer/Q2 Predictions Min                              -6.16132\n",
      "trainer/Q Targets Mean                                  -1.63218\n",
      "trainer/Q Targets Std                                    1.0066\n",
      "trainer/Q Targets Max                                   -0.0595577\n",
      "trainer/Q Targets Min                                   -6.2031\n",
      "trainer/Log Pis Mean                                     2.04664\n",
      "trainer/Log Pis Std                                      1.28886\n",
      "trainer/Log Pis Max                                      5.20349\n",
      "trainer/Log Pis Min                                     -2.11795\n",
      "trainer/Policy mu Mean                                   0.00162103\n",
      "trainer/Policy mu Std                                    0.246505\n",
      "trainer/Policy mu Max                                    2.37764\n",
      "trainer/Policy mu Min                                   -2.10731\n",
      "trainer/Policy log std Mean                             -2.39335\n",
      "trainer/Policy log std Std                               0.441233\n",
      "trainer/Policy log std Max                              -0.248241\n",
      "trainer/Policy log std Min                              -3.21867\n",
      "trainer/Alpha                                            0.0200744\n",
      "trainer/Alpha Loss                                       0.182321\n",
      "exploration/num steps total                           3300\n",
      "exploration/num paths total                            165\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0914819\n",
      "exploration/Rewards Std                                  0.0905991\n",
      "exploration/Rewards Max                                  0.0907386\n",
      "exploration/Rewards Min                                 -0.390972\n",
      "exploration/Returns Mean                                -1.82964\n",
      "exploration/Returns Std                                  1.2724\n",
      "exploration/Returns Max                                  0.109419\n",
      "exploration/Returns Min                                 -3.28081\n",
      "exploration/Actions Mean                                 0.00345199\n",
      "exploration/Actions Std                                  0.0972037\n",
      "exploration/Actions Max                                  0.330593\n",
      "exploration/Actions Min                                 -0.250378\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.82964\n",
      "exploration/env_infos/final/reward_dist Mean             0.0707797\n",
      "exploration/env_infos/final/reward_dist Std              0.137737\n",
      "exploration/env_infos/final/reward_dist Max              0.346211\n",
      "exploration/env_infos/final/reward_dist Min              3.72994e-35\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0150057\n",
      "exploration/env_infos/initial/reward_dist Std            0.0182832\n",
      "exploration/env_infos/initial/reward_dist Max            0.0434135\n",
      "exploration/env_infos/initial/reward_dist Min            1.13222e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.163333\n",
      "exploration/env_infos/reward_dist Std                    0.252853\n",
      "exploration/env_infos/reward_dist Max                    0.980687\n",
      "exploration/env_infos/reward_dist Min                    3.72994e-35\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0887638\n",
      "exploration/env_infos/final/reward_energy Std            0.0347684\n",
      "exploration/env_infos/final/reward_energy Max           -0.0281889\n",
      "exploration/env_infos/final/reward_energy Min           -0.121169\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.20304\n",
      "exploration/env_infos/initial/reward_energy Std          0.0760875\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0987474\n",
      "exploration/env_infos/initial/reward_energy Min         -0.330613\n",
      "exploration/env_infos/reward_energy Mean                -0.119357\n",
      "exploration/env_infos/reward_energy Std                  0.0683724\n",
      "exploration/env_infos/reward_energy Max                 -0.0116462\n",
      "exploration/env_infos/reward_energy Min                 -0.350358\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.166991\n",
      "exploration/env_infos/final/end_effector_loc Std         0.315079\n",
      "exploration/env_infos/final/end_effector_loc Max         0.803847\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.306483\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00346441\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00683858\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0165297\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00982475\n",
      "exploration/env_infos/end_effector_loc Mean              0.0858867\n",
      "exploration/env_infos/end_effector_loc Std               0.173915\n",
      "exploration/env_infos/end_effector_loc Max               0.803847\n",
      "exploration/env_infos/end_effector_loc Min              -0.306483\n",
      "evaluation/num steps total                           23000\n",
      "evaluation/num paths total                            1150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0823196\n",
      "evaluation/Rewards Std                                   0.0681208\n",
      "evaluation/Rewards Max                                   0.127273\n",
      "evaluation/Rewards Min                                  -0.391238\n",
      "evaluation/Returns Mean                                 -1.64639\n",
      "evaluation/Returns Std                                   1.14259\n",
      "evaluation/Returns Max                                   1.50547\n",
      "evaluation/Returns Min                                  -4.2164\n",
      "evaluation/Actions Mean                                 -0.00348344\n",
      "evaluation/Actions Std                                   0.0552982\n",
      "evaluation/Actions Max                                   0.394784\n",
      "evaluation/Actions Min                                  -0.822481\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.64639\n",
      "evaluation/env_infos/final/reward_dist Mean              0.18094\n",
      "evaluation/env_infos/final/reward_dist Std               0.31012\n",
      "evaluation/env_infos/final/reward_dist Max               0.979428\n",
      "evaluation/env_infos/final/reward_dist Min               2.35261e-22\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00445483\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00894614\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0436059\n",
      "evaluation/env_infos/initial/reward_dist Min             1.39161e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.122351\n",
      "evaluation/env_infos/reward_dist Std                     0.221854\n",
      "evaluation/env_infos/reward_dist Max                     0.999277\n",
      "evaluation/env_infos/reward_dist Min                     2.35261e-22\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0586889\n",
      "evaluation/env_infos/final/reward_energy Std             0.0636247\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0119675\n",
      "evaluation/env_infos/final/reward_energy Min            -0.345051\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.154708\n",
      "evaluation/env_infos/initial/reward_energy Std           0.147983\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0217239\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.830936\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0523917\n",
      "evaluation/env_infos/reward_energy Std                   0.058268\n",
      "evaluation/env_infos/reward_energy Max                  -0.000632169\n",
      "evaluation/env_infos/reward_energy Min                  -0.830936\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00954474\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.252928\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.749336\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.585671\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00049892\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00755267\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0197392\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0411241\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0129256\n",
      "evaluation/env_infos/end_effector_loc Std                0.149668\n",
      "evaluation/env_infos/end_effector_loc Max                0.749336\n",
      "evaluation/env_infos/end_effector_loc Min               -0.585671\n",
      "time/data storing (s)                                    0.00303623\n",
      "time/evaluation sampling (s)                             1.03329\n",
      "time/exploration sampling (s)                            0.125712\n",
      "time/logging (s)                                         0.0198914\n",
      "time/saving (s)                                          0.0274667\n",
      "time/training (s)                                       46.4538\n",
      "time/epoch (s)                                          47.6632\n",
      "time/total (s)                                        1079.16\n",
      "Epoch                                                   22\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:43:42.909944 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 23 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000845264\r\n",
      "trainer/QF2 Loss                                         0.000738536\r\n",
      "trainer/Policy Loss                                      3.24365\r\n",
      "trainer/Q1 Predictions Mean                             -1.47428\r\n",
      "trainer/Q1 Predictions Std                               0.926854\r\n",
      "trainer/Q1 Predictions Max                               0.0180639\r\n",
      "trainer/Q1 Predictions Min                              -3.9728\r\n",
      "trainer/Q2 Predictions Mean                             -1.46816\r\n",
      "trainer/Q2 Predictions Std                               0.923123\r\n",
      "trainer/Q2 Predictions Max                               0.0499661\r\n",
      "trainer/Q2 Predictions Min                              -3.9255\r\n",
      "trainer/Q Targets Mean                                  -1.46658\r\n",
      "trainer/Q Targets Std                                    0.92405\r\n",
      "trainer/Q Targets Max                                    0.0203456\r\n",
      "trainer/Q Targets Min                                   -3.91251\r\n",
      "trainer/Log Pis Mean                                     1.7836\r\n",
      "trainer/Log Pis Std                                      1.25738\r\n",
      "trainer/Log Pis Max                                      4.06054\r\n",
      "trainer/Log Pis Min                                     -6.73606\r\n",
      "trainer/Policy mu Mean                                   0.00486339\r\n",
      "trainer/Policy mu Std                                    0.214854\r\n",
      "trainer/Policy mu Max                                    2.09769\r\n",
      "trainer/Policy mu Min                                   -0.540247\r\n",
      "trainer/Policy log std Mean                             -2.29203\r\n",
      "trainer/Policy log std Std                               0.464893\r\n",
      "trainer/Policy log std Max                              -0.62013\r\n",
      "trainer/Policy log std Min                              -3.0852\r\n",
      "trainer/Alpha                                            0.021282\r\n",
      "trainer/Alpha Loss                                      -0.832993\r\n",
      "exploration/num steps total                           3400\r\n",
      "exploration/num paths total                            170\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.121694\r\n",
      "exploration/Rewards Std                                  0.0714952\r\n",
      "exploration/Rewards Max                                 -0.0272096\r\n",
      "exploration/Rewards Min                                 -0.376006\r\n",
      "exploration/Returns Mean                                -2.43387\r\n",
      "exploration/Returns Std                                  0.759646\r\n",
      "exploration/Returns Max                                 -1.61762\r\n",
      "exploration/Returns Min                                 -3.60402\r\n",
      "exploration/Actions Mean                                -0.00160453\r\n",
      "exploration/Actions Std                                  0.148671\r\n",
      "exploration/Actions Max                                  0.573589\r\n",
      "exploration/Actions Min                                 -0.392601\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.43387\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0357856\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0382002\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0975672\r\n",
      "exploration/env_infos/final/reward_dist Min              1.33244e-36\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00778208\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0106843\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.027118\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.49011e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.136185\r\n",
      "exploration/env_infos/reward_dist Std                    0.209742\r\n",
      "exploration/env_infos/reward_dist Max                    0.944358\r\n",
      "exploration/env_infos/reward_dist Min                    1.33244e-36\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.133502\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0319811\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.079353\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.172768\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.332912\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.159562\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0907183\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.579413\r\n",
      "exploration/env_infos/reward_energy Mean                -0.184057\r\n",
      "exploration/env_infos/reward_energy Std                  0.101658\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0215264\r\n",
      "exploration/env_infos/reward_energy Min                 -0.579413\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0112904\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.327038\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.477011\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.755059\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00506901\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0120278\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0286795\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0138935\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0114126\r\n",
      "exploration/env_infos/end_effector_loc Std               0.196484\r\n",
      "exploration/env_infos/end_effector_loc Max               0.477011\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.755059\r\n",
      "evaluation/num steps total                           24000\r\n",
      "evaluation/num paths total                            1200\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0668052\r\n",
      "evaluation/Rewards Std                                   0.0905556\r\n",
      "evaluation/Rewards Max                                   0.117957\r\n",
      "evaluation/Rewards Min                                  -0.587621\r\n",
      "evaluation/Returns Mean                                 -1.3361\r\n",
      "evaluation/Returns Std                                   1.32925\r\n",
      "evaluation/Returns Max                                   0.637623\r\n",
      "evaluation/Returns Min                                  -5.46529\r\n",
      "evaluation/Actions Mean                                 -0.0045642\r\n",
      "evaluation/Actions Std                                   0.070436\r\n",
      "evaluation/Actions Max                                   0.427441\r\n",
      "evaluation/Actions Min                                  -0.789231\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.3361\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.103558\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.205055\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.734825\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.50247e-44\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00467618\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00749599\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0327052\r\n",
      "evaluation/env_infos/initial/reward_dist Min             6.04762e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.106973\r\n",
      "evaluation/env_infos/reward_dist Std                     0.18091\r\n",
      "evaluation/env_infos/reward_dist Max                     0.988565\r\n",
      "evaluation/env_infos/reward_dist Min                     4.50247e-44\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0779269\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0656574\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00832536\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.418494\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.189587\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.168144\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0310469\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.799563\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0689684\r\n",
      "evaluation/env_infos/reward_energy Std                   0.072163\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00133464\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.799563\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0959188\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.299924\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.757402\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.683907\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00286836\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00848777\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0163386\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0394615\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.05306\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.18292\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.757402\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.683907\r\n",
      "time/data storing (s)                                    0.00310695\r\n",
      "time/evaluation sampling (s)                             0.977444\r\n",
      "time/exploration sampling (s)                            0.115789\r\n",
      "time/logging (s)                                         0.0196122\r\n",
      "time/saving (s)                                          0.0297334\r\n",
      "time/training (s)                                       45.7869\r\n",
      "time/epoch (s)                                          46.9326\r\n",
      "time/total (s)                                        1126.42\r\n",
      "Epoch                                                   23\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:44:30.773688 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 24 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00103472\r\n",
      "trainer/QF2 Loss                                         0.00262344\r\n",
      "trainer/Policy Loss                                      3.38863\r\n",
      "trainer/Q1 Predictions Mean                             -1.50708\r\n",
      "trainer/Q1 Predictions Std                               0.903178\r\n",
      "trainer/Q1 Predictions Max                               0.028404\r\n",
      "trainer/Q1 Predictions Min                              -3.87102\r\n",
      "trainer/Q2 Predictions Mean                             -1.49204\r\n",
      "trainer/Q2 Predictions Std                               0.897398\r\n",
      "trainer/Q2 Predictions Max                               0.0144268\r\n",
      "trainer/Q2 Predictions Min                              -3.91756\r\n",
      "trainer/Q Targets Mean                                  -1.49779\r\n",
      "trainer/Q Targets Std                                    0.90045\r\n",
      "trainer/Q Targets Max                                    0.0203456\r\n",
      "trainer/Q Targets Min                                   -3.89184\r\n",
      "trainer/Log Pis Mean                                     1.89091\r\n",
      "trainer/Log Pis Std                                      1.35864\r\n",
      "trainer/Log Pis Max                                      4.39738\r\n",
      "trainer/Log Pis Min                                     -3.28733\r\n",
      "trainer/Policy mu Mean                                  -0.0201926\r\n",
      "trainer/Policy mu Std                                    0.303422\r\n",
      "trainer/Policy mu Max                                    2.2092\r\n",
      "trainer/Policy mu Min                                   -2.16765\r\n",
      "trainer/Policy log std Mean                             -2.2858\r\n",
      "trainer/Policy log std Std                               0.519817\r\n",
      "trainer/Policy log std Max                              -0.418502\r\n",
      "trainer/Policy log std Min                              -3.35979\r\n",
      "trainer/Alpha                                            0.0203439\r\n",
      "trainer/Alpha Loss                                      -0.424652\r\n",
      "exploration/num steps total                           3500\r\n",
      "exploration/num paths total                            175\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.119444\r\n",
      "exploration/Rewards Std                                  0.0577276\r\n",
      "exploration/Rewards Max                                 -0.0380516\r\n",
      "exploration/Rewards Min                                 -0.390374\r\n",
      "exploration/Returns Mean                                -2.38888\r\n",
      "exploration/Returns Std                                  0.314438\r\n",
      "exploration/Returns Max                                 -1.99009\r\n",
      "exploration/Returns Min                                 -2.7715\r\n",
      "exploration/Actions Mean                                -0.00777254\r\n",
      "exploration/Actions Std                                  0.080656\r\n",
      "exploration/Actions Max                                  0.257109\r\n",
      "exploration/Actions Min                                 -0.31836\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.38888\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.000831564\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00160144\r\n",
      "exploration/env_infos/final/reward_dist Max              0.00403375\r\n",
      "exploration/env_infos/final/reward_dist Min              2.83523e-14\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00235145\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00446793\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0112845\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.70187e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0256169\r\n",
      "exploration/env_infos/reward_dist Std                    0.0713609\r\n",
      "exploration/env_infos/reward_dist Max                    0.46613\r\n",
      "exploration/env_infos/reward_dist Min                    2.83523e-14\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.162422\r\n",
      "exploration/env_infos/final/reward_energy Std            0.100143\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0511202\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.345145\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.151178\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0734582\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0657675\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.257935\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0980831\r\n",
      "exploration/env_infos/reward_energy Std                  0.0592564\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0136538\r\n",
      "exploration/env_infos/reward_energy Min                 -0.345145\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0624545\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.254441\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.316923\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.556561\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00187483\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00563901\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0128554\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00828111\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0306966\r\n",
      "exploration/env_infos/end_effector_loc Std               0.14491\r\n",
      "exploration/env_infos/end_effector_loc Max               0.316923\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.556561\r\n",
      "evaluation/num steps total                           25000\r\n",
      "evaluation/num paths total                            1250\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0664353\r\n",
      "evaluation/Rewards Std                                   0.0728691\r\n",
      "evaluation/Rewards Max                                   0.12772\r\n",
      "evaluation/Rewards Min                                  -0.597638\r\n",
      "evaluation/Returns Mean                                 -1.32871\r\n",
      "evaluation/Returns Std                                   1.04792\r\n",
      "evaluation/Returns Max                                   0.472059\r\n",
      "evaluation/Returns Min                                  -3.803\r\n",
      "evaluation/Actions Mean                                 -0.00219815\r\n",
      "evaluation/Actions Std                                   0.0759196\r\n",
      "evaluation/Actions Max                                   0.933951\r\n",
      "evaluation/Actions Min                                  -0.919217\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.32871\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0767016\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.178398\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.83875\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.18975e-37\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00639741\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0192363\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.132789\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0043e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.103883\r\n",
      "evaluation/env_infos/reward_dist Std                     0.197216\r\n",
      "evaluation/env_infos/reward_dist Max                     0.973228\r\n",
      "evaluation/env_infos/reward_dist Min                     1.18975e-37\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.068687\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0368885\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0120729\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.148536\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.215859\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.263531\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.028544\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.31043\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0665136\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0843396\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000437994\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.31043\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0437687\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.305752\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.757639\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.739353\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00162892\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0119332\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0466975\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0459608\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0248649\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.189145\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.757639\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.739353\r\n",
      "time/data storing (s)                                    0.00295545\r\n",
      "time/evaluation sampling (s)                             1.05557\r\n",
      "time/exploration sampling (s)                            0.121922\r\n",
      "time/logging (s)                                         0.0196189\r\n",
      "time/saving (s)                                          0.0281453\r\n",
      "time/training (s)                                       46.2969\r\n",
      "time/epoch (s)                                          47.5251\r\n",
      "time/total (s)                                        1174.28\r\n",
      "Epoch                                                   24\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:45:18.462529 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 25 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00192642\n",
      "trainer/QF2 Loss                                         0.0037633\n",
      "trainer/Policy Loss                                      3.61657\n",
      "trainer/Q1 Predictions Mean                             -1.44162\n",
      "trainer/Q1 Predictions Std                               0.874737\n",
      "trainer/Q1 Predictions Max                              -0.000672296\n",
      "trainer/Q1 Predictions Min                              -4.15411\n",
      "trainer/Q2 Predictions Mean                             -1.4362\n",
      "trainer/Q2 Predictions Std                               0.886233\n",
      "trainer/Q2 Predictions Max                               0.0374026\n",
      "trainer/Q2 Predictions Min                              -4.28273\n",
      "trainer/Q Targets Mean                                  -1.43645\n",
      "trainer/Q Targets Std                                    0.880393\n",
      "trainer/Q Targets Max                                    0.0350086\n",
      "trainer/Q Targets Min                                   -4.17356\n",
      "trainer/Log Pis Mean                                     2.19224\n",
      "trainer/Log Pis Std                                      1.34198\n",
      "trainer/Log Pis Max                                      4.39552\n",
      "trainer/Log Pis Min                                     -4.02113\n",
      "trainer/Policy mu Mean                                  -0.00145799\n",
      "trainer/Policy mu Std                                    0.292317\n",
      "trainer/Policy mu Max                                    2.3997\n",
      "trainer/Policy mu Min                                   -2.14093\n",
      "trainer/Policy log std Mean                             -2.41379\n",
      "trainer/Policy log std Std                               0.548914\n",
      "trainer/Policy log std Max                              -0.257925\n",
      "trainer/Policy log std Min                              -3.27876\n",
      "trainer/Alpha                                            0.020647\n",
      "trainer/Alpha Loss                                       0.745945\n",
      "exploration/num steps total                           3600\n",
      "exploration/num paths total                            180\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.188863\n",
      "exploration/Rewards Std                                  0.122258\n",
      "exploration/Rewards Max                                 -0.0628688\n",
      "exploration/Rewards Min                                 -0.742205\n",
      "exploration/Returns Mean                                -3.77726\n",
      "exploration/Returns Std                                  1.49607\n",
      "exploration/Returns Max                                 -2.45397\n",
      "exploration/Returns Min                                 -6.47996\n",
      "exploration/Actions Mean                                -0.0105934\n",
      "exploration/Actions Std                                  0.166051\n",
      "exploration/Actions Max                                  0.831515\n",
      "exploration/Actions Min                                 -0.707685\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.77726\n",
      "exploration/env_infos/final/reward_dist Mean             0.0199669\n",
      "exploration/env_infos/final/reward_dist Std              0.0399338\n",
      "exploration/env_infos/final/reward_dist Max              0.0998346\n",
      "exploration/env_infos/final/reward_dist Min              1.02853e-83\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00363422\n",
      "exploration/env_infos/initial/reward_dist Std            0.00333577\n",
      "exploration/env_infos/initial/reward_dist Max            0.00870617\n",
      "exploration/env_infos/initial/reward_dist Min            2.07171e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.029488\n",
      "exploration/env_infos/reward_dist Std                    0.0977123\n",
      "exploration/env_infos/reward_dist Max                    0.748551\n",
      "exploration/env_infos/reward_dist Min                    1.02853e-83\n",
      "exploration/env_infos/final/reward_energy Mean          -0.172607\n",
      "exploration/env_infos/final/reward_energy Std            0.0955641\n",
      "exploration/env_infos/final/reward_energy Max           -0.0616236\n",
      "exploration/env_infos/final/reward_energy Min           -0.321439\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.393064\n",
      "exploration/env_infos/initial/reward_energy Std          0.382627\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0980722\n",
      "exploration/env_infos/initial/reward_energy Min         -1.0919\n",
      "exploration/env_infos/reward_energy Mean                -0.175356\n",
      "exploration/env_infos/reward_energy Std                  0.156909\n",
      "exploration/env_infos/reward_energy Max                 -0.0251731\n",
      "exploration/env_infos/reward_energy Min                 -1.0919\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0834723\n",
      "exploration/env_infos/final/end_effector_loc Std         0.526149\n",
      "exploration/env_infos/final/end_effector_loc Max         0.634092\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.919464\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000959299\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0193703\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0415758\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0353842\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0260919\n",
      "exploration/env_infos/end_effector_loc Std               0.301315\n",
      "exploration/env_infos/end_effector_loc Max               0.634092\n",
      "exploration/env_infos/end_effector_loc Min              -0.919464\n",
      "evaluation/num steps total                           26000\n",
      "evaluation/num paths total                            1300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0819103\n",
      "evaluation/Rewards Std                                   0.115202\n",
      "evaluation/Rewards Max                                   0.141297\n",
      "evaluation/Rewards Min                                  -0.749245\n",
      "evaluation/Returns Mean                                 -1.63821\n",
      "evaluation/Returns Std                                   1.90608\n",
      "evaluation/Returns Max                                   1.0731\n",
      "evaluation/Returns Min                                  -9.08768\n",
      "evaluation/Actions Mean                                  0.00163399\n",
      "evaluation/Actions Std                                   0.0945268\n",
      "evaluation/Actions Max                                   0.697414\n",
      "evaluation/Actions Min                                  -0.914458\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.63821\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0589946\n",
      "evaluation/env_infos/final/reward_dist Std               0.152116\n",
      "evaluation/env_infos/final/reward_dist Max               0.89552\n",
      "evaluation/env_infos/final/reward_dist Min               8.33754e-73\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0054505\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00889244\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0362764\n",
      "evaluation/env_infos/initial/reward_dist Min             1.58227e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.110301\n",
      "evaluation/env_infos/reward_dist Std                     0.210319\n",
      "evaluation/env_infos/reward_dist Max                     0.997319\n",
      "evaluation/env_infos/reward_dist Min                     8.33754e-73\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0667749\n",
      "evaluation/env_infos/final/reward_energy Std             0.0548726\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00602081\n",
      "evaluation/env_infos/final/reward_energy Min            -0.2698\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.24335\n",
      "evaluation/env_infos/initial/reward_energy Std           0.257038\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0147075\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.971556\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0795428\n",
      "evaluation/env_infos/reward_energy Std                   0.107466\n",
      "evaluation/env_infos/reward_energy Max                  -0.000613486\n",
      "evaluation/env_infos/reward_energy Min                  -1.10715\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0362375\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.301432\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.839747\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.844749\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00369727\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0119557\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0302735\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0457229\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0377041\n",
      "evaluation/env_infos/end_effector_loc Std                0.198189\n",
      "evaluation/env_infos/end_effector_loc Max                0.839747\n",
      "evaluation/env_infos/end_effector_loc Min               -0.844749\n",
      "time/data storing (s)                                    0.00344935\n",
      "time/evaluation sampling (s)                             0.939588\n",
      "time/exploration sampling (s)                            0.12251\n",
      "time/logging (s)                                         0.0211877\n",
      "time/saving (s)                                          0.0284435\n",
      "time/training (s)                                       46.1892\n",
      "time/epoch (s)                                          47.3044\n",
      "time/total (s)                                        1221.97\n",
      "Epoch                                                   25\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:46:06.819076 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 26 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00228066\r\n",
      "trainer/QF2 Loss                                         0.00218909\r\n",
      "trainer/Policy Loss                                      3.46209\r\n",
      "trainer/Q1 Predictions Mean                             -1.43709\r\n",
      "trainer/Q1 Predictions Std                               0.846088\r\n",
      "trainer/Q1 Predictions Max                               0.115269\r\n",
      "trainer/Q1 Predictions Min                              -3.89597\r\n",
      "trainer/Q2 Predictions Mean                             -1.40883\r\n",
      "trainer/Q2 Predictions Std                               0.837883\r\n",
      "trainer/Q2 Predictions Max                               0.0753636\r\n",
      "trainer/Q2 Predictions Min                              -3.87255\r\n",
      "trainer/Q Targets Mean                                  -1.42218\r\n",
      "trainer/Q Targets Std                                    0.848074\r\n",
      "trainer/Q Targets Max                                    0.126095\r\n",
      "trainer/Q Targets Min                                   -3.9578\r\n",
      "trainer/Log Pis Mean                                     2.0403\r\n",
      "trainer/Log Pis Std                                      1.21157\r\n",
      "trainer/Log Pis Max                                      4.0894\r\n",
      "trainer/Log Pis Min                                     -3.38142\r\n",
      "trainer/Policy mu Mean                                   0.00570102\r\n",
      "trainer/Policy mu Std                                    0.235485\r\n",
      "trainer/Policy mu Max                                    1.83575\r\n",
      "trainer/Policy mu Min                                   -1.07412\r\n",
      "trainer/Policy log std Mean                             -2.37641\r\n",
      "trainer/Policy log std Std                               0.473602\r\n",
      "trainer/Policy log std Max                              -0.612779\r\n",
      "trainer/Policy log std Min                              -3.316\r\n",
      "trainer/Alpha                                            0.0202056\r\n",
      "trainer/Alpha Loss                                       0.157148\r\n",
      "exploration/num steps total                           3700\r\n",
      "exploration/num paths total                            185\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0719371\r\n",
      "exploration/Rewards Std                                  0.0726162\r\n",
      "exploration/Rewards Max                                  0.0450167\r\n",
      "exploration/Rewards Min                                 -0.318041\r\n",
      "exploration/Returns Mean                                -1.43874\r\n",
      "exploration/Returns Std                                  0.652951\r\n",
      "exploration/Returns Max                                 -0.519682\r\n",
      "exploration/Returns Min                                 -2.48418\r\n",
      "exploration/Actions Mean                                -0.00549966\r\n",
      "exploration/Actions Std                                  0.135288\r\n",
      "exploration/Actions Max                                  0.439915\r\n",
      "exploration/Actions Min                                 -0.443709\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.43874\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00355946\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00412964\r\n",
      "exploration/env_infos/final/reward_dist Max              0.00906741\r\n",
      "exploration/env_infos/final/reward_dist Min              1.01275e-10\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0227187\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0265253\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0724936\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.30809e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.132084\r\n",
      "exploration/env_infos/reward_dist Std                    0.239816\r\n",
      "exploration/env_infos/reward_dist Max                    0.952683\r\n",
      "exploration/env_infos/reward_dist Min                    1.01275e-10\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0974783\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0707078\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.016771\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.19294\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.29403\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.148949\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.095127\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.473352\r\n",
      "exploration/env_infos/reward_energy Mean                -0.153649\r\n",
      "exploration/env_infos/reward_energy Std                  0.114272\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00377446\r\n",
      "exploration/env_infos/reward_energy Min                 -0.473352\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.147408\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.261906\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.339589\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.526127\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00759073\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00884194\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00780837\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0221855\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0985716\r\n",
      "exploration/env_infos/end_effector_loc Std               0.186107\r\n",
      "exploration/env_infos/end_effector_loc Max               0.339589\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.526127\r\n",
      "evaluation/num steps total                           27000\r\n",
      "evaluation/num paths total                            1350\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0824593\r\n",
      "evaluation/Rewards Std                                   0.0807482\r\n",
      "evaluation/Rewards Max                                   0.109391\r\n",
      "evaluation/Rewards Min                                  -0.496691\r\n",
      "evaluation/Returns Mean                                 -1.64919\r\n",
      "evaluation/Returns Std                                   1.25101\r\n",
      "evaluation/Returns Max                                   0.80004\r\n",
      "evaluation/Returns Min                                  -6.2604\r\n",
      "evaluation/Actions Mean                                  0.00734949\r\n",
      "evaluation/Actions Std                                   0.0895855\r\n",
      "evaluation/Actions Max                                   0.868865\r\n",
      "evaluation/Actions Min                                  -0.552673\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.64919\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.050003\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.113415\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.452838\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.13232e-41\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00736621\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0180794\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0981235\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.07941e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.110812\r\n",
      "evaluation/env_infos/reward_dist Std                     0.21178\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993859\r\n",
      "evaluation/env_infos/reward_dist Min                     5.13232e-41\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.060528\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0485656\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00707899\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.210945\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.251763\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.221763\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0139842\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.877523\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0870708\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0926166\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00169746\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.877523\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0302717\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.294874\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.582197\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.831018\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00337334\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0113721\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0434432\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0276336\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0446409\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.196958\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.582197\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.831018\r\n",
      "time/data storing (s)                                    0.00289835\r\n",
      "time/evaluation sampling (s)                             1.13078\r\n",
      "time/exploration sampling (s)                            0.124635\r\n",
      "time/logging (s)                                         0.0192274\r\n",
      "time/saving (s)                                          0.0276353\r\n",
      "time/training (s)                                       46.6736\r\n",
      "time/epoch (s)                                          47.9788\r\n",
      "time/total (s)                                        1270.32\r\n",
      "Epoch                                                   26\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:46:54.671616 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 27 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00117056\n",
      "trainer/QF2 Loss                                         0.000933309\n",
      "trainer/Policy Loss                                      3.38432\n",
      "trainer/Q1 Predictions Mean                             -1.43432\n",
      "trainer/Q1 Predictions Std                               0.837177\n",
      "trainer/Q1 Predictions Max                               0.272819\n",
      "trainer/Q1 Predictions Min                              -3.93213\n",
      "trainer/Q2 Predictions Mean                             -1.43306\n",
      "trainer/Q2 Predictions Std                               0.83038\n",
      "trainer/Q2 Predictions Max                               0.233711\n",
      "trainer/Q2 Predictions Min                              -3.85239\n",
      "trainer/Q Targets Mean                                  -1.43132\n",
      "trainer/Q Targets Std                                    0.831348\n",
      "trainer/Q Targets Max                                    0.243764\n",
      "trainer/Q Targets Min                                   -3.87282\n",
      "trainer/Log Pis Mean                                     1.96409\n",
      "trainer/Log Pis Std                                      1.2736\n",
      "trainer/Log Pis Max                                      4.53841\n",
      "trainer/Log Pis Min                                     -2.97317\n",
      "trainer/Policy mu Mean                                   0.0194747\n",
      "trainer/Policy mu Std                                    0.376847\n",
      "trainer/Policy mu Max                                    2.04941\n",
      "trainer/Policy mu Min                                   -2.04462\n",
      "trainer/Policy log std Mean                             -2.2632\n",
      "trainer/Policy log std Std                               0.586772\n",
      "trainer/Policy log std Max                              -0.363287\n",
      "trainer/Policy log std Min                              -3.12931\n",
      "trainer/Alpha                                            0.0206545\n",
      "trainer/Alpha Loss                                      -0.139351\n",
      "exploration/num steps total                           3800\n",
      "exploration/num paths total                            190\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0936505\n",
      "exploration/Rewards Std                                  0.0833835\n",
      "exploration/Rewards Max                                  0.119909\n",
      "exploration/Rewards Min                                 -0.242174\n",
      "exploration/Returns Mean                                -1.87301\n",
      "exploration/Returns Std                                  1.18356\n",
      "exploration/Returns Max                                  0.381063\n",
      "exploration/Returns Min                                 -3.0046\n",
      "exploration/Actions Mean                                -9.28162e-05\n",
      "exploration/Actions Std                                  0.147226\n",
      "exploration/Actions Max                                  0.731528\n",
      "exploration/Actions Min                                 -0.424503\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.87301\n",
      "exploration/env_infos/final/reward_dist Mean             0.14122\n",
      "exploration/env_infos/final/reward_dist Std              0.197063\n",
      "exploration/env_infos/final/reward_dist Max              0.502831\n",
      "exploration/env_infos/final/reward_dist Min              2.50857e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00665475\n",
      "exploration/env_infos/initial/reward_dist Std            0.0129119\n",
      "exploration/env_infos/initial/reward_dist Max            0.0324748\n",
      "exploration/env_infos/initial/reward_dist Min            6.036e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.170704\n",
      "exploration/env_infos/reward_dist Std                    0.280902\n",
      "exploration/env_infos/reward_dist Max                    0.894967\n",
      "exploration/env_infos/reward_dist Min                    2.50857e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.117\n",
      "exploration/env_infos/final/reward_energy Std            0.02686\n",
      "exploration/env_infos/final/reward_energy Max           -0.0773788\n",
      "exploration/env_infos/final/reward_energy Min           -0.14586\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.316058\n",
      "exploration/env_infos/initial/reward_energy Std          0.316338\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0763855\n",
      "exploration/env_infos/initial/reward_energy Min         -0.93236\n",
      "exploration/env_infos/reward_energy Mean                -0.155975\n",
      "exploration/env_infos/reward_energy Std                  0.137924\n",
      "exploration/env_infos/reward_energy Max                 -0.0136486\n",
      "exploration/env_infos/reward_energy Min                 -0.93236\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0689644\n",
      "exploration/env_infos/final/end_effector_loc Std         0.284833\n",
      "exploration/env_infos/final/end_effector_loc Max         0.753108\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.2885\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00292423\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0155371\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0365764\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0142319\n",
      "exploration/env_infos/end_effector_loc Mean              0.0355849\n",
      "exploration/env_infos/end_effector_loc Std               0.193668\n",
      "exploration/env_infos/end_effector_loc Max               0.753108\n",
      "exploration/env_infos/end_effector_loc Min              -0.2885\n",
      "evaluation/num steps total                           28000\n",
      "evaluation/num paths total                            1400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0778595\n",
      "evaluation/Rewards Std                                   0.121249\n",
      "evaluation/Rewards Max                                   0.175705\n",
      "evaluation/Rewards Min                                  -0.856595\n",
      "evaluation/Returns Mean                                 -1.55719\n",
      "evaluation/Returns Std                                   2.1308\n",
      "evaluation/Returns Max                                   1.38935\n",
      "evaluation/Returns Min                                 -13.1545\n",
      "evaluation/Actions Mean                                  0.00542355\n",
      "evaluation/Actions Std                                   0.0728291\n",
      "evaluation/Actions Max                                   0.543115\n",
      "evaluation/Actions Min                                  -0.776873\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.55719\n",
      "evaluation/env_infos/final/reward_dist Mean              0.121078\n",
      "evaluation/env_infos/final/reward_dist Std               0.227312\n",
      "evaluation/env_infos/final/reward_dist Max               0.84511\n",
      "evaluation/env_infos/final/reward_dist Min               3.69766e-110\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00300244\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00723685\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0472442\n",
      "evaluation/env_infos/initial/reward_dist Min             1.49442e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.144645\n",
      "evaluation/env_infos/reward_dist Std                     0.250774\n",
      "evaluation/env_infos/reward_dist Max                     0.996554\n",
      "evaluation/env_infos/reward_dist Min                     3.69766e-110\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0578203\n",
      "evaluation/env_infos/final/reward_energy Std             0.0604497\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00587563\n",
      "evaluation/env_infos/final/reward_energy Min            -0.296658\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.178263\n",
      "evaluation/env_infos/initial/reward_energy Std           0.199052\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.011272\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.945953\n",
      "evaluation/env_infos/reward_energy Mean                 -0.061335\n",
      "evaluation/env_infos/reward_energy Std                   0.0830965\n",
      "evaluation/env_infos/reward_energy Max                  -0.000613207\n",
      "evaluation/env_infos/reward_energy Min                  -0.945953\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.046963\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.288993\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.694169\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000632911\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00942597\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0206334\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0388437\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00851982\n",
      "evaluation/env_infos/end_effector_loc Std                0.187776\n",
      "evaluation/env_infos/end_effector_loc Max                0.694169\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00293677\n",
      "time/evaluation sampling (s)                             0.954434\n",
      "time/exploration sampling (s)                            0.124643\n",
      "time/logging (s)                                         0.0186633\n",
      "time/saving (s)                                          0.0273865\n",
      "time/training (s)                                       46.3614\n",
      "time/epoch (s)                                          47.4894\n",
      "time/total (s)                                        1318.17\n",
      "Epoch                                                   27\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:47:42.051056 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 28 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000836844\r\n",
      "trainer/QF2 Loss                                         0.00103203\r\n",
      "trainer/Policy Loss                                      3.42811\r\n",
      "trainer/Q1 Predictions Mean                             -1.37537\r\n",
      "trainer/Q1 Predictions Std                               0.864069\r\n",
      "trainer/Q1 Predictions Max                               0.472538\r\n",
      "trainer/Q1 Predictions Min                              -3.8801\r\n",
      "trainer/Q2 Predictions Mean                             -1.39298\r\n",
      "trainer/Q2 Predictions Std                               0.871996\r\n",
      "trainer/Q2 Predictions Max                               0.40889\r\n",
      "trainer/Q2 Predictions Min                              -4.02032\r\n",
      "trainer/Q Targets Mean                                  -1.37427\r\n",
      "trainer/Q Targets Std                                    0.868458\r\n",
      "trainer/Q Targets Max                                    0.4348\r\n",
      "trainer/Q Targets Min                                   -4.00653\r\n",
      "trainer/Log Pis Mean                                     2.05911\r\n",
      "trainer/Log Pis Std                                      1.15668\r\n",
      "trainer/Log Pis Max                                      6.28543\r\n",
      "trainer/Log Pis Min                                     -3.77694\r\n",
      "trainer/Policy mu Mean                                   0.0183732\r\n",
      "trainer/Policy mu Std                                    0.367676\r\n",
      "trainer/Policy mu Max                                    2.24762\r\n",
      "trainer/Policy mu Min                                   -2.01615\r\n",
      "trainer/Policy log std Mean                             -2.3182\r\n",
      "trainer/Policy log std Std                               0.514683\r\n",
      "trainer/Policy log std Max                              -0.280837\r\n",
      "trainer/Policy log std Min                              -3.23126\r\n",
      "trainer/Alpha                                            0.0202401\r\n",
      "trainer/Alpha Loss                                       0.230551\r\n",
      "exploration/num steps total                           3900\r\n",
      "exploration/num paths total                            195\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.055702\r\n",
      "exploration/Rewards Std                                  0.0625389\r\n",
      "exploration/Rewards Max                                  0.0752089\r\n",
      "exploration/Rewards Min                                 -0.228344\r\n",
      "exploration/Returns Mean                                -1.11404\r\n",
      "exploration/Returns Std                                  0.917538\r\n",
      "exploration/Returns Max                                  0.421251\r\n",
      "exploration/Returns Min                                 -2.14373\r\n",
      "exploration/Actions Mean                                 0.00608474\r\n",
      "exploration/Actions Std                                  0.100982\r\n",
      "exploration/Actions Max                                  0.430569\r\n",
      "exploration/Actions Min                                 -0.226309\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.11404\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.139011\r\n",
      "exploration/env_infos/final/reward_dist Std              0.221973\r\n",
      "exploration/env_infos/final/reward_dist Max              0.578239\r\n",
      "exploration/env_infos/final/reward_dist Min              3.05102e-08\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0119336\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0130206\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0307964\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000392136\r\n",
      "exploration/env_infos/reward_dist Mean                   0.275087\r\n",
      "exploration/env_infos/reward_dist Std                    0.285184\r\n",
      "exploration/env_infos/reward_dist Max                    0.930571\r\n",
      "exploration/env_infos/reward_dist Min                    3.05102e-08\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.11755\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0451618\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.065436\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.196319\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.11201\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0449788\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0557613\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.18078\r\n",
      "exploration/env_infos/reward_energy Mean                -0.117153\r\n",
      "exploration/env_infos/reward_energy Std                  0.0821204\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0105278\r\n",
      "exploration/env_infos/reward_energy Min                 -0.430907\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0929161\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.176735\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.336813\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.226047\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00296963\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00306481\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00833534\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00322619\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0446034\r\n",
      "exploration/env_infos/end_effector_loc Std               0.104675\r\n",
      "exploration/env_infos/end_effector_loc Max               0.336813\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.226047\r\n",
      "evaluation/num steps total                           29000\r\n",
      "evaluation/num paths total                            1450\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0775506\r\n",
      "evaluation/Rewards Std                                   0.0889816\r\n",
      "evaluation/Rewards Max                                   0.150317\r\n",
      "evaluation/Rewards Min                                  -0.563364\r\n",
      "evaluation/Returns Mean                                 -1.55101\r\n",
      "evaluation/Returns Std                                   1.45118\r\n",
      "evaluation/Returns Max                                   1.20005\r\n",
      "evaluation/Returns Min                                  -8.71153\r\n",
      "evaluation/Actions Mean                                  0.00568594\r\n",
      "evaluation/Actions Std                                   0.0851227\r\n",
      "evaluation/Actions Max                                   0.510682\r\n",
      "evaluation/Actions Min                                  -0.961774\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.55101\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0536298\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.151092\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.688131\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.36428e-86\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00708609\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117662\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0420442\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.01616e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0916732\r\n",
      "evaluation/env_infos/reward_dist Std                     0.188689\r\n",
      "evaluation/env_infos/reward_dist Max                     0.984362\r\n",
      "evaluation/env_infos/reward_dist Min                     1.71307e-88\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0823674\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0842128\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00826024\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.322326\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.216973\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.215967\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0215917\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.27582\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0783424\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0917545\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00152775\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.27582\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0173664\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.290968\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.543374\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00183255\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106673\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0255341\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0480887\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00579446\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.198391\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.543374\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00315859\r\n",
      "time/evaluation sampling (s)                             0.955819\r\n",
      "time/exploration sampling (s)                            0.121984\r\n",
      "time/logging (s)                                         0.0192455\r\n",
      "time/saving (s)                                          0.0298329\r\n",
      "time/training (s)                                       45.8728\r\n",
      "time/epoch (s)                                          47.0028\r\n",
      "time/total (s)                                        1365.55\r\n",
      "Epoch                                                   28\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:48:29.850608 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 29 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00143346\n",
      "trainer/QF2 Loss                                         0.00145426\n",
      "trainer/Policy Loss                                      3.40982\n",
      "trainer/Q1 Predictions Mean                             -1.34385\n",
      "trainer/Q1 Predictions Std                               0.794182\n",
      "trainer/Q1 Predictions Max                               0.338798\n",
      "trainer/Q1 Predictions Min                              -3.89966\n",
      "trainer/Q2 Predictions Mean                             -1.32362\n",
      "trainer/Q2 Predictions Std                               0.784791\n",
      "trainer/Q2 Predictions Max                               0.381921\n",
      "trainer/Q2 Predictions Min                              -3.87196\n",
      "trainer/Q Targets Mean                                  -1.33658\n",
      "trainer/Q Targets Std                                    0.784752\n",
      "trainer/Q Targets Max                                    0.396806\n",
      "trainer/Q Targets Min                                   -3.93741\n",
      "trainer/Log Pis Mean                                     2.08061\n",
      "trainer/Log Pis Std                                      1.28496\n",
      "trainer/Log Pis Max                                      4.50485\n",
      "trainer/Log Pis Min                                     -3.01126\n",
      "trainer/Policy mu Mean                                   0.00242612\n",
      "trainer/Policy mu Std                                    0.235283\n",
      "trainer/Policy mu Max                                    1.7354\n",
      "trainer/Policy mu Min                                   -1.14995\n",
      "trainer/Policy log std Mean                             -2.39065\n",
      "trainer/Policy log std Std                               0.500581\n",
      "trainer/Policy log std Max                              -0.535807\n",
      "trainer/Policy log std Min                              -3.18573\n",
      "trainer/Alpha                                            0.0207749\n",
      "trainer/Alpha Loss                                       0.312314\n",
      "exploration/num steps total                           4000\n",
      "exploration/num paths total                            200\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.101811\n",
      "exploration/Rewards Std                                  0.0752361\n",
      "exploration/Rewards Max                                  0.0854098\n",
      "exploration/Rewards Min                                 -0.320172\n",
      "exploration/Returns Mean                                -2.03623\n",
      "exploration/Returns Std                                  1.00636\n",
      "exploration/Returns Max                                 -0.826922\n",
      "exploration/Returns Min                                 -3.51336\n",
      "exploration/Actions Mean                                -0.00267165\n",
      "exploration/Actions Std                                  0.125441\n",
      "exploration/Actions Max                                  0.512903\n",
      "exploration/Actions Min                                 -0.451126\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.03623\n",
      "exploration/env_infos/final/reward_dist Mean             0.185554\n",
      "exploration/env_infos/final/reward_dist Std              0.364861\n",
      "exploration/env_infos/final/reward_dist Max              0.915219\n",
      "exploration/env_infos/final/reward_dist Min              1.05527e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103528\n",
      "exploration/env_infos/initial/reward_dist Std            0.0125724\n",
      "exploration/env_infos/initial/reward_dist Max            0.0339781\n",
      "exploration/env_infos/initial/reward_dist Min            0.000198093\n",
      "exploration/env_infos/reward_dist Mean                   0.134622\n",
      "exploration/env_infos/reward_dist Std                    0.249953\n",
      "exploration/env_infos/reward_dist Max                    0.96125\n",
      "exploration/env_infos/reward_dist Min                    1.05527e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.112694\n",
      "exploration/env_infos/final/reward_energy Std            0.0892674\n",
      "exploration/env_infos/final/reward_energy Max           -0.0205583\n",
      "exploration/env_infos/final/reward_energy Min           -0.270084\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.295558\n",
      "exploration/env_infos/initial/reward_energy Std          0.193242\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0610989\n",
      "exploration/env_infos/initial/reward_energy Min         -0.634772\n",
      "exploration/env_infos/reward_energy Mean                -0.142418\n",
      "exploration/env_infos/reward_energy Std                  0.10584\n",
      "exploration/env_infos/reward_energy Max                 -0.00904316\n",
      "exploration/env_infos/reward_energy Min                 -0.634772\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0252763\n",
      "exploration/env_infos/final/end_effector_loc Std         0.299038\n",
      "exploration/env_infos/final/end_effector_loc Max         0.4248\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.673905\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00717461\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0102174\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0256451\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00838616\n",
      "exploration/env_infos/end_effector_loc Mean              0.0192251\n",
      "exploration/env_infos/end_effector_loc Std               0.195176\n",
      "exploration/env_infos/end_effector_loc Max               0.4248\n",
      "exploration/env_infos/end_effector_loc Min              -0.673905\n",
      "evaluation/num steps total                           30000\n",
      "evaluation/num paths total                            1500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0476888\n",
      "evaluation/Rewards Std                                   0.076602\n",
      "evaluation/Rewards Max                                   0.137631\n",
      "evaluation/Rewards Min                                  -0.487326\n",
      "evaluation/Returns Mean                                 -0.953775\n",
      "evaluation/Returns Std                                   1.10766\n",
      "evaluation/Returns Max                                   1.03744\n",
      "evaluation/Returns Min                                  -2.87491\n",
      "evaluation/Actions Mean                                 -0.00248753\n",
      "evaluation/Actions Std                                   0.0801856\n",
      "evaluation/Actions Max                                   0.911033\n",
      "evaluation/Actions Min                                  -0.625356\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.953775\n",
      "evaluation/env_infos/final/reward_dist Mean              0.137634\n",
      "evaluation/env_infos/final/reward_dist Std               0.251972\n",
      "evaluation/env_infos/final/reward_dist Max               0.890035\n",
      "evaluation/env_infos/final/reward_dist Min               2.05399e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00587226\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00960469\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0422461\n",
      "evaluation/env_infos/initial/reward_dist Min             2.35855e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.150229\n",
      "evaluation/env_infos/reward_dist Std                     0.243486\n",
      "evaluation/env_infos/reward_dist Max                     0.99847\n",
      "evaluation/env_infos/reward_dist Min                     2.05399e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0898408\n",
      "evaluation/env_infos/final/reward_energy Std             0.0686247\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00632332\n",
      "evaluation/env_infos/final/reward_energy Min            -0.297953\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.205826\n",
      "evaluation/env_infos/initial/reward_energy Std           0.2116\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0105307\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.921956\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0734961\n",
      "evaluation/env_infos/reward_energy Std                   0.0864301\n",
      "evaluation/env_infos/reward_energy Max                  -0.00111476\n",
      "evaluation/env_infos/reward_energy Min                  -0.921956\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0226645\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.240071\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.466689\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.576811\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000952578\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0103931\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0455516\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0312678\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0100615\n",
      "evaluation/env_infos/end_effector_loc Std                0.157375\n",
      "evaluation/env_infos/end_effector_loc Max                0.49087\n",
      "evaluation/env_infos/end_effector_loc Min               -0.576811\n",
      "time/data storing (s)                                    0.00300334\n",
      "time/evaluation sampling (s)                             0.931769\n",
      "time/exploration sampling (s)                            0.120044\n",
      "time/logging (s)                                         0.0187717\n",
      "time/saving (s)                                          0.0274834\n",
      "time/training (s)                                       46.3177\n",
      "time/epoch (s)                                          47.4188\n",
      "time/total (s)                                        1413.35\n",
      "Epoch                                                   29\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:49:17.360204 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 30 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000868392\n",
      "trainer/QF2 Loss                                         0.00092665\n",
      "trainer/Policy Loss                                      3.17587\n",
      "trainer/Q1 Predictions Mean                             -1.23718\n",
      "trainer/Q1 Predictions Std                               0.813709\n",
      "trainer/Q1 Predictions Max                               0.556193\n",
      "trainer/Q1 Predictions Min                              -3.25137\n",
      "trainer/Q2 Predictions Mean                             -1.2578\n",
      "trainer/Q2 Predictions Std                               0.813467\n",
      "trainer/Q2 Predictions Max                               0.578312\n",
      "trainer/Q2 Predictions Min                              -3.3113\n",
      "trainer/Q Targets Mean                                  -1.25083\n",
      "trainer/Q Targets Std                                    0.816737\n",
      "trainer/Q Targets Max                                    0.578554\n",
      "trainer/Q Targets Min                                   -3.28503\n",
      "trainer/Log Pis Mean                                     1.92624\n",
      "trainer/Log Pis Std                                      1.33906\n",
      "trainer/Log Pis Max                                      4.33227\n",
      "trainer/Log Pis Min                                     -3.99861\n",
      "trainer/Policy mu Mean                                   0.0291556\n",
      "trainer/Policy mu Std                                    0.302697\n",
      "trainer/Policy mu Max                                    1.92191\n",
      "trainer/Policy mu Min                                   -1.52025\n",
      "trainer/Policy log std Mean                             -2.34375\n",
      "trainer/Policy log std Std                               0.552229\n",
      "trainer/Policy log std Max                              -0.321568\n",
      "trainer/Policy log std Min                              -3.19551\n",
      "trainer/Alpha                                            0.0219214\n",
      "trainer/Alpha Loss                                      -0.281782\n",
      "exploration/num steps total                           4100\n",
      "exploration/num paths total                            205\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.124742\n",
      "exploration/Rewards Std                                  0.101751\n",
      "exploration/Rewards Max                                  0.0233141\n",
      "exploration/Rewards Min                                 -0.566925\n",
      "exploration/Returns Mean                                -2.49483\n",
      "exploration/Returns Std                                  1.10729\n",
      "exploration/Returns Max                                 -0.990037\n",
      "exploration/Returns Min                                 -4.37202\n",
      "exploration/Actions Mean                                -0.0192899\n",
      "exploration/Actions Std                                  0.131652\n",
      "exploration/Actions Max                                  0.557396\n",
      "exploration/Actions Min                                 -0.515605\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.49483\n",
      "exploration/env_infos/final/reward_dist Mean             0.000115024\n",
      "exploration/env_infos/final/reward_dist Std              0.000162676\n",
      "exploration/env_infos/final/reward_dist Max              0.000416184\n",
      "exploration/env_infos/final/reward_dist Min              3.0369e-69\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00445585\n",
      "exploration/env_infos/initial/reward_dist Std            0.00507897\n",
      "exploration/env_infos/initial/reward_dist Max            0.0132387\n",
      "exploration/env_infos/initial/reward_dist Min            1.48586e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0473141\n",
      "exploration/env_infos/reward_dist Std                    0.132383\n",
      "exploration/env_infos/reward_dist Max                    0.820441\n",
      "exploration/env_infos/reward_dist Min                    3.0369e-69\n",
      "exploration/env_infos/final/reward_energy Mean          -0.176782\n",
      "exploration/env_infos/final/reward_energy Std            0.101689\n",
      "exploration/env_infos/final/reward_energy Max           -0.0817857\n",
      "exploration/env_infos/final/reward_energy Min           -0.368716\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.11499\n",
      "exploration/env_infos/initial/reward_energy Std          0.0772959\n",
      "exploration/env_infos/initial/reward_energy Max         -0.039025\n",
      "exploration/env_infos/initial/reward_energy Min         -0.209132\n",
      "exploration/env_infos/reward_energy Mean                -0.140006\n",
      "exploration/env_infos/reward_energy Std                  0.125726\n",
      "exploration/env_infos/reward_energy Max                 -0.0039966\n",
      "exploration/env_infos/reward_energy Min                 -0.565691\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.188437\n",
      "exploration/env_infos/final/end_effector_loc Std         0.341598\n",
      "exploration/env_infos/final/end_effector_loc Max         0.329339\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.802257\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00087698\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0048195\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00969946\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00420323\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0700567\n",
      "exploration/env_infos/end_effector_loc Std               0.173904\n",
      "exploration/env_infos/end_effector_loc Max               0.329339\n",
      "exploration/env_infos/end_effector_loc Min              -0.802257\n",
      "evaluation/num steps total                           31000\n",
      "evaluation/num paths total                            1550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0729044\n",
      "evaluation/Rewards Std                                   0.0687716\n",
      "evaluation/Rewards Max                                   0.133651\n",
      "evaluation/Rewards Min                                  -0.363407\n",
      "evaluation/Returns Mean                                 -1.45809\n",
      "evaluation/Returns Std                                   0.958826\n",
      "evaluation/Returns Max                                   0.420791\n",
      "evaluation/Returns Min                                  -3.30132\n",
      "evaluation/Actions Mean                                  0.000209947\n",
      "evaluation/Actions Std                                   0.101427\n",
      "evaluation/Actions Max                                   0.954438\n",
      "evaluation/Actions Min                                  -0.886743\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.45809\n",
      "evaluation/env_infos/final/reward_dist Mean              0.172209\n",
      "evaluation/env_infos/final/reward_dist Std               0.294937\n",
      "evaluation/env_infos/final/reward_dist Max               0.900849\n",
      "evaluation/env_infos/final/reward_dist Min               5.27157e-26\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00578265\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0102414\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0462467\n",
      "evaluation/env_infos/initial/reward_dist Min             1.52378e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.121241\n",
      "evaluation/env_infos/reward_dist Std                     0.22821\n",
      "evaluation/env_infos/reward_dist Max                     0.992421\n",
      "evaluation/env_infos/reward_dist Min                     5.27157e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0603641\n",
      "evaluation/env_infos/final/reward_energy Std             0.0601814\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00366965\n",
      "evaluation/env_infos/final/reward_energy Min            -0.284695\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.23473\n",
      "evaluation/env_infos/initial/reward_energy Std           0.289546\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00779645\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.30279\n",
      "evaluation/env_infos/reward_energy Mean                 -0.084739\n",
      "evaluation/env_infos/reward_energy Std                   0.115734\n",
      "evaluation/env_infos/reward_energy Max                  -0.00101156\n",
      "evaluation/env_infos/reward_energy Min                  -1.30279\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0283729\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.249677\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.542368\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.762873\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000518219\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0131681\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0477219\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0443371\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0177462\n",
      "evaluation/env_infos/end_effector_loc Std                0.16411\n",
      "evaluation/env_infos/end_effector_loc Max                0.548663\n",
      "evaluation/env_infos/end_effector_loc Min               -0.762873\n",
      "time/data storing (s)                                    0.00299595\n",
      "time/evaluation sampling (s)                             0.96168\n",
      "time/exploration sampling (s)                            0.127034\n",
      "time/logging (s)                                         0.0200868\n",
      "time/saving (s)                                          0.0273687\n",
      "time/training (s)                                       45.9843\n",
      "time/epoch (s)                                          47.1235\n",
      "time/total (s)                                        1460.86\n",
      "Epoch                                                   30\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:50:05.177174 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 31 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00105319\n",
      "trainer/QF2 Loss                                         0.00170622\n",
      "trainer/Policy Loss                                      3.33576\n",
      "trainer/Q1 Predictions Mean                             -1.30821\n",
      "trainer/Q1 Predictions Std                               0.841368\n",
      "trainer/Q1 Predictions Max                               0.773772\n",
      "trainer/Q1 Predictions Min                              -3.40724\n",
      "trainer/Q2 Predictions Mean                             -1.31752\n",
      "trainer/Q2 Predictions Std                               0.840216\n",
      "trainer/Q2 Predictions Max                               0.692052\n",
      "trainer/Q2 Predictions Min                              -3.38911\n",
      "trainer/Q Targets Mean                                  -1.30253\n",
      "trainer/Q Targets Std                                    0.842284\n",
      "trainer/Q Targets Max                                    0.78719\n",
      "trainer/Q Targets Min                                   -3.42878\n",
      "trainer/Log Pis Mean                                     2.02568\n",
      "trainer/Log Pis Std                                      1.28525\n",
      "trainer/Log Pis Max                                      4.27801\n",
      "trainer/Log Pis Min                                     -4.08691\n",
      "trainer/Policy mu Mean                                   0.0266079\n",
      "trainer/Policy mu Std                                    0.284967\n",
      "trainer/Policy mu Max                                    2.2716\n",
      "trainer/Policy mu Min                                   -1.65812\n",
      "trainer/Policy log std Mean                             -2.30338\n",
      "trainer/Policy log std Std                               0.558107\n",
      "trainer/Policy log std Max                              -0.192994\n",
      "trainer/Policy log std Min                              -3.22668\n",
      "trainer/Alpha                                            0.0208096\n",
      "trainer/Alpha Loss                                       0.0994424\n",
      "exploration/num steps total                           4200\n",
      "exploration/num paths total                            210\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.136397\n",
      "exploration/Rewards Std                                  0.0449336\n",
      "exploration/Rewards Max                                 -0.00982367\n",
      "exploration/Rewards Min                                 -0.268115\n",
      "exploration/Returns Mean                                -2.72794\n",
      "exploration/Returns Std                                  0.598944\n",
      "exploration/Returns Max                                 -1.74675\n",
      "exploration/Returns Min                                 -3.46367\n",
      "exploration/Actions Mean                                 0.00441041\n",
      "exploration/Actions Std                                  0.0754965\n",
      "exploration/Actions Max                                  0.279963\n",
      "exploration/Actions Min                                 -0.205511\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.72794\n",
      "exploration/env_infos/final/reward_dist Mean             0.023512\n",
      "exploration/env_infos/final/reward_dist Std              0.0355496\n",
      "exploration/env_infos/final/reward_dist Max              0.0927815\n",
      "exploration/env_infos/final/reward_dist Min              4.00104e-13\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0042544\n",
      "exploration/env_infos/initial/reward_dist Std            0.00673618\n",
      "exploration/env_infos/initial/reward_dist Max            0.0176893\n",
      "exploration/env_infos/initial/reward_dist Min            1.28221e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0522036\n",
      "exploration/env_infos/reward_dist Std                    0.153622\n",
      "exploration/env_infos/reward_dist Max                    0.862431\n",
      "exploration/env_infos/reward_dist Min                    4.00104e-13\n",
      "exploration/env_infos/final/reward_energy Mean          -0.139108\n",
      "exploration/env_infos/final/reward_energy Std            0.0435108\n",
      "exploration/env_infos/final/reward_energy Max           -0.0916708\n",
      "exploration/env_infos/final/reward_energy Min           -0.206007\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.146633\n",
      "exploration/env_infos/initial/reward_energy Std          0.0673603\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0331241\n",
      "exploration/env_infos/initial/reward_energy Min         -0.233346\n",
      "exploration/env_infos/reward_energy Mean                -0.0953342\n",
      "exploration/env_infos/reward_energy Std                  0.0484742\n",
      "exploration/env_infos/reward_energy Max                 -0.0240482\n",
      "exploration/env_infos/reward_energy Min                 -0.293867\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0853826\n",
      "exploration/env_infos/final/end_effector_loc Std         0.222733\n",
      "exploration/env_infos/final/end_effector_loc Max         0.457031\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.169338\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00191643\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00537361\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0104349\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00677597\n",
      "exploration/env_infos/end_effector_loc Mean              0.040722\n",
      "exploration/env_infos/end_effector_loc Std               0.135335\n",
      "exploration/env_infos/end_effector_loc Max               0.457031\n",
      "exploration/env_infos/end_effector_loc Min              -0.169338\n",
      "evaluation/num steps total                           32000\n",
      "evaluation/num paths total                            1600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0670496\n",
      "evaluation/Rewards Std                                   0.0762236\n",
      "evaluation/Rewards Max                                   0.138701\n",
      "evaluation/Rewards Min                                  -0.502041\n",
      "evaluation/Returns Mean                                 -1.34099\n",
      "evaluation/Returns Std                                   1.00427\n",
      "evaluation/Returns Max                                   0.819274\n",
      "evaluation/Returns Min                                  -3.85441\n",
      "evaluation/Actions Mean                                  0.00661723\n",
      "evaluation/Actions Std                                   0.0779997\n",
      "evaluation/Actions Max                                   0.724893\n",
      "evaluation/Actions Min                                  -0.605992\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.34099\n",
      "evaluation/env_infos/final/reward_dist Mean              0.10896\n",
      "evaluation/env_infos/final/reward_dist Std               0.199922\n",
      "evaluation/env_infos/final/reward_dist Max               0.896509\n",
      "evaluation/env_infos/final/reward_dist Min               1.64797e-26\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00685253\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0112947\n",
      "evaluation/env_infos/initial/reward_dist Max             0.043635\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97466e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.152013\n",
      "evaluation/env_infos/reward_dist Std                     0.246042\n",
      "evaluation/env_infos/reward_dist Max                     0.995809\n",
      "evaluation/env_infos/reward_dist Min                     1.64797e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0478993\n",
      "evaluation/env_infos/final/reward_energy Std             0.0527738\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00340879\n",
      "evaluation/env_infos/final/reward_energy Min            -0.231816\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.192832\n",
      "evaluation/env_infos/initial/reward_energy Std           0.189688\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0174097\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.81228\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0698787\n",
      "evaluation/env_infos/reward_energy Std                   0.085863\n",
      "evaluation/env_infos/reward_energy Max                  -0.00327803\n",
      "evaluation/env_infos/reward_energy Min                  -0.81228\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0637486\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.28143\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.812117\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.475605\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000404155\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00955479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0362446\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0302996\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0207534\n",
      "evaluation/env_infos/end_effector_loc Std                0.177362\n",
      "evaluation/env_infos/end_effector_loc Max                0.812117\n",
      "evaluation/env_infos/end_effector_loc Min               -0.475605\n",
      "time/data storing (s)                                    0.00294456\n",
      "time/evaluation sampling (s)                             0.979122\n",
      "time/exploration sampling (s)                            0.124152\n",
      "time/logging (s)                                         0.0210123\n",
      "time/saving (s)                                          0.0300203\n",
      "time/training (s)                                       46.242\n",
      "time/epoch (s)                                          47.3992\n",
      "time/total (s)                                        1508.67\n",
      "Epoch                                                   31\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:50:53.506924 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 32 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00075019\n",
      "trainer/QF2 Loss                                         0.000711884\n",
      "trainer/Policy Loss                                      3.25488\n",
      "trainer/Q1 Predictions Mean                             -1.25544\n",
      "trainer/Q1 Predictions Std                               0.785948\n",
      "trainer/Q1 Predictions Max                               0.821711\n",
      "trainer/Q1 Predictions Min                              -3.24505\n",
      "trainer/Q2 Predictions Mean                             -1.25161\n",
      "trainer/Q2 Predictions Std                               0.783929\n",
      "trainer/Q2 Predictions Max                               0.821986\n",
      "trainer/Q2 Predictions Min                              -3.24602\n",
      "trainer/Q Targets Mean                                  -1.2596\n",
      "trainer/Q Targets Std                                    0.792916\n",
      "trainer/Q Targets Max                                    0.840977\n",
      "trainer/Q Targets Min                                   -3.27868\n",
      "trainer/Log Pis Mean                                     2.01259\n",
      "trainer/Log Pis Std                                      1.27611\n",
      "trainer/Log Pis Max                                      4.23461\n",
      "trainer/Log Pis Min                                     -3.20308\n",
      "trainer/Policy mu Mean                                   0.0494255\n",
      "trainer/Policy mu Std                                    0.262388\n",
      "trainer/Policy mu Max                                    2.25552\n",
      "trainer/Policy mu Min                                   -0.902589\n",
      "trainer/Policy log std Mean                             -2.34858\n",
      "trainer/Policy log std Std                               0.543264\n",
      "trainer/Policy log std Max                              -0.265509\n",
      "trainer/Policy log std Min                              -3.14608\n",
      "trainer/Alpha                                            0.0229554\n",
      "trainer/Alpha Loss                                       0.0475195\n",
      "exploration/num steps total                           4300\n",
      "exploration/num paths total                            215\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.103459\n",
      "exploration/Rewards Std                                  0.0947882\n",
      "exploration/Rewards Max                                  0.081175\n",
      "exploration/Rewards Min                                 -0.336021\n",
      "exploration/Returns Mean                                -2.06919\n",
      "exploration/Returns Std                                  1.3674\n",
      "exploration/Returns Max                                  0.265208\n",
      "exploration/Returns Min                                 -3.75538\n",
      "exploration/Actions Mean                                 0.00442631\n",
      "exploration/Actions Std                                  0.203609\n",
      "exploration/Actions Max                                  0.910404\n",
      "exploration/Actions Min                                 -0.879292\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.06919\n",
      "exploration/env_infos/final/reward_dist Mean             0.0819507\n",
      "exploration/env_infos/final/reward_dist Std              0.103409\n",
      "exploration/env_infos/final/reward_dist Max              0.244303\n",
      "exploration/env_infos/final/reward_dist Min              1.84696e-20\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0172631\n",
      "exploration/env_infos/initial/reward_dist Std            0.0113907\n",
      "exploration/env_infos/initial/reward_dist Max            0.0363455\n",
      "exploration/env_infos/initial/reward_dist Min            0.00545669\n",
      "exploration/env_infos/reward_dist Mean                   0.0924413\n",
      "exploration/env_infos/reward_dist Std                    0.17842\n",
      "exploration/env_infos/reward_dist Max                    0.720027\n",
      "exploration/env_infos/reward_dist Min                    1.84696e-20\n",
      "exploration/env_infos/final/reward_energy Mean          -0.102541\n",
      "exploration/env_infos/final/reward_energy Std            0.0596053\n",
      "exploration/env_infos/final/reward_energy Max           -0.0280562\n",
      "exploration/env_infos/final/reward_energy Min           -0.210534\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.510908\n",
      "exploration/env_infos/initial/reward_energy Std          0.487991\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0457979\n",
      "exploration/env_infos/initial/reward_energy Min         -1.2657\n",
      "exploration/env_infos/reward_energy Mean                -0.207598\n",
      "exploration/env_infos/reward_energy Std                  0.199638\n",
      "exploration/env_infos/reward_energy Max                 -0.0112263\n",
      "exploration/env_infos/reward_energy Min                 -1.2657\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.185495\n",
      "exploration/env_infos/final/end_effector_loc Std         0.212326\n",
      "exploration/env_infos/final/end_effector_loc Max         0.72325\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.0334009\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00754701\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0238117\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0455202\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0439646\n",
      "exploration/env_infos/end_effector_loc Mean              0.102439\n",
      "exploration/env_infos/end_effector_loc Std               0.204913\n",
      "exploration/env_infos/end_effector_loc Max               0.72325\n",
      "exploration/env_infos/end_effector_loc Min              -0.424709\n",
      "evaluation/num steps total                           33000\n",
      "evaluation/num paths total                            1650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0597704\n",
      "evaluation/Rewards Std                                   0.0790739\n",
      "evaluation/Rewards Max                                   0.122847\n",
      "evaluation/Rewards Min                                  -0.456664\n",
      "evaluation/Returns Mean                                 -1.19541\n",
      "evaluation/Returns Std                                   1.17868\n",
      "evaluation/Returns Max                                   1.16564\n",
      "evaluation/Returns Min                                  -3.30228\n",
      "evaluation/Actions Mean                                  0.00946841\n",
      "evaluation/Actions Std                                   0.0817082\n",
      "evaluation/Actions Max                                   0.98935\n",
      "evaluation/Actions Min                                  -0.758869\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.19541\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0795869\n",
      "evaluation/env_infos/final/reward_dist Std               0.195023\n",
      "evaluation/env_infos/final/reward_dist Max               0.952876\n",
      "evaluation/env_infos/final/reward_dist Min               6.67184e-31\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00708472\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0109615\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0487913\n",
      "evaluation/env_infos/initial/reward_dist Min             1.2844e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.137477\n",
      "evaluation/env_infos/reward_dist Std                     0.228536\n",
      "evaluation/env_infos/reward_dist Max                     0.990842\n",
      "evaluation/env_infos/reward_dist Min                     6.67184e-31\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0360925\n",
      "evaluation/env_infos/final/reward_energy Std             0.0434452\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00639578\n",
      "evaluation/env_infos/final/reward_energy Min            -0.262577\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.204623\n",
      "evaluation/env_infos/initial/reward_energy Std           0.221104\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0057304\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.24687\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0726766\n",
      "evaluation/env_infos/reward_energy Std                   0.0908289\n",
      "evaluation/env_infos/reward_energy Max                  -0.00103516\n",
      "evaluation/env_infos/reward_energy Min                  -1.24687\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0655513\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.264282\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.805094\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.649783\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000438643\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106421\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0494675\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0379435\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0121129\n",
      "evaluation/env_infos/end_effector_loc Std                0.176808\n",
      "evaluation/env_infos/end_effector_loc Max                0.805094\n",
      "evaluation/env_infos/end_effector_loc Min               -0.649783\n",
      "time/data storing (s)                                    0.00304569\n",
      "time/evaluation sampling (s)                             1.03075\n",
      "time/exploration sampling (s)                            0.121423\n",
      "time/logging (s)                                         0.0205147\n",
      "time/saving (s)                                          0.0296664\n",
      "time/training (s)                                       46.6471\n",
      "time/epoch (s)                                          47.8525\n",
      "time/total (s)                                        1557\n",
      "Epoch                                                   32\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:51:41.688999 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 33 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00109556\n",
      "trainer/QF2 Loss                                         0.00108879\n",
      "trainer/Policy Loss                                      3.17819\n",
      "trainer/Q1 Predictions Mean                             -1.28395\n",
      "trainer/Q1 Predictions Std                               0.817455\n",
      "trainer/Q1 Predictions Max                               0.853905\n",
      "trainer/Q1 Predictions Min                              -3.31159\n",
      "trainer/Q2 Predictions Mean                             -1.29418\n",
      "trainer/Q2 Predictions Std                               0.816651\n",
      "trainer/Q2 Predictions Max                               0.871622\n",
      "trainer/Q2 Predictions Min                              -3.38168\n",
      "trainer/Q Targets Mean                                  -1.28498\n",
      "trainer/Q Targets Std                                    0.821735\n",
      "trainer/Q Targets Max                                    0.852357\n",
      "trainer/Q Targets Min                                   -3.3464\n",
      "trainer/Log Pis Mean                                     1.88772\n",
      "trainer/Log Pis Std                                      1.21953\n",
      "trainer/Log Pis Max                                      4.80274\n",
      "trainer/Log Pis Min                                     -2.60258\n",
      "trainer/Policy mu Mean                                   0.0277613\n",
      "trainer/Policy mu Std                                    0.277339\n",
      "trainer/Policy mu Max                                    2.29856\n",
      "trainer/Policy mu Min                                   -1.79327\n",
      "trainer/Policy log std Mean                             -2.29707\n",
      "trainer/Policy log std Std                               0.531072\n",
      "trainer/Policy log std Max                              -0.413334\n",
      "trainer/Policy log std Min                              -3.1734\n",
      "trainer/Alpha                                            0.021398\n",
      "trainer/Alpha Loss                                      -0.4316\n",
      "exploration/num steps total                           4400\n",
      "exploration/num paths total                            220\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.127169\n",
      "exploration/Rewards Std                                  0.068734\n",
      "exploration/Rewards Max                                 -0.0110828\n",
      "exploration/Rewards Min                                 -0.305158\n",
      "exploration/Returns Mean                                -2.54338\n",
      "exploration/Returns Std                                  0.540036\n",
      "exploration/Returns Max                                 -1.68848\n",
      "exploration/Returns Min                                 -3.21045\n",
      "exploration/Actions Mean                                 0.00333491\n",
      "exploration/Actions Std                                  0.1516\n",
      "exploration/Actions Max                                  0.584744\n",
      "exploration/Actions Min                                 -0.53535\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.54338\n",
      "exploration/env_infos/final/reward_dist Mean             0.00256698\n",
      "exploration/env_infos/final/reward_dist Std              0.00489217\n",
      "exploration/env_infos/final/reward_dist Max              0.0123439\n",
      "exploration/env_infos/final/reward_dist Min              5.6276e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0066095\n",
      "exploration/env_infos/initial/reward_dist Std            0.00741077\n",
      "exploration/env_infos/initial/reward_dist Max            0.0163727\n",
      "exploration/env_infos/initial/reward_dist Min            0.000131801\n",
      "exploration/env_infos/reward_dist Mean                   0.0315535\n",
      "exploration/env_infos/reward_dist Std                    0.0797907\n",
      "exploration/env_infos/reward_dist Max                    0.393827\n",
      "exploration/env_infos/reward_dist Min                    5.6276e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.161087\n",
      "exploration/env_infos/final/reward_energy Std            0.0405295\n",
      "exploration/env_infos/final/reward_energy Max           -0.101114\n",
      "exploration/env_infos/final/reward_energy Min           -0.221528\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.243577\n",
      "exploration/env_infos/initial/reward_energy Std          0.130683\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0813547\n",
      "exploration/env_infos/initial/reward_energy Min         -0.375911\n",
      "exploration/env_infos/reward_energy Mean                -0.163187\n",
      "exploration/env_infos/reward_energy Std                  0.139131\n",
      "exploration/env_infos/reward_energy Max                 -0.0153714\n",
      "exploration/env_infos/reward_energy Min                 -0.686673\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.127949\n",
      "exploration/env_infos/final/end_effector_loc Std         0.260978\n",
      "exploration/env_infos/final/end_effector_loc Max         0.269642\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.562699\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00716842\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00664254\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0018379\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0187743\n",
      "exploration/env_infos/end_effector_loc Mean             -0.111732\n",
      "exploration/env_infos/end_effector_loc Std               0.172079\n",
      "exploration/env_infos/end_effector_loc Max               0.269642\n",
      "exploration/env_infos/end_effector_loc Min              -0.562699\n",
      "evaluation/num steps total                           34000\n",
      "evaluation/num paths total                            1700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0538329\n",
      "evaluation/Rewards Std                                   0.0730123\n",
      "evaluation/Rewards Max                                   0.140252\n",
      "evaluation/Rewards Min                                  -0.360975\n",
      "evaluation/Returns Mean                                 -1.07666\n",
      "evaluation/Returns Std                                   1.16411\n",
      "evaluation/Returns Max                                   1.23055\n",
      "evaluation/Returns Min                                  -3.85707\n",
      "evaluation/Actions Mean                                  0.00152176\n",
      "evaluation/Actions Std                                   0.0728253\n",
      "evaluation/Actions Max                                   0.945597\n",
      "evaluation/Actions Min                                  -0.339191\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.07666\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0862982\n",
      "evaluation/env_infos/final/reward_dist Std               0.187931\n",
      "evaluation/env_infos/final/reward_dist Max               0.879575\n",
      "evaluation/env_infos/final/reward_dist Min               4.95935e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00678808\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0149283\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0788802\n",
      "evaluation/env_infos/initial/reward_dist Min             1.58291e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.140438\n",
      "evaluation/env_infos/reward_dist Std                     0.224561\n",
      "evaluation/env_infos/reward_dist Max                     0.991858\n",
      "evaluation/env_infos/reward_dist Min                     4.95935e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0387068\n",
      "evaluation/env_infos/final/reward_energy Std             0.0371852\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00278421\n",
      "evaluation/env_infos/final/reward_energy Min            -0.177427\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.206786\n",
      "evaluation/env_infos/initial/reward_energy Std           0.230985\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0180013\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.967905\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0600531\n",
      "evaluation/env_infos/reward_energy Std                   0.0836978\n",
      "evaluation/env_infos/reward_energy Max                  -9.70881e-05\n",
      "evaluation/env_infos/reward_energy Min                  -0.967905\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.046359\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.254531\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.609016\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.544094\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00307715\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0105202\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0472798\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0169596\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0300307\n",
      "evaluation/env_infos/end_effector_loc Std                0.163301\n",
      "evaluation/env_infos/end_effector_loc Max                0.609016\n",
      "evaluation/env_infos/end_effector_loc Min               -0.544094\n",
      "time/data storing (s)                                    0.00297949\n",
      "time/evaluation sampling (s)                             1.09667\n",
      "time/exploration sampling (s)                            0.12444\n",
      "time/logging (s)                                         0.0194861\n",
      "time/saving (s)                                          0.0277493\n",
      "time/training (s)                                       46.4329\n",
      "time/epoch (s)                                          47.7042\n",
      "time/total (s)                                        1605.18\n",
      "Epoch                                                   33\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:52:29.977929 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 34 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00123744\n",
      "trainer/QF2 Loss                                         0.00117208\n",
      "trainer/Policy Loss                                      3.32772\n",
      "trainer/Q1 Predictions Mean                             -1.28273\n",
      "trainer/Q1 Predictions Std                               0.84898\n",
      "trainer/Q1 Predictions Max                               0.672572\n",
      "trainer/Q1 Predictions Min                              -3.11149\n",
      "trainer/Q2 Predictions Mean                             -1.275\n",
      "trainer/Q2 Predictions Std                               0.841071\n",
      "trainer/Q2 Predictions Max                               0.661838\n",
      "trainer/Q2 Predictions Min                              -3.11998\n",
      "trainer/Q Targets Mean                                  -1.27041\n",
      "trainer/Q Targets Std                                    0.845921\n",
      "trainer/Q Targets Max                                    0.656677\n",
      "trainer/Q Targets Min                                   -3.36522\n",
      "trainer/Log Pis Mean                                     2.04894\n",
      "trainer/Log Pis Std                                      1.10214\n",
      "trainer/Log Pis Max                                      4.00603\n",
      "trainer/Log Pis Min                                     -2.7506\n",
      "trainer/Policy mu Mean                                   0.0252811\n",
      "trainer/Policy mu Std                                    0.292933\n",
      "trainer/Policy mu Max                                    2.1663\n",
      "trainer/Policy mu Min                                   -1.55671\n",
      "trainer/Policy log std Mean                             -2.33273\n",
      "trainer/Policy log std Std                               0.54674\n",
      "trainer/Policy log std Max                               0.0783681\n",
      "trainer/Policy log std Min                              -3.25117\n",
      "trainer/Alpha                                            0.0206205\n",
      "trainer/Alpha Loss                                       0.189974\n",
      "exploration/num steps total                           4500\n",
      "exploration/num paths total                            225\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.10358\n",
      "exploration/Rewards Std                                  0.0530624\n",
      "exploration/Rewards Max                                 -0.010319\n",
      "exploration/Rewards Min                                 -0.260495\n",
      "exploration/Returns Mean                                -2.0716\n",
      "exploration/Returns Std                                  0.605178\n",
      "exploration/Returns Max                                 -1.2873\n",
      "exploration/Returns Min                                 -2.7749\n",
      "exploration/Actions Mean                                 0.00846258\n",
      "exploration/Actions Std                                  0.136673\n",
      "exploration/Actions Max                                  0.659234\n",
      "exploration/Actions Min                                 -0.568087\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.0716\n",
      "exploration/env_infos/final/reward_dist Mean             0.015277\n",
      "exploration/env_infos/final/reward_dist Std              0.0221676\n",
      "exploration/env_infos/final/reward_dist Max              0.0570412\n",
      "exploration/env_infos/final/reward_dist Min              2.64592e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00646447\n",
      "exploration/env_infos/initial/reward_dist Std            0.0128014\n",
      "exploration/env_infos/initial/reward_dist Max            0.0320671\n",
      "exploration/env_infos/initial/reward_dist Min            1.75148e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.110006\n",
      "exploration/env_infos/reward_dist Std                    0.197375\n",
      "exploration/env_infos/reward_dist Max                    0.914457\n",
      "exploration/env_infos/reward_dist Min                    2.64592e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.13615\n",
      "exploration/env_infos/final/reward_energy Std            0.140604\n",
      "exploration/env_infos/final/reward_energy Max           -0.00226691\n",
      "exploration/env_infos/final/reward_energy Min           -0.382436\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.147605\n",
      "exploration/env_infos/initial/reward_energy Std          0.0844157\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0599493\n",
      "exploration/env_infos/initial/reward_energy Min         -0.280021\n",
      "exploration/env_infos/reward_energy Mean                -0.145985\n",
      "exploration/env_infos/reward_energy Std                  0.127242\n",
      "exploration/env_infos/reward_energy Max                 -0.00226691\n",
      "exploration/env_infos/reward_energy Min                 -0.666892\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.136851\n",
      "exploration/env_infos/final/end_effector_loc Std         0.227358\n",
      "exploration/env_infos/final/end_effector_loc Max         0.489115\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.27221\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000436647\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00599591\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0118256\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0100258\n",
      "exploration/env_infos/end_effector_loc Mean              0.0616875\n",
      "exploration/env_infos/end_effector_loc Std               0.162682\n",
      "exploration/env_infos/end_effector_loc Max               0.489115\n",
      "exploration/env_infos/end_effector_loc Min              -0.324256\n",
      "evaluation/num steps total                           35000\n",
      "evaluation/num paths total                            1750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0720636\n",
      "evaluation/Rewards Std                                   0.0964161\n",
      "evaluation/Rewards Max                                   0.12641\n",
      "evaluation/Rewards Min                                  -0.842333\n",
      "evaluation/Returns Mean                                 -1.44127\n",
      "evaluation/Returns Std                                   1.38452\n",
      "evaluation/Returns Max                                   1.29367\n",
      "evaluation/Returns Min                                  -6.45703\n",
      "evaluation/Actions Mean                                  0.00649545\n",
      "evaluation/Actions Std                                   0.0819222\n",
      "evaluation/Actions Max                                   0.986867\n",
      "evaluation/Actions Min                                  -0.536424\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.44127\n",
      "evaluation/env_infos/final/reward_dist Mean              0.138868\n",
      "evaluation/env_infos/final/reward_dist Std               0.222794\n",
      "evaluation/env_infos/final/reward_dist Max               0.889928\n",
      "evaluation/env_infos/final/reward_dist Min               1.6727e-116\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00768201\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117812\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0425665\n",
      "evaluation/env_infos/initial/reward_dist Min             9.07773e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.155045\n",
      "evaluation/env_infos/reward_dist Std                     0.245595\n",
      "evaluation/env_infos/reward_dist Max                     0.976948\n",
      "evaluation/env_infos/reward_dist Min                     1.6727e-116\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0393703\n",
      "evaluation/env_infos/final/reward_energy Std             0.0433413\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00190631\n",
      "evaluation/env_infos/final/reward_energy Min            -0.25295\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.192133\n",
      "evaluation/env_infos/initial/reward_energy Std           0.259236\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00540779\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.09411\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0612043\n",
      "evaluation/env_infos/reward_energy Std                   0.0987973\n",
      "evaluation/env_infos/reward_energy Max                  -0.00190631\n",
      "evaluation/env_infos/reward_energy Min                  -1.09411\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.113839\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.264476\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.688718\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00374914\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107746\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0493433\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0268212\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0567187\n",
      "evaluation/env_infos/end_effector_loc Std                0.165499\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.688718\n",
      "time/data storing (s)                                    0.00294383\n",
      "time/evaluation sampling (s)                             0.985696\n",
      "time/exploration sampling (s)                            0.121794\n",
      "time/logging (s)                                         0.0194978\n",
      "time/saving (s)                                          0.0272527\n",
      "time/training (s)                                       46.68\n",
      "time/epoch (s)                                          47.8371\n",
      "time/total (s)                                        1653.47\n",
      "Epoch                                                   34\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:53:18.341202 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 35 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000918973\r\n",
      "trainer/QF2 Loss                                         0.000874934\r\n",
      "trainer/Policy Loss                                      3.22342\r\n",
      "trainer/Q1 Predictions Mean                             -1.30244\r\n",
      "trainer/Q1 Predictions Std                               0.844705\r\n",
      "trainer/Q1 Predictions Max                               0.869148\r\n",
      "trainer/Q1 Predictions Min                              -3.63314\r\n",
      "trainer/Q2 Predictions Mean                             -1.2915\r\n",
      "trainer/Q2 Predictions Std                               0.846021\r\n",
      "trainer/Q2 Predictions Max                               0.877048\r\n",
      "trainer/Q2 Predictions Min                              -3.64759\r\n",
      "trainer/Q Targets Mean                                  -1.29583\r\n",
      "trainer/Q Targets Std                                    0.846769\r\n",
      "trainer/Q Targets Max                                    0.850652\r\n",
      "trainer/Q Targets Min                                   -3.65207\r\n",
      "trainer/Log Pis Mean                                     1.936\r\n",
      "trainer/Log Pis Std                                      1.23043\r\n",
      "trainer/Log Pis Max                                      4.27421\r\n",
      "trainer/Log Pis Min                                     -2.82327\r\n",
      "trainer/Policy mu Mean                                   0.0236413\r\n",
      "trainer/Policy mu Std                                    0.314129\r\n",
      "trainer/Policy mu Max                                    2.25861\r\n",
      "trainer/Policy mu Min                                   -1.72985\r\n",
      "trainer/Policy log std Mean                             -2.27345\r\n",
      "trainer/Policy log std Std                               0.557868\r\n",
      "trainer/Policy log std Max                              -0.3534\r\n",
      "trainer/Policy log std Min                              -3.12462\r\n",
      "trainer/Alpha                                            0.0210955\r\n",
      "trainer/Alpha Loss                                      -0.246942\r\n",
      "exploration/num steps total                           4600\r\n",
      "exploration/num paths total                            230\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0795748\r\n",
      "exploration/Rewards Std                                  0.0944403\r\n",
      "exploration/Rewards Max                                  0.112282\r\n",
      "exploration/Rewards Min                                 -0.341413\r\n",
      "exploration/Returns Mean                                -1.5915\r\n",
      "exploration/Returns Std                                  1.63505\r\n",
      "exploration/Returns Max                                  1.13\r\n",
      "exploration/Returns Min                                 -3.5346\r\n",
      "exploration/Actions Mean                                -0.00763083\r\n",
      "exploration/Actions Std                                  0.13031\r\n",
      "exploration/Actions Max                                  0.43694\r\n",
      "exploration/Actions Min                                 -0.492049\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.5915\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00227609\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00438705\r\n",
      "exploration/env_infos/final/reward_dist Max              0.011047\r\n",
      "exploration/env_infos/final/reward_dist Min              6.17515e-12\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00723898\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0141428\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0355232\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.29277e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.149889\r\n",
      "exploration/env_infos/reward_dist Std                    0.230354\r\n",
      "exploration/env_infos/reward_dist Max                    0.808675\r\n",
      "exploration/env_infos/reward_dist Min                    6.17515e-12\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.215276\r\n",
      "exploration/env_infos/final/reward_energy Std            0.159544\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0520282\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.492142\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.270284\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.145091\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.12556\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.540685\r\n",
      "exploration/env_infos/reward_energy Mean                -0.153594\r\n",
      "exploration/env_infos/reward_energy Std                  0.102404\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0113371\r\n",
      "exploration/env_infos/reward_energy Min                 -0.540685\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0141\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.224407\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.305665\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.411739\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00540735\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0094017\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.021847\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0063274\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00832377\r\n",
      "exploration/env_infos/end_effector_loc Std               0.139918\r\n",
      "exploration/env_infos/end_effector_loc Max               0.305665\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.411739\r\n",
      "evaluation/num steps total                           36000\r\n",
      "evaluation/num paths total                            1800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0726216\r\n",
      "evaluation/Rewards Std                                   0.0805609\r\n",
      "evaluation/Rewards Max                                   0.143964\r\n",
      "evaluation/Rewards Min                                  -0.567815\r\n",
      "evaluation/Returns Mean                                 -1.45243\r\n",
      "evaluation/Returns Std                                   1.29939\r\n",
      "evaluation/Returns Max                                   1.10687\r\n",
      "evaluation/Returns Min                                  -4.63886\r\n",
      "evaluation/Actions Mean                                 -0.000746167\r\n",
      "evaluation/Actions Std                                   0.0728619\r\n",
      "evaluation/Actions Max                                   0.743739\r\n",
      "evaluation/Actions Min                                  -0.597957\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.45243\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.125576\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.212716\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.86688\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.93355e-37\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00533571\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116613\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0751584\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.45765e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.134894\r\n",
      "evaluation/env_infos/reward_dist Std                     0.23056\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997463\r\n",
      "evaluation/env_infos/reward_dist Min                     2.93355e-37\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0482072\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0466425\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000284555\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.253728\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.1745\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.204381\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0187585\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.834149\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0644544\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0804018\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000284555\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.834149\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0342238\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.26522\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.442488\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.641251\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000869935\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00946152\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.037187\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0298979\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0184406\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.17705\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.442488\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.689336\r\n",
      "time/data storing (s)                                    0.00308145\r\n",
      "time/evaluation sampling (s)                             0.957582\r\n",
      "time/exploration sampling (s)                            0.126669\r\n",
      "time/logging (s)                                         0.0199518\r\n",
      "time/saving (s)                                          0.0334987\r\n",
      "time/training (s)                                       46.7208\r\n",
      "time/epoch (s)                                          47.8616\r\n",
      "time/total (s)                                        1701.83\r\n",
      "Epoch                                                   35\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:54:06.893160 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 36 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000605631\n",
      "trainer/QF2 Loss                                         0.000626211\n",
      "trainer/Policy Loss                                      3.27922\n",
      "trainer/Q1 Predictions Mean                             -1.2923\n",
      "trainer/Q1 Predictions Std                               0.770117\n",
      "trainer/Q1 Predictions Max                               0.316088\n",
      "trainer/Q1 Predictions Min                              -3.11537\n",
      "trainer/Q2 Predictions Mean                             -1.28661\n",
      "trainer/Q2 Predictions Std                               0.76713\n",
      "trainer/Q2 Predictions Max                               0.320936\n",
      "trainer/Q2 Predictions Min                              -3.06836\n",
      "trainer/Q Targets Mean                                  -1.29106\n",
      "trainer/Q Targets Std                                    0.770174\n",
      "trainer/Q Targets Max                                    0.33578\n",
      "trainer/Q Targets Min                                   -3.12505\n",
      "trainer/Log Pis Mean                                     1.99836\n",
      "trainer/Log Pis Std                                      1.30368\n",
      "trainer/Log Pis Max                                      4.56045\n",
      "trainer/Log Pis Min                                     -2.66712\n",
      "trainer/Policy mu Mean                                   0.0320942\n",
      "trainer/Policy mu Std                                    0.265762\n",
      "trainer/Policy mu Max                                    2.19052\n",
      "trainer/Policy mu Min                                   -1.17318\n",
      "trainer/Policy log std Mean                             -2.34607\n",
      "trainer/Policy log std Std                               0.539033\n",
      "trainer/Policy log std Max                              -0.520894\n",
      "trainer/Policy log std Min                              -3.2558\n",
      "trainer/Alpha                                            0.020808\n",
      "trainer/Alpha Loss                                      -0.0063619\n",
      "exploration/num steps total                           4700\n",
      "exploration/num paths total                            235\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.122402\n",
      "exploration/Rewards Std                                  0.0694997\n",
      "exploration/Rewards Max                                  0.0336042\n",
      "exploration/Rewards Min                                 -0.351463\n",
      "exploration/Returns Mean                                -2.44804\n",
      "exploration/Returns Std                                  0.916239\n",
      "exploration/Returns Max                                 -0.971969\n",
      "exploration/Returns Min                                 -3.5443\n",
      "exploration/Actions Mean                                 0.00713647\n",
      "exploration/Actions Std                                  0.0897322\n",
      "exploration/Actions Max                                  0.462318\n",
      "exploration/Actions Min                                 -0.26242\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.44804\n",
      "exploration/env_infos/final/reward_dist Mean             0.0353323\n",
      "exploration/env_infos/final/reward_dist Std              0.0706645\n",
      "exploration/env_infos/final/reward_dist Max              0.176661\n",
      "exploration/env_infos/final/reward_dist Min              2.54528e-33\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0045831\n",
      "exploration/env_infos/initial/reward_dist Std            0.0081152\n",
      "exploration/env_infos/initial/reward_dist Max            0.020783\n",
      "exploration/env_infos/initial/reward_dist Min            1.58025e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0461514\n",
      "exploration/env_infos/reward_dist Std                    0.153784\n",
      "exploration/env_infos/reward_dist Max                    0.838086\n",
      "exploration/env_infos/reward_dist Min                    2.54528e-33\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0893728\n",
      "exploration/env_infos/final/reward_energy Std            0.0278585\n",
      "exploration/env_infos/final/reward_energy Max           -0.0431968\n",
      "exploration/env_infos/final/reward_energy Min           -0.123315\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.15062\n",
      "exploration/env_infos/initial/reward_energy Std          0.158345\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0282364\n",
      "exploration/env_infos/initial/reward_energy Min         -0.462479\n",
      "exploration/env_infos/reward_energy Mean                -0.0997835\n",
      "exploration/env_infos/reward_energy Std                  0.0790497\n",
      "exploration/env_infos/reward_energy Max                 -0.00578606\n",
      "exploration/env_infos/reward_energy Min                 -0.462479\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0663997\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275907\n",
      "exploration/env_infos/final/end_effector_loc Max         0.493978\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.392582\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00188392\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00749334\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0231159\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00475101\n",
      "exploration/env_infos/end_effector_loc Mean              0.0265999\n",
      "exploration/env_infos/end_effector_loc Std               0.160198\n",
      "exploration/env_infos/end_effector_loc Max               0.493978\n",
      "exploration/env_infos/end_effector_loc Min              -0.392582\n",
      "evaluation/num steps total                           37000\n",
      "evaluation/num paths total                            1850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0675885\n",
      "evaluation/Rewards Std                                   0.0827899\n",
      "evaluation/Rewards Max                                   0.160845\n",
      "evaluation/Rewards Min                                  -0.480762\n",
      "evaluation/Returns Mean                                 -1.35177\n",
      "evaluation/Returns Std                                   1.32298\n",
      "evaluation/Returns Max                                   1.52933\n",
      "evaluation/Returns Min                                  -4.65521\n",
      "evaluation/Actions Mean                                  0.00619177\n",
      "evaluation/Actions Std                                   0.0895627\n",
      "evaluation/Actions Max                                   0.933886\n",
      "evaluation/Actions Min                                  -0.648239\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.35177\n",
      "evaluation/env_infos/final/reward_dist Mean              0.130794\n",
      "evaluation/env_infos/final/reward_dist Std               0.245266\n",
      "evaluation/env_infos/final/reward_dist Max               0.967942\n",
      "evaluation/env_infos/final/reward_dist Min               3.13799e-53\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00716443\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00966465\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0367492\n",
      "evaluation/env_infos/initial/reward_dist Min             1.39948e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.134682\n",
      "evaluation/env_infos/reward_dist Std                     0.22906\n",
      "evaluation/env_infos/reward_dist Max                     0.997701\n",
      "evaluation/env_infos/reward_dist Min                     3.13799e-53\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0406197\n",
      "evaluation/env_infos/final/reward_energy Std             0.035818\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00428035\n",
      "evaluation/env_infos/final/reward_energy Min            -0.205486\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.246199\n",
      "evaluation/env_infos/initial/reward_energy Std           0.261537\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0156449\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.97082\n",
      "evaluation/env_infos/reward_energy Mean                 -0.075024\n",
      "evaluation/env_infos/reward_energy Std                   0.102426\n",
      "evaluation/env_infos/reward_energy Max                  -0.00118013\n",
      "evaluation/env_infos/reward_energy Min                  -0.97082\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.105188\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.247231\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.941617\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.448258\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00412429\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0120108\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0466943\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0324119\n",
      "evaluation/env_infos/end_effector_loc Mean               0.055323\n",
      "evaluation/env_infos/end_effector_loc Std                0.176673\n",
      "evaluation/env_infos/end_effector_loc Max                0.941617\n",
      "evaluation/env_infos/end_effector_loc Min               -0.562808\n",
      "time/data storing (s)                                    0.00297228\n",
      "time/evaluation sampling (s)                             1.07534\n",
      "time/exploration sampling (s)                            0.132309\n",
      "time/logging (s)                                         0.0191431\n",
      "time/saving (s)                                          0.0281384\n",
      "time/training (s)                                       46.797\n",
      "time/epoch (s)                                          48.0549\n",
      "time/total (s)                                        1750.38\n",
      "Epoch                                                   36\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:54:55.585387 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 37 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000478914\n",
      "trainer/QF2 Loss                                         0.00061002\n",
      "trainer/Policy Loss                                      3.20777\n",
      "trainer/Q1 Predictions Mean                             -1.25784\n",
      "trainer/Q1 Predictions Std                               0.780462\n",
      "trainer/Q1 Predictions Max                               0.10543\n",
      "trainer/Q1 Predictions Min                              -3.36135\n",
      "trainer/Q2 Predictions Mean                             -1.26037\n",
      "trainer/Q2 Predictions Std                               0.784712\n",
      "trainer/Q2 Predictions Max                               0.0904833\n",
      "trainer/Q2 Predictions Min                              -3.37937\n",
      "trainer/Q Targets Mean                                  -1.25452\n",
      "trainer/Q Targets Std                                    0.78277\n",
      "trainer/Q Targets Max                                    0.140852\n",
      "trainer/Q Targets Min                                   -3.3592\n",
      "trainer/Log Pis Mean                                     1.95308\n",
      "trainer/Log Pis Std                                      1.39233\n",
      "trainer/Log Pis Max                                      4.49226\n",
      "trainer/Log Pis Min                                     -4.11478\n",
      "trainer/Policy mu Mean                                   0.0185315\n",
      "trainer/Policy mu Std                                    0.29171\n",
      "trainer/Policy mu Max                                    2.04366\n",
      "trainer/Policy mu Min                                   -1.71033\n",
      "trainer/Policy log std Mean                             -2.35222\n",
      "trainer/Policy log std Std                               0.553983\n",
      "trainer/Policy log std Max                              -0.417703\n",
      "trainer/Policy log std Min                              -3.30163\n",
      "trainer/Alpha                                            0.0210571\n",
      "trainer/Alpha Loss                                      -0.181161\n",
      "exploration/num steps total                           4800\n",
      "exploration/num paths total                            240\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.114128\n",
      "exploration/Rewards Std                                  0.073588\n",
      "exploration/Rewards Max                                  0.024674\n",
      "exploration/Rewards Min                                 -0.338285\n",
      "exploration/Returns Mean                                -2.28257\n",
      "exploration/Returns Std                                  1.04006\n",
      "exploration/Returns Max                                 -0.948907\n",
      "exploration/Returns Min                                 -4.10179\n",
      "exploration/Actions Mean                                 0.005233\n",
      "exploration/Actions Std                                  0.0944893\n",
      "exploration/Actions Max                                  0.429682\n",
      "exploration/Actions Min                                 -0.38747\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.28257\n",
      "exploration/env_infos/final/reward_dist Mean             0.213596\n",
      "exploration/env_infos/final/reward_dist Std              0.373236\n",
      "exploration/env_infos/final/reward_dist Max              0.956741\n",
      "exploration/env_infos/final/reward_dist Min              3.59703e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00584195\n",
      "exploration/env_infos/initial/reward_dist Std            0.011241\n",
      "exploration/env_infos/initial/reward_dist Max            0.0283149\n",
      "exploration/env_infos/initial/reward_dist Min            1.49293e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.171957\n",
      "exploration/env_infos/reward_dist Std                    0.277847\n",
      "exploration/env_infos/reward_dist Max                    0.99032\n",
      "exploration/env_infos/reward_dist Min                    1.49293e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.112556\n",
      "exploration/env_infos/final/reward_energy Std            0.0197922\n",
      "exploration/env_infos/final/reward_energy Max           -0.0876452\n",
      "exploration/env_infos/final/reward_energy Min           -0.143331\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.17816\n",
      "exploration/env_infos/initial/reward_energy Std          0.138614\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0575201\n",
      "exploration/env_infos/initial/reward_energy Min         -0.440084\n",
      "exploration/env_infos/reward_energy Mean                -0.113082\n",
      "exploration/env_infos/reward_energy Std                  0.0715795\n",
      "exploration/env_infos/reward_energy Max                 -0.0170477\n",
      "exploration/env_infos/reward_energy Min                 -0.440084\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.100949\n",
      "exploration/env_infos/final/end_effector_loc Std         0.208494\n",
      "exploration/env_infos/final/end_effector_loc Max         0.348982\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.255912\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0045759\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00653871\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0214841\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00248805\n",
      "exploration/env_infos/end_effector_loc Mean              0.0566068\n",
      "exploration/env_infos/end_effector_loc Std               0.1448\n",
      "exploration/env_infos/end_effector_loc Max               0.348982\n",
      "exploration/env_infos/end_effector_loc Min              -0.255912\n",
      "evaluation/num steps total                           38000\n",
      "evaluation/num paths total                            1900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.072401\n",
      "evaluation/Rewards Std                                   0.0788888\n",
      "evaluation/Rewards Max                                   0.145412\n",
      "evaluation/Rewards Min                                  -0.451531\n",
      "evaluation/Returns Mean                                 -1.44802\n",
      "evaluation/Returns Std                                   1.25622\n",
      "evaluation/Returns Max                                   1.63671\n",
      "evaluation/Returns Min                                  -4.53131\n",
      "evaluation/Actions Mean                                  0.00359544\n",
      "evaluation/Actions Std                                   0.0750294\n",
      "evaluation/Actions Max                                   0.889578\n",
      "evaluation/Actions Min                                  -0.556226\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.44802\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0830005\n",
      "evaluation/env_infos/final/reward_dist Std               0.186767\n",
      "evaluation/env_infos/final/reward_dist Max               0.916391\n",
      "evaluation/env_infos/final/reward_dist Min               2.58885e-29\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00542448\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0089922\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0453284\n",
      "evaluation/env_infos/initial/reward_dist Min             1.62572e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.14984\n",
      "evaluation/env_infos/reward_dist Std                     0.254803\n",
      "evaluation/env_infos/reward_dist Max                     0.995023\n",
      "evaluation/env_infos/reward_dist Min                     2.58885e-29\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0627122\n",
      "evaluation/env_infos/final/reward_energy Std             0.0710985\n",
      "evaluation/env_infos/final/reward_energy Max            -0.003445\n",
      "evaluation/env_infos/final/reward_energy Min            -0.378386\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.202046\n",
      "evaluation/env_infos/initial/reward_energy Std           0.23204\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.000982548\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.04916\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0649262\n",
      "evaluation/env_infos/reward_energy Std                   0.084079\n",
      "evaluation/env_infos/reward_energy Max                  -0.000982548\n",
      "evaluation/env_infos/reward_energy Min                  -1.04916\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0927944\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.231278\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.592756\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.416722\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00228665\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010635\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0444789\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0278113\n",
      "evaluation/env_infos/end_effector_loc Mean               0.045263\n",
      "evaluation/env_infos/end_effector_loc Std                0.152889\n",
      "evaluation/env_infos/end_effector_loc Max                0.592756\n",
      "evaluation/env_infos/end_effector_loc Min               -0.416722\n",
      "time/data storing (s)                                    0.0046635\n",
      "time/evaluation sampling (s)                             0.945931\n",
      "time/exploration sampling (s)                            0.122238\n",
      "time/logging (s)                                         0.0189699\n",
      "time/saving (s)                                          0.0287787\n",
      "time/training (s)                                       47.0396\n",
      "time/epoch (s)                                          48.1602\n",
      "time/total (s)                                        1799.07\n",
      "Epoch                                                   37\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:55:44.965920 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 38 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000709893\n",
      "trainer/QF2 Loss                                         0.000669694\n",
      "trainer/Policy Loss                                      3.16473\n",
      "trainer/Q1 Predictions Mean                             -1.24494\n",
      "trainer/Q1 Predictions Std                               0.806771\n",
      "trainer/Q1 Predictions Max                               0.823547\n",
      "trainer/Q1 Predictions Min                              -3.09551\n",
      "trainer/Q2 Predictions Mean                             -1.24495\n",
      "trainer/Q2 Predictions Std                               0.808075\n",
      "trainer/Q2 Predictions Max                               0.825101\n",
      "trainer/Q2 Predictions Min                              -3.06\n",
      "trainer/Q Targets Mean                                  -1.2452\n",
      "trainer/Q Targets Std                                    0.810376\n",
      "trainer/Q Targets Max                                    0.823385\n",
      "trainer/Q Targets Min                                   -3.14826\n",
      "trainer/Log Pis Mean                                     1.91563\n",
      "trainer/Log Pis Std                                      1.34852\n",
      "trainer/Log Pis Max                                      4.1649\n",
      "trainer/Log Pis Min                                     -4.68074\n",
      "trainer/Policy mu Mean                                   0.0130495\n",
      "trainer/Policy mu Std                                    0.230862\n",
      "trainer/Policy mu Max                                    1.7075\n",
      "trainer/Policy mu Min                                   -1.62628\n",
      "trainer/Policy log std Mean                             -2.36426\n",
      "trainer/Policy log std Std                               0.534829\n",
      "trainer/Policy log std Max                              -0.507135\n",
      "trainer/Policy log std Min                              -3.12252\n",
      "trainer/Alpha                                            0.0216825\n",
      "trainer/Alpha Loss                                      -0.323071\n",
      "exploration/num steps total                           4900\n",
      "exploration/num paths total                            245\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.119867\n",
      "exploration/Rewards Std                                  0.0591079\n",
      "exploration/Rewards Max                                  0.00324871\n",
      "exploration/Rewards Min                                 -0.337458\n",
      "exploration/Returns Mean                                -2.39735\n",
      "exploration/Returns Std                                  0.67777\n",
      "exploration/Returns Max                                 -1.43607\n",
      "exploration/Returns Min                                 -3.05605\n",
      "exploration/Actions Mean                                -0.00741399\n",
      "exploration/Actions Std                                  0.145643\n",
      "exploration/Actions Max                                  0.538114\n",
      "exploration/Actions Min                                 -0.873221\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.39735\n",
      "exploration/env_infos/final/reward_dist Mean             0.0037306\n",
      "exploration/env_infos/final/reward_dist Std              0.00720544\n",
      "exploration/env_infos/final/reward_dist Max              0.0181367\n",
      "exploration/env_infos/final/reward_dist Min              2.23936e-23\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00439563\n",
      "exploration/env_infos/initial/reward_dist Std            0.00501227\n",
      "exploration/env_infos/initial/reward_dist Max            0.0115229\n",
      "exploration/env_infos/initial/reward_dist Min            2.31067e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0215425\n",
      "exploration/env_infos/reward_dist Std                    0.045021\n",
      "exploration/env_infos/reward_dist Max                    0.219322\n",
      "exploration/env_infos/reward_dist Min                    2.23936e-23\n",
      "exploration/env_infos/final/reward_energy Mean          -0.118173\n",
      "exploration/env_infos/final/reward_energy Std            0.0338859\n",
      "exploration/env_infos/final/reward_energy Max           -0.0563318\n",
      "exploration/env_infos/final/reward_energy Min           -0.159307\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.418842\n",
      "exploration/env_infos/initial/reward_energy Std          0.30533\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0627019\n",
      "exploration/env_infos/initial/reward_energy Min         -0.912623\n",
      "exploration/env_infos/reward_energy Mean                -0.151197\n",
      "exploration/env_infos/reward_energy Std                  0.140261\n",
      "exploration/env_infos/reward_energy Max                 -0.0100802\n",
      "exploration/env_infos/reward_energy Min                 -0.912623\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0723041\n",
      "exploration/env_infos/final/end_effector_loc Std         0.301271\n",
      "exploration/env_infos/final/end_effector_loc Max         0.572492\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.542393\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00326211\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0180327\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0211642\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0436611\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0334776\n",
      "exploration/env_infos/end_effector_loc Std               0.208733\n",
      "exploration/env_infos/end_effector_loc Max               0.572492\n",
      "exploration/env_infos/end_effector_loc Min              -0.542393\n",
      "evaluation/num steps total                           39000\n",
      "evaluation/num paths total                            1950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0677237\n",
      "evaluation/Rewards Std                                   0.0785379\n",
      "evaluation/Rewards Max                                   0.15477\n",
      "evaluation/Rewards Min                                  -0.485431\n",
      "evaluation/Returns Mean                                 -1.35447\n",
      "evaluation/Returns Std                                   1.24179\n",
      "evaluation/Returns Max                                   1.4804\n",
      "evaluation/Returns Min                                  -3.63833\n",
      "evaluation/Actions Mean                                  0.00500738\n",
      "evaluation/Actions Std                                   0.0877938\n",
      "evaluation/Actions Max                                   0.788563\n",
      "evaluation/Actions Min                                  -0.710088\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.35447\n",
      "evaluation/env_infos/final/reward_dist Mean              0.115904\n",
      "evaluation/env_infos/final/reward_dist Std               0.197942\n",
      "evaluation/env_infos/final/reward_dist Max               0.849919\n",
      "evaluation/env_infos/final/reward_dist Min               2.40591e-15\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00524921\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0122458\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0767013\n",
      "evaluation/env_infos/initial/reward_dist Min             1.37391e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.154826\n",
      "evaluation/env_infos/reward_dist Std                     0.241374\n",
      "evaluation/env_infos/reward_dist Max                     0.996201\n",
      "evaluation/env_infos/reward_dist Min                     2.40591e-15\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0340604\n",
      "evaluation/env_infos/final/reward_energy Std             0.0304063\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00191928\n",
      "evaluation/env_infos/final/reward_energy Min            -0.177506\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.300135\n",
      "evaluation/env_infos/initial/reward_energy Std           0.258532\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00992488\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.955547\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0760907\n",
      "evaluation/env_infos/reward_energy Std                   0.0983659\n",
      "evaluation/env_infos/reward_energy Max                  -0.00136889\n",
      "evaluation/env_infos/reward_energy Min                  -0.955547\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0665309\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.238011\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.631617\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.506816\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000112451\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0140049\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0394282\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0355044\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0230084\n",
      "evaluation/env_infos/end_effector_loc Std                0.168105\n",
      "evaluation/env_infos/end_effector_loc Max                0.631617\n",
      "evaluation/env_infos/end_effector_loc Min               -0.506816\n",
      "time/data storing (s)                                    0.00303799\n",
      "time/evaluation sampling (s)                             0.977547\n",
      "time/exploration sampling (s)                            0.128789\n",
      "time/logging (s)                                         0.0208401\n",
      "time/saving (s)                                          0.0263786\n",
      "time/training (s)                                       47.7263\n",
      "time/epoch (s)                                          48.8828\n",
      "time/total (s)                                        1848.45\n",
      "Epoch                                                   38\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:56:34.198910 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 39 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000599681\n",
      "trainer/QF2 Loss                                         0.000993493\n",
      "trainer/Policy Loss                                      3.20033\n",
      "trainer/Q1 Predictions Mean                             -1.17695\n",
      "trainer/Q1 Predictions Std                               0.742562\n",
      "trainer/Q1 Predictions Max                               0.465623\n",
      "trainer/Q1 Predictions Min                              -3.06184\n",
      "trainer/Q2 Predictions Mean                             -1.17315\n",
      "trainer/Q2 Predictions Std                               0.741192\n",
      "trainer/Q2 Predictions Max                               0.458362\n",
      "trainer/Q2 Predictions Min                              -3.05362\n",
      "trainer/Q Targets Mean                                  -1.18372\n",
      "trainer/Q Targets Std                                    0.739572\n",
      "trainer/Q Targets Max                                    0.453457\n",
      "trainer/Q Targets Min                                   -3.05665\n",
      "trainer/Log Pis Mean                                     2.02124\n",
      "trainer/Log Pis Std                                      1.40537\n",
      "trainer/Log Pis Max                                      4.15086\n",
      "trainer/Log Pis Min                                     -3.56272\n",
      "trainer/Policy mu Mean                                  -0.0114388\n",
      "trainer/Policy mu Std                                    0.193637\n",
      "trainer/Policy mu Max                                    1.02487\n",
      "trainer/Policy mu Min                                   -1.74578\n",
      "trainer/Policy log std Mean                             -2.40552\n",
      "trainer/Policy log std Std                               0.533291\n",
      "trainer/Policy log std Max                              -0.620102\n",
      "trainer/Policy log std Min                              -3.16634\n",
      "trainer/Alpha                                            0.0212451\n",
      "trainer/Alpha Loss                                       0.0818544\n",
      "exploration/num steps total                           5000\n",
      "exploration/num paths total                            250\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0878613\n",
      "exploration/Rewards Std                                  0.0479503\n",
      "exploration/Rewards Max                                  0.000648297\n",
      "exploration/Rewards Min                                 -0.231784\n",
      "exploration/Returns Mean                                -1.75723\n",
      "exploration/Returns Std                                  0.620113\n",
      "exploration/Returns Max                                 -0.96165\n",
      "exploration/Returns Min                                 -2.62138\n",
      "exploration/Actions Mean                                -0.000760485\n",
      "exploration/Actions Std                                  0.076859\n",
      "exploration/Actions Max                                  0.243629\n",
      "exploration/Actions Min                                 -0.286061\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.75723\n",
      "exploration/env_infos/final/reward_dist Mean             0.144432\n",
      "exploration/env_infos/final/reward_dist Std              0.144781\n",
      "exploration/env_infos/final/reward_dist Max              0.377852\n",
      "exploration/env_infos/final/reward_dist Min              1.58067e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00690956\n",
      "exploration/env_infos/initial/reward_dist Std            0.0049987\n",
      "exploration/env_infos/initial/reward_dist Max            0.0140139\n",
      "exploration/env_infos/initial/reward_dist Min            6.5385e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0743844\n",
      "exploration/env_infos/reward_dist Std                    0.114904\n",
      "exploration/env_infos/reward_dist Max                    0.448983\n",
      "exploration/env_infos/reward_dist Min                    1.58067e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0575939\n",
      "exploration/env_infos/final/reward_energy Std            0.0319958\n",
      "exploration/env_infos/final/reward_energy Max           -0.0294491\n",
      "exploration/env_infos/final/reward_energy Min           -0.107363\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0802943\n",
      "exploration/env_infos/initial/reward_energy Std          0.0554765\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0246659\n",
      "exploration/env_infos/initial/reward_energy Min         -0.148315\n",
      "exploration/env_infos/reward_energy Mean                -0.0932643\n",
      "exploration/env_infos/reward_energy Std                  0.055835\n",
      "exploration/env_infos/reward_energy Max                 -0.0177855\n",
      "exploration/env_infos/reward_energy Min                 -0.289577\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00213125\n",
      "exploration/env_infos/final/end_effector_loc Std         0.172134\n",
      "exploration/env_infos/final/end_effector_loc Max         0.198807\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.341642\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000158937\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00344685\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00496141\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00630682\n",
      "exploration/env_infos/end_effector_loc Mean              0.00260502\n",
      "exploration/env_infos/end_effector_loc Std               0.109488\n",
      "exploration/env_infos/end_effector_loc Max               0.240129\n",
      "exploration/env_infos/end_effector_loc Min              -0.341642\n",
      "evaluation/num steps total                           40000\n",
      "evaluation/num paths total                            2000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0620722\n",
      "evaluation/Rewards Std                                   0.0652007\n",
      "evaluation/Rewards Max                                   0.113681\n",
      "evaluation/Rewards Min                                  -0.295942\n",
      "evaluation/Returns Mean                                 -1.24144\n",
      "evaluation/Returns Std                                   1.04463\n",
      "evaluation/Returns Max                                   1.14738\n",
      "evaluation/Returns Min                                  -3.26202\n",
      "evaluation/Actions Mean                                 -0.000104147\n",
      "evaluation/Actions Std                                   0.0592643\n",
      "evaluation/Actions Max                                   0.581588\n",
      "evaluation/Actions Min                                  -0.670397\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.24144\n",
      "evaluation/env_infos/final/reward_dist Mean              0.217677\n",
      "evaluation/env_infos/final/reward_dist Std               0.308197\n",
      "evaluation/env_infos/final/reward_dist Max               0.980079\n",
      "evaluation/env_infos/final/reward_dist Min               1.10536e-12\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00620091\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0153886\n",
      "evaluation/env_infos/initial/reward_dist Max             0.080859\n",
      "evaluation/env_infos/initial/reward_dist Min             1.59024e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.189278\n",
      "evaluation/env_infos/reward_dist Std                     0.277066\n",
      "evaluation/env_infos/reward_dist Max                     0.995948\n",
      "evaluation/env_infos/reward_dist Min                     1.10536e-12\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0319272\n",
      "evaluation/env_infos/final/reward_energy Std             0.0403296\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00124271\n",
      "evaluation/env_infos/final/reward_energy Min            -0.207715\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.147048\n",
      "evaluation/env_infos/initial/reward_energy Std           0.152158\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0134551\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.685786\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0508773\n",
      "evaluation/env_infos/reward_energy Std                   0.0666036\n",
      "evaluation/env_infos/reward_energy Max                  -0.000832061\n",
      "evaluation/env_infos/reward_energy Min                  -0.685786\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00149691\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.212658\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.388273\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.535446\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000127981\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00748014\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0290794\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0321602\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00113944\n",
      "evaluation/env_infos/end_effector_loc Std                0.139535\n",
      "evaluation/env_infos/end_effector_loc Max                0.388273\n",
      "evaluation/env_infos/end_effector_loc Min               -0.535446\n",
      "time/data storing (s)                                    0.00303274\n",
      "time/evaluation sampling (s)                             1.04249\n",
      "time/exploration sampling (s)                            0.128576\n",
      "time/logging (s)                                         0.0198837\n",
      "time/saving (s)                                          0.0273886\n",
      "time/training (s)                                       47.5236\n",
      "time/epoch (s)                                          48.745\n",
      "time/total (s)                                        1897.68\n",
      "Epoch                                                   39\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:57:23.996077 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 40 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000472922\r\n",
      "trainer/QF2 Loss                                         0.000627347\r\n",
      "trainer/Policy Loss                                      3.57672\r\n",
      "trainer/Q1 Predictions Mean                             -1.24836\r\n",
      "trainer/Q1 Predictions Std                               0.849226\r\n",
      "trainer/Q1 Predictions Max                               0.812357\r\n",
      "trainer/Q1 Predictions Min                              -3.49059\r\n",
      "trainer/Q2 Predictions Mean                             -1.25727\r\n",
      "trainer/Q2 Predictions Std                               0.854329\r\n",
      "trainer/Q2 Predictions Max                               0.795855\r\n",
      "trainer/Q2 Predictions Min                              -3.52104\r\n",
      "trainer/Q Targets Mean                                  -1.25259\r\n",
      "trainer/Q Targets Std                                    0.852995\r\n",
      "trainer/Q Targets Max                                    0.815927\r\n",
      "trainer/Q Targets Min                                   -3.53714\r\n",
      "trainer/Log Pis Mean                                     2.33122\r\n",
      "trainer/Log Pis Std                                      1.14259\r\n",
      "trainer/Log Pis Max                                      4.52423\r\n",
      "trainer/Log Pis Min                                     -1.42159\r\n",
      "trainer/Policy mu Mean                                   0.00227673\r\n",
      "trainer/Policy mu Std                                    0.181763\r\n",
      "trainer/Policy mu Max                                    0.959961\r\n",
      "trainer/Policy mu Min                                   -1.69594\r\n",
      "trainer/Policy log std Mean                             -2.48799\r\n",
      "trainer/Policy log std Std                               0.468314\r\n",
      "trainer/Policy log std Max                              -0.298582\r\n",
      "trainer/Policy log std Min                              -3.27929\r\n",
      "trainer/Alpha                                            0.0219262\r\n",
      "trainer/Alpha Loss                                       1.26528\r\n",
      "exploration/num steps total                           5100\r\n",
      "exploration/num paths total                            255\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.128128\r\n",
      "exploration/Rewards Std                                  0.0565535\r\n",
      "exploration/Rewards Max                                  0.0141675\r\n",
      "exploration/Rewards Min                                 -0.276301\r\n",
      "exploration/Returns Mean                                -2.56256\r\n",
      "exploration/Returns Std                                  0.769104\r\n",
      "exploration/Returns Max                                 -1.10142\r\n",
      "exploration/Returns Min                                 -3.21845\r\n",
      "exploration/Actions Mean                                 0.00499555\r\n",
      "exploration/Actions Std                                  0.174426\r\n",
      "exploration/Actions Max                                  0.543405\r\n",
      "exploration/Actions Min                                 -0.684274\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.56256\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.166963\r\n",
      "exploration/env_infos/final/reward_dist Std              0.307587\r\n",
      "exploration/env_infos/final/reward_dist Max              0.780948\r\n",
      "exploration/env_infos/final/reward_dist Min              1.14756e-11\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00702498\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0139059\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0348364\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.38913e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.104056\r\n",
      "exploration/env_infos/reward_dist Std                    0.198211\r\n",
      "exploration/env_infos/reward_dist Max                    0.84455\r\n",
      "exploration/env_infos/reward_dist Min                    1.14756e-11\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0685416\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0528541\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.00923155\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.166851\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.266674\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.252018\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0588264\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.684386\r\n",
      "exploration/env_infos/reward_energy Mean                -0.18475\r\n",
      "exploration/env_infos/reward_energy Std                  0.163605\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00923155\r\n",
      "exploration/env_infos/reward_energy Min                 -0.72312\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0322486\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.262746\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.37748\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.454455\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00541435\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0117885\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00395661\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0342137\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0513177\r\n",
      "exploration/env_infos/end_effector_loc Std               0.135811\r\n",
      "exploration/env_infos/end_effector_loc Max               0.37748\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.454455\r\n",
      "evaluation/num steps total                           41000\r\n",
      "evaluation/num paths total                            2050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0448905\r\n",
      "evaluation/Rewards Std                                   0.0774582\r\n",
      "evaluation/Rewards Max                                   0.142465\r\n",
      "evaluation/Rewards Min                                  -0.273529\r\n",
      "evaluation/Returns Mean                                 -0.89781\r\n",
      "evaluation/Returns Std                                   1.22358\r\n",
      "evaluation/Returns Max                                   1.57651\r\n",
      "evaluation/Returns Min                                  -3.41745\r\n",
      "evaluation/Actions Mean                                  0.00543332\r\n",
      "evaluation/Actions Std                                   0.0591869\r\n",
      "evaluation/Actions Max                                   0.397638\r\n",
      "evaluation/Actions Min                                  -0.451406\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.89781\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.199916\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.286096\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.950568\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.71141e-43\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00522038\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00776596\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0300724\r\n",
      "evaluation/env_infos/initial/reward_dist Min             9.61035e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.19379\r\n",
      "evaluation/env_infos/reward_dist Std                     0.275412\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999862\r\n",
      "evaluation/env_infos/reward_dist Min                     9.71141e-43\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0355357\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.026707\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00295833\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.106319\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.155964\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.127927\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0117579\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.480844\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.056391\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0623319\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000854299\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.484473\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0627012\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.24452\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.852845\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.4289\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000719107\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00709545\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0198819\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0225703\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0231385\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.154933\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.852845\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.4289\r\n",
      "time/data storing (s)                                    0.00296992\r\n",
      "time/evaluation sampling (s)                             0.980987\r\n",
      "time/exploration sampling (s)                            0.133396\r\n",
      "time/logging (s)                                         0.0191136\r\n",
      "time/saving (s)                                          0.0262884\r\n",
      "time/training (s)                                       48.1154\r\n",
      "time/epoch (s)                                          49.2781\r\n",
      "time/total (s)                                        1947.48\r\n",
      "Epoch                                                   40\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:58:14.034606 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 41 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000747012\r\n",
      "trainer/QF2 Loss                                         0.000716691\r\n",
      "trainer/Policy Loss                                      3.22332\r\n",
      "trainer/Q1 Predictions Mean                             -1.24725\r\n",
      "trainer/Q1 Predictions Std                               0.819687\r\n",
      "trainer/Q1 Predictions Max                               0.372572\r\n",
      "trainer/Q1 Predictions Min                              -3.02345\r\n",
      "trainer/Q2 Predictions Mean                             -1.24745\r\n",
      "trainer/Q2 Predictions Std                               0.817364\r\n",
      "trainer/Q2 Predictions Max                               0.35331\r\n",
      "trainer/Q2 Predictions Min                              -3.00914\r\n",
      "trainer/Q Targets Mean                                  -1.24107\r\n",
      "trainer/Q Targets Std                                    0.815637\r\n",
      "trainer/Q Targets Max                                    0.345243\r\n",
      "trainer/Q Targets Min                                   -2.98973\r\n",
      "trainer/Log Pis Mean                                     1.9767\r\n",
      "trainer/Log Pis Std                                      1.37722\r\n",
      "trainer/Log Pis Max                                      4.21065\r\n",
      "trainer/Log Pis Min                                     -3.74701\r\n",
      "trainer/Policy mu Mean                                  -0.014389\r\n",
      "trainer/Policy mu Std                                    0.226946\r\n",
      "trainer/Policy mu Max                                    0.952786\r\n",
      "trainer/Policy mu Min                                   -1.92989\r\n",
      "trainer/Policy log std Mean                             -2.3508\r\n",
      "trainer/Policy log std Std                               0.525453\r\n",
      "trainer/Policy log std Max                              -0.118352\r\n",
      "trainer/Policy log std Min                              -3.08316\r\n",
      "trainer/Alpha                                            0.0222943\r\n",
      "trainer/Alpha Loss                                      -0.088621\r\n",
      "exploration/num steps total                           5200\r\n",
      "exploration/num paths total                            260\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.108586\r\n",
      "exploration/Rewards Std                                  0.089061\r\n",
      "exploration/Rewards Max                                  0.0780773\r\n",
      "exploration/Rewards Min                                 -0.363096\r\n",
      "exploration/Returns Mean                                -2.17172\r\n",
      "exploration/Returns Std                                  0.476525\r\n",
      "exploration/Returns Max                                 -1.513\r\n",
      "exploration/Returns Min                                 -2.99671\r\n",
      "exploration/Actions Mean                                -0.000347208\r\n",
      "exploration/Actions Std                                  0.126289\r\n",
      "exploration/Actions Max                                  0.595977\r\n",
      "exploration/Actions Min                                 -0.368301\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.17172\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0515242\r\n",
      "exploration/env_infos/final/reward_dist Std              0.096562\r\n",
      "exploration/env_infos/final/reward_dist Max              0.244467\r\n",
      "exploration/env_infos/final/reward_dist Min              1.37169e-21\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00943608\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00567965\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0159991\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000409653\r\n",
      "exploration/env_infos/reward_dist Mean                   0.161945\r\n",
      "exploration/env_infos/reward_dist Std                    0.253269\r\n",
      "exploration/env_infos/reward_dist Max                    0.988925\r\n",
      "exploration/env_infos/reward_dist Min                    1.37169e-21\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0975759\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0689862\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0261655\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.22667\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.186507\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.112063\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0618262\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.368315\r\n",
      "exploration/env_infos/reward_energy Mean                -0.141516\r\n",
      "exploration/env_infos/reward_energy Std                  0.108955\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00648332\r\n",
      "exploration/env_infos/reward_energy Min                 -0.597092\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0242527\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.294609\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.661142\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.379402\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000176617\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00769073\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0130303\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.018415\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00394223\r\n",
      "exploration/env_infos/end_effector_loc Std               0.200957\r\n",
      "exploration/env_infos/end_effector_loc Max               0.670875\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.379402\r\n",
      "evaluation/num steps total                           42000\r\n",
      "evaluation/num paths total                            2100\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0378611\r\n",
      "evaluation/Rewards Std                                   0.0735909\r\n",
      "evaluation/Rewards Max                                   0.168612\r\n",
      "evaluation/Rewards Min                                  -0.359448\r\n",
      "evaluation/Returns Mean                                 -0.757221\r\n",
      "evaluation/Returns Std                                   1.16719\r\n",
      "evaluation/Returns Max                                   2.23396\r\n",
      "evaluation/Returns Min                                  -3.95632\r\n",
      "evaluation/Actions Mean                                  0.00121715\r\n",
      "evaluation/Actions Std                                   0.0748966\r\n",
      "evaluation/Actions Max                                   0.696414\r\n",
      "evaluation/Actions Min                                  -0.63322\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.757221\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.153036\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.245556\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.984794\r\n",
      "evaluation/env_infos/final/reward_dist Min               7.12841e-24\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00544539\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0114839\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0635339\r\n",
      "evaluation/env_infos/initial/reward_dist Min             4.06273e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.162361\r\n",
      "evaluation/env_infos/reward_dist Std                     0.244943\r\n",
      "evaluation/env_infos/reward_dist Max                     0.986074\r\n",
      "evaluation/env_infos/reward_dist Min                     7.12841e-24\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0454797\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0661946\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00126028\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.334627\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.22456\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.217492\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0186805\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.748676\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0646599\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0839111\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00113334\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.748676\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0621651\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.220084\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.633095\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.505645\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0027727\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106993\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0348207\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.031661\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0388569\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.148087\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.633095\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.505645\r\n",
      "time/data storing (s)                                    0.00293002\r\n",
      "time/evaluation sampling (s)                             1.16047\r\n",
      "time/exploration sampling (s)                            0.124084\r\n",
      "time/logging (s)                                         0.0198531\r\n",
      "time/saving (s)                                          0.0281677\r\n",
      "time/training (s)                                       48.1775\r\n",
      "time/epoch (s)                                          49.513\r\n",
      "time/total (s)                                        1997.52\r\n",
      "Epoch                                                   41\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:59:02.992978 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 42 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000850917\r\n",
      "trainer/QF2 Loss                                         0.000875528\r\n",
      "trainer/Policy Loss                                      3.30849\r\n",
      "trainer/Q1 Predictions Mean                             -1.26606\r\n",
      "trainer/Q1 Predictions Std                               0.840976\r\n",
      "trainer/Q1 Predictions Max                               0.398435\r\n",
      "trainer/Q1 Predictions Min                              -3.44398\r\n",
      "trainer/Q2 Predictions Mean                             -1.27072\r\n",
      "trainer/Q2 Predictions Std                               0.83733\r\n",
      "trainer/Q2 Predictions Max                               0.383272\r\n",
      "trainer/Q2 Predictions Min                              -3.41526\r\n",
      "trainer/Q Targets Mean                                  -1.26838\r\n",
      "trainer/Q Targets Std                                    0.842464\r\n",
      "trainer/Q Targets Max                                    0.432687\r\n",
      "trainer/Q Targets Min                                   -3.4265\r\n",
      "trainer/Log Pis Mean                                     2.04169\r\n",
      "trainer/Log Pis Std                                      1.29696\r\n",
      "trainer/Log Pis Max                                      4.18876\r\n",
      "trainer/Log Pis Min                                     -3.54574\r\n",
      "trainer/Policy mu Mean                                  -0.0471084\r\n",
      "trainer/Policy mu Std                                    0.2494\r\n",
      "trainer/Policy mu Max                                    0.849315\r\n",
      "trainer/Policy mu Min                                   -2.06625\r\n",
      "trainer/Policy log std Mean                             -2.34516\r\n",
      "trainer/Policy log std Std                               0.555005\r\n",
      "trainer/Policy log std Max                              -0.40944\r\n",
      "trainer/Policy log std Min                              -3.24337\r\n",
      "trainer/Alpha                                            0.0223974\r\n",
      "trainer/Alpha Loss                                       0.158394\r\n",
      "exploration/num steps total                           5300\r\n",
      "exploration/num paths total                            265\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0431316\r\n",
      "exploration/Rewards Std                                  0.0976292\r\n",
      "exploration/Rewards Max                                  0.156133\r\n",
      "exploration/Rewards Min                                 -0.231852\r\n",
      "exploration/Returns Mean                                -0.862632\r\n",
      "exploration/Returns Std                                  1.60861\r\n",
      "exploration/Returns Max                                  1.11907\r\n",
      "exploration/Returns Min                                 -2.83472\r\n",
      "exploration/Actions Mean                                 0.00843724\r\n",
      "exploration/Actions Std                                  0.146911\r\n",
      "exploration/Actions Max                                  0.72211\r\n",
      "exploration/Actions Min                                 -0.54174\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.862632\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.20898\r\n",
      "exploration/env_infos/final/reward_dist Std              0.371134\r\n",
      "exploration/env_infos/final/reward_dist Max              0.950795\r\n",
      "exploration/env_infos/final/reward_dist Min              0.0104473\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0069302\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.010399\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0274205\r\n",
      "exploration/env_infos/initial/reward_dist Min            8.05678e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.255152\r\n",
      "exploration/env_infos/reward_dist Std                    0.292324\r\n",
      "exploration/env_infos/reward_dist Max                    0.994649\r\n",
      "exploration/env_infos/reward_dist Min                    3.32693e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.150789\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0299189\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0921139\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.17491\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.217276\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.20601\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0943882\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.628005\r\n",
      "exploration/env_infos/reward_energy Mean                -0.166247\r\n",
      "exploration/env_infos/reward_energy Std                  0.125181\r\n",
      "exploration/env_infos/reward_energy Max                 -0.013504\r\n",
      "exploration/env_infos/reward_energy Min                 -0.726907\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0832773\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.20932\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.367984\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.253348\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00335396\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0100405\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0264398\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00558222\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0477682\r\n",
      "exploration/env_infos/end_effector_loc Std               0.142517\r\n",
      "exploration/env_infos/end_effector_loc Max               0.367984\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.279784\r\n",
      "evaluation/num steps total                           43000\r\n",
      "evaluation/num paths total                            2150\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0523175\r\n",
      "evaluation/Rewards Std                                   0.0680046\r\n",
      "evaluation/Rewards Max                                   0.113837\r\n",
      "evaluation/Rewards Min                                  -0.316292\r\n",
      "evaluation/Returns Mean                                 -1.04635\r\n",
      "evaluation/Returns Std                                   1.03263\r\n",
      "evaluation/Returns Max                                   1.00249\r\n",
      "evaluation/Returns Min                                  -3.51896\r\n",
      "evaluation/Actions Mean                                 -0.00314619\r\n",
      "evaluation/Actions Std                                   0.0586854\r\n",
      "evaluation/Actions Max                                   0.371142\r\n",
      "evaluation/Actions Min                                  -0.53612\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.04635\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.119154\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.202417\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.834441\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.55731e-22\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00609618\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0104669\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0528491\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.54296e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.138369\r\n",
      "evaluation/env_infos/reward_dist Std                     0.221431\r\n",
      "evaluation/env_infos/reward_dist Max                     0.995074\r\n",
      "evaluation/env_infos/reward_dist Min                     9.55731e-22\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0400891\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0471676\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00107949\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.247729\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.153027\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.132133\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0169855\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.560632\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0537658\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0633797\r\n",
      "evaluation/env_infos/reward_energy Max                  -8.70494e-05\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.593021\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0523264\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.251819\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.66553\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.578466\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00152484\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00698358\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0185571\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.026806\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0283597\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.155945\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.66553\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.578466\r\n",
      "time/data storing (s)                                    0.00296025\r\n",
      "time/evaluation sampling (s)                             0.959177\r\n",
      "time/exploration sampling (s)                            0.121693\r\n",
      "time/logging (s)                                         0.0201969\r\n",
      "time/saving (s)                                          0.0277381\r\n",
      "time/training (s)                                       47.2361\r\n",
      "time/epoch (s)                                          48.3679\r\n",
      "time/total (s)                                        2046.47\r\n",
      "Epoch                                                   42\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:59:53.639985 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 43 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000701614\n",
      "trainer/QF2 Loss                                         0.000675195\n",
      "trainer/Policy Loss                                      3.07476\n",
      "trainer/Q1 Predictions Mean                             -1.2415\n",
      "trainer/Q1 Predictions Std                               0.786815\n",
      "trainer/Q1 Predictions Max                               0.240751\n",
      "trainer/Q1 Predictions Min                              -3.42513\n",
      "trainer/Q2 Predictions Mean                             -1.24719\n",
      "trainer/Q2 Predictions Std                               0.788789\n",
      "trainer/Q2 Predictions Max                               0.24001\n",
      "trainer/Q2 Predictions Min                              -3.4348\n",
      "trainer/Q Targets Mean                                  -1.24297\n",
      "trainer/Q Targets Std                                    0.793317\n",
      "trainer/Q Targets Max                                    0.26578\n",
      "trainer/Q Targets Min                                   -3.43635\n",
      "trainer/Log Pis Mean                                     1.83313\n",
      "trainer/Log Pis Std                                      1.28414\n",
      "trainer/Log Pis Max                                      4.44941\n",
      "trainer/Log Pis Min                                     -2.81072\n",
      "trainer/Policy mu Mean                                  -0.0552155\n",
      "trainer/Policy mu Std                                    0.328977\n",
      "trainer/Policy mu Max                                    0.936544\n",
      "trainer/Policy mu Min                                   -2.62048\n",
      "trainer/Policy log std Mean                             -2.24213\n",
      "trainer/Policy log std Std                               0.615682\n",
      "trainer/Policy log std Max                              -0.169917\n",
      "trainer/Policy log std Min                              -3.19286\n",
      "trainer/Alpha                                            0.0219053\n",
      "trainer/Alpha Loss                                      -0.63744\n",
      "exploration/num steps total                           5400\n",
      "exploration/num paths total                            270\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0858878\n",
      "exploration/Rewards Std                                  0.0689896\n",
      "exploration/Rewards Max                                  0.0639313\n",
      "exploration/Rewards Min                                 -0.269376\n",
      "exploration/Returns Mean                                -1.71776\n",
      "exploration/Returns Std                                  0.999047\n",
      "exploration/Returns Max                                 -0.0894993\n",
      "exploration/Returns Min                                 -2.7722\n",
      "exploration/Actions Mean                                -0.00211149\n",
      "exploration/Actions Std                                  0.107425\n",
      "exploration/Actions Max                                  0.296136\n",
      "exploration/Actions Min                                 -0.472578\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.71776\n",
      "exploration/env_infos/final/reward_dist Mean             0.00769153\n",
      "exploration/env_infos/final/reward_dist Std              0.00788375\n",
      "exploration/env_infos/final/reward_dist Max              0.0180249\n",
      "exploration/env_infos/final/reward_dist Min              2.68527e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00136561\n",
      "exploration/env_infos/initial/reward_dist Std            0.00154054\n",
      "exploration/env_infos/initial/reward_dist Max            0.00411628\n",
      "exploration/env_infos/initial/reward_dist Min            1.95066e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.132176\n",
      "exploration/env_infos/reward_dist Std                    0.217536\n",
      "exploration/env_infos/reward_dist Max                    0.877957\n",
      "exploration/env_infos/reward_dist Min                    2.68527e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.116339\n",
      "exploration/env_infos/final/reward_energy Std            0.0507543\n",
      "exploration/env_infos/final/reward_energy Max           -0.0734598\n",
      "exploration/env_infos/final/reward_energy Min           -0.215515\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.193666\n",
      "exploration/env_infos/initial/reward_energy Std          0.179065\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0332592\n",
      "exploration/env_infos/initial/reward_energy Min         -0.526532\n",
      "exploration/env_infos/reward_energy Mean                -0.122136\n",
      "exploration/env_infos/reward_energy Std                  0.0903985\n",
      "exploration/env_infos/reward_energy Max                 -0.0179513\n",
      "exploration/env_infos/reward_energy Min                 -0.526532\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0371892\n",
      "exploration/env_infos/final/end_effector_loc Std         0.211536\n",
      "exploration/env_infos/final/end_effector_loc Max         0.347999\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.325093\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000255723\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0093219\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0116088\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0236289\n",
      "exploration/env_infos/end_effector_loc Mean              0.0223937\n",
      "exploration/env_infos/end_effector_loc Std               0.127985\n",
      "exploration/env_infos/end_effector_loc Max               0.347999\n",
      "exploration/env_infos/end_effector_loc Min              -0.325093\n",
      "evaluation/num steps total                           44000\n",
      "evaluation/num paths total                            2200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0586977\n",
      "evaluation/Rewards Std                                   0.0775934\n",
      "evaluation/Rewards Max                                   0.104497\n",
      "evaluation/Rewards Min                                  -0.463564\n",
      "evaluation/Returns Mean                                 -1.17395\n",
      "evaluation/Returns Std                                   1.26158\n",
      "evaluation/Returns Max                                   1.04113\n",
      "evaluation/Returns Min                                  -5.61074\n",
      "evaluation/Actions Mean                                 -0.0041032\n",
      "evaluation/Actions Std                                   0.0771713\n",
      "evaluation/Actions Max                                   0.477392\n",
      "evaluation/Actions Min                                  -0.648114\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.17395\n",
      "evaluation/env_infos/final/reward_dist Mean              0.126497\n",
      "evaluation/env_infos/final/reward_dist Std               0.256468\n",
      "evaluation/env_infos/final/reward_dist Max               0.959871\n",
      "evaluation/env_infos/final/reward_dist Min               5.46059e-40\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00519711\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126464\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0833445\n",
      "evaluation/env_infos/initial/reward_dist Min             1.02164e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.136444\n",
      "evaluation/env_infos/reward_dist Std                     0.230421\n",
      "evaluation/env_infos/reward_dist Max                     0.99041\n",
      "evaluation/env_infos/reward_dist Min                     5.46059e-40\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0452094\n",
      "evaluation/env_infos/final/reward_energy Std             0.0382867\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00628401\n",
      "evaluation/env_infos/final/reward_energy Min            -0.216881\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.175961\n",
      "evaluation/env_infos/initial/reward_energy Std           0.17863\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00673251\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.683875\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0692886\n",
      "evaluation/env_infos/reward_energy Std                   0.0845196\n",
      "evaluation/env_infos/reward_energy Max                  -0.0022403\n",
      "evaluation/env_infos/reward_energy Min                  -0.697775\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0322904\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.234744\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.691879\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.470574\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00112831\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00879294\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0238696\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0324057\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0144875\n",
      "evaluation/env_infos/end_effector_loc Std                0.156009\n",
      "evaluation/env_infos/end_effector_loc Max                0.691879\n",
      "evaluation/env_infos/end_effector_loc Min               -0.470574\n",
      "time/data storing (s)                                    0.00295611\n",
      "time/evaluation sampling (s)                             0.956482\n",
      "time/exploration sampling (s)                            0.119601\n",
      "time/logging (s)                                         0.0200073\n",
      "time/saving (s)                                          0.0274302\n",
      "time/training (s)                                       48.9737\n",
      "time/epoch (s)                                          50.1002\n",
      "time/total (s)                                        2097.12\n",
      "Epoch                                                   43\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:00:44.292976 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 44 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000748182\n",
      "trainer/QF2 Loss                                         0.00102367\n",
      "trainer/Policy Loss                                      3.47922\n",
      "trainer/Q1 Predictions Mean                             -1.23141\n",
      "trainer/Q1 Predictions Std                               0.798353\n",
      "trainer/Q1 Predictions Max                               0.464253\n",
      "trainer/Q1 Predictions Min                              -3.03907\n",
      "trainer/Q2 Predictions Mean                             -1.23692\n",
      "trainer/Q2 Predictions Std                               0.79921\n",
      "trainer/Q2 Predictions Max                               0.424\n",
      "trainer/Q2 Predictions Min                              -3.01259\n",
      "trainer/Q Targets Mean                                  -1.21919\n",
      "trainer/Q Targets Std                                    0.795592\n",
      "trainer/Q Targets Max                                    0.476011\n",
      "trainer/Q Targets Min                                   -3.00801\n",
      "trainer/Log Pis Mean                                     2.25467\n",
      "trainer/Log Pis Std                                      1.17681\n",
      "trainer/Log Pis Max                                      4.18947\n",
      "trainer/Log Pis Min                                     -2.71596\n",
      "trainer/Policy mu Mean                                  -0.00766301\n",
      "trainer/Policy mu Std                                    0.261707\n",
      "trainer/Policy mu Max                                    1.55361\n",
      "trainer/Policy mu Min                                   -1.77585\n",
      "trainer/Policy log std Mean                             -2.38917\n",
      "trainer/Policy log std Std                               0.561883\n",
      "trainer/Policy log std Max                               0.134769\n",
      "trainer/Policy log std Min                              -3.23257\n",
      "trainer/Alpha                                            0.0236469\n",
      "trainer/Alpha Loss                                       0.953495\n",
      "exploration/num steps total                           5500\n",
      "exploration/num paths total                            275\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0479709\n",
      "exploration/Rewards Std                                  0.082753\n",
      "exploration/Rewards Max                                  0.103674\n",
      "exploration/Rewards Min                                 -0.279683\n",
      "exploration/Returns Mean                                -0.959418\n",
      "exploration/Returns Std                                  1.42687\n",
      "exploration/Returns Max                                  0.840224\n",
      "exploration/Returns Min                                 -3.09562\n",
      "exploration/Actions Mean                                -0.00842861\n",
      "exploration/Actions Std                                  0.171472\n",
      "exploration/Actions Max                                  0.534538\n",
      "exploration/Actions Min                                 -0.52086\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.959418\n",
      "exploration/env_infos/final/reward_dist Mean             0.432434\n",
      "exploration/env_infos/final/reward_dist Std              0.39814\n",
      "exploration/env_infos/final/reward_dist Max              0.998297\n",
      "exploration/env_infos/final/reward_dist Min              1.11105e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00589768\n",
      "exploration/env_infos/initial/reward_dist Std            0.00722452\n",
      "exploration/env_infos/initial/reward_dist Max            0.0196201\n",
      "exploration/env_infos/initial/reward_dist Min            1.67762e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.337003\n",
      "exploration/env_infos/reward_dist Std                    0.355849\n",
      "exploration/env_infos/reward_dist Max                    0.999681\n",
      "exploration/env_infos/reward_dist Min                    1.11105e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.125655\n",
      "exploration/env_infos/final/reward_energy Std            0.0993442\n",
      "exploration/env_infos/final/reward_energy Max           -0.0186167\n",
      "exploration/env_infos/final/reward_energy Min           -0.300706\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.312951\n",
      "exploration/env_infos/initial/reward_energy Std          0.281435\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0352025\n",
      "exploration/env_infos/initial/reward_energy Min         -0.724098\n",
      "exploration/env_infos/reward_energy Mean                -0.196518\n",
      "exploration/env_infos/reward_energy Std                  0.142577\n",
      "exploration/env_infos/reward_energy Max                 -0.00647169\n",
      "exploration/env_infos/reward_energy Min                 -0.724098\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0622881\n",
      "exploration/env_infos/final/end_effector_loc Std         0.168431\n",
      "exploration/env_infos/final/end_effector_loc Max         0.194777\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.371081\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00201908\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0147429\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0267269\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.026043\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0321917\n",
      "exploration/env_infos/end_effector_loc Std               0.142831\n",
      "exploration/env_infos/end_effector_loc Max               0.230049\n",
      "exploration/env_infos/end_effector_loc Min              -0.371081\n",
      "evaluation/num steps total                           45000\n",
      "evaluation/num paths total                            2250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0468926\n",
      "evaluation/Rewards Std                                   0.0632369\n",
      "evaluation/Rewards Max                                   0.12741\n",
      "evaluation/Rewards Min                                  -0.271024\n",
      "evaluation/Returns Mean                                 -0.937851\n",
      "evaluation/Returns Std                                   0.942558\n",
      "evaluation/Returns Max                                   1.45192\n",
      "evaluation/Returns Min                                  -3.01985\n",
      "evaluation/Actions Mean                                  0.0032215\n",
      "evaluation/Actions Std                                   0.0638241\n",
      "evaluation/Actions Max                                   0.461854\n",
      "evaluation/Actions Min                                  -0.476166\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.937851\n",
      "evaluation/env_infos/final/reward_dist Mean              0.106158\n",
      "evaluation/env_infos/final/reward_dist Std               0.215342\n",
      "evaluation/env_infos/final/reward_dist Max               0.848232\n",
      "evaluation/env_infos/final/reward_dist Min               1.91472e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00487184\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0103034\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0579828\n",
      "evaluation/env_infos/initial/reward_dist Min             1.80714e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.188391\n",
      "evaluation/env_infos/reward_dist Std                     0.276533\n",
      "evaluation/env_infos/reward_dist Max                     0.997011\n",
      "evaluation/env_infos/reward_dist Min                     1.91472e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0453677\n",
      "evaluation/env_infos/final/reward_energy Std             0.0395575\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00274374\n",
      "evaluation/env_infos/final/reward_energy Min            -0.153075\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.173995\n",
      "evaluation/env_infos/initial/reward_energy Std           0.161721\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0110972\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.608174\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0619193\n",
      "evaluation/env_infos/reward_energy Std                   0.0658315\n",
      "evaluation/env_infos/reward_energy Max                  -0.000763547\n",
      "evaluation/env_infos/reward_energy Min                  -0.608174\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0537029\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.246393\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.601495\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.580937\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0006205\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00837553\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0230927\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0238083\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0233474\n",
      "evaluation/env_infos/end_effector_loc Std                0.152319\n",
      "evaluation/env_infos/end_effector_loc Max                0.601495\n",
      "evaluation/env_infos/end_effector_loc Min               -0.580937\n",
      "time/data storing (s)                                    0.00305638\n",
      "time/evaluation sampling (s)                             1.02163\n",
      "time/exploration sampling (s)                            0.139308\n",
      "time/logging (s)                                         0.0197278\n",
      "time/saving (s)                                          0.0281175\n",
      "time/training (s)                                       48.8621\n",
      "time/epoch (s)                                          50.0739\n",
      "time/total (s)                                        2147.77\n",
      "Epoch                                                   44\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:01:35.251941 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 45 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000711372\r\n",
      "trainer/QF2 Loss                                         0.000584052\r\n",
      "trainer/Policy Loss                                      3.23355\r\n",
      "trainer/Q1 Predictions Mean                             -1.17799\r\n",
      "trainer/Q1 Predictions Std                               0.765086\r\n",
      "trainer/Q1 Predictions Max                               0.494181\r\n",
      "trainer/Q1 Predictions Min                              -3.04878\r\n",
      "trainer/Q2 Predictions Mean                             -1.19311\r\n",
      "trainer/Q2 Predictions Std                               0.764668\r\n",
      "trainer/Q2 Predictions Max                               0.47158\r\n",
      "trainer/Q2 Predictions Min                              -3.02392\r\n",
      "trainer/Q Targets Mean                                  -1.18521\r\n",
      "trainer/Q Targets Std                                    0.758649\r\n",
      "trainer/Q Targets Max                                    0.421255\r\n",
      "trainer/Q Targets Min                                   -3.01927\r\n",
      "trainer/Log Pis Mean                                     2.05547\r\n",
      "trainer/Log Pis Std                                      1.47473\r\n",
      "trainer/Log Pis Max                                      4.33901\r\n",
      "trainer/Log Pis Min                                     -4.75551\r\n",
      "trainer/Policy mu Mean                                  -0.0376243\r\n",
      "trainer/Policy mu Std                                    0.269961\r\n",
      "trainer/Policy mu Max                                    1.65856\r\n",
      "trainer/Policy mu Min                                   -1.66792\r\n",
      "trainer/Policy log std Mean                             -2.3677\r\n",
      "trainer/Policy log std Std                               0.573836\r\n",
      "trainer/Policy log std Max                               0.106461\r\n",
      "trainer/Policy log std Min                              -3.24439\r\n",
      "trainer/Alpha                                            0.0219387\r\n",
      "trainer/Alpha Loss                                       0.211859\r\n",
      "exploration/num steps total                           5600\r\n",
      "exploration/num paths total                            280\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.047284\r\n",
      "exploration/Rewards Std                                  0.0528119\r\n",
      "exploration/Rewards Max                                  0.0788098\r\n",
      "exploration/Rewards Min                                 -0.17006\r\n",
      "exploration/Returns Mean                                -0.945679\r\n",
      "exploration/Returns Std                                  0.612891\r\n",
      "exploration/Returns Max                                 -0.139005\r\n",
      "exploration/Returns Min                                 -1.76834\r\n",
      "exploration/Actions Mean                                -0.0117357\r\n",
      "exploration/Actions Std                                  0.0845483\r\n",
      "exploration/Actions Max                                  0.291401\r\n",
      "exploration/Actions Min                                 -0.330207\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.945679\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.225307\r\n",
      "exploration/env_infos/final/reward_dist Std              0.179444\r\n",
      "exploration/env_infos/final/reward_dist Max              0.514492\r\n",
      "exploration/env_infos/final/reward_dist Min              3.88946e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00614593\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00614189\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0166657\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.1473e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.244683\r\n",
      "exploration/env_infos/reward_dist Std                    0.293866\r\n",
      "exploration/env_infos/reward_dist Max                    0.994921\r\n",
      "exploration/env_infos/reward_dist Min                    3.88946e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.137586\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0724317\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0577703\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.25774\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0724467\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0456279\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.026533\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.157387\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0997514\r\n",
      "exploration/env_infos/reward_energy Std                  0.0679847\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00510332\r\n",
      "exploration/env_infos/reward_energy Min                 -0.385931\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0400647\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.169492\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.254897\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.303288\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000930076\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00288063\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00531375\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0058044\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0086292\r\n",
      "exploration/env_infos/end_effector_loc Std               0.0976801\r\n",
      "exploration/env_infos/end_effector_loc Max               0.254897\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.303288\r\n",
      "evaluation/num steps total                           46000\r\n",
      "evaluation/num paths total                            2300\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0530775\r\n",
      "evaluation/Rewards Std                                   0.0932497\r\n",
      "evaluation/Rewards Max                                   0.14873\r\n",
      "evaluation/Rewards Min                                  -0.465293\r\n",
      "evaluation/Returns Mean                                 -1.06155\r\n",
      "evaluation/Returns Std                                   1.58134\r\n",
      "evaluation/Returns Max                                   2.01854\r\n",
      "evaluation/Returns Min                                  -6.25032\r\n",
      "evaluation/Actions Mean                                  0.000519342\r\n",
      "evaluation/Actions Std                                   0.0869449\r\n",
      "evaluation/Actions Max                                   0.633883\r\n",
      "evaluation/Actions Min                                  -0.684968\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.06155\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.144997\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.228853\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.954783\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.91716e-48\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0081041\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0135097\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0651729\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.87412e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.191669\r\n",
      "evaluation/env_infos/reward_dist Std                     0.271851\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998671\r\n",
      "evaluation/env_infos/reward_dist Min                     1.91716e-48\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0656291\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0767479\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00926416\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.293782\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.242403\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.229313\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0165183\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.876969\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0787594\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0944263\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00126959\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.876969\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0629101\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.22908\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.723135\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.384817\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00107889\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.011748\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0316941\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0342484\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0262031\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.158691\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.723135\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.384817\r\n",
      "time/data storing (s)                                    0.00295078\r\n",
      "time/evaluation sampling (s)                             1.08838\r\n",
      "time/exploration sampling (s)                            0.132017\r\n",
      "time/logging (s)                                         0.0192029\r\n",
      "time/saving (s)                                          0.0273022\r\n",
      "time/training (s)                                       49.0983\r\n",
      "time/epoch (s)                                          50.3681\r\n",
      "time/total (s)                                        2198.73\r\n",
      "Epoch                                                   45\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:02:25.710948 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 46 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000731135\n",
      "trainer/QF2 Loss                                         0.0011455\n",
      "trainer/Policy Loss                                      3.24412\n",
      "trainer/Q1 Predictions Mean                             -1.08897\n",
      "trainer/Q1 Predictions Std                               0.826447\n",
      "trainer/Q1 Predictions Max                               0.776581\n",
      "trainer/Q1 Predictions Min                              -3.24344\n",
      "trainer/Q2 Predictions Mean                             -1.09035\n",
      "trainer/Q2 Predictions Std                               0.831832\n",
      "trainer/Q2 Predictions Max                               0.792003\n",
      "trainer/Q2 Predictions Min                              -3.24359\n",
      "trainer/Q Targets Mean                                  -1.09619\n",
      "trainer/Q Targets Std                                    0.828363\n",
      "trainer/Q Targets Max                                    0.810937\n",
      "trainer/Q Targets Min                                   -3.25975\n",
      "trainer/Log Pis Mean                                     2.16483\n",
      "trainer/Log Pis Std                                      1.27434\n",
      "trainer/Log Pis Max                                      4.41629\n",
      "trainer/Log Pis Min                                     -3.43697\n",
      "trainer/Policy mu Mean                                  -0.0261343\n",
      "trainer/Policy mu Std                                    0.350219\n",
      "trainer/Policy mu Max                                    1.53506\n",
      "trainer/Policy mu Min                                   -2.28192\n",
      "trainer/Policy log std Mean                             -2.33425\n",
      "trainer/Policy log std Std                               0.56743\n",
      "trainer/Policy log std Max                              -0.372923\n",
      "trainer/Policy log std Min                              -3.24036\n",
      "trainer/Alpha                                            0.0234274\n",
      "trainer/Alpha Loss                                       0.618813\n",
      "exploration/num steps total                           5700\n",
      "exploration/num paths total                            285\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0307633\n",
      "exploration/Rewards Std                                  0.10012\n",
      "exploration/Rewards Max                                  0.166475\n",
      "exploration/Rewards Min                                 -0.214093\n",
      "exploration/Returns Mean                                -0.615266\n",
      "exploration/Returns Std                                  1.82093\n",
      "exploration/Returns Max                                  2.323\n",
      "exploration/Returns Min                                 -2.53005\n",
      "exploration/Actions Mean                                 0.00610728\n",
      "exploration/Actions Std                                  0.15783\n",
      "exploration/Actions Max                                  0.520756\n",
      "exploration/Actions Min                                 -0.520308\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.615266\n",
      "exploration/env_infos/final/reward_dist Mean             0.0705296\n",
      "exploration/env_infos/final/reward_dist Std              0.0666859\n",
      "exploration/env_infos/final/reward_dist Max              0.173321\n",
      "exploration/env_infos/final/reward_dist Min              1.54049e-23\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0147467\n",
      "exploration/env_infos/initial/reward_dist Std            0.017984\n",
      "exploration/env_infos/initial/reward_dist Max            0.0503523\n",
      "exploration/env_infos/initial/reward_dist Min            0.00350369\n",
      "exploration/env_infos/reward_dist Mean                   0.222839\n",
      "exploration/env_infos/reward_dist Std                    0.31309\n",
      "exploration/env_infos/reward_dist Max                    0.997014\n",
      "exploration/env_infos/reward_dist Min                    1.54049e-23\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135396\n",
      "exploration/env_infos/final/reward_energy Std            0.0543594\n",
      "exploration/env_infos/final/reward_energy Max           -0.0738233\n",
      "exploration/env_infos/final/reward_energy Min           -0.219418\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.291178\n",
      "exploration/env_infos/initial/reward_energy Std          0.241808\n",
      "exploration/env_infos/initial/reward_energy Max         -0.00166394\n",
      "exploration/env_infos/initial/reward_energy Min         -0.613194\n",
      "exploration/env_infos/reward_energy Mean                -0.171586\n",
      "exploration/env_infos/reward_energy Std                  0.143016\n",
      "exploration/env_infos/reward_energy Max                 -0.00166394\n",
      "exploration/env_infos/reward_energy Min                 -0.617841\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0167907\n",
      "exploration/env_infos/final/end_effector_loc Std         0.277538\n",
      "exploration/env_infos/final/end_effector_loc Max         0.382034\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.648924\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00770275\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0109425\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00301881\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0260154\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0363368\n",
      "exploration/env_infos/end_effector_loc Std               0.176114\n",
      "exploration/env_infos/end_effector_loc Max               0.382034\n",
      "exploration/env_infos/end_effector_loc Min              -0.648924\n",
      "evaluation/num steps total                           47000\n",
      "evaluation/num paths total                            2350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0418353\n",
      "evaluation/Rewards Std                                   0.0687402\n",
      "evaluation/Rewards Max                                   0.165579\n",
      "evaluation/Rewards Min                                  -0.355658\n",
      "evaluation/Returns Mean                                 -0.836705\n",
      "evaluation/Returns Std                                   1.09906\n",
      "evaluation/Returns Max                                   1.28144\n",
      "evaluation/Returns Min                                  -3.28158\n",
      "evaluation/Actions Mean                                 -0.00293082\n",
      "evaluation/Actions Std                                   0.0597479\n",
      "evaluation/Actions Max                                   0.527181\n",
      "evaluation/Actions Min                                  -0.497226\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.836705\n",
      "evaluation/env_infos/final/reward_dist Mean              0.174195\n",
      "evaluation/env_infos/final/reward_dist Std               0.260631\n",
      "evaluation/env_infos/final/reward_dist Max               0.973332\n",
      "evaluation/env_infos/final/reward_dist Min               8.92665e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00675324\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0129852\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0599654\n",
      "evaluation/env_infos/initial/reward_dist Min             2.00439e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.181861\n",
      "evaluation/env_infos/reward_dist Std                     0.268184\n",
      "evaluation/env_infos/reward_dist Max                     0.998878\n",
      "evaluation/env_infos/reward_dist Min                     8.92665e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0349873\n",
      "evaluation/env_infos/final/reward_energy Std             0.0438961\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000704192\n",
      "evaluation/env_infos/final/reward_energy Min            -0.218094\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.142178\n",
      "evaluation/env_infos/initial/reward_energy Std           0.15502\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0184667\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.628238\n",
      "evaluation/env_infos/reward_energy Mean                 -0.053339\n",
      "evaluation/env_infos/reward_energy Std                   0.0656638\n",
      "evaluation/env_infos/reward_energy Max                  -0.000704192\n",
      "evaluation/env_infos/reward_energy Min                  -0.628238\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.011154\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.208737\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.386924\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.438762\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       2.45855e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00743684\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0263591\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0248613\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0010924\n",
      "evaluation/env_infos/end_effector_loc Std                0.135737\n",
      "evaluation/env_infos/end_effector_loc Max                0.386924\n",
      "evaluation/env_infos/end_effector_loc Min               -0.438762\n",
      "time/data storing (s)                                    0.00290145\n",
      "time/evaluation sampling (s)                             0.987762\n",
      "time/exploration sampling (s)                            0.140735\n",
      "time/logging (s)                                         0.0195866\n",
      "time/saving (s)                                          0.0415051\n",
      "time/training (s)                                       48.6614\n",
      "time/epoch (s)                                          49.8539\n",
      "time/total (s)                                        2249.19\n",
      "Epoch                                                   46\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:03:16.325036 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 47 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000774705\n",
      "trainer/QF2 Loss                                         0.000982771\n",
      "trainer/Policy Loss                                      3.06444\n",
      "trainer/Q1 Predictions Mean                             -1.14256\n",
      "trainer/Q1 Predictions Std                               0.847949\n",
      "trainer/Q1 Predictions Max                               0.584781\n",
      "trainer/Q1 Predictions Min                              -3.37716\n",
      "trainer/Q2 Predictions Mean                             -1.13176\n",
      "trainer/Q2 Predictions Std                               0.846155\n",
      "trainer/Q2 Predictions Max                               0.68082\n",
      "trainer/Q2 Predictions Min                              -3.35201\n",
      "trainer/Q Targets Mean                                  -1.14642\n",
      "trainer/Q Targets Std                                    0.850951\n",
      "trainer/Q Targets Max                                    0.621236\n",
      "trainer/Q Targets Min                                   -3.39536\n",
      "trainer/Log Pis Mean                                     1.92306\n",
      "trainer/Log Pis Std                                      1.39018\n",
      "trainer/Log Pis Max                                      4.06419\n",
      "trainer/Log Pis Min                                     -3.18251\n",
      "trainer/Policy mu Mean                                   0.00438545\n",
      "trainer/Policy mu Std                                    0.219616\n",
      "trainer/Policy mu Max                                    1.36695\n",
      "trainer/Policy mu Min                                   -1.24017\n",
      "trainer/Policy log std Mean                             -2.35851\n",
      "trainer/Policy log std Std                               0.537844\n",
      "trainer/Policy log std Max                              -0.630345\n",
      "trainer/Policy log std Min                              -3.17348\n",
      "trainer/Alpha                                            0.0222845\n",
      "trainer/Alpha Loss                                      -0.292622\n",
      "exploration/num steps total                           5800\n",
      "exploration/num paths total                            290\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.108283\n",
      "exploration/Rewards Std                                  0.0750876\n",
      "exploration/Rewards Max                                  0.0599312\n",
      "exploration/Rewards Min                                 -0.394537\n",
      "exploration/Returns Mean                                -2.16566\n",
      "exploration/Returns Std                                  0.461911\n",
      "exploration/Returns Max                                 -1.6923\n",
      "exploration/Returns Min                                 -3.05098\n",
      "exploration/Actions Mean                                 0.0026477\n",
      "exploration/Actions Std                                  0.127673\n",
      "exploration/Actions Max                                  0.56105\n",
      "exploration/Actions Min                                 -0.758632\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.16566\n",
      "exploration/env_infos/final/reward_dist Mean             0.0130267\n",
      "exploration/env_infos/final/reward_dist Std              0.0208692\n",
      "exploration/env_infos/final/reward_dist Max              0.0538719\n",
      "exploration/env_infos/final/reward_dist Min              9.44704e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00165926\n",
      "exploration/env_infos/initial/reward_dist Std            0.00202333\n",
      "exploration/env_infos/initial/reward_dist Max            0.00434629\n",
      "exploration/env_infos/initial/reward_dist Min            2.85964e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0874414\n",
      "exploration/env_infos/reward_dist Std                    0.175431\n",
      "exploration/env_infos/reward_dist Max                    0.870125\n",
      "exploration/env_infos/reward_dist Min                    9.44704e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0990031\n",
      "exploration/env_infos/final/reward_energy Std            0.0625659\n",
      "exploration/env_infos/final/reward_energy Max           -0.0131941\n",
      "exploration/env_infos/final/reward_energy Min           -0.161805\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.148668\n",
      "exploration/env_infos/initial/reward_energy Std          0.179622\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0276055\n",
      "exploration/env_infos/initial/reward_energy Min         -0.501463\n",
      "exploration/env_infos/reward_energy Mean                -0.1275\n",
      "exploration/env_infos/reward_energy Std                  0.127901\n",
      "exploration/env_infos/reward_energy Max                 -0.0109609\n",
      "exploration/env_infos/reward_energy Min                 -0.760287\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0609577\n",
      "exploration/env_infos/final/end_effector_loc Std         0.277129\n",
      "exploration/env_infos/final/end_effector_loc Max         0.338086\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.492948\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00317513\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00760766\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0249635\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00234285\n",
      "exploration/env_infos/end_effector_loc Mean              0.0363605\n",
      "exploration/env_infos/end_effector_loc Std               0.180014\n",
      "exploration/env_infos/end_effector_loc Max               0.552751\n",
      "exploration/env_infos/end_effector_loc Min              -0.492948\n",
      "evaluation/num steps total                           48000\n",
      "evaluation/num paths total                            2400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0569282\n",
      "evaluation/Rewards Std                                   0.0905301\n",
      "evaluation/Rewards Max                                   0.159303\n",
      "evaluation/Rewards Min                                  -0.64312\n",
      "evaluation/Returns Mean                                 -1.13856\n",
      "evaluation/Returns Std                                   1.47482\n",
      "evaluation/Returns Max                                   2.39439\n",
      "evaluation/Returns Min                                  -8.0949\n",
      "evaluation/Actions Mean                                 -0.00245383\n",
      "evaluation/Actions Std                                   0.0776349\n",
      "evaluation/Actions Max                                   0.794481\n",
      "evaluation/Actions Min                                  -0.580887\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.13856\n",
      "evaluation/env_infos/final/reward_dist Mean              0.185527\n",
      "evaluation/env_infos/final/reward_dist Std               0.276476\n",
      "evaluation/env_infos/final/reward_dist Max               0.905666\n",
      "evaluation/env_infos/final/reward_dist Min               6.06237e-55\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00615371\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00906853\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0366437\n",
      "evaluation/env_infos/initial/reward_dist Min             3.13296e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.163577\n",
      "evaluation/env_infos/reward_dist Std                     0.245385\n",
      "evaluation/env_infos/reward_dist Max                     0.988988\n",
      "evaluation/env_infos/reward_dist Min                     6.05746e-55\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0570947\n",
      "evaluation/env_infos/final/reward_energy Std             0.0839103\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00445416\n",
      "evaluation/env_infos/final/reward_energy Min            -0.423823\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.180761\n",
      "evaluation/env_infos/initial/reward_energy Std           0.194831\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0191576\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.794753\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0656209\n",
      "evaluation/env_infos/reward_energy Std                   0.0880926\n",
      "evaluation/env_infos/reward_energy Max                  -0.00219267\n",
      "evaluation/env_infos/reward_energy Min                  -0.794753\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0159003\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.272365\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.812746\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000841064\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00935866\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0397241\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0277926\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0111766\n",
      "evaluation/env_infos/end_effector_loc Std                0.173308\n",
      "evaluation/env_infos/end_effector_loc Max                0.826669\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00285794\n",
      "time/evaluation sampling (s)                             1.005\n",
      "time/exploration sampling (s)                            0.123496\n",
      "time/logging (s)                                         0.0203152\n",
      "time/saving (s)                                          0.0286376\n",
      "time/training (s)                                       48.7519\n",
      "time/epoch (s)                                          49.9323\n",
      "time/total (s)                                        2299.8\n",
      "Epoch                                                   47\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:04:07.084234 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 48 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000812128\n",
      "trainer/QF2 Loss                                         0.000894113\n",
      "trainer/Policy Loss                                      2.96098\n",
      "trainer/Q1 Predictions Mean                             -1.05747\n",
      "trainer/Q1 Predictions Std                               0.87824\n",
      "trainer/Q1 Predictions Max                               0.949891\n",
      "trainer/Q1 Predictions Min                              -3.4402\n",
      "trainer/Q2 Predictions Mean                             -1.04787\n",
      "trainer/Q2 Predictions Std                               0.875799\n",
      "trainer/Q2 Predictions Max                               0.918833\n",
      "trainer/Q2 Predictions Min                              -3.43295\n",
      "trainer/Q Targets Mean                                  -1.04485\n",
      "trainer/Q Targets Std                                    0.876854\n",
      "trainer/Q Targets Max                                    0.919985\n",
      "trainer/Q Targets Min                                   -3.38668\n",
      "trainer/Log Pis Mean                                     1.90101\n",
      "trainer/Log Pis Std                                      1.32671\n",
      "trainer/Log Pis Max                                      4.28463\n",
      "trainer/Log Pis Min                                     -2.79723\n",
      "trainer/Policy mu Mean                                  -0.0416288\n",
      "trainer/Policy mu Std                                    0.237069\n",
      "trainer/Policy mu Max                                    0.841938\n",
      "trainer/Policy mu Min                                   -1.63776\n",
      "trainer/Policy log std Mean                             -2.32188\n",
      "trainer/Policy log std Std                               0.548151\n",
      "trainer/Policy log std Max                              -0.451461\n",
      "trainer/Policy log std Min                              -3.21882\n",
      "trainer/Alpha                                            0.0235794\n",
      "trainer/Alpha Loss                                      -0.37087\n",
      "exploration/num steps total                           5900\n",
      "exploration/num paths total                            295\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.146647\n",
      "exploration/Rewards Std                                  0.0479953\n",
      "exploration/Rewards Max                                 -0.037941\n",
      "exploration/Rewards Min                                 -0.310911\n",
      "exploration/Returns Mean                                -2.93294\n",
      "exploration/Returns Std                                  0.546128\n",
      "exploration/Returns Max                                 -2.38807\n",
      "exploration/Returns Min                                 -3.88707\n",
      "exploration/Actions Mean                                -0.0157726\n",
      "exploration/Actions Std                                  0.0801929\n",
      "exploration/Actions Max                                  0.272226\n",
      "exploration/Actions Min                                 -0.271025\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.93294\n",
      "exploration/env_infos/final/reward_dist Mean             0.183623\n",
      "exploration/env_infos/final/reward_dist Std              0.264698\n",
      "exploration/env_infos/final/reward_dist Max              0.679839\n",
      "exploration/env_infos/final/reward_dist Min              2.64084e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00628251\n",
      "exploration/env_infos/initial/reward_dist Std            0.00897646\n",
      "exploration/env_infos/initial/reward_dist Max            0.0230351\n",
      "exploration/env_infos/initial/reward_dist Min            6.445e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0394018\n",
      "exploration/env_infos/reward_dist Std                    0.106806\n",
      "exploration/env_infos/reward_dist Max                    0.679839\n",
      "exploration/env_infos/reward_dist Min                    2.64084e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0649841\n",
      "exploration/env_infos/final/reward_energy Std            0.0248938\n",
      "exploration/env_infos/final/reward_energy Max           -0.0240804\n",
      "exploration/env_infos/final/reward_energy Min           -0.0927007\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.139215\n",
      "exploration/env_infos/initial/reward_energy Std          0.0586564\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0783967\n",
      "exploration/env_infos/initial/reward_energy Min         -0.243009\n",
      "exploration/env_infos/reward_energy Mean                -0.0983774\n",
      "exploration/env_infos/reward_energy Std                  0.0606733\n",
      "exploration/env_infos/reward_energy Max                 -0.00709111\n",
      "exploration/env_infos/reward_energy Min                 -0.293829\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.17989\n",
      "exploration/env_infos/final/end_effector_loc Std         0.228178\n",
      "exploration/env_infos/final/end_effector_loc Max         0.222055\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.595208\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000749055\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00528826\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0095394\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00729682\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0631703\n",
      "exploration/env_infos/end_effector_loc Std               0.135452\n",
      "exploration/env_infos/end_effector_loc Max               0.222055\n",
      "exploration/env_infos/end_effector_loc Min              -0.595208\n",
      "evaluation/num steps total                           49000\n",
      "evaluation/num paths total                            2450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0541914\n",
      "evaluation/Rewards Std                                   0.0821416\n",
      "evaluation/Rewards Max                                   0.173135\n",
      "evaluation/Rewards Min                                  -0.626226\n",
      "evaluation/Returns Mean                                 -1.08383\n",
      "evaluation/Returns Std                                   1.27306\n",
      "evaluation/Returns Max                                   2.13822\n",
      "evaluation/Returns Min                                  -4.21051\n",
      "evaluation/Actions Mean                                 -0.0037657\n",
      "evaluation/Actions Std                                   0.0695748\n",
      "evaluation/Actions Max                                   0.48012\n",
      "evaluation/Actions Min                                  -0.724343\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.08383\n",
      "evaluation/env_infos/final/reward_dist Mean              0.113957\n",
      "evaluation/env_infos/final/reward_dist Std               0.19304\n",
      "evaluation/env_infos/final/reward_dist Max               0.878264\n",
      "evaluation/env_infos/final/reward_dist Min               1.35699e-49\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00516275\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00930889\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0381481\n",
      "evaluation/env_infos/initial/reward_dist Min             1.6002e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.145049\n",
      "evaluation/env_infos/reward_dist Std                     0.234339\n",
      "evaluation/env_infos/reward_dist Max                     0.998005\n",
      "evaluation/env_infos/reward_dist Min                     1.35699e-49\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0613291\n",
      "evaluation/env_infos/final/reward_energy Std             0.127723\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00420815\n",
      "evaluation/env_infos/final/reward_energy Min            -0.728221\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.149562\n",
      "evaluation/env_infos/initial/reward_energy Std           0.157299\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00662381\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.696372\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0552113\n",
      "evaluation/env_infos/reward_energy Std                   0.0816174\n",
      "evaluation/env_infos/reward_energy Max                  -0.000392736\n",
      "evaluation/env_infos/reward_energy Min                  -0.728221\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0230599\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.26\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.580109\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00100097\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0076084\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.024006\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0321826\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0128841\n",
      "evaluation/env_infos/end_effector_loc Std                0.155643\n",
      "evaluation/env_infos/end_effector_loc Max                0.580109\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00290828\n",
      "time/evaluation sampling (s)                             0.99231\n",
      "time/exploration sampling (s)                            0.124966\n",
      "time/logging (s)                                         0.0194402\n",
      "time/saving (s)                                          0.0275424\n",
      "time/training (s)                                       48.9671\n",
      "time/epoch (s)                                          50.1342\n",
      "time/total (s)                                        2350.56\n",
      "Epoch                                                   48\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:04:57.930368 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 49 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000790089\r\n",
      "trainer/QF2 Loss                                         0.000909833\r\n",
      "trainer/Policy Loss                                      3.14872\r\n",
      "trainer/Q1 Predictions Mean                             -1.17362\r\n",
      "trainer/Q1 Predictions Std                               0.819447\r\n",
      "trainer/Q1 Predictions Max                               1.10229\r\n",
      "trainer/Q1 Predictions Min                              -3.27301\r\n",
      "trainer/Q2 Predictions Mean                             -1.17793\r\n",
      "trainer/Q2 Predictions Std                               0.819963\r\n",
      "trainer/Q2 Predictions Max                               1.07898\r\n",
      "trainer/Q2 Predictions Min                              -3.29812\r\n",
      "trainer/Q Targets Mean                                  -1.16026\r\n",
      "trainer/Q Targets Std                                    0.817276\r\n",
      "trainer/Q Targets Max                                    1.12376\r\n",
      "trainer/Q Targets Min                                   -3.26364\r\n",
      "trainer/Log Pis Mean                                     1.97034\r\n",
      "trainer/Log Pis Std                                      1.25284\r\n",
      "trainer/Log Pis Max                                      4.09925\r\n",
      "trainer/Log Pis Min                                     -2.16304\r\n",
      "trainer/Policy mu Mean                                  -0.0109839\r\n",
      "trainer/Policy mu Std                                    0.251774\r\n",
      "trainer/Policy mu Max                                    1.38071\r\n",
      "trainer/Policy mu Min                                   -1.64826\r\n",
      "trainer/Policy log std Mean                             -2.35616\r\n",
      "trainer/Policy log std Std                               0.557364\r\n",
      "trainer/Policy log std Max                              -0.373805\r\n",
      "trainer/Policy log std Min                              -3.25251\r\n",
      "trainer/Alpha                                            0.0230331\r\n",
      "trainer/Alpha Loss                                      -0.111848\r\n",
      "exploration/num steps total                           6000\r\n",
      "exploration/num paths total                            300\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0918758\r\n",
      "exploration/Rewards Std                                  0.079307\r\n",
      "exploration/Rewards Max                                  0.0731935\r\n",
      "exploration/Rewards Min                                 -0.253194\r\n",
      "exploration/Returns Mean                                -1.83752\r\n",
      "exploration/Returns Std                                  1.22137\r\n",
      "exploration/Returns Max                                 -0.210576\r\n",
      "exploration/Returns Min                                 -3.81122\r\n",
      "exploration/Actions Mean                                 0.0150513\r\n",
      "exploration/Actions Std                                  0.142931\r\n",
      "exploration/Actions Max                                  0.51355\r\n",
      "exploration/Actions Min                                 -0.476604\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.83752\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.227148\r\n",
      "exploration/env_infos/final/reward_dist Std              0.283692\r\n",
      "exploration/env_infos/final/reward_dist Max              0.695794\r\n",
      "exploration/env_infos/final/reward_dist Min              0.000102888\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00511143\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.009444\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0239766\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.89836e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.250638\r\n",
      "exploration/env_infos/reward_dist Std                    0.303744\r\n",
      "exploration/env_infos/reward_dist Max                    0.985887\r\n",
      "exploration/env_infos/reward_dist Min                    5.89836e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.142058\r\n",
      "exploration/env_infos/final/reward_energy Std            0.04869\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0927796\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.226253\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.210934\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.140638\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0809642\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.479591\r\n",
      "exploration/env_infos/reward_energy Mean                -0.16182\r\n",
      "exploration/env_infos/reward_energy Std                  0.122988\r\n",
      "exploration/env_infos/reward_energy Max                 -0.015301\r\n",
      "exploration/env_infos/reward_energy Min                 -0.585933\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0688178\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.204546\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.437379\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.273206\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00334609\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00831529\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00758251\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0238302\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00272038\r\n",
      "exploration/env_infos/end_effector_loc Std               0.14418\r\n",
      "exploration/env_infos/end_effector_loc Max               0.437379\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.273206\r\n",
      "evaluation/num steps total                           50000\r\n",
      "evaluation/num paths total                            2500\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0645907\r\n",
      "evaluation/Rewards Std                                   0.0694995\r\n",
      "evaluation/Rewards Max                                   0.121869\r\n",
      "evaluation/Rewards Min                                  -0.51852\r\n",
      "evaluation/Returns Mean                                 -1.29181\r\n",
      "evaluation/Returns Std                                   1.04072\r\n",
      "evaluation/Returns Max                                   1.48893\r\n",
      "evaluation/Returns Min                                  -3.67947\r\n",
      "evaluation/Actions Mean                                  0.00312979\r\n",
      "evaluation/Actions Std                                   0.0800667\r\n",
      "evaluation/Actions Max                                   0.919365\r\n",
      "evaluation/Actions Min                                  -0.600676\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.29181\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.133643\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.258193\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.944719\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.63041e-41\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0056937\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00796159\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0305873\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.87776e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.141818\r\n",
      "evaluation/env_infos/reward_dist Std                     0.238998\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999341\r\n",
      "evaluation/env_infos/reward_dist Min                     2.63041e-41\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0553054\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0678317\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00378141\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.367493\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.207392\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.226891\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0119363\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.961091\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0723403\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0872229\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00240827\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.961091\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0672111\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.237864\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.512369\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.876454\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00135258\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107835\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0459682\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0300338\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0340352\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.153224\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.512369\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.876454\r\n",
      "time/data storing (s)                                    0.00304457\r\n",
      "time/evaluation sampling (s)                             1.08001\r\n",
      "time/exploration sampling (s)                            0.122393\r\n",
      "time/logging (s)                                         0.0191793\r\n",
      "time/saving (s)                                          0.0293789\r\n",
      "time/training (s)                                       48.9631\r\n",
      "time/epoch (s)                                          50.2171\r\n",
      "time/total (s)                                        2401.4\r\n",
      "Epoch                                                   49\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:05:51.283340 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 50 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000646107\r\n",
      "trainer/QF2 Loss                                         0.000763435\r\n",
      "trainer/Policy Loss                                      3.08049\r\n",
      "trainer/Q1 Predictions Mean                             -1.12613\r\n",
      "trainer/Q1 Predictions Std                               0.847626\r\n",
      "trainer/Q1 Predictions Max                               1.23249\r\n",
      "trainer/Q1 Predictions Min                              -3.1243\r\n",
      "trainer/Q2 Predictions Mean                             -1.13239\r\n",
      "trainer/Q2 Predictions Std                               0.84789\r\n",
      "trainer/Q2 Predictions Max                               1.23346\r\n",
      "trainer/Q2 Predictions Min                              -3.14379\r\n",
      "trainer/Q Targets Mean                                  -1.12996\r\n",
      "trainer/Q Targets Std                                    0.846965\r\n",
      "trainer/Q Targets Max                                    1.27236\r\n",
      "trainer/Q Targets Min                                   -3.1479\r\n",
      "trainer/Log Pis Mean                                     1.94655\r\n",
      "trainer/Log Pis Std                                      1.29308\r\n",
      "trainer/Log Pis Max                                      4.42344\r\n",
      "trainer/Log Pis Min                                     -3.75637\r\n",
      "trainer/Policy mu Mean                                  -0.0159303\r\n",
      "trainer/Policy mu Std                                    0.25567\r\n",
      "trainer/Policy mu Max                                    1.33346\r\n",
      "trainer/Policy mu Min                                   -1.84889\r\n",
      "trainer/Policy log std Mean                             -2.33046\r\n",
      "trainer/Policy log std Std                               0.551454\r\n",
      "trainer/Policy log std Max                              -0.00246775\r\n",
      "trainer/Policy log std Min                              -3.22621\r\n",
      "trainer/Alpha                                            0.0226247\r\n",
      "trainer/Alpha Loss                                      -0.20253\r\n",
      "exploration/num steps total                           6100\r\n",
      "exploration/num paths total                            305\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0694542\r\n",
      "exploration/Rewards Std                                  0.0700169\r\n",
      "exploration/Rewards Max                                  0.107779\r\n",
      "exploration/Rewards Min                                 -0.236101\r\n",
      "exploration/Returns Mean                                -1.38908\r\n",
      "exploration/Returns Std                                  0.996937\r\n",
      "exploration/Returns Max                                  0.188735\r\n",
      "exploration/Returns Min                                 -2.8631\r\n",
      "exploration/Actions Mean                                -0.00530229\r\n",
      "exploration/Actions Std                                  0.0883433\r\n",
      "exploration/Actions Max                                  0.351393\r\n",
      "exploration/Actions Min                                 -0.403368\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.38908\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0459113\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0604781\r\n",
      "exploration/env_infos/final/reward_dist Max              0.162781\r\n",
      "exploration/env_infos/final/reward_dist Min              1.28882e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00874509\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0094665\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.026402\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.81177e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.246123\r\n",
      "exploration/env_infos/reward_dist Std                    0.298077\r\n",
      "exploration/env_infos/reward_dist Max                    0.991014\r\n",
      "exploration/env_infos/reward_dist Min                    1.28882e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.160259\r\n",
      "exploration/env_infos/final/reward_energy Std            0.126515\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0814626\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.410211\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.124858\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.128788\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0269304\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.37607\r\n",
      "exploration/env_infos/reward_energy Mean                -0.102595\r\n",
      "exploration/env_infos/reward_energy Std                  0.0716914\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00549877\r\n",
      "exploration/env_infos/reward_energy Min                 -0.410211\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.112286\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.203921\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.255814\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.372387\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000279998\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00633573\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0175697\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00669907\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0508431\r\n",
      "exploration/env_infos/end_effector_loc Std               0.12762\r\n",
      "exploration/env_infos/end_effector_loc Max               0.255814\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.372387\r\n",
      "evaluation/num steps total                           51000\r\n",
      "evaluation/num paths total                            2550\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0394188\r\n",
      "evaluation/Rewards Std                                   0.068004\r\n",
      "evaluation/Rewards Max                                   0.159538\r\n",
      "evaluation/Rewards Min                                  -0.378376\r\n",
      "evaluation/Returns Mean                                 -0.788376\r\n",
      "evaluation/Returns Std                                   1.08727\r\n",
      "evaluation/Returns Max                                   1.71132\r\n",
      "evaluation/Returns Min                                  -2.56107\r\n",
      "evaluation/Actions Mean                                  8.65382e-05\r\n",
      "evaluation/Actions Std                                   0.0566716\r\n",
      "evaluation/Actions Max                                   0.715245\r\n",
      "evaluation/Actions Min                                  -0.668209\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.788376\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.226187\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.286432\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.915394\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.85922e-13\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00605323\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00964549\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0418206\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.85426e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.171169\r\n",
      "evaluation/env_infos/reward_dist Std                     0.271545\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993102\r\n",
      "evaluation/env_infos/reward_dist Min                     5.85922e-13\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0470937\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0466738\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00944293\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.197267\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.153707\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.18391\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.014149\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.755233\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0459281\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0656808\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00090866\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.755233\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00523209\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.209677\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.638184\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.527048\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000843556\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00843206\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0357622\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0334105\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00479586\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.131875\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.638184\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.527048\r\n",
      "time/data storing (s)                                    0.00407339\r\n",
      "time/evaluation sampling (s)                             1.04371\r\n",
      "time/exploration sampling (s)                            0.140701\r\n",
      "time/logging (s)                                         0.0196148\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/saving (s)                                          0.0553031\n",
      "time/training (s)                                       51.4698\n",
      "time/epoch (s)                                          52.7332\n",
      "time/total (s)                                        2454.75\n",
      "Epoch                                                   50\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 11:06:41.676805 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 51 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00193865\n",
      "trainer/QF2 Loss                                         0.00061397\n",
      "trainer/Policy Loss                                      3.22226\n",
      "trainer/Q1 Predictions Mean                             -1.13893\n",
      "trainer/Q1 Predictions Std                               0.884325\n",
      "trainer/Q1 Predictions Max                               1.38836\n",
      "trainer/Q1 Predictions Min                              -3.1857\n",
      "trainer/Q2 Predictions Mean                             -1.14884\n",
      "trainer/Q2 Predictions Std                               0.887894\n",
      "trainer/Q2 Predictions Max                               1.36732\n",
      "trainer/Q2 Predictions Min                              -3.21057\n",
      "trainer/Q Targets Mean                                  -1.14358\n",
      "trainer/Q Targets Std                                    0.88556\n",
      "trainer/Q Targets Max                                    1.41657\n",
      "trainer/Q Targets Min                                   -3.18895\n",
      "trainer/Log Pis Mean                                     2.07997\n",
      "trainer/Log Pis Std                                      1.36571\n",
      "trainer/Log Pis Max                                      4.57685\n",
      "trainer/Log Pis Min                                     -4.41859\n",
      "trainer/Policy mu Mean                                  -0.00798388\n",
      "trainer/Policy mu Std                                    0.212919\n",
      "trainer/Policy mu Max                                    1.4918\n",
      "trainer/Policy mu Min                                   -1.49872\n",
      "trainer/Policy log std Mean                             -2.3728\n",
      "trainer/Policy log std Std                               0.555353\n",
      "trainer/Policy log std Max                              -0.622116\n",
      "trainer/Policy log std Min                              -3.26376\n",
      "trainer/Alpha                                            0.0228128\n",
      "trainer/Alpha Loss                                       0.302467\n",
      "exploration/num steps total                           6200\n",
      "exploration/num paths total                            310\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.100228\n",
      "exploration/Rewards Std                                  0.0910357\n",
      "exploration/Rewards Max                                  0.0732557\n",
      "exploration/Rewards Min                                 -0.317285\n",
      "exploration/Returns Mean                                -2.00456\n",
      "exploration/Returns Std                                  1.59692\n",
      "exploration/Returns Max                                  0.409792\n",
      "exploration/Returns Min                                 -4.09928\n",
      "exploration/Actions Mean                                -0.0099438\n",
      "exploration/Actions Std                                  0.182871\n",
      "exploration/Actions Max                                  0.431977\n",
      "exploration/Actions Min                                 -0.664595\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.00456\n",
      "exploration/env_infos/final/reward_dist Mean             0.178916\n",
      "exploration/env_infos/final/reward_dist Std              0.357577\n",
      "exploration/env_infos/final/reward_dist Max              0.894069\n",
      "exploration/env_infos/final/reward_dist Min              2.02335e-31\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0160775\n",
      "exploration/env_infos/initial/reward_dist Std            0.0227473\n",
      "exploration/env_infos/initial/reward_dist Max            0.0606214\n",
      "exploration/env_infos/initial/reward_dist Min            3.5386e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.113754\n",
      "exploration/env_infos/reward_dist Std                    0.213596\n",
      "exploration/env_infos/reward_dist Max                    0.914231\n",
      "exploration/env_infos/reward_dist Min                    2.02335e-31\n",
      "exploration/env_infos/final/reward_energy Mean          -0.301809\n",
      "exploration/env_infos/final/reward_energy Std            0.196542\n",
      "exploration/env_infos/final/reward_energy Max           -0.151391\n",
      "exploration/env_infos/final/reward_energy Min           -0.683236\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.367712\n",
      "exploration/env_infos/initial/reward_energy Std          0.26079\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0804808\n",
      "exploration/env_infos/initial/reward_energy Min         -0.755714\n",
      "exploration/env_infos/reward_energy Mean                -0.214166\n",
      "exploration/env_infos/reward_energy Std                  0.145652\n",
      "exploration/env_infos/reward_energy Max                 -0.018483\n",
      "exploration/env_infos/reward_energy Min                 -0.755714\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00417504\n",
      "exploration/env_infos/final/end_effector_loc Std         0.271127\n",
      "exploration/env_infos/final/end_effector_loc Max         0.357213\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.490395\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00850626\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0134786\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00852682\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0300337\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00311436\n",
      "exploration/env_infos/end_effector_loc Std               0.146163\n",
      "exploration/env_infos/end_effector_loc Max               0.357213\n",
      "exploration/env_infos/end_effector_loc Min              -0.490395\n",
      "evaluation/num steps total                           52000\n",
      "evaluation/num paths total                            2600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0423782\n",
      "evaluation/Rewards Std                                   0.0763285\n",
      "evaluation/Rewards Max                                   0.16563\n",
      "evaluation/Rewards Min                                  -0.310916\n",
      "evaluation/Returns Mean                                 -0.847564\n",
      "evaluation/Returns Std                                   1.1552\n",
      "evaluation/Returns Max                                   2.12833\n",
      "evaluation/Returns Min                                  -3.09494\n",
      "evaluation/Actions Mean                                  0.000836034\n",
      "evaluation/Actions Std                                   0.0746768\n",
      "evaluation/Actions Max                                   0.493386\n",
      "evaluation/Actions Min                                  -0.66004\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.847564\n",
      "evaluation/env_infos/final/reward_dist Mean              0.148473\n",
      "evaluation/env_infos/final/reward_dist Std               0.266156\n",
      "evaluation/env_infos/final/reward_dist Max               0.906336\n",
      "evaluation/env_infos/final/reward_dist Min               9.88054e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00543535\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0105028\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0478917\n",
      "evaluation/env_infos/initial/reward_dist Min             1.27419e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.165136\n",
      "evaluation/env_infos/reward_dist Std                     0.252478\n",
      "evaluation/env_infos/reward_dist Max                     0.998643\n",
      "evaluation/env_infos/reward_dist Min                     9.88054e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0714105\n",
      "evaluation/env_infos/final/reward_energy Std             0.0690687\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00940822\n",
      "evaluation/env_infos/final/reward_energy Min            -0.370618\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.209358\n",
      "evaluation/env_infos/initial/reward_energy Std           0.18823\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00724747\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.661129\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0647582\n",
      "evaluation/env_infos/reward_energy Std                   0.0834327\n",
      "evaluation/env_infos/reward_energy Max                  -0.001005\n",
      "evaluation/env_infos/reward_energy Min                  -0.661129\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.023684\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.221197\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.427228\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.704078\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000745616\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00992576\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0212594\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.033002\n",
      "evaluation/env_infos/end_effector_loc Mean               0.010156\n",
      "evaluation/env_infos/end_effector_loc Std                0.142869\n",
      "evaluation/env_infos/end_effector_loc Max                0.427228\n",
      "evaluation/env_infos/end_effector_loc Min               -0.704078\n",
      "time/data storing (s)                                    0.00310105\n",
      "time/evaluation sampling (s)                             1.09496\n",
      "time/exploration sampling (s)                            0.124947\n",
      "time/logging (s)                                         0.0188341\n",
      "time/saving (s)                                          0.0270247\n",
      "time/training (s)                                       48.4462\n",
      "time/epoch (s)                                          49.7151\n",
      "time/total (s)                                        2505.14\n",
      "Epoch                                                   51\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:07:32.943008 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 52 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000564138\n",
      "trainer/QF2 Loss                                         0.000882073\n",
      "trainer/Policy Loss                                      3.13608\n",
      "trainer/Q1 Predictions Mean                             -1.06428\n",
      "trainer/Q1 Predictions Std                               0.811253\n",
      "trainer/Q1 Predictions Max                               0.961758\n",
      "trainer/Q1 Predictions Min                              -3.43856\n",
      "trainer/Q2 Predictions Mean                             -1.06456\n",
      "trainer/Q2 Predictions Std                               0.810862\n",
      "trainer/Q2 Predictions Max                               0.965712\n",
      "trainer/Q2 Predictions Min                              -3.43368\n",
      "trainer/Q Targets Mean                                  -1.06752\n",
      "trainer/Q Targets Std                                    0.814868\n",
      "trainer/Q Targets Max                                    0.953399\n",
      "trainer/Q Targets Min                                   -3.43996\n",
      "trainer/Log Pis Mean                                     2.07794\n",
      "trainer/Log Pis Std                                      1.32794\n",
      "trainer/Log Pis Max                                      4.4231\n",
      "trainer/Log Pis Min                                     -5.59204\n",
      "trainer/Policy mu Mean                                  -0.0514461\n",
      "trainer/Policy mu Std                                    0.259162\n",
      "trainer/Policy mu Max                                    1.008\n",
      "trainer/Policy mu Min                                   -1.90385\n",
      "trainer/Policy log std Mean                             -2.34018\n",
      "trainer/Policy log std Std                               0.548303\n",
      "trainer/Policy log std Max                              -0.401168\n",
      "trainer/Policy log std Min                              -3.25115\n",
      "trainer/Alpha                                            0.0214122\n",
      "trainer/Alpha Loss                                       0.299631\n",
      "exploration/num steps total                           6300\n",
      "exploration/num paths total                            315\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117094\n",
      "exploration/Rewards Std                                  0.0536121\n",
      "exploration/Rewards Max                                 -0.00310531\n",
      "exploration/Rewards Min                                 -0.294406\n",
      "exploration/Returns Mean                                -2.34188\n",
      "exploration/Returns Std                                  0.548216\n",
      "exploration/Returns Max                                 -1.35404\n",
      "exploration/Returns Min                                 -3.02284\n",
      "exploration/Actions Mean                                 0.00205229\n",
      "exploration/Actions Std                                  0.0755326\n",
      "exploration/Actions Max                                  0.277145\n",
      "exploration/Actions Min                                 -0.228927\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.34188\n",
      "exploration/env_infos/final/reward_dist Mean             0.0734465\n",
      "exploration/env_infos/final/reward_dist Std              0.127777\n",
      "exploration/env_infos/final/reward_dist Max              0.328035\n",
      "exploration/env_infos/final/reward_dist Min              5.05137e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000439209\n",
      "exploration/env_infos/initial/reward_dist Std            0.000292858\n",
      "exploration/env_infos/initial/reward_dist Max            0.000739874\n",
      "exploration/env_infos/initial/reward_dist Min            5.38416e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.05603\n",
      "exploration/env_infos/reward_dist Std                    0.174253\n",
      "exploration/env_infos/reward_dist Max                    0.915356\n",
      "exploration/env_infos/reward_dist Min                    1.09447e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.14332\n",
      "exploration/env_infos/final/reward_energy Std            0.0622155\n",
      "exploration/env_infos/final/reward_energy Max           -0.0716452\n",
      "exploration/env_infos/final/reward_energy Min           -0.233507\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0806226\n",
      "exploration/env_infos/initial/reward_energy Std          0.0288107\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0509701\n",
      "exploration/env_infos/initial/reward_energy Min         -0.131429\n",
      "exploration/env_infos/reward_energy Mean                -0.0907904\n",
      "exploration/env_infos/reward_energy Std                  0.0563549\n",
      "exploration/env_infos/reward_energy Max                 -0.00697259\n",
      "exploration/env_infos/reward_energy Min                 -0.307387\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.022086\n",
      "exploration/env_infos/final/end_effector_loc Std         0.167831\n",
      "exploration/env_infos/final/end_effector_loc Max         0.267382\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.372321\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00135697\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00270577\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00404251\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00469231\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0226019\n",
      "exploration/env_infos/end_effector_loc Std               0.0910463\n",
      "exploration/env_infos/end_effector_loc Max               0.267382\n",
      "exploration/env_infos/end_effector_loc Min              -0.372321\n",
      "evaluation/num steps total                           53000\n",
      "evaluation/num paths total                            2650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0570043\n",
      "evaluation/Rewards Std                                   0.0678026\n",
      "evaluation/Rewards Max                                   0.144139\n",
      "evaluation/Rewards Min                                  -0.337982\n",
      "evaluation/Returns Mean                                 -1.14009\n",
      "evaluation/Returns Std                                   1.0589\n",
      "evaluation/Returns Max                                   1.15055\n",
      "evaluation/Returns Min                                  -3.65873\n",
      "evaluation/Actions Mean                                 -0.00157285\n",
      "evaluation/Actions Std                                   0.0630739\n",
      "evaluation/Actions Max                                   0.674026\n",
      "evaluation/Actions Min                                  -0.586379\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.14009\n",
      "evaluation/env_infos/final/reward_dist Mean              0.183789\n",
      "evaluation/env_infos/final/reward_dist Std               0.28059\n",
      "evaluation/env_infos/final/reward_dist Max               0.980919\n",
      "evaluation/env_infos/final/reward_dist Min               2.05697e-26\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00562016\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117248\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0707175\n",
      "evaluation/env_infos/initial/reward_dist Min             3.60645e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.155375\n",
      "evaluation/env_infos/reward_dist Std                     0.236562\n",
      "evaluation/env_infos/reward_dist Max                     0.996748\n",
      "evaluation/env_infos/reward_dist Min                     2.05697e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0751938\n",
      "evaluation/env_infos/final/reward_energy Std             0.0723688\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00520269\n",
      "evaluation/env_infos/final/reward_energy Min            -0.293956\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.1643\n",
      "evaluation/env_infos/initial/reward_energy Std           0.184008\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0142004\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.790587\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0525415\n",
      "evaluation/env_infos/reward_energy Std                   0.0721177\n",
      "evaluation/env_infos/reward_energy Max                  -0.000646712\n",
      "evaluation/env_infos/reward_energy Min                  -0.790587\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0372193\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.215862\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.317112\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.645088\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000574897\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00870265\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0337013\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0293189\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0190383\n",
      "evaluation/env_infos/end_effector_loc Std                0.136677\n",
      "evaluation/env_infos/end_effector_loc Max                0.317112\n",
      "evaluation/env_infos/end_effector_loc Min               -0.645088\n",
      "time/data storing (s)                                    0.00316824\n",
      "time/evaluation sampling (s)                             0.993103\n",
      "time/exploration sampling (s)                            0.127051\n",
      "time/logging (s)                                         0.0185869\n",
      "time/saving (s)                                          0.0264119\n",
      "time/training (s)                                       49.4501\n",
      "time/epoch (s)                                          50.6184\n",
      "time/total (s)                                        2556.41\n",
      "Epoch                                                   52\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:08:23.608887 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 53 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000572624\n",
      "trainer/QF2 Loss                                         0.000973084\n",
      "trainer/Policy Loss                                      2.99688\n",
      "trainer/Q1 Predictions Mean                             -1.15497\n",
      "trainer/Q1 Predictions Std                               0.849146\n",
      "trainer/Q1 Predictions Max                               0.737176\n",
      "trainer/Q1 Predictions Min                              -3.27979\n",
      "trainer/Q2 Predictions Mean                             -1.1408\n",
      "trainer/Q2 Predictions Std                               0.844036\n",
      "trainer/Q2 Predictions Max                               0.728662\n",
      "trainer/Q2 Predictions Min                              -3.27321\n",
      "trainer/Q Targets Mean                                  -1.15099\n",
      "trainer/Q Targets Std                                    0.852005\n",
      "trainer/Q Targets Max                                    0.70616\n",
      "trainer/Q Targets Min                                   -3.2992\n",
      "trainer/Log Pis Mean                                     1.84293\n",
      "trainer/Log Pis Std                                      1.39176\n",
      "trainer/Log Pis Max                                      4.22056\n",
      "trainer/Log Pis Min                                     -2.56069\n",
      "trainer/Policy mu Mean                                  -0.0383871\n",
      "trainer/Policy mu Std                                    0.241176\n",
      "trainer/Policy mu Max                                    1.00329\n",
      "trainer/Policy mu Min                                   -2.02289\n",
      "trainer/Policy log std Mean                             -2.32471\n",
      "trainer/Policy log std Std                               0.527772\n",
      "trainer/Policy log std Max                              -0.315075\n",
      "trainer/Policy log std Min                              -3.20793\n",
      "trainer/Alpha                                            0.0227258\n",
      "trainer/Alpha Loss                                      -0.59412\n",
      "exploration/num steps total                           6400\n",
      "exploration/num paths total                            320\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0904083\n",
      "exploration/Rewards Std                                  0.0934899\n",
      "exploration/Rewards Max                                  0.0521065\n",
      "exploration/Rewards Min                                 -0.464915\n",
      "exploration/Returns Mean                                -1.80817\n",
      "exploration/Returns Std                                  1.38176\n",
      "exploration/Returns Max                                 -0.662538\n",
      "exploration/Returns Min                                 -3.53301\n",
      "exploration/Actions Mean                                 0.0212716\n",
      "exploration/Actions Std                                  0.164074\n",
      "exploration/Actions Max                                  0.715719\n",
      "exploration/Actions Min                                 -0.518244\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.80817\n",
      "exploration/env_infos/final/reward_dist Mean             0.0416745\n",
      "exploration/env_infos/final/reward_dist Std              0.0833418\n",
      "exploration/env_infos/final/reward_dist Max              0.208358\n",
      "exploration/env_infos/final/reward_dist Min              1.15508e-93\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00106381\n",
      "exploration/env_infos/initial/reward_dist Std            0.00125184\n",
      "exploration/env_infos/initial/reward_dist Max            0.00350076\n",
      "exploration/env_infos/initial/reward_dist Min            1.16555e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0454611\n",
      "exploration/env_infos/reward_dist Std                    0.0882724\n",
      "exploration/env_infos/reward_dist Max                    0.454585\n",
      "exploration/env_infos/reward_dist Min                    1.15508e-93\n",
      "exploration/env_infos/final/reward_energy Mean          -0.184593\n",
      "exploration/env_infos/final/reward_energy Std            0.0831683\n",
      "exploration/env_infos/final/reward_energy Max           -0.117049\n",
      "exploration/env_infos/final/reward_energy Min           -0.33386\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.404757\n",
      "exploration/env_infos/initial/reward_energy Std          0.223312\n",
      "exploration/env_infos/initial/reward_energy Max         -0.102838\n",
      "exploration/env_infos/initial/reward_energy Min         -0.685325\n",
      "exploration/env_infos/reward_energy Mean                -0.193288\n",
      "exploration/env_infos/reward_energy Std                  0.131853\n",
      "exploration/env_infos/reward_energy Max                 -0.0238137\n",
      "exploration/env_infos/reward_energy Min                 -0.723145\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.18155\n",
      "exploration/env_infos/final/end_effector_loc Std         0.389973\n",
      "exploration/env_infos/final/end_effector_loc Max         0.900878\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.294195\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00160495\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0162648\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0319206\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0259122\n",
      "exploration/env_infos/end_effector_loc Mean              0.0613797\n",
      "exploration/env_infos/end_effector_loc Std               0.236086\n",
      "exploration/env_infos/end_effector_loc Max               0.900878\n",
      "exploration/env_infos/end_effector_loc Min              -0.294195\n",
      "evaluation/num steps total                           54000\n",
      "evaluation/num paths total                            2700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0615927\n",
      "evaluation/Rewards Std                                   0.0875146\n",
      "evaluation/Rewards Max                                   0.129299\n",
      "evaluation/Rewards Min                                  -0.47225\n",
      "evaluation/Returns Mean                                 -1.23185\n",
      "evaluation/Returns Std                                   1.39009\n",
      "evaluation/Returns Max                                   1.51972\n",
      "evaluation/Returns Min                                  -5.00488\n",
      "evaluation/Actions Mean                                 -0.0032988\n",
      "evaluation/Actions Std                                   0.0786492\n",
      "evaluation/Actions Max                                   0.776927\n",
      "evaluation/Actions Min                                  -0.852732\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.23185\n",
      "evaluation/env_infos/final/reward_dist Mean              0.177501\n",
      "evaluation/env_infos/final/reward_dist Std               0.263068\n",
      "evaluation/env_infos/final/reward_dist Max               0.970842\n",
      "evaluation/env_infos/final/reward_dist Min               3.14615e-39\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00688088\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0099455\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0422336\n",
      "evaluation/env_infos/initial/reward_dist Min             1.33985e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.176659\n",
      "evaluation/env_infos/reward_dist Std                     0.259607\n",
      "evaluation/env_infos/reward_dist Max                     0.997383\n",
      "evaluation/env_infos/reward_dist Min                     3.14615e-39\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0719284\n",
      "evaluation/env_infos/final/reward_energy Std             0.0703338\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00300241\n",
      "evaluation/env_infos/final/reward_energy Min            -0.293106\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.248596\n",
      "evaluation/env_infos/initial/reward_energy Std           0.246405\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.012014\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.956006\n",
      "evaluation/env_infos/reward_energy Mean                 -0.067307\n",
      "evaluation/env_infos/reward_energy Std                   0.0886732\n",
      "evaluation/env_infos/reward_energy Max                  -0.00120531\n",
      "evaluation/env_infos/reward_energy Min                  -0.956006\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00501051\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.239866\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.532985\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.614261\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00100322\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123344\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0388463\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0426366\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00247718\n",
      "evaluation/env_infos/end_effector_loc Std                0.160254\n",
      "evaluation/env_infos/end_effector_loc Max                0.532985\n",
      "evaluation/env_infos/end_effector_loc Min               -0.614261\n",
      "time/data storing (s)                                    0.00288061\n",
      "time/evaluation sampling (s)                             0.96954\n",
      "time/exploration sampling (s)                            0.134335\n",
      "time/logging (s)                                         0.0214879\n",
      "time/saving (s)                                          0.0276615\n",
      "time/training (s)                                       48.8595\n",
      "time/epoch (s)                                          50.0154\n",
      "time/total (s)                                        2607.07\n",
      "Epoch                                                   53\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:09:13.806202 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 54 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000602111\n",
      "trainer/QF2 Loss                                         0.000698058\n",
      "trainer/Policy Loss                                      3.1455\n",
      "trainer/Q1 Predictions Mean                             -1.06032\n",
      "trainer/Q1 Predictions Std                               0.813\n",
      "trainer/Q1 Predictions Max                               1.46217\n",
      "trainer/Q1 Predictions Min                              -3.10419\n",
      "trainer/Q2 Predictions Mean                             -1.05144\n",
      "trainer/Q2 Predictions Std                               0.812666\n",
      "trainer/Q2 Predictions Max                               1.49839\n",
      "trainer/Q2 Predictions Min                              -3.06528\n",
      "trainer/Q Targets Mean                                  -1.05529\n",
      "trainer/Q Targets Std                                    0.812686\n",
      "trainer/Q Targets Max                                    1.42107\n",
      "trainer/Q Targets Min                                   -3.08488\n",
      "trainer/Log Pis Mean                                     2.07855\n",
      "trainer/Log Pis Std                                      1.36986\n",
      "trainer/Log Pis Max                                      4.36102\n",
      "trainer/Log Pis Min                                     -4.29018\n",
      "trainer/Policy mu Mean                                  -0.0166128\n",
      "trainer/Policy mu Std                                    0.166369\n",
      "trainer/Policy mu Max                                    0.695085\n",
      "trainer/Policy mu Min                                   -2.10868\n",
      "trainer/Policy log std Mean                             -2.4178\n",
      "trainer/Policy log std Std                               0.501994\n",
      "trainer/Policy log std Max                              -0.518801\n",
      "trainer/Policy log std Min                              -3.25803\n",
      "trainer/Alpha                                            0.0224601\n",
      "trainer/Alpha Loss                                       0.298174\n",
      "exploration/num steps total                           6500\n",
      "exploration/num paths total                            325\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0896921\n",
      "exploration/Rewards Std                                  0.071465\n",
      "exploration/Rewards Max                                  0.0365829\n",
      "exploration/Rewards Min                                 -0.371033\n",
      "exploration/Returns Mean                                -1.79384\n",
      "exploration/Returns Std                                  0.485314\n",
      "exploration/Returns Max                                 -1.16159\n",
      "exploration/Returns Min                                 -2.61891\n",
      "exploration/Actions Mean                                 0.00977882\n",
      "exploration/Actions Std                                  0.108313\n",
      "exploration/Actions Max                                  0.431954\n",
      "exploration/Actions Min                                 -0.345894\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.79384\n",
      "exploration/env_infos/final/reward_dist Mean             0.150734\n",
      "exploration/env_infos/final/reward_dist Std              0.242625\n",
      "exploration/env_infos/final/reward_dist Max              0.628069\n",
      "exploration/env_infos/final/reward_dist Min              2.50777e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00534776\n",
      "exploration/env_infos/initial/reward_dist Std            0.00594692\n",
      "exploration/env_infos/initial/reward_dist Max            0.0150082\n",
      "exploration/env_infos/initial/reward_dist Min            1.95935e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.11308\n",
      "exploration/env_infos/reward_dist Std                    0.154352\n",
      "exploration/env_infos/reward_dist Max                    0.628069\n",
      "exploration/env_infos/reward_dist Min                    2.50777e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.255023\n",
      "exploration/env_infos/final/reward_energy Std            0.143467\n",
      "exploration/env_infos/final/reward_energy Max           -0.118\n",
      "exploration/env_infos/final/reward_energy Min           -0.517729\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.178196\n",
      "exploration/env_infos/initial/reward_energy Std          0.0945371\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0520423\n",
      "exploration/env_infos/initial/reward_energy Min         -0.265767\n",
      "exploration/env_infos/reward_energy Mean                -0.129746\n",
      "exploration/env_infos/reward_energy Std                  0.0825865\n",
      "exploration/env_infos/reward_energy Max                 -0.0190906\n",
      "exploration/env_infos/reward_energy Min                 -0.517729\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0316354\n",
      "exploration/env_infos/final/end_effector_loc Std         0.215032\n",
      "exploration/env_infos/final/end_effector_loc Max         0.36066\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.307917\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000861554\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00707965\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0132239\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0116155\n",
      "exploration/env_infos/end_effector_loc Mean              0.0059525\n",
      "exploration/env_infos/end_effector_loc Std               0.135553\n",
      "exploration/env_infos/end_effector_loc Max               0.36066\n",
      "exploration/env_infos/end_effector_loc Min              -0.311552\n",
      "evaluation/num steps total                           55000\n",
      "evaluation/num paths total                            2750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.052766\n",
      "evaluation/Rewards Std                                   0.0717795\n",
      "evaluation/Rewards Max                                   0.128068\n",
      "evaluation/Rewards Min                                  -0.521683\n",
      "evaluation/Returns Mean                                 -1.05532\n",
      "evaluation/Returns Std                                   1.13114\n",
      "evaluation/Returns Max                                   1.14936\n",
      "evaluation/Returns Min                                  -5.28174\n",
      "evaluation/Actions Mean                                  0.00251551\n",
      "evaluation/Actions Std                                   0.065148\n",
      "evaluation/Actions Max                                   0.549805\n",
      "evaluation/Actions Min                                  -0.553513\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.05532\n",
      "evaluation/env_infos/final/reward_dist Mean              0.21628\n",
      "evaluation/env_infos/final/reward_dist Std               0.292008\n",
      "evaluation/env_infos/final/reward_dist Max               0.952999\n",
      "evaluation/env_infos/final/reward_dist Min               1.47572e-93\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00708446\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116563\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0486307\n",
      "evaluation/env_infos/initial/reward_dist Min             8.98009e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.199824\n",
      "evaluation/env_infos/reward_dist Std                     0.292146\n",
      "evaluation/env_infos/reward_dist Max                     0.998379\n",
      "evaluation/env_infos/reward_dist Min                     1.47572e-93\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0685116\n",
      "evaluation/env_infos/final/reward_energy Std             0.0876042\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00785327\n",
      "evaluation/env_infos/final/reward_energy Min            -0.587198\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.169714\n",
      "evaluation/env_infos/initial/reward_energy Std           0.16625\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00259799\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.749208\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0569569\n",
      "evaluation/env_infos/reward_energy Std                   0.0725058\n",
      "evaluation/env_infos/reward_energy Max                  -0.00109553\n",
      "evaluation/env_infos/reward_energy Min                  -0.749208\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0494042\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.250392\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.573149\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00150277\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00826401\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0274903\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.02698\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0244095\n",
      "evaluation/env_infos/end_effector_loc Std                0.155295\n",
      "evaluation/env_infos/end_effector_loc Max                0.573149\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00289018\n",
      "time/evaluation sampling (s)                             0.985175\n",
      "time/exploration sampling (s)                            0.119853\n",
      "time/logging (s)                                         0.0196587\n",
      "time/saving (s)                                          0.0273358\n",
      "time/training (s)                                       48.4307\n",
      "time/epoch (s)                                          49.5856\n",
      "time/total (s)                                        2657.27\n",
      "Epoch                                                   54\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:10:04.066068 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 55 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000593128\n",
      "trainer/QF2 Loss                                         0.000621741\n",
      "trainer/Policy Loss                                      3.03282\n",
      "trainer/Q1 Predictions Mean                             -1.07043\n",
      "trainer/Q1 Predictions Std                               0.834907\n",
      "trainer/Q1 Predictions Max                               0.695357\n",
      "trainer/Q1 Predictions Min                              -3.25502\n",
      "trainer/Q2 Predictions Mean                             -1.0609\n",
      "trainer/Q2 Predictions Std                               0.82877\n",
      "trainer/Q2 Predictions Max                               0.70105\n",
      "trainer/Q2 Predictions Min                              -3.2107\n",
      "trainer/Q Targets Mean                                  -1.06613\n",
      "trainer/Q Targets Std                                    0.836254\n",
      "trainer/Q Targets Max                                    0.716177\n",
      "trainer/Q Targets Min                                   -3.24291\n",
      "trainer/Log Pis Mean                                     1.96523\n",
      "trainer/Log Pis Std                                      1.2988\n",
      "trainer/Log Pis Max                                      4.62568\n",
      "trainer/Log Pis Min                                     -2.57531\n",
      "trainer/Policy mu Mean                                  -0.027083\n",
      "trainer/Policy mu Std                                    0.265751\n",
      "trainer/Policy mu Max                                    0.895987\n",
      "trainer/Policy mu Min                                   -1.82981\n",
      "trainer/Policy log std Mean                             -2.29266\n",
      "trainer/Policy log std Std                               0.590715\n",
      "trainer/Policy log std Max                              -0.195252\n",
      "trainer/Policy log std Min                              -3.23085\n",
      "trainer/Alpha                                            0.0230214\n",
      "trainer/Alpha Loss                                      -0.131146\n",
      "exploration/num steps total                           6600\n",
      "exploration/num paths total                            330\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0764029\n",
      "exploration/Rewards Std                                  0.104132\n",
      "exploration/Rewards Max                                  0.16182\n",
      "exploration/Rewards Min                                 -0.238052\n",
      "exploration/Returns Mean                                -1.52806\n",
      "exploration/Returns Std                                  1.93502\n",
      "exploration/Returns Max                                  1.98621\n",
      "exploration/Returns Min                                 -3.47602\n",
      "exploration/Actions Mean                                 0.00125412\n",
      "exploration/Actions Std                                  0.134406\n",
      "exploration/Actions Max                                  0.473603\n",
      "exploration/Actions Min                                 -0.880081\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.52806\n",
      "exploration/env_infos/final/reward_dist Mean             0.186837\n",
      "exploration/env_infos/final/reward_dist Std              0.324201\n",
      "exploration/env_infos/final/reward_dist Max              0.831045\n",
      "exploration/env_infos/final/reward_dist Min              4.90153e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0149268\n",
      "exploration/env_infos/initial/reward_dist Std            0.0238764\n",
      "exploration/env_infos/initial/reward_dist Max            0.0623438\n",
      "exploration/env_infos/initial/reward_dist Min            5.19226e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0850211\n",
      "exploration/env_infos/reward_dist Std                    0.17913\n",
      "exploration/env_infos/reward_dist Max                    0.838509\n",
      "exploration/env_infos/reward_dist Min                    4.90153e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.180544\n",
      "exploration/env_infos/final/reward_energy Std            0.0990073\n",
      "exploration/env_infos/final/reward_energy Max           -0.0667163\n",
      "exploration/env_infos/final/reward_energy Min           -0.349078\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.269136\n",
      "exploration/env_infos/initial/reward_energy Std          0.330088\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0220289\n",
      "exploration/env_infos/initial/reward_energy Min         -0.892314\n",
      "exploration/env_infos/reward_energy Mean                -0.131652\n",
      "exploration/env_infos/reward_energy Std                  0.137116\n",
      "exploration/env_infos/reward_energy Max                 -0.00727495\n",
      "exploration/env_infos/reward_energy Min                 -0.892314\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.033585\n",
      "exploration/env_infos/final/end_effector_loc Std         0.262782\n",
      "exploration/env_infos/final/end_effector_loc Max         0.639494\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.301857\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00335862\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0146785\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0160162\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.044004\n",
      "exploration/env_infos/end_effector_loc Mean              0.0107712\n",
      "exploration/env_infos/end_effector_loc Std               0.168243\n",
      "exploration/env_infos/end_effector_loc Max               0.639494\n",
      "exploration/env_infos/end_effector_loc Min              -0.303993\n",
      "evaluation/num steps total                           56000\n",
      "evaluation/num paths total                            2800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0480364\n",
      "evaluation/Rewards Std                                   0.0750632\n",
      "evaluation/Rewards Max                                   0.126706\n",
      "evaluation/Rewards Min                                  -0.456857\n",
      "evaluation/Returns Mean                                 -0.960727\n",
      "evaluation/Returns Std                                   1.12656\n",
      "evaluation/Returns Max                                   1.77404\n",
      "evaluation/Returns Min                                  -4.81758\n",
      "evaluation/Actions Mean                                  0.000799285\n",
      "evaluation/Actions Std                                   0.0720022\n",
      "evaluation/Actions Max                                   0.393275\n",
      "evaluation/Actions Min                                  -0.844084\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.960727\n",
      "evaluation/env_infos/final/reward_dist Mean              0.130547\n",
      "evaluation/env_infos/final/reward_dist Std               0.233542\n",
      "evaluation/env_infos/final/reward_dist Max               0.878043\n",
      "evaluation/env_infos/final/reward_dist Min               6.89677e-78\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00950609\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0171082\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0907809\n",
      "evaluation/env_infos/initial/reward_dist Min             9.45511e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.188513\n",
      "evaluation/env_infos/reward_dist Std                     0.259368\n",
      "evaluation/env_infos/reward_dist Max                     0.997478\n",
      "evaluation/env_infos/reward_dist Min                     6.89677e-78\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0778148\n",
      "evaluation/env_infos/final/reward_energy Std             0.0925562\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00349504\n",
      "evaluation/env_infos/final/reward_energy Min            -0.548578\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.194743\n",
      "evaluation/env_infos/initial/reward_energy Std           0.192678\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0289425\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.9069\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0620299\n",
      "evaluation/env_infos/reward_energy Std                   0.0807601\n",
      "evaluation/env_infos/reward_energy Max                  -0.00182103\n",
      "evaluation/env_infos/reward_energy Min                  -0.9069\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0408104\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27453\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.471268\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000318141\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00968045\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0196637\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0422042\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0181344\n",
      "evaluation/env_infos/end_effector_loc Std                0.172707\n",
      "evaluation/env_infos/end_effector_loc Max                0.471268\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00299232\n",
      "time/evaluation sampling (s)                             1.02214\n",
      "time/exploration sampling (s)                            0.14017\n",
      "time/logging (s)                                         0.0197715\n",
      "time/saving (s)                                          0.03487\n",
      "time/training (s)                                       48.328\n",
      "time/epoch (s)                                          49.548\n",
      "time/total (s)                                        2707.53\n",
      "Epoch                                                   55\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:10:55.944776 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 56 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00103821\r\n",
      "trainer/QF2 Loss                                         0.000697852\r\n",
      "trainer/Policy Loss                                      3.18031\r\n",
      "trainer/Q1 Predictions Mean                             -1.11693\r\n",
      "trainer/Q1 Predictions Std                               0.864548\r\n",
      "trainer/Q1 Predictions Max                               1.37104\r\n",
      "trainer/Q1 Predictions Min                              -2.99365\r\n",
      "trainer/Q2 Predictions Mean                             -1.13266\r\n",
      "trainer/Q2 Predictions Std                               0.872073\r\n",
      "trainer/Q2 Predictions Max                               1.38206\r\n",
      "trainer/Q2 Predictions Min                              -3.01791\r\n",
      "trainer/Q Targets Mean                                  -1.12416\r\n",
      "trainer/Q Targets Std                                    0.864477\r\n",
      "trainer/Q Targets Max                                    1.36514\r\n",
      "trainer/Q Targets Min                                   -3.00417\r\n",
      "trainer/Log Pis Mean                                     2.05176\r\n",
      "trainer/Log Pis Std                                      1.30808\r\n",
      "trainer/Log Pis Max                                      4.28097\r\n",
      "trainer/Log Pis Min                                     -2.14099\r\n",
      "trainer/Policy mu Mean                                  -0.0145926\r\n",
      "trainer/Policy mu Std                                    0.176873\r\n",
      "trainer/Policy mu Max                                    0.677015\r\n",
      "trainer/Policy mu Min                                   -1.3311\r\n",
      "trainer/Policy log std Mean                             -2.36058\r\n",
      "trainer/Policy log std Std                               0.559371\r\n",
      "trainer/Policy log std Max                              -0.578373\r\n",
      "trainer/Policy log std Min                              -3.36633\r\n",
      "trainer/Alpha                                            0.0234853\r\n",
      "trainer/Alpha Loss                                       0.19414\r\n",
      "exploration/num steps total                           6700\r\n",
      "exploration/num paths total                            335\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0919842\r\n",
      "exploration/Rewards Std                                  0.11143\r\n",
      "exploration/Rewards Max                                  0.111324\r\n",
      "exploration/Rewards Min                                 -0.540389\r\n",
      "exploration/Returns Mean                                -1.83968\r\n",
      "exploration/Returns Std                                  1.36444\r\n",
      "exploration/Returns Max                                  0.0752275\r\n",
      "exploration/Returns Min                                 -3.78938\r\n",
      "exploration/Actions Mean                                 0.00801843\r\n",
      "exploration/Actions Std                                  0.105194\r\n",
      "exploration/Actions Max                                  0.532639\r\n",
      "exploration/Actions Min                                 -0.437578\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.83968\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0544772\r\n",
      "exploration/env_infos/final/reward_dist Std              0.108954\r\n",
      "exploration/env_infos/final/reward_dist Max              0.272386\r\n",
      "exploration/env_infos/final/reward_dist Min              4.81574e-22\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00540059\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00649916\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0179335\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.76774e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.175592\r\n",
      "exploration/env_infos/reward_dist Std                    0.287512\r\n",
      "exploration/env_infos/reward_dist Max                    0.982445\r\n",
      "exploration/env_infos/reward_dist Min                    4.81574e-22\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.209329\r\n",
      "exploration/env_infos/final/reward_energy Std            0.193591\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.05436\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.591051\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.185301\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.142165\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0188189\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.437981\r\n",
      "exploration/env_infos/reward_energy Mean                -0.122424\r\n",
      "exploration/env_infos/reward_energy Std                  0.0852795\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00298507\r\n",
      "exploration/env_infos/reward_energy Min                 -0.591051\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00189352\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.374657\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.495658\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.759981\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      2.25967e-05\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00825732\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00994174\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0218789\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0140311\r\n",
      "exploration/env_infos/end_effector_loc Std               0.212849\r\n",
      "exploration/env_infos/end_effector_loc Max               0.495658\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.759981\r\n",
      "evaluation/num steps total                           57000\r\n",
      "evaluation/num paths total                            2850\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0460892\r\n",
      "evaluation/Rewards Std                                   0.107942\r\n",
      "evaluation/Rewards Max                                   0.144132\r\n",
      "evaluation/Rewards Min                                  -0.896632\r\n",
      "evaluation/Returns Mean                                 -0.921784\r\n",
      "evaluation/Returns Std                                   1.7149\r\n",
      "evaluation/Returns Max                                   1.88585\r\n",
      "evaluation/Returns Min                                  -9.6688\r\n",
      "evaluation/Actions Mean                                 -0.000575136\r\n",
      "evaluation/Actions Std                                   0.0851187\r\n",
      "evaluation/Actions Max                                   0.434438\r\n",
      "evaluation/Actions Min                                  -0.774292\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.921784\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.154316\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.264287\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.991114\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.78169e-77\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00856504\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0243941\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.160821\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.79399e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.228195\r\n",
      "evaluation/env_infos/reward_dist Std                     0.296986\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997694\r\n",
      "evaluation/env_infos/reward_dist Min                     3.78169e-77\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0711521\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.105453\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00555379\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.596977\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.26154\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.224334\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0222993\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.860797\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0705596\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0975315\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178729\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.860797\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0369735\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.286926\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.669286\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00131289\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121115\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0217219\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0387146\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.015287\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.18299\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.669286\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00300688\r\n",
      "time/evaluation sampling (s)                             1.07989\r\n",
      "time/exploration sampling (s)                            0.1281\r\n",
      "time/logging (s)                                         0.0193335\r\n",
      "time/saving (s)                                          0.0281824\r\n",
      "time/training (s)                                       49.8395\r\n",
      "time/epoch (s)                                          51.098\r\n",
      "time/total (s)                                        2759.41\r\n",
      "Epoch                                                   56\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:11:47.670761 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 57 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000736703\n",
      "trainer/QF2 Loss                                         0.00139431\n",
      "trainer/Policy Loss                                      3.07645\n",
      "trainer/Q1 Predictions Mean                             -1.11824\n",
      "trainer/Q1 Predictions Std                               0.848807\n",
      "trainer/Q1 Predictions Max                               1.3714\n",
      "trainer/Q1 Predictions Min                              -3.08349\n",
      "trainer/Q2 Predictions Mean                             -1.13871\n",
      "trainer/Q2 Predictions Std                               0.854693\n",
      "trainer/Q2 Predictions Max                               1.36565\n",
      "trainer/Q2 Predictions Min                              -3.14645\n",
      "trainer/Q Targets Mean                                  -1.11986\n",
      "trainer/Q Targets Std                                    0.850055\n",
      "trainer/Q Targets Max                                    1.36535\n",
      "trainer/Q Targets Min                                   -3.10122\n",
      "trainer/Log Pis Mean                                     1.93985\n",
      "trainer/Log Pis Std                                      1.39609\n",
      "trainer/Log Pis Max                                      4.15604\n",
      "trainer/Log Pis Min                                     -2.32912\n",
      "trainer/Policy mu Mean                                  -0.0272378\n",
      "trainer/Policy mu Std                                    0.218933\n",
      "trainer/Policy mu Max                                    0.883041\n",
      "trainer/Policy mu Min                                   -1.63654\n",
      "trainer/Policy log std Mean                             -2.361\n",
      "trainer/Policy log std Std                               0.612402\n",
      "trainer/Policy log std Max                               0.165138\n",
      "trainer/Policy log std Min                              -3.25185\n",
      "trainer/Alpha                                            0.0238173\n",
      "trainer/Alpha Loss                                      -0.224707\n",
      "exploration/num steps total                           6800\n",
      "exploration/num paths total                            340\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.107745\n",
      "exploration/Rewards Std                                  0.0765248\n",
      "exploration/Rewards Max                                  0.0742506\n",
      "exploration/Rewards Min                                 -0.340967\n",
      "exploration/Returns Mean                                -2.15491\n",
      "exploration/Returns Std                                  1.07939\n",
      "exploration/Returns Max                                 -0.139814\n",
      "exploration/Returns Min                                 -3.1687\n",
      "exploration/Actions Mean                                 0.0111467\n",
      "exploration/Actions Std                                  0.132408\n",
      "exploration/Actions Max                                  0.521979\n",
      "exploration/Actions Min                                 -0.439911\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.15491\n",
      "exploration/env_infos/final/reward_dist Mean             0.141021\n",
      "exploration/env_infos/final/reward_dist Std              0.206453\n",
      "exploration/env_infos/final/reward_dist Max              0.531406\n",
      "exploration/env_infos/final/reward_dist Min              8.0462e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000176101\n",
      "exploration/env_infos/initial/reward_dist Std            0.000147691\n",
      "exploration/env_infos/initial/reward_dist Max            0.000420959\n",
      "exploration/env_infos/initial/reward_dist Min            1.01247e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.19069\n",
      "exploration/env_infos/reward_dist Std                    0.291601\n",
      "exploration/env_infos/reward_dist Max                    0.967412\n",
      "exploration/env_infos/reward_dist Min                    8.0462e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.115605\n",
      "exploration/env_infos/final/reward_energy Std            0.0603252\n",
      "exploration/env_infos/final/reward_energy Max           -0.0468414\n",
      "exploration/env_infos/final/reward_energy Min           -0.223197\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.325379\n",
      "exploration/env_infos/initial/reward_energy Std          0.167988\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0845183\n",
      "exploration/env_infos/initial/reward_energy Min         -0.57063\n",
      "exploration/env_infos/reward_energy Mean                -0.153218\n",
      "exploration/env_infos/reward_energy Std                  0.108795\n",
      "exploration/env_infos/reward_energy Max                 -0.0131685\n",
      "exploration/env_infos/reward_energy Min                 -0.57063\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0460593\n",
      "exploration/env_infos/final/end_effector_loc Std         0.321278\n",
      "exploration/env_infos/final/end_effector_loc Max         0.572841\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.348545\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00305663\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0125806\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0260989\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0219955\n",
      "exploration/env_infos/end_effector_loc Mean              0.01469\n",
      "exploration/env_infos/end_effector_loc Std               0.211635\n",
      "exploration/env_infos/end_effector_loc Max               0.572841\n",
      "exploration/env_infos/end_effector_loc Min              -0.393952\n",
      "evaluation/num steps total                           58000\n",
      "evaluation/num paths total                            2900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0628498\n",
      "evaluation/Rewards Std                                   0.0752698\n",
      "evaluation/Rewards Max                                   0.147402\n",
      "evaluation/Rewards Min                                  -0.558639\n",
      "evaluation/Returns Mean                                 -1.257\n",
      "evaluation/Returns Std                                   1.06187\n",
      "evaluation/Returns Max                                   0.936845\n",
      "evaluation/Returns Min                                  -3.73891\n",
      "evaluation/Actions Mean                                 -0.00691471\n",
      "evaluation/Actions Std                                   0.0696975\n",
      "evaluation/Actions Max                                   0.430153\n",
      "evaluation/Actions Min                                  -0.696\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.257\n",
      "evaluation/env_infos/final/reward_dist Mean              0.130742\n",
      "evaluation/env_infos/final/reward_dist Std               0.247027\n",
      "evaluation/env_infos/final/reward_dist Max               0.95122\n",
      "evaluation/env_infos/final/reward_dist Min               3.98917e-53\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00562622\n",
      "evaluation/env_infos/initial/reward_dist Std             0.01203\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0605262\n",
      "evaluation/env_infos/initial/reward_dist Min             1.13677e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.157453\n",
      "evaluation/env_infos/reward_dist Std                     0.260578\n",
      "evaluation/env_infos/reward_dist Max                     0.995197\n",
      "evaluation/env_infos/reward_dist Min                     3.98917e-53\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0693599\n",
      "evaluation/env_infos/final/reward_energy Std             0.0884549\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00376967\n",
      "evaluation/env_infos/final/reward_energy Min            -0.410207\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.186663\n",
      "evaluation/env_infos/initial/reward_energy Std           0.182228\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0117582\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.797529\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0584573\n",
      "evaluation/env_infos/reward_energy Std                   0.0799616\n",
      "evaluation/env_infos/reward_energy Max                  -0.000445657\n",
      "evaluation/env_infos/reward_energy Min                  -0.797529\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0719455\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.286102\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.561632\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.954515\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00153214\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00909479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0215076\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0348\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0310406\n",
      "evaluation/env_infos/end_effector_loc Std                0.166966\n",
      "evaluation/env_infos/end_effector_loc Max                0.561632\n",
      "evaluation/env_infos/end_effector_loc Min               -0.954515\n",
      "time/data storing (s)                                    0.00295063\n",
      "time/evaluation sampling (s)                             0.986685\n",
      "time/exploration sampling (s)                            0.122752\n",
      "time/logging (s)                                         0.0227919\n",
      "time/saving (s)                                          0.0292932\n",
      "time/training (s)                                       49.7764\n",
      "time/epoch (s)                                          50.9409\n",
      "time/total (s)                                        2811.13\n",
      "Epoch                                                   57\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:12:39.177089 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 58 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000976209\n",
      "trainer/QF2 Loss                                         0.000556483\n",
      "trainer/Policy Loss                                      3.10385\n",
      "trainer/Q1 Predictions Mean                             -1.13657\n",
      "trainer/Q1 Predictions Std                               0.913435\n",
      "trainer/Q1 Predictions Max                               1.4514\n",
      "trainer/Q1 Predictions Min                              -3.15312\n",
      "trainer/Q2 Predictions Mean                             -1.13386\n",
      "trainer/Q2 Predictions Std                               0.906887\n",
      "trainer/Q2 Predictions Max                               1.41451\n",
      "trainer/Q2 Predictions Min                              -3.12819\n",
      "trainer/Q Targets Mean                                  -1.13305\n",
      "trainer/Q Targets Std                                    0.906633\n",
      "trainer/Q Targets Max                                    1.41203\n",
      "trainer/Q Targets Min                                   -3.12796\n",
      "trainer/Log Pis Mean                                     1.96587\n",
      "trainer/Log Pis Std                                      1.36929\n",
      "trainer/Log Pis Max                                      4.27113\n",
      "trainer/Log Pis Min                                     -2.901\n",
      "trainer/Policy mu Mean                                  -0.0107412\n",
      "trainer/Policy mu Std                                    0.289848\n",
      "trainer/Policy mu Max                                    2.03518\n",
      "trainer/Policy mu Min                                   -2.12561\n",
      "trainer/Policy log std Mean                             -2.34451\n",
      "trainer/Policy log std Std                               0.555469\n",
      "trainer/Policy log std Max                              -0.483498\n",
      "trainer/Policy log std Min                              -3.17089\n",
      "trainer/Alpha                                            0.0237251\n",
      "trainer/Alpha Loss                                      -0.127666\n",
      "exploration/num steps total                           6900\n",
      "exploration/num paths total                            345\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0637124\n",
      "exploration/Rewards Std                                  0.0641904\n",
      "exploration/Rewards Max                                  0.0882144\n",
      "exploration/Rewards Min                                 -0.232782\n",
      "exploration/Returns Mean                                -1.27425\n",
      "exploration/Returns Std                                  0.839123\n",
      "exploration/Returns Max                                 -0.545506\n",
      "exploration/Returns Min                                 -2.78509\n",
      "exploration/Actions Mean                                -0.00674216\n",
      "exploration/Actions Std                                  0.10708\n",
      "exploration/Actions Max                                  0.399047\n",
      "exploration/Actions Min                                 -0.369892\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.27425\n",
      "exploration/env_infos/final/reward_dist Mean             0.225606\n",
      "exploration/env_infos/final/reward_dist Std              0.201701\n",
      "exploration/env_infos/final/reward_dist Max              0.478563\n",
      "exploration/env_infos/final/reward_dist Min              1.75888e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0102332\n",
      "exploration/env_infos/initial/reward_dist Std            0.0187589\n",
      "exploration/env_infos/initial/reward_dist Max            0.04772\n",
      "exploration/env_infos/initial/reward_dist Min            1.12261e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.218896\n",
      "exploration/env_infos/reward_dist Std                    0.298107\n",
      "exploration/env_infos/reward_dist Max                    0.969618\n",
      "exploration/env_infos/reward_dist Min                    1.75888e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.140288\n",
      "exploration/env_infos/final/reward_energy Std            0.081693\n",
      "exploration/env_infos/final/reward_energy Max           -0.0446855\n",
      "exploration/env_infos/final/reward_energy Min           -0.268648\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.174828\n",
      "exploration/env_infos/initial/reward_energy Std          0.160067\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0265545\n",
      "exploration/env_infos/initial/reward_energy Min         -0.451147\n",
      "exploration/env_infos/reward_energy Mean                -0.120361\n",
      "exploration/env_infos/reward_energy Std                  0.092392\n",
      "exploration/env_infos/reward_energy Max                 -0.00737304\n",
      "exploration/env_infos/reward_energy Min                 -0.451147\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0113732\n",
      "exploration/env_infos/final/end_effector_loc Std         0.219228\n",
      "exploration/env_infos/final/end_effector_loc Max         0.415162\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.319477\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000944217\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00832714\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0199523\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0105232\n",
      "exploration/env_infos/end_effector_loc Mean              0.0129345\n",
      "exploration/env_infos/end_effector_loc Std               0.138301\n",
      "exploration/env_infos/end_effector_loc Max               0.415162\n",
      "exploration/env_infos/end_effector_loc Min              -0.319477\n",
      "evaluation/num steps total                           59000\n",
      "evaluation/num paths total                            2950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0580023\n",
      "evaluation/Rewards Std                                   0.0984882\n",
      "evaluation/Rewards Max                                   0.149515\n",
      "evaluation/Rewards Min                                  -0.625158\n",
      "evaluation/Returns Mean                                 -1.16005\n",
      "evaluation/Returns Std                                   1.535\n",
      "evaluation/Returns Max                                   1.24575\n",
      "evaluation/Returns Min                                  -6.05891\n",
      "evaluation/Actions Mean                                 -0.00258568\n",
      "evaluation/Actions Std                                   0.0869172\n",
      "evaluation/Actions Max                                   0.848333\n",
      "evaluation/Actions Min                                  -0.75162\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.16005\n",
      "evaluation/env_infos/final/reward_dist Mean              0.172223\n",
      "evaluation/env_infos/final/reward_dist Std               0.278525\n",
      "evaluation/env_infos/final/reward_dist Max               0.993757\n",
      "evaluation/env_infos/final/reward_dist Min               4.2049e-42\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00447183\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00827516\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0426785\n",
      "evaluation/env_infos/initial/reward_dist Min             4.92073e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.182244\n",
      "evaluation/env_infos/reward_dist Std                     0.270307\n",
      "evaluation/env_infos/reward_dist Max                     0.998749\n",
      "evaluation/env_infos/reward_dist Min                     4.2049e-42\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0623619\n",
      "evaluation/env_infos/final/reward_energy Std             0.0708515\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00242769\n",
      "evaluation/env_infos/final/reward_energy Min            -0.30588\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.265714\n",
      "evaluation/env_infos/initial/reward_energy Std           0.246532\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0175726\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.0769\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0700888\n",
      "evaluation/env_infos/reward_energy Std                   0.101045\n",
      "evaluation/env_infos/reward_energy Max                  -0.000338659\n",
      "evaluation/env_infos/reward_energy Min                  -1.0769\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00128081\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.283692\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.754932\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.731241\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00163343\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127106\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0424167\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.037581\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0126277\n",
      "evaluation/env_infos/end_effector_loc Std                0.188323\n",
      "evaluation/env_infos/end_effector_loc Max                0.754932\n",
      "evaluation/env_infos/end_effector_loc Min               -0.731241\n",
      "time/data storing (s)                                    0.00298244\n",
      "time/evaluation sampling (s)                             1.19395\n",
      "time/exploration sampling (s)                            0.142179\n",
      "time/logging (s)                                         0.0202151\n",
      "time/saving (s)                                          0.0322962\n",
      "time/training (s)                                       49.2897\n",
      "time/epoch (s)                                          50.6813\n",
      "time/total (s)                                        2862.63\n",
      "Epoch                                                   58\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:13:30.595475 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 59 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000782608\r\n",
      "trainer/QF2 Loss                                         0.000914096\r\n",
      "trainer/Policy Loss                                      3.0445\r\n",
      "trainer/Q1 Predictions Mean                             -1.0383\r\n",
      "trainer/Q1 Predictions Std                               0.875598\r\n",
      "trainer/Q1 Predictions Max                               1.14027\r\n",
      "trainer/Q1 Predictions Min                              -3.20058\r\n",
      "trainer/Q2 Predictions Mean                             -1.03925\r\n",
      "trainer/Q2 Predictions Std                               0.867133\r\n",
      "trainer/Q2 Predictions Max                               1.09196\r\n",
      "trainer/Q2 Predictions Min                              -3.14994\r\n",
      "trainer/Q Targets Mean                                  -1.03407\r\n",
      "trainer/Q Targets Std                                    0.868148\r\n",
      "trainer/Q Targets Max                                    1.12238\r\n",
      "trainer/Q Targets Min                                   -3.15873\r\n",
      "trainer/Log Pis Mean                                     2.00652\r\n",
      "trainer/Log Pis Std                                      1.33477\r\n",
      "trainer/Log Pis Max                                      4.30143\r\n",
      "trainer/Log Pis Min                                     -3.96297\r\n",
      "trainer/Policy mu Mean                                   0.00192278\r\n",
      "trainer/Policy mu Std                                    0.244312\r\n",
      "trainer/Policy mu Max                                    1.40005\r\n",
      "trainer/Policy mu Min                                   -1.86538\r\n",
      "trainer/Policy log std Mean                             -2.3422\r\n",
      "trainer/Policy log std Std                               0.574239\r\n",
      "trainer/Policy log std Max                              -0.494797\r\n",
      "trainer/Policy log std Min                              -3.3438\r\n",
      "trainer/Alpha                                            0.0246097\r\n",
      "trainer/Alpha Loss                                       0.0241708\r\n",
      "exploration/num steps total                           7000\r\n",
      "exploration/num paths total                            350\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.106712\r\n",
      "exploration/Rewards Std                                  0.063485\r\n",
      "exploration/Rewards Max                                  0.0208196\r\n",
      "exploration/Rewards Min                                 -0.299185\r\n",
      "exploration/Returns Mean                                -2.13424\r\n",
      "exploration/Returns Std                                  0.724292\r\n",
      "exploration/Returns Max                                 -0.834191\r\n",
      "exploration/Returns Min                                 -2.89898\r\n",
      "exploration/Actions Mean                                -0.0113687\r\n",
      "exploration/Actions Std                                  0.0964343\r\n",
      "exploration/Actions Max                                  0.491671\r\n",
      "exploration/Actions Min                                 -0.342736\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.13424\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0055943\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0111753\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0279448\r\n",
      "exploration/env_infos/final/reward_dist Min              4.81377e-18\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0115378\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0144357\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0387283\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.96002e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.178113\r\n",
      "exploration/env_infos/reward_dist Std                    0.261905\r\n",
      "exploration/env_infos/reward_dist Max                    0.923988\r\n",
      "exploration/env_infos/reward_dist Min                    4.81377e-18\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.200212\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0742213\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0821549\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.285096\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.166925\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.109903\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0626359\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.344821\r\n",
      "exploration/env_infos/reward_energy Mean                -0.112005\r\n",
      "exploration/env_infos/reward_energy Std                  0.0794508\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00264672\r\n",
      "exploration/env_infos/reward_energy Min                 -0.496845\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0859518\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.330942\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.648032\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.40822\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00121525\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00696071\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0113109\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.012883\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0270654\r\n",
      "exploration/env_infos/end_effector_loc Std               0.199471\r\n",
      "exploration/env_infos/end_effector_loc Max               0.648032\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.40822\r\n",
      "evaluation/num steps total                           60000\r\n",
      "evaluation/num paths total                            3000\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0538115\r\n",
      "evaluation/Rewards Std                                   0.0700771\r\n",
      "evaluation/Rewards Max                                   0.152198\r\n",
      "evaluation/Rewards Min                                  -0.327423\r\n",
      "evaluation/Returns Mean                                 -1.07623\r\n",
      "evaluation/Returns Std                                   1.0822\r\n",
      "evaluation/Returns Max                                   1.15248\r\n",
      "evaluation/Returns Min                                  -3.57458\r\n",
      "evaluation/Actions Mean                                  0.00159971\r\n",
      "evaluation/Actions Std                                   0.0588618\r\n",
      "evaluation/Actions Max                                   0.579086\r\n",
      "evaluation/Actions Min                                  -0.658098\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.07623\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.163761\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.240206\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.943602\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.57219e-28\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00632625\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0118294\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0648479\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.21272e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.176089\r\n",
      "evaluation/env_infos/reward_dist Std                     0.237339\r\n",
      "evaluation/env_infos/reward_dist Max                     0.991186\r\n",
      "evaluation/env_infos/reward_dist Min                     1.57219e-28\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0439887\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0433368\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00175422\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.231913\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.165517\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.168008\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0144981\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.725906\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.05179\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0652099\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000944197\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.725906\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0350867\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.228897\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.767799\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.50932\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00153801\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00819529\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0289543\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0329049\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0202614\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.146125\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.767799\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.50932\r\n",
      "time/data storing (s)                                    0.00286541\r\n",
      "time/evaluation sampling (s)                             0.946543\r\n",
      "time/exploration sampling (s)                            0.125678\r\n",
      "time/logging (s)                                         0.0190117\r\n",
      "time/saving (s)                                          0.027809\r\n",
      "time/training (s)                                       49.544\r\n",
      "time/epoch (s)                                          50.666\r\n",
      "time/total (s)                                        2914.05\r\n",
      "Epoch                                                   59\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:14:22.186877 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 60 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000758772\r\n",
      "trainer/QF2 Loss                                         0.00123063\r\n",
      "trainer/Policy Loss                                      3.00413\r\n",
      "trainer/Q1 Predictions Mean                             -1.0597\r\n",
      "trainer/Q1 Predictions Std                               0.871687\r\n",
      "trainer/Q1 Predictions Max                               0.854861\r\n",
      "trainer/Q1 Predictions Min                              -3.29896\r\n",
      "trainer/Q2 Predictions Mean                             -1.0733\r\n",
      "trainer/Q2 Predictions Std                               0.871997\r\n",
      "trainer/Q2 Predictions Max                               0.820439\r\n",
      "trainer/Q2 Predictions Min                              -3.35344\r\n",
      "trainer/Q Targets Mean                                  -1.05068\r\n",
      "trainer/Q Targets Std                                    0.869957\r\n",
      "trainer/Q Targets Max                                    0.835995\r\n",
      "trainer/Q Targets Min                                   -3.41339\r\n",
      "trainer/Log Pis Mean                                     1.9389\r\n",
      "trainer/Log Pis Std                                      1.3448\r\n",
      "trainer/Log Pis Max                                      4.24585\r\n",
      "trainer/Log Pis Min                                     -2.92971\r\n",
      "trainer/Policy mu Mean                                  -0.00811573\r\n",
      "trainer/Policy mu Std                                    0.266706\r\n",
      "trainer/Policy mu Max                                    1.74583\r\n",
      "trainer/Policy mu Min                                   -1.85107\r\n",
      "trainer/Policy log std Mean                             -2.27666\r\n",
      "trainer/Policy log std Std                               0.559586\r\n",
      "trainer/Policy log std Max                              -0.270967\r\n",
      "trainer/Policy log std Min                              -3.30598\r\n",
      "trainer/Alpha                                            0.0239073\r\n",
      "trainer/Alpha Loss                                      -0.22809\r\n",
      "exploration/num steps total                           7100\r\n",
      "exploration/num paths total                            355\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0826143\r\n",
      "exploration/Rewards Std                                  0.0992189\r\n",
      "exploration/Rewards Max                                  0.112004\r\n",
      "exploration/Rewards Min                                 -0.444243\r\n",
      "exploration/Returns Mean                                -1.65229\r\n",
      "exploration/Returns Std                                  1.37535\r\n",
      "exploration/Returns Max                                  0.639751\r\n",
      "exploration/Returns Min                                 -3.38329\r\n",
      "exploration/Actions Mean                                 0.0265292\r\n",
      "exploration/Actions Std                                  0.20738\r\n",
      "exploration/Actions Max                                  0.839389\r\n",
      "exploration/Actions Min                                 -0.615247\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.65229\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.11065\r\n",
      "exploration/env_infos/final/reward_dist Std              0.141825\r\n",
      "exploration/env_infos/final/reward_dist Max              0.342756\r\n",
      "exploration/env_infos/final/reward_dist Min              3.12357e-39\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00251599\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00384338\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00994946\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.30252e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.142978\r\n",
      "exploration/env_infos/reward_dist Std                    0.25517\r\n",
      "exploration/env_infos/reward_dist Max                    0.944986\r\n",
      "exploration/env_infos/reward_dist Min                    3.12357e-39\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.344343\r\n",
      "exploration/env_infos/final/reward_energy Std            0.180763\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0719853\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.570626\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.363396\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.266043\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.111392\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.860793\r\n",
      "exploration/env_infos/reward_energy Mean                -0.233865\r\n",
      "exploration/env_infos/reward_energy Std                  0.180909\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0200967\r\n",
      "exploration/env_infos/reward_energy Min                 -0.860793\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.204942\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.341885\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.746935\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.158708\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00604595\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0147306\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0419695\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0198725\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.101134\r\n",
      "exploration/env_infos/end_effector_loc Std               0.215568\r\n",
      "exploration/env_infos/end_effector_loc Max               0.746935\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.239929\r\n",
      "evaluation/num steps total                           61000\r\n",
      "evaluation/num paths total                            3050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.065043\r\n",
      "evaluation/Rewards Std                                   0.0998449\r\n",
      "evaluation/Rewards Max                                   0.145244\r\n",
      "evaluation/Rewards Min                                  -0.958442\r\n",
      "evaluation/Returns Mean                                 -1.30086\r\n",
      "evaluation/Returns Std                                   1.5564\r\n",
      "evaluation/Returns Max                                   1.77085\r\n",
      "evaluation/Returns Min                                  -7.72202\r\n",
      "evaluation/Actions Mean                                 -0.00232126\r\n",
      "evaluation/Actions Std                                   0.0773594\r\n",
      "evaluation/Actions Max                                   0.392522\r\n",
      "evaluation/Actions Min                                  -0.836239\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.30086\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.110964\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.184167\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.801857\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.65837e-80\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00695834\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0153477\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0891525\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.01567e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.142707\r\n",
      "evaluation/env_infos/reward_dist Std                     0.228521\r\n",
      "evaluation/env_infos/reward_dist Max                     0.991639\r\n",
      "evaluation/env_infos/reward_dist Min                     2.65837e-80\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0532923\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0682248\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00497583\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.405238\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.218493\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.239794\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00649765\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.929943\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0625255\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0898349\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000964328\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.929943\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0824807\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.296863\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.513556\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00470444\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0104603\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0196261\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.041812\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0583683\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.188871\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.513556\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00302425\r\n",
      "time/evaluation sampling (s)                             1.04908\r\n",
      "time/exploration sampling (s)                            0.121913\r\n",
      "time/logging (s)                                         0.0188005\r\n",
      "time/saving (s)                                          0.0266367\r\n",
      "time/training (s)                                       49.5222\r\n",
      "time/epoch (s)                                          50.7417\r\n",
      "time/total (s)                                        2965.64\r\n",
      "Epoch                                                   60\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:15:14.076004 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 61 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0012214\n",
      "trainer/QF2 Loss                                         0.000619664\n",
      "trainer/Policy Loss                                      3.28278\n",
      "trainer/Q1 Predictions Mean                             -1.16082\n",
      "trainer/Q1 Predictions Std                               0.840847\n",
      "trainer/Q1 Predictions Max                               1.28081\n",
      "trainer/Q1 Predictions Min                              -3.25172\n",
      "trainer/Q2 Predictions Mean                             -1.16785\n",
      "trainer/Q2 Predictions Std                               0.84773\n",
      "trainer/Q2 Predictions Max                               1.24534\n",
      "trainer/Q2 Predictions Min                              -3.2589\n",
      "trainer/Q Targets Mean                                  -1.16183\n",
      "trainer/Q Targets Std                                    0.843013\n",
      "trainer/Q Targets Max                                    1.23618\n",
      "trainer/Q Targets Min                                   -3.24207\n",
      "trainer/Log Pis Mean                                     2.11582\n",
      "trainer/Log Pis Std                                      1.28537\n",
      "trainer/Log Pis Max                                      4.27045\n",
      "trainer/Log Pis Min                                     -2.6927\n",
      "trainer/Policy mu Mean                                  -0.0272072\n",
      "trainer/Policy mu Std                                    0.291138\n",
      "trainer/Policy mu Max                                    1.64686\n",
      "trainer/Policy mu Min                                   -2.60137\n",
      "trainer/Policy log std Mean                             -2.3803\n",
      "trainer/Policy log std Std                               0.554214\n",
      "trainer/Policy log std Max                               0.476095\n",
      "trainer/Policy log std Min                              -3.29479\n",
      "trainer/Alpha                                            0.023034\n",
      "trainer/Alpha Loss                                       0.436729\n",
      "exploration/num steps total                           7200\n",
      "exploration/num paths total                            360\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0865088\n",
      "exploration/Rewards Std                                  0.0776299\n",
      "exploration/Rewards Max                                  0.0716411\n",
      "exploration/Rewards Min                                 -0.337966\n",
      "exploration/Returns Mean                                -1.73018\n",
      "exploration/Returns Std                                  0.946684\n",
      "exploration/Returns Max                                 -0.550522\n",
      "exploration/Returns Min                                 -3.23919\n",
      "exploration/Actions Mean                                -6.5368e-05\n",
      "exploration/Actions Std                                  0.167597\n",
      "exploration/Actions Max                                  0.645976\n",
      "exploration/Actions Min                                 -0.746401\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.73018\n",
      "exploration/env_infos/final/reward_dist Mean             0.1487\n",
      "exploration/env_infos/final/reward_dist Std              0.292279\n",
      "exploration/env_infos/final/reward_dist Max              0.733225\n",
      "exploration/env_infos/final/reward_dist Min              5.06713e-13\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0100371\n",
      "exploration/env_infos/initial/reward_dist Std            0.0146866\n",
      "exploration/env_infos/initial/reward_dist Max            0.0381545\n",
      "exploration/env_infos/initial/reward_dist Min            8.16505e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.170701\n",
      "exploration/env_infos/reward_dist Std                    0.276374\n",
      "exploration/env_infos/reward_dist Max                    0.945495\n",
      "exploration/env_infos/reward_dist Min                    2.81892e-13\n",
      "exploration/env_infos/final/reward_energy Mean          -0.224791\n",
      "exploration/env_infos/final/reward_energy Std            0.115314\n",
      "exploration/env_infos/final/reward_energy Max           -0.0611035\n",
      "exploration/env_infos/final/reward_energy Min           -0.384905\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.366105\n",
      "exploration/env_infos/initial/reward_energy Std          0.286352\n",
      "exploration/env_infos/initial/reward_energy Max         -0.124378\n",
      "exploration/env_infos/initial/reward_energy Min         -0.923587\n",
      "exploration/env_infos/reward_energy Mean                -0.186817\n",
      "exploration/env_infos/reward_energy Std                  0.145867\n",
      "exploration/env_infos/reward_energy Max                 -0.0280366\n",
      "exploration/env_infos/reward_energy Min                 -0.923587\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0293502\n",
      "exploration/env_infos/final/end_effector_loc Std         0.235665\n",
      "exploration/env_infos/final/end_effector_loc Max         0.306177\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.50276\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00500199\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0156531\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0109284\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0373201\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0286744\n",
      "exploration/env_infos/end_effector_loc Std               0.166731\n",
      "exploration/env_infos/end_effector_loc Max               0.306177\n",
      "exploration/env_infos/end_effector_loc Min              -0.505157\n",
      "evaluation/num steps total                           62000\n",
      "evaluation/num paths total                            3100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0511719\n",
      "evaluation/Rewards Std                                   0.0959191\n",
      "evaluation/Rewards Max                                   0.174871\n",
      "evaluation/Rewards Min                                  -0.790145\n",
      "evaluation/Returns Mean                                 -1.02344\n",
      "evaluation/Returns Std                                   1.43992\n",
      "evaluation/Returns Max                                   2.24638\n",
      "evaluation/Returns Min                                  -7.16074\n",
      "evaluation/Actions Mean                                 -0.00581291\n",
      "evaluation/Actions Std                                   0.0783518\n",
      "evaluation/Actions Max                                   0.528124\n",
      "evaluation/Actions Min                                  -0.880301\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.02344\n",
      "evaluation/env_infos/final/reward_dist Mean              0.178361\n",
      "evaluation/env_infos/final/reward_dist Std               0.269663\n",
      "evaluation/env_infos/final/reward_dist Max               0.910572\n",
      "evaluation/env_infos/final/reward_dist Min               3.23083e-63\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00778542\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0121605\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0510564\n",
      "evaluation/env_infos/initial/reward_dist Min             1.71364e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.207906\n",
      "evaluation/env_infos/reward_dist Std                     0.279442\n",
      "evaluation/env_infos/reward_dist Max                     0.992906\n",
      "evaluation/env_infos/reward_dist Min                     3.23083e-63\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0563884\n",
      "evaluation/env_infos/final/reward_energy Std             0.0577781\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00594085\n",
      "evaluation/env_infos/final/reward_energy Min            -0.339755\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.236974\n",
      "evaluation/env_infos/initial/reward_energy Std           0.22122\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00544037\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.914412\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0706185\n",
      "evaluation/env_infos/reward_energy Std                   0.0857824\n",
      "evaluation/env_infos/reward_energy Max                  -0.00037715\n",
      "evaluation/env_infos/reward_energy Min                  -0.914412\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0697611\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.250344\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.373101\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00319469\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0110074\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0264062\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0440151\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0375194\n",
      "evaluation/env_infos/end_effector_loc Std                0.170166\n",
      "evaluation/env_infos/end_effector_loc Max                0.373101\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00311045\n",
      "time/evaluation sampling (s)                             0.973542\n",
      "time/exploration sampling (s)                            0.133126\n",
      "time/logging (s)                                         0.019252\n",
      "time/saving (s)                                          0.0274365\n",
      "time/training (s)                                       49.9696\n",
      "time/epoch (s)                                          51.1261\n",
      "time/total (s)                                        3017.52\n",
      "Epoch                                                   61\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:16:08.738929 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 62 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000634692\n",
      "trainer/QF2 Loss                                         0.000627221\n",
      "trainer/Policy Loss                                      2.91604\n",
      "trainer/Q1 Predictions Mean                             -0.997989\n",
      "trainer/Q1 Predictions Std                               0.871205\n",
      "trainer/Q1 Predictions Max                               1.2956\n",
      "trainer/Q1 Predictions Min                              -3.42981\n",
      "trainer/Q2 Predictions Mean                             -0.998782\n",
      "trainer/Q2 Predictions Std                               0.872183\n",
      "trainer/Q2 Predictions Max                               1.29475\n",
      "trainer/Q2 Predictions Min                              -3.37904\n",
      "trainer/Q Targets Mean                                  -0.996887\n",
      "trainer/Q Targets Std                                    0.871294\n",
      "trainer/Q Targets Max                                    1.25639\n",
      "trainer/Q Targets Min                                   -3.41908\n",
      "trainer/Log Pis Mean                                     1.91374\n",
      "trainer/Log Pis Std                                      1.28089\n",
      "trainer/Log Pis Max                                      5.30689\n",
      "trainer/Log Pis Min                                     -2.26128\n",
      "trainer/Policy mu Mean                                  -0.00171755\n",
      "trainer/Policy mu Std                                    0.300823\n",
      "trainer/Policy mu Max                                    2.14798\n",
      "trainer/Policy mu Min                                   -2.33292\n",
      "trainer/Policy log std Mean                             -2.32926\n",
      "trainer/Policy log std Std                               0.536209\n",
      "trainer/Policy log std Max                              -0.296443\n",
      "trainer/Policy log std Min                              -3.27395\n",
      "trainer/Alpha                                            0.0224037\n",
      "trainer/Alpha Loss                                      -0.327583\n",
      "exploration/num steps total                           7300\n",
      "exploration/num paths total                            365\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.103345\n",
      "exploration/Rewards Std                                  0.126713\n",
      "exploration/Rewards Max                                  0.116165\n",
      "exploration/Rewards Min                                 -0.393746\n",
      "exploration/Returns Mean                                -2.06691\n",
      "exploration/Returns Std                                  2.15482\n",
      "exploration/Returns Max                                  0.494875\n",
      "exploration/Returns Min                                 -5.61846\n",
      "exploration/Actions Mean                                -0.0226414\n",
      "exploration/Actions Std                                  0.154638\n",
      "exploration/Actions Max                                  0.511255\n",
      "exploration/Actions Min                                 -0.784482\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.06691\n",
      "exploration/env_infos/final/reward_dist Mean             0.0740026\n",
      "exploration/env_infos/final/reward_dist Std              0.139397\n",
      "exploration/env_infos/final/reward_dist Max              0.352465\n",
      "exploration/env_infos/final/reward_dist Min              2.8596e-56\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00767819\n",
      "exploration/env_infos/initial/reward_dist Std            0.0113012\n",
      "exploration/env_infos/initial/reward_dist Max            0.0300937\n",
      "exploration/env_infos/initial/reward_dist Min            5.81273e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.204585\n",
      "exploration/env_infos/reward_dist Std                    0.326299\n",
      "exploration/env_infos/reward_dist Max                    0.995997\n",
      "exploration/env_infos/reward_dist Min                    1.41095e-57\n",
      "exploration/env_infos/final/reward_energy Mean          -0.236766\n",
      "exploration/env_infos/final/reward_energy Std            0.0976469\n",
      "exploration/env_infos/final/reward_energy Max           -0.129846\n",
      "exploration/env_infos/final/reward_energy Min           -0.386049\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.361647\n",
      "exploration/env_infos/initial/reward_energy Std          0.286251\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0770294\n",
      "exploration/env_infos/initial/reward_energy Min         -0.787053\n",
      "exploration/env_infos/reward_energy Mean                -0.174346\n",
      "exploration/env_infos/reward_energy Std                  0.135848\n",
      "exploration/env_infos/reward_energy Max                 -0.00539338\n",
      "exploration/env_infos/reward_energy Min                 -0.787053\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.283214\n",
      "exploration/env_infos/final/end_effector_loc Std         0.415396\n",
      "exploration/env_infos/final/end_effector_loc Max         0.202268\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00581344\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0152353\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00858226\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0392241\n",
      "exploration/env_infos/end_effector_loc Mean             -0.16644\n",
      "exploration/env_infos/end_effector_loc Std               0.324875\n",
      "exploration/env_infos/end_effector_loc Max               0.202268\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           63000\n",
      "evaluation/num paths total                            3150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0621365\n",
      "evaluation/Rewards Std                                   0.0926426\n",
      "evaluation/Rewards Max                                   0.161391\n",
      "evaluation/Rewards Min                                  -0.826141\n",
      "evaluation/Returns Mean                                 -1.24273\n",
      "evaluation/Returns Std                                   1.37406\n",
      "evaluation/Returns Max                                   1.89437\n",
      "evaluation/Returns Min                                  -7.17047\n",
      "evaluation/Actions Mean                                 -0.00401785\n",
      "evaluation/Actions Std                                   0.065011\n",
      "evaluation/Actions Max                                   0.409663\n",
      "evaluation/Actions Min                                  -0.661217\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.24273\n",
      "evaluation/env_infos/final/reward_dist Mean              0.210435\n",
      "evaluation/env_infos/final/reward_dist Std               0.284031\n",
      "evaluation/env_infos/final/reward_dist Max               0.970319\n",
      "evaluation/env_infos/final/reward_dist Min               2.15102e-70\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00690663\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111925\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0350946\n",
      "evaluation/env_infos/initial/reward_dist Min             1.24206e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.188523\n",
      "evaluation/env_infos/reward_dist Std                     0.266471\n",
      "evaluation/env_infos/reward_dist Max                     0.996731\n",
      "evaluation/env_infos/reward_dist Min                     2.15102e-70\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0532387\n",
      "evaluation/env_infos/final/reward_energy Std             0.0676217\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00431878\n",
      "evaluation/env_infos/final/reward_energy Min            -0.428684\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.182515\n",
      "evaluation/env_infos/initial/reward_energy Std           0.172348\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0174584\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.751374\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0571062\n",
      "evaluation/env_infos/reward_energy Std                   0.0722774\n",
      "evaluation/env_infos/reward_energy Max                  -0.00110665\n",
      "evaluation/env_infos/reward_energy Min                  -0.751374\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0597645\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.259055\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.638273\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00161082\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00872781\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0204832\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0330609\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0315314\n",
      "evaluation/env_infos/end_effector_loc Std                0.167403\n",
      "evaluation/env_infos/end_effector_loc Max                0.638273\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00299993\n",
      "time/evaluation sampling (s)                             1.06472\n",
      "time/exploration sampling (s)                            0.152037\n",
      "time/logging (s)                                         0.0277487\n",
      "time/saving (s)                                          0.0280732\n",
      "time/training (s)                                       52.5623\n",
      "time/epoch (s)                                          53.8379\n",
      "time/total (s)                                        3072.19\n",
      "Epoch                                                   62\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:17:04.384801 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 63 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00106572\n",
      "trainer/QF2 Loss                                         0.000923544\n",
      "trainer/Policy Loss                                      2.98495\n",
      "trainer/Q1 Predictions Mean                             -0.98084\n",
      "trainer/Q1 Predictions Std                               0.863898\n",
      "trainer/Q1 Predictions Max                               1.26089\n",
      "trainer/Q1 Predictions Min                              -3.51502\n",
      "trainer/Q2 Predictions Mean                             -0.986053\n",
      "trainer/Q2 Predictions Std                               0.864993\n",
      "trainer/Q2 Predictions Max                               1.25423\n",
      "trainer/Q2 Predictions Min                              -3.55425\n",
      "trainer/Q Targets Mean                                  -0.982552\n",
      "trainer/Q Targets Std                                    0.862493\n",
      "trainer/Q Targets Max                                    1.26187\n",
      "trainer/Q Targets Min                                   -3.50164\n",
      "trainer/Log Pis Mean                                     1.99904\n",
      "trainer/Log Pis Std                                      1.48848\n",
      "trainer/Log Pis Max                                      4.51535\n",
      "trainer/Log Pis Min                                     -4.5159\n",
      "trainer/Policy mu Mean                                  -0.0435677\n",
      "trainer/Policy mu Std                                    0.300672\n",
      "trainer/Policy mu Max                                    1.33234\n",
      "trainer/Policy mu Min                                   -1.96731\n",
      "trainer/Policy log std Mean                             -2.35595\n",
      "trainer/Policy log std Std                               0.644975\n",
      "trainer/Policy log std Max                              -0.0728238\n",
      "trainer/Policy log std Min                              -3.29243\n",
      "trainer/Alpha                                            0.0219921\n",
      "trainer/Alpha Loss                                      -0.00367916\n",
      "exploration/num steps total                           7400\n",
      "exploration/num paths total                            370\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0892055\n",
      "exploration/Rewards Std                                  0.0449565\n",
      "exploration/Rewards Max                                  0.00168235\n",
      "exploration/Rewards Min                                 -0.211877\n",
      "exploration/Returns Mean                                -1.78411\n",
      "exploration/Returns Std                                  0.243676\n",
      "exploration/Returns Max                                 -1.55699\n",
      "exploration/Returns Min                                 -2.14896\n",
      "exploration/Actions Mean                                 0.00524486\n",
      "exploration/Actions Std                                  0.0660066\n",
      "exploration/Actions Max                                  0.175894\n",
      "exploration/Actions Min                                 -0.244261\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.78411\n",
      "exploration/env_infos/final/reward_dist Mean             0.117463\n",
      "exploration/env_infos/final/reward_dist Std              0.177927\n",
      "exploration/env_infos/final/reward_dist Max              0.470724\n",
      "exploration/env_infos/final/reward_dist Min              0.00337346\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00246474\n",
      "exploration/env_infos/initial/reward_dist Std            0.00344925\n",
      "exploration/env_infos/initial/reward_dist Max            0.00929892\n",
      "exploration/env_infos/initial/reward_dist Min            0.000224256\n",
      "exploration/env_infos/reward_dist Mean                   0.174359\n",
      "exploration/env_infos/reward_dist Std                    0.280385\n",
      "exploration/env_infos/reward_dist Max                    0.967187\n",
      "exploration/env_infos/reward_dist Min                    0.000224256\n",
      "exploration/env_infos/final/reward_energy Mean          -0.119796\n",
      "exploration/env_infos/final/reward_energy Std            0.0752\n",
      "exploration/env_infos/final/reward_energy Max           -0.0389277\n",
      "exploration/env_infos/final/reward_energy Min           -0.251704\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.120366\n",
      "exploration/env_infos/initial/reward_energy Std          0.0347554\n",
      "exploration/env_infos/initial/reward_energy Max         -0.086151\n",
      "exploration/env_infos/initial/reward_energy Min         -0.172405\n",
      "exploration/env_infos/reward_energy Mean                -0.0816451\n",
      "exploration/env_infos/reward_energy Std                  0.0458567\n",
      "exploration/env_infos/reward_energy Max                 -0.00361415\n",
      "exploration/env_infos/reward_energy Min                 -0.251704\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0960533\n",
      "exploration/env_infos/final/end_effector_loc Std         0.17954\n",
      "exploration/env_infos/final/end_effector_loc Max         0.310679\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.17368\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00115417\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00427643\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00791804\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00523494\n",
      "exploration/env_infos/end_effector_loc Mean              0.0432469\n",
      "exploration/env_infos/end_effector_loc Std               0.100648\n",
      "exploration/env_infos/end_effector_loc Max               0.310679\n",
      "exploration/env_infos/end_effector_loc Min              -0.17368\n",
      "evaluation/num steps total                           64000\n",
      "evaluation/num paths total                            3200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0729532\n",
      "evaluation/Rewards Std                                   0.119224\n",
      "evaluation/Rewards Max                                   0.132267\n",
      "evaluation/Rewards Min                                  -0.998626\n",
      "evaluation/Returns Mean                                 -1.45906\n",
      "evaluation/Returns Std                                   1.94882\n",
      "evaluation/Returns Max                                   1.05038\n",
      "evaluation/Returns Min                                 -12.8342\n",
      "evaluation/Actions Mean                                 -0.00645751\n",
      "evaluation/Actions Std                                   0.0909429\n",
      "evaluation/Actions Max                                   0.731594\n",
      "evaluation/Actions Min                                  -0.938262\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.45906\n",
      "evaluation/env_infos/final/reward_dist Mean              0.053989\n",
      "evaluation/env_infos/final/reward_dist Std               0.108936\n",
      "evaluation/env_infos/final/reward_dist Max               0.59619\n",
      "evaluation/env_infos/final/reward_dist Min               2.61209e-82\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00641776\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116615\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0504439\n",
      "evaluation/env_infos/initial/reward_dist Min             1.43281e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.128383\n",
      "evaluation/env_infos/reward_dist Std                     0.224211\n",
      "evaluation/env_infos/reward_dist Max                     0.993204\n",
      "evaluation/env_infos/reward_dist Min                     2.99727e-111\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0520669\n",
      "evaluation/env_infos/final/reward_energy Std             0.0679275\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00124606\n",
      "evaluation/env_infos/final/reward_energy Min            -0.290667\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.245273\n",
      "evaluation/env_infos/initial/reward_energy Std           0.260467\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00425869\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.956161\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0671221\n",
      "evaluation/env_infos/reward_energy Std                   0.110087\n",
      "evaluation/env_infos/reward_energy Max                  -0.000193892\n",
      "evaluation/env_infos/reward_energy Min                  -0.956161\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0874433\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.307375\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.597116\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00122363\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0125899\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0365797\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0469131\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0404215\n",
      "evaluation/env_infos/end_effector_loc Std                0.205834\n",
      "evaluation/env_infos/end_effector_loc Max                0.597116\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00310708\n",
      "time/evaluation sampling (s)                             1.0403\n",
      "time/exploration sampling (s)                            0.127455\n",
      "time/logging (s)                                         0.0203977\n",
      "time/saving (s)                                          0.0346161\n",
      "time/training (s)                                       53.4961\n",
      "time/epoch (s)                                          54.7219\n",
      "time/total (s)                                        3127.83\n",
      "Epoch                                                   63\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:18:00.175788 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 64 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00146345\r\n",
      "trainer/QF2 Loss                                         0.000959836\r\n",
      "trainer/Policy Loss                                      2.95088\r\n",
      "trainer/Q1 Predictions Mean                             -0.985467\r\n",
      "trainer/Q1 Predictions Std                               0.837433\r\n",
      "trainer/Q1 Predictions Max                               1.41956\r\n",
      "trainer/Q1 Predictions Min                              -2.83056\r\n",
      "trainer/Q2 Predictions Mean                             -0.980667\r\n",
      "trainer/Q2 Predictions Std                               0.840654\r\n",
      "trainer/Q2 Predictions Max                               1.40363\r\n",
      "trainer/Q2 Predictions Min                              -2.85136\r\n",
      "trainer/Q Targets Mean                                  -0.983918\r\n",
      "trainer/Q Targets Std                                    0.83444\r\n",
      "trainer/Q Targets Max                                    1.39927\r\n",
      "trainer/Q Targets Min                                   -2.84695\r\n",
      "trainer/Log Pis Mean                                     1.97365\r\n",
      "trainer/Log Pis Std                                      1.38198\r\n",
      "trainer/Log Pis Max                                      4.30566\r\n",
      "trainer/Log Pis Min                                     -3.60185\r\n",
      "trainer/Policy mu Mean                                  -0.0430279\r\n",
      "trainer/Policy mu Std                                    0.312673\r\n",
      "trainer/Policy mu Max                                    1.56305\r\n",
      "trainer/Policy mu Min                                   -1.5937\r\n",
      "trainer/Policy log std Mean                             -2.29679\r\n",
      "trainer/Policy log std Std                               0.629537\r\n",
      "trainer/Policy log std Max                              -0.351987\r\n",
      "trainer/Policy log std Min                              -3.29245\r\n",
      "trainer/Alpha                                            0.0225247\r\n",
      "trainer/Alpha Loss                                      -0.0999372\r\n",
      "exploration/num steps total                           7500\r\n",
      "exploration/num paths total                            375\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0918094\r\n",
      "exploration/Rewards Std                                  0.0751304\r\n",
      "exploration/Rewards Max                                  0.0656775\r\n",
      "exploration/Rewards Min                                 -0.305532\r\n",
      "exploration/Returns Mean                                -1.83619\r\n",
      "exploration/Returns Std                                  1.16984\r\n",
      "exploration/Returns Max                                 -0.964449\r\n",
      "exploration/Returns Min                                 -3.98421\r\n",
      "exploration/Actions Mean                                -0.00280096\r\n",
      "exploration/Actions Std                                  0.156518\r\n",
      "exploration/Actions Max                                  0.701314\r\n",
      "exploration/Actions Min                                 -0.713328\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.83619\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.356948\r\n",
      "exploration/env_infos/final/reward_dist Std              0.381137\r\n",
      "exploration/env_infos/final/reward_dist Max              0.904839\r\n",
      "exploration/env_infos/final/reward_dist Min              3.28296e-08\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00833009\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0118972\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0306929\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.95232e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.238959\r\n",
      "exploration/env_infos/reward_dist Std                    0.324704\r\n",
      "exploration/env_infos/reward_dist Max                    0.986571\r\n",
      "exploration/env_infos/reward_dist Min                    3.28296e-08\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.110257\r\n",
      "exploration/env_infos/final/reward_energy Std            0.134651\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0056416\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.375983\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.181091\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.161119\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0251869\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.48828\r\n",
      "exploration/env_infos/reward_energy Mean                -0.158781\r\n",
      "exploration/env_infos/reward_energy Std                  0.154273\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0056416\r\n",
      "exploration/env_infos/reward_energy Min                 -0.831194\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0140176\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.24606\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.342668\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.358879\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00124133\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00847943\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0234854\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00666941\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0118163\r\n",
      "exploration/env_infos/end_effector_loc Std               0.163333\r\n",
      "exploration/env_infos/end_effector_loc Max               0.342668\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.358879\r\n",
      "evaluation/num steps total                           65000\r\n",
      "evaluation/num paths total                            3250\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0592212\r\n",
      "evaluation/Rewards Std                                   0.0819531\r\n",
      "evaluation/Rewards Max                                   0.144569\r\n",
      "evaluation/Rewards Min                                  -0.575305\r\n",
      "evaluation/Returns Mean                                 -1.18442\r\n",
      "evaluation/Returns Std                                   1.29447\r\n",
      "evaluation/Returns Max                                   1.40656\r\n",
      "evaluation/Returns Min                                  -5.3822\r\n",
      "evaluation/Actions Mean                                 -0.00310106\r\n",
      "evaluation/Actions Std                                   0.0782224\r\n",
      "evaluation/Actions Max                                   0.757156\r\n",
      "evaluation/Actions Min                                  -0.893266\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.18442\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.103723\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.215073\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.933461\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.08878e-32\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00727164\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00949375\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0374116\r\n",
      "evaluation/env_infos/initial/reward_dist Min             9.22651e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.163312\r\n",
      "evaluation/env_infos/reward_dist Std                     0.255998\r\n",
      "evaluation/env_infos/reward_dist Max                     0.990845\r\n",
      "evaluation/env_infos/reward_dist Min                     4.08878e-32\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0328447\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0488877\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00564629\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.326877\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.220762\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255839\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0124398\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.939393\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0606826\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0925978\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00117461\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.939393\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0927931\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.250779\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.417388\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.695747\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00256128\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116695\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0378578\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0446633\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.048938\n",
      "evaluation/env_infos/end_effector_loc Std                0.170599\n",
      "evaluation/env_infos/end_effector_loc Max                0.417388\n",
      "evaluation/env_infos/end_effector_loc Min               -0.695747\n",
      "time/data storing (s)                                    0.00313944\n",
      "time/evaluation sampling (s)                             1.32119\n",
      "time/exploration sampling (s)                            0.131314\n",
      "time/logging (s)                                         0.0194275\n",
      "time/saving (s)                                          0.0288478\n",
      "time/training (s)                                       53.336\n",
      "time/epoch (s)                                          54.8399\n",
      "time/total (s)                                        3183.62\n",
      "Epoch                                                   64\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 11:18:54.743043 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 65 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00111018\n",
      "trainer/QF2 Loss                                         0.0011456\n",
      "trainer/Policy Loss                                      2.99685\n",
      "trainer/Q1 Predictions Mean                             -1.11838\n",
      "trainer/Q1 Predictions Std                               0.872252\n",
      "trainer/Q1 Predictions Max                               1.03295\n",
      "trainer/Q1 Predictions Min                              -3.53844\n",
      "trainer/Q2 Predictions Mean                             -1.12378\n",
      "trainer/Q2 Predictions Std                               0.873145\n",
      "trainer/Q2 Predictions Max                               1.05597\n",
      "trainer/Q2 Predictions Min                              -3.54662\n",
      "trainer/Q Targets Mean                                  -1.12275\n",
      "trainer/Q Targets Std                                    0.870789\n",
      "trainer/Q Targets Max                                    0.995587\n",
      "trainer/Q Targets Min                                   -3.54026\n",
      "trainer/Log Pis Mean                                     1.87769\n",
      "trainer/Log Pis Std                                      1.41446\n",
      "trainer/Log Pis Max                                      4.43374\n",
      "trainer/Log Pis Min                                     -5.73508\n",
      "trainer/Policy mu Mean                                  -0.035743\n",
      "trainer/Policy mu Std                                    0.290049\n",
      "trainer/Policy mu Max                                    1.43821\n",
      "trainer/Policy mu Min                                   -1.74043\n",
      "trainer/Policy log std Mean                             -2.29613\n",
      "trainer/Policy log std Std                               0.624719\n",
      "trainer/Policy log std Max                              -0.317344\n",
      "trainer/Policy log std Min                              -3.29857\n",
      "trainer/Alpha                                            0.0226675\n",
      "trainer/Alpha Loss                                      -0.463112\n",
      "exploration/num steps total                           7600\n",
      "exploration/num paths total                            380\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0904829\n",
      "exploration/Rewards Std                                  0.0723519\n",
      "exploration/Rewards Max                                  0.0842216\n",
      "exploration/Rewards Min                                 -0.319509\n",
      "exploration/Returns Mean                                -1.80966\n",
      "exploration/Returns Std                                  0.637551\n",
      "exploration/Returns Max                                 -0.868496\n",
      "exploration/Returns Min                                 -2.53577\n",
      "exploration/Actions Mean                                 0.00380706\n",
      "exploration/Actions Std                                  0.162694\n",
      "exploration/Actions Max                                  0.435481\n",
      "exploration/Actions Min                                 -0.533255\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.80966\n",
      "exploration/env_infos/final/reward_dist Mean             0.0867236\n",
      "exploration/env_infos/final/reward_dist Std              0.101871\n",
      "exploration/env_infos/final/reward_dist Max              0.275649\n",
      "exploration/env_infos/final/reward_dist Min              1.191e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00586635\n",
      "exploration/env_infos/initial/reward_dist Std            0.00878111\n",
      "exploration/env_infos/initial/reward_dist Max            0.0227394\n",
      "exploration/env_infos/initial/reward_dist Min            2.90312e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.112475\n",
      "exploration/env_infos/reward_dist Std                    0.199378\n",
      "exploration/env_infos/reward_dist Max                    0.948732\n",
      "exploration/env_infos/reward_dist Min                    1.191e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.224514\n",
      "exploration/env_infos/final/reward_energy Std            0.126094\n",
      "exploration/env_infos/final/reward_energy Max           -0.0981367\n",
      "exploration/env_infos/final/reward_energy Min           -0.449233\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.274589\n",
      "exploration/env_infos/initial/reward_energy Std          0.0983146\n",
      "exploration/env_infos/initial/reward_energy Max         -0.121018\n",
      "exploration/env_infos/initial/reward_energy Min         -0.419779\n",
      "exploration/env_infos/reward_energy Mean                -0.192393\n",
      "exploration/env_infos/reward_energy Std                  0.126303\n",
      "exploration/env_infos/reward_energy Max                 -0.0189515\n",
      "exploration/env_infos/reward_energy Min                 -0.604316\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0716393\n",
      "exploration/env_infos/final/end_effector_loc Std         0.253295\n",
      "exploration/env_infos/final/end_effector_loc Max         0.554907\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.252142\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00347871\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00970721\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0157297\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0146996\n",
      "exploration/env_infos/end_effector_loc Mean              0.0356649\n",
      "exploration/env_infos/end_effector_loc Std               0.178066\n",
      "exploration/env_infos/end_effector_loc Max               0.554907\n",
      "exploration/env_infos/end_effector_loc Min              -0.276975\n",
      "evaluation/num steps total                           66000\n",
      "evaluation/num paths total                            3300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0672046\n",
      "evaluation/Rewards Std                                   0.0765211\n",
      "evaluation/Rewards Max                                   0.113239\n",
      "evaluation/Rewards Min                                  -0.437832\n",
      "evaluation/Returns Mean                                 -1.34409\n",
      "evaluation/Returns Std                                   1.22498\n",
      "evaluation/Returns Max                                   0.806703\n",
      "evaluation/Returns Min                                  -4.07027\n",
      "evaluation/Actions Mean                                 -0.00254544\n",
      "evaluation/Actions Std                                   0.0704033\n",
      "evaluation/Actions Max                                   0.386287\n",
      "evaluation/Actions Min                                  -0.822902\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.34409\n",
      "evaluation/env_infos/final/reward_dist Mean              0.108959\n",
      "evaluation/env_infos/final/reward_dist Std               0.226537\n",
      "evaluation/env_infos/final/reward_dist Max               0.983472\n",
      "evaluation/env_infos/final/reward_dist Min               2.0964e-42\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00516189\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00870364\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0499665\n",
      "evaluation/env_infos/initial/reward_dist Min             1.21681e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.144947\n",
      "evaluation/env_infos/reward_dist Std                     0.231538\n",
      "evaluation/env_infos/reward_dist Max                     0.989499\n",
      "evaluation/env_infos/reward_dist Min                     2.0964e-42\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0327534\n",
      "evaluation/env_infos/final/reward_energy Std             0.0287464\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00019799\n",
      "evaluation/env_infos/final/reward_energy Min            -0.17201\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.207138\n",
      "evaluation/env_infos/initial/reward_energy Std           0.237337\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0112435\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.904715\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0540245\n",
      "evaluation/env_infos/reward_energy Std                   0.0837111\n",
      "evaluation/env_infos/reward_energy Max                  -0.00019799\n",
      "evaluation/env_infos/reward_energy Min                  -0.904715\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0588115\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27292\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.548047\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.994406\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00244908\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0108649\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0193143\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0411451\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0307573\n",
      "evaluation/env_infos/end_effector_loc Std                0.174223\n",
      "evaluation/env_infos/end_effector_loc Max                0.548047\n",
      "evaluation/env_infos/end_effector_loc Min               -0.994406\n",
      "time/data storing (s)                                    0.00308966\n",
      "time/evaluation sampling (s)                             1.05659\n",
      "time/exploration sampling (s)                            0.119199\n",
      "time/logging (s)                                         0.0224172\n",
      "time/saving (s)                                          0.0277751\n",
      "time/training (s)                                       52.4682\n",
      "time/epoch (s)                                          53.6973\n",
      "time/total (s)                                        3238.18\n",
      "Epoch                                                   65\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:19:49.578002 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 66 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000759035\n",
      "trainer/QF2 Loss                                         0.000647573\n",
      "trainer/Policy Loss                                      3.02721\n",
      "trainer/Q1 Predictions Mean                             -1.01486\n",
      "trainer/Q1 Predictions Std                               0.809172\n",
      "trainer/Q1 Predictions Max                               1.45411\n",
      "trainer/Q1 Predictions Min                              -3.25609\n",
      "trainer/Q2 Predictions Mean                             -1.02544\n",
      "trainer/Q2 Predictions Std                               0.817793\n",
      "trainer/Q2 Predictions Max                               1.46488\n",
      "trainer/Q2 Predictions Min                              -3.26749\n",
      "trainer/Q Targets Mean                                  -1.02067\n",
      "trainer/Q Targets Std                                    0.816052\n",
      "trainer/Q Targets Max                                    1.46706\n",
      "trainer/Q Targets Min                                   -3.28621\n",
      "trainer/Log Pis Mean                                     2.01245\n",
      "trainer/Log Pis Std                                      1.41913\n",
      "trainer/Log Pis Max                                      4.26551\n",
      "trainer/Log Pis Min                                     -6.26971\n",
      "trainer/Policy mu Mean                                  -0.0241908\n",
      "trainer/Policy mu Std                                    0.356843\n",
      "trainer/Policy mu Max                                    2.2086\n",
      "trainer/Policy mu Min                                   -1.60176\n",
      "trainer/Policy log std Mean                             -2.28841\n",
      "trainer/Policy log std Std                               0.63886\n",
      "trainer/Policy log std Max                              -0.259482\n",
      "trainer/Policy log std Min                              -3.2367\n",
      "trainer/Alpha                                            0.0225177\n",
      "trainer/Alpha Loss                                       0.0472552\n",
      "exploration/num steps total                           7700\n",
      "exploration/num paths total                            385\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.108111\n",
      "exploration/Rewards Std                                  0.059479\n",
      "exploration/Rewards Max                                  0.00093035\n",
      "exploration/Rewards Min                                 -0.218068\n",
      "exploration/Returns Mean                                -2.16223\n",
      "exploration/Returns Std                                  0.948068\n",
      "exploration/Returns Max                                 -1.00734\n",
      "exploration/Returns Min                                 -3.39744\n",
      "exploration/Actions Mean                                -0.00118048\n",
      "exploration/Actions Std                                  0.0629602\n",
      "exploration/Actions Max                                  0.147206\n",
      "exploration/Actions Min                                 -0.239547\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.16223\n",
      "exploration/env_infos/final/reward_dist Mean             0.106805\n",
      "exploration/env_infos/final/reward_dist Std              0.117899\n",
      "exploration/env_infos/final/reward_dist Max              0.264615\n",
      "exploration/env_infos/final/reward_dist Min              0.00100071\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00371643\n",
      "exploration/env_infos/initial/reward_dist Std            0.00484532\n",
      "exploration/env_infos/initial/reward_dist Max            0.0120798\n",
      "exploration/env_infos/initial/reward_dist Min            4.24627e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0624837\n",
      "exploration/env_infos/reward_dist Std                    0.110854\n",
      "exploration/env_infos/reward_dist Max                    0.52481\n",
      "exploration/env_infos/reward_dist Min                    4.24627e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0790392\n",
      "exploration/env_infos/final/reward_energy Std            0.0658179\n",
      "exploration/env_infos/final/reward_energy Max           -0.0215403\n",
      "exploration/env_infos/final/reward_energy Min           -0.203739\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.114406\n",
      "exploration/env_infos/initial/reward_energy Std          0.0379014\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0653145\n",
      "exploration/env_infos/initial/reward_energy Min         -0.165814\n",
      "exploration/env_infos/reward_energy Mean                -0.0771334\n",
      "exploration/env_infos/reward_energy Std                  0.0445106\n",
      "exploration/env_infos/reward_energy Max                 -0.00861514\n",
      "exploration/env_infos/reward_energy Min                 -0.241468\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.011071\n",
      "exploration/env_infos/final/end_effector_loc Std         0.154221\n",
      "exploration/env_infos/final/end_effector_loc Max         0.360802\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.181624\n",
      "exploration/env_infos/initial/end_effector_loc Mean      5.5099e-05\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00426069\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0067763\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00660314\n",
      "exploration/env_infos/end_effector_loc Mean              0.00610171\n",
      "exploration/env_infos/end_effector_loc Std               0.0868714\n",
      "exploration/env_infos/end_effector_loc Max               0.360802\n",
      "exploration/env_infos/end_effector_loc Min              -0.181624\n",
      "evaluation/num steps total                           67000\n",
      "evaluation/num paths total                            3350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0560858\n",
      "evaluation/Rewards Std                                   0.0728026\n",
      "evaluation/Rewards Max                                   0.144882\n",
      "evaluation/Rewards Min                                  -0.451024\n",
      "evaluation/Returns Mean                                 -1.12172\n",
      "evaluation/Returns Std                                   1.1978\n",
      "evaluation/Returns Max                                   2.02571\n",
      "evaluation/Returns Min                                  -3.89042\n",
      "evaluation/Actions Mean                                 -0.00236113\n",
      "evaluation/Actions Std                                   0.0746259\n",
      "evaluation/Actions Max                                   0.731236\n",
      "evaluation/Actions Min                                  -0.933932\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.12172\n",
      "evaluation/env_infos/final/reward_dist Mean              0.114248\n",
      "evaluation/env_infos/final/reward_dist Std               0.223237\n",
      "evaluation/env_infos/final/reward_dist Max               0.955604\n",
      "evaluation/env_infos/final/reward_dist Min               7.40154e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00761311\n",
      "evaluation/env_infos/initial/reward_dist Std             0.018282\n",
      "evaluation/env_infos/initial/reward_dist Max             0.110233\n",
      "evaluation/env_infos/initial/reward_dist Min             1.12122e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.14064\n",
      "evaluation/env_infos/reward_dist Std                     0.247623\n",
      "evaluation/env_infos/reward_dist Max                     0.999347\n",
      "evaluation/env_infos/reward_dist Min                     7.40154e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0400683\n",
      "evaluation/env_infos/final/reward_energy Std             0.0387507\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0020063\n",
      "evaluation/env_infos/final/reward_energy Min            -0.184611\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.215374\n",
      "evaluation/env_infos/initial/reward_energy Std           0.272116\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00197372\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.965591\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0541837\n",
      "evaluation/env_infos/reward_energy Std                   0.0906274\n",
      "evaluation/env_infos/reward_energy Max                  -0.000797285\n",
      "evaluation/env_infos/reward_energy Min                  -0.965591\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0739885\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.209977\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.470421\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.456137\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0033332\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0118081\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0365618\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0466966\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0447561\n",
      "evaluation/env_infos/end_effector_loc Std                0.141382\n",
      "evaluation/env_infos/end_effector_loc Max                0.470421\n",
      "evaluation/env_infos/end_effector_loc Min               -0.539406\n",
      "time/data storing (s)                                    0.00291019\n",
      "time/evaluation sampling (s)                             1.0474\n",
      "time/exploration sampling (s)                            0.132867\n",
      "time/logging (s)                                         0.0283029\n",
      "time/saving (s)                                          0.0345324\n",
      "time/training (s)                                       52.618\n",
      "time/epoch (s)                                          53.864\n",
      "time/total (s)                                        3293.02\n",
      "Epoch                                                   66\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:20:42.839717 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 67 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00138736\n",
      "trainer/QF2 Loss                                         0.00131741\n",
      "trainer/Policy Loss                                      3.22321\n",
      "trainer/Q1 Predictions Mean                             -1.17844\n",
      "trainer/Q1 Predictions Std                               0.822925\n",
      "trainer/Q1 Predictions Max                               1.12253\n",
      "trainer/Q1 Predictions Min                              -3.28685\n",
      "trainer/Q2 Predictions Mean                             -1.18412\n",
      "trainer/Q2 Predictions Std                               0.831319\n",
      "trainer/Q2 Predictions Max                               1.17507\n",
      "trainer/Q2 Predictions Min                              -3.31397\n",
      "trainer/Q Targets Mean                                  -1.19035\n",
      "trainer/Q Targets Std                                    0.833043\n",
      "trainer/Q Targets Max                                    1.09842\n",
      "trainer/Q Targets Min                                   -3.29937\n",
      "trainer/Log Pis Mean                                     2.04886\n",
      "trainer/Log Pis Std                                      1.26423\n",
      "trainer/Log Pis Max                                      4.15641\n",
      "trainer/Log Pis Min                                     -2.24546\n",
      "trainer/Policy mu Mean                                  -0.0110192\n",
      "trainer/Policy mu Std                                    0.381566\n",
      "trainer/Policy mu Max                                    2.09145\n",
      "trainer/Policy mu Min                                   -1.73722\n",
      "trainer/Policy log std Mean                             -2.24308\n",
      "trainer/Policy log std Std                               0.667282\n",
      "trainer/Policy log std Max                               0.0805831\n",
      "trainer/Policy log std Min                              -3.12712\n",
      "trainer/Alpha                                            0.0236443\n",
      "trainer/Alpha Loss                                       0.183072\n",
      "exploration/num steps total                           7800\n",
      "exploration/num paths total                            390\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.119585\n",
      "exploration/Rewards Std                                  0.0843299\n",
      "exploration/Rewards Max                                  0.073158\n",
      "exploration/Rewards Min                                 -0.332855\n",
      "exploration/Returns Mean                                -2.3917\n",
      "exploration/Returns Std                                  1.29422\n",
      "exploration/Returns Max                                 -0.457542\n",
      "exploration/Returns Min                                 -4.27129\n",
      "exploration/Actions Mean                                 0.00250776\n",
      "exploration/Actions Std                                  0.0782392\n",
      "exploration/Actions Max                                  0.243326\n",
      "exploration/Actions Min                                 -0.385816\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.3917\n",
      "exploration/env_infos/final/reward_dist Mean             0.166595\n",
      "exploration/env_infos/final/reward_dist Std              0.186827\n",
      "exploration/env_infos/final/reward_dist Max              0.435503\n",
      "exploration/env_infos/final/reward_dist Min              0.000106283\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00575197\n",
      "exploration/env_infos/initial/reward_dist Std            0.010757\n",
      "exploration/env_infos/initial/reward_dist Max            0.0272548\n",
      "exploration/env_infos/initial/reward_dist Min            3.07816e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0942637\n",
      "exploration/env_infos/reward_dist Std                    0.141065\n",
      "exploration/env_infos/reward_dist Max                    0.500407\n",
      "exploration/env_infos/reward_dist Min                    3.07816e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.065404\n",
      "exploration/env_infos/final/reward_energy Std            0.021657\n",
      "exploration/env_infos/final/reward_energy Max           -0.0396475\n",
      "exploration/env_infos/final/reward_energy Min           -0.0934896\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.122478\n",
      "exploration/env_infos/initial/reward_energy Std          0.145565\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0261379\n",
      "exploration/env_infos/initial/reward_energy Min         -0.407206\n",
      "exploration/env_infos/reward_energy Mean                -0.0903008\n",
      "exploration/env_infos/reward_energy Std                  0.0640397\n",
      "exploration/env_infos/reward_energy Max                 -0.00138165\n",
      "exploration/env_infos/reward_energy Min                 -0.407206\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0245658\n",
      "exploration/env_infos/final/end_effector_loc Std         0.167298\n",
      "exploration/env_infos/final/end_effector_loc Max         0.147302\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.355609\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00251515\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00623792\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00343912\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0192908\n",
      "exploration/env_infos/end_effector_loc Mean             -0.021229\n",
      "exploration/env_infos/end_effector_loc Std               0.111621\n",
      "exploration/env_infos/end_effector_loc Max               0.154897\n",
      "exploration/env_infos/end_effector_loc Min              -0.401651\n",
      "evaluation/num steps total                           68000\n",
      "evaluation/num paths total                            3400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0463511\n",
      "evaluation/Rewards Std                                   0.0761563\n",
      "evaluation/Rewards Max                                   0.15573\n",
      "evaluation/Rewards Min                                  -0.740937\n",
      "evaluation/Returns Mean                                 -0.927022\n",
      "evaluation/Returns Std                                   1.0236\n",
      "evaluation/Returns Max                                   1.52996\n",
      "evaluation/Returns Min                                  -4.07462\n",
      "evaluation/Actions Mean                                  0.00486511\n",
      "evaluation/Actions Std                                   0.0776408\n",
      "evaluation/Actions Max                                   0.492104\n",
      "evaluation/Actions Min                                  -0.905732\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.927022\n",
      "evaluation/env_infos/final/reward_dist Mean              0.163287\n",
      "evaluation/env_infos/final/reward_dist Std               0.241353\n",
      "evaluation/env_infos/final/reward_dist Max               0.947334\n",
      "evaluation/env_infos/final/reward_dist Min               8.00598e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00490888\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00936141\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0458326\n",
      "evaluation/env_infos/initial/reward_dist Min             1.12382e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.183521\n",
      "evaluation/env_infos/reward_dist Std                     0.254716\n",
      "evaluation/env_infos/reward_dist Max                     0.98955\n",
      "evaluation/env_infos/reward_dist Min                     8.00598e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0652995\n",
      "evaluation/env_infos/final/reward_energy Std             0.102098\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00237647\n",
      "evaluation/env_infos/final/reward_energy Min            -0.497693\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.221858\n",
      "evaluation/env_infos/initial/reward_energy Std           0.249592\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0124987\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.988887\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0640896\n",
      "evaluation/env_infos/reward_energy Std                   0.0894207\n",
      "evaluation/env_infos/reward_energy Max                  -0.0010393\n",
      "evaluation/env_infos/reward_energy Min                  -0.988887\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0153496\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.248185\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.545767\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.563308\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0021244\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116139\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0229086\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0452866\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00409039\n",
      "evaluation/env_infos/end_effector_loc Std                0.163927\n",
      "evaluation/env_infos/end_effector_loc Max                0.545767\n",
      "evaluation/env_infos/end_effector_loc Min               -0.579242\n",
      "time/data storing (s)                                    0.00298185\n",
      "time/evaluation sampling (s)                             1.04222\n",
      "time/exploration sampling (s)                            0.127583\n",
      "time/logging (s)                                         0.0192208\n",
      "time/saving (s)                                          0.0292508\n",
      "time/training (s)                                       51.1149\n",
      "time/epoch (s)                                          52.3361\n",
      "time/total (s)                                        3346.27\n",
      "Epoch                                                   67\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:21:34.494471 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 68 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00172722\n",
      "trainer/QF2 Loss                                         0.000777282\n",
      "trainer/Policy Loss                                      3.22243\n",
      "trainer/Q1 Predictions Mean                             -1.13803\n",
      "trainer/Q1 Predictions Std                               0.760872\n",
      "trainer/Q1 Predictions Max                               0.676283\n",
      "trainer/Q1 Predictions Min                              -3.28053\n",
      "trainer/Q2 Predictions Mean                             -1.12633\n",
      "trainer/Q2 Predictions Std                               0.755157\n",
      "trainer/Q2 Predictions Max                               0.671782\n",
      "trainer/Q2 Predictions Min                              -3.29702\n",
      "trainer/Q Targets Mean                                  -1.12269\n",
      "trainer/Q Targets Std                                    0.755176\n",
      "trainer/Q Targets Max                                    0.654324\n",
      "trainer/Q Targets Min                                   -3.28438\n",
      "trainer/Log Pis Mean                                     2.09192\n",
      "trainer/Log Pis Std                                      1.32213\n",
      "trainer/Log Pis Max                                      4.543\n",
      "trainer/Log Pis Min                                     -2.33965\n",
      "trainer/Policy mu Mean                                   0.00024379\n",
      "trainer/Policy mu Std                                    0.384885\n",
      "trainer/Policy mu Max                                    1.97334\n",
      "trainer/Policy mu Min                                   -1.74154\n",
      "trainer/Policy log std Mean                             -2.30036\n",
      "trainer/Policy log std Std                               0.655229\n",
      "trainer/Policy log std Max                               0.104348\n",
      "trainer/Policy log std Min                              -3.20241\n",
      "trainer/Alpha                                            0.0234401\n",
      "trainer/Alpha Loss                                       0.345104\n",
      "exploration/num steps total                           7900\n",
      "exploration/num paths total                            395\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.115336\n",
      "exploration/Rewards Std                                  0.147503\n",
      "exploration/Rewards Max                                  0.06014\n",
      "exploration/Rewards Min                                 -0.632688\n",
      "exploration/Returns Mean                                -2.30672\n",
      "exploration/Returns Std                                  2.5205\n",
      "exploration/Returns Max                                  0.292477\n",
      "exploration/Returns Min                                 -6.73799\n",
      "exploration/Actions Mean                                -0.00740298\n",
      "exploration/Actions Std                                  0.19192\n",
      "exploration/Actions Max                                  0.598612\n",
      "exploration/Actions Min                                 -0.712987\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.30672\n",
      "exploration/env_infos/final/reward_dist Mean             0.273079\n",
      "exploration/env_infos/final/reward_dist Std              0.364135\n",
      "exploration/env_infos/final/reward_dist Max              0.968844\n",
      "exploration/env_infos/final/reward_dist Min              1.30113e-31\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00736255\n",
      "exploration/env_infos/initial/reward_dist Std            0.00654993\n",
      "exploration/env_infos/initial/reward_dist Max            0.0176867\n",
      "exploration/env_infos/initial/reward_dist Min            0.000226222\n",
      "exploration/env_infos/reward_dist Mean                   0.227021\n",
      "exploration/env_infos/reward_dist Std                    0.324032\n",
      "exploration/env_infos/reward_dist Max                    0.999999\n",
      "exploration/env_infos/reward_dist Min                    1.30113e-31\n",
      "exploration/env_infos/final/reward_energy Mean          -0.328671\n",
      "exploration/env_infos/final/reward_energy Std            0.19416\n",
      "exploration/env_infos/final/reward_energy Max           -0.115863\n",
      "exploration/env_infos/final/reward_energy Min           -0.681776\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.324851\n",
      "exploration/env_infos/initial/reward_energy Std          0.0874401\n",
      "exploration/env_infos/initial/reward_energy Max         -0.200166\n",
      "exploration/env_infos/initial/reward_energy Min         -0.413335\n",
      "exploration/env_infos/reward_energy Mean                -0.228935\n",
      "exploration/env_infos/reward_energy Std                  0.146167\n",
      "exploration/env_infos/reward_energy Max                 -0.0211623\n",
      "exploration/env_infos/reward_energy Min                 -0.807032\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.135059\n",
      "exploration/env_infos/final/end_effector_loc Std         0.264153\n",
      "exploration/env_infos/final/end_effector_loc Max         0.269729\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.537556\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00227542\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0116743\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0183508\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0180107\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0749623\n",
      "exploration/env_infos/end_effector_loc Std               0.18569\n",
      "exploration/env_infos/end_effector_loc Max               0.276773\n",
      "exploration/env_infos/end_effector_loc Min              -0.537556\n",
      "evaluation/num steps total                           69000\n",
      "evaluation/num paths total                            3450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0629682\n",
      "evaluation/Rewards Std                                   0.0732746\n",
      "evaluation/Rewards Max                                   0.0909085\n",
      "evaluation/Rewards Min                                  -0.422571\n",
      "evaluation/Returns Mean                                 -1.25936\n",
      "evaluation/Returns Std                                   1.03871\n",
      "evaluation/Returns Max                                   0.727792\n",
      "evaluation/Returns Min                                  -3.40581\n",
      "evaluation/Actions Mean                                 -0.000187787\n",
      "evaluation/Actions Std                                   0.0813938\n",
      "evaluation/Actions Max                                   0.730927\n",
      "evaluation/Actions Min                                  -0.903367\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.25936\n",
      "evaluation/env_infos/final/reward_dist Mean              0.135996\n",
      "evaluation/env_infos/final/reward_dist Std               0.224241\n",
      "evaluation/env_infos/final/reward_dist Max               0.896155\n",
      "evaluation/env_infos/final/reward_dist Min               2.10406e-23\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705512\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0171036\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0899132\n",
      "evaluation/env_infos/initial/reward_dist Min             1.4271e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.165155\n",
      "evaluation/env_infos/reward_dist Std                     0.247343\n",
      "evaluation/env_infos/reward_dist Max                     0.996618\n",
      "evaluation/env_infos/reward_dist Min                     2.10406e-23\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0458434\n",
      "evaluation/env_infos/final/reward_energy Std             0.0398149\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000655285\n",
      "evaluation/env_infos/final/reward_energy Min            -0.170266\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.246643\n",
      "evaluation/env_infos/initial/reward_energy Std           0.234207\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0161244\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.958\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0680376\n",
      "evaluation/env_infos/reward_energy Std                   0.0928486\n",
      "evaluation/env_infos/reward_energy Max                  -0.00027507\n",
      "evaluation/env_infos/reward_energy Min                  -0.958\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0159061\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.261089\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.564158\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.630649\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -9.83276e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0120249\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0365463\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0451683\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00828856\n",
      "evaluation/env_infos/end_effector_loc Std                0.167835\n",
      "evaluation/env_infos/end_effector_loc Max                0.564158\n",
      "evaluation/env_infos/end_effector_loc Min               -0.630649\n",
      "time/data storing (s)                                    0.00303915\n",
      "time/evaluation sampling (s)                             1.03508\n",
      "time/exploration sampling (s)                            0.142272\n",
      "time/logging (s)                                         0.0193741\n",
      "time/saving (s)                                          0.0283905\n",
      "time/training (s)                                       49.5159\n",
      "time/epoch (s)                                          50.7441\n",
      "time/total (s)                                        3397.93\n",
      "Epoch                                                   68\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:22:25.228978 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 69 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000933154\n",
      "trainer/QF2 Loss                                         0.000682523\n",
      "trainer/Policy Loss                                      3.07447\n",
      "trainer/Q1 Predictions Mean                             -1.11253\n",
      "trainer/Q1 Predictions Std                               0.843683\n",
      "trainer/Q1 Predictions Max                               0.838345\n",
      "trainer/Q1 Predictions Min                              -3.43732\n",
      "trainer/Q2 Predictions Mean                             -1.11363\n",
      "trainer/Q2 Predictions Std                               0.842185\n",
      "trainer/Q2 Predictions Max                               0.849407\n",
      "trainer/Q2 Predictions Min                              -3.43083\n",
      "trainer/Q Targets Mean                                  -1.11499\n",
      "trainer/Q Targets Std                                    0.844636\n",
      "trainer/Q Targets Max                                    0.845606\n",
      "trainer/Q Targets Min                                   -3.41585\n",
      "trainer/Log Pis Mean                                     1.97151\n",
      "trainer/Log Pis Std                                      1.47977\n",
      "trainer/Log Pis Max                                      4.3675\n",
      "trainer/Log Pis Min                                     -7.78597\n",
      "trainer/Policy mu Mean                                   0.0141081\n",
      "trainer/Policy mu Std                                    0.308223\n",
      "trainer/Policy mu Max                                    1.42802\n",
      "trainer/Policy mu Min                                   -1.58876\n",
      "trainer/Policy log std Mean                             -2.30164\n",
      "trainer/Policy log std Std                               0.672328\n",
      "trainer/Policy log std Max                               0.53859\n",
      "trainer/Policy log std Min                              -3.23721\n",
      "trainer/Alpha                                            0.0224914\n",
      "trainer/Alpha Loss                                      -0.108064\n",
      "exploration/num steps total                           8000\n",
      "exploration/num paths total                            400\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.10127\n",
      "exploration/Rewards Std                                  0.0577385\n",
      "exploration/Rewards Max                                  0.012892\n",
      "exploration/Rewards Min                                 -0.285635\n",
      "exploration/Returns Mean                                -2.02541\n",
      "exploration/Returns Std                                  0.250597\n",
      "exploration/Returns Max                                 -1.75535\n",
      "exploration/Returns Min                                 -2.42366\n",
      "exploration/Actions Mean                                 0.00349451\n",
      "exploration/Actions Std                                  0.0994182\n",
      "exploration/Actions Max                                  0.332261\n",
      "exploration/Actions Min                                 -0.360477\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.02541\n",
      "exploration/env_infos/final/reward_dist Mean             0.0537053\n",
      "exploration/env_infos/final/reward_dist Std              0.106457\n",
      "exploration/env_infos/final/reward_dist Max              0.266616\n",
      "exploration/env_infos/final/reward_dist Min              2.96158e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000434665\n",
      "exploration/env_infos/initial/reward_dist Std            0.000494509\n",
      "exploration/env_infos/initial/reward_dist Max            0.0013831\n",
      "exploration/env_infos/initial/reward_dist Min            4.68113e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.110503\n",
      "exploration/env_infos/reward_dist Std                    0.212478\n",
      "exploration/env_infos/reward_dist Max                    0.979295\n",
      "exploration/env_infos/reward_dist Min                    2.96158e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.121605\n",
      "exploration/env_infos/final/reward_energy Std            0.0442026\n",
      "exploration/env_infos/final/reward_energy Max           -0.0591545\n",
      "exploration/env_infos/final/reward_energy Min           -0.182283\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.19603\n",
      "exploration/env_infos/initial/reward_energy Std          0.0890731\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0961772\n",
      "exploration/env_infos/initial/reward_energy Min         -0.348777\n",
      "exploration/env_infos/reward_energy Mean                -0.114617\n",
      "exploration/env_infos/reward_energy Std                  0.08158\n",
      "exploration/env_infos/reward_energy Max                 -0.00967994\n",
      "exploration/env_infos/reward_energy Min                 -0.408036\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.172271\n",
      "exploration/env_infos/final/end_effector_loc Std         0.184701\n",
      "exploration/env_infos/final/end_effector_loc Max         0.467517\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.163024\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00358183\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00671733\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.013139\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0114665\n",
      "exploration/env_infos/end_effector_loc Mean              0.105474\n",
      "exploration/env_infos/end_effector_loc Std               0.121177\n",
      "exploration/env_infos/end_effector_loc Max               0.467517\n",
      "exploration/env_infos/end_effector_loc Min              -0.163024\n",
      "evaluation/num steps total                           70000\n",
      "evaluation/num paths total                            3500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0532707\n",
      "evaluation/Rewards Std                                   0.0712409\n",
      "evaluation/Rewards Max                                   0.111361\n",
      "evaluation/Rewards Min                                  -0.629201\n",
      "evaluation/Returns Mean                                 -1.06541\n",
      "evaluation/Returns Std                                   1.02132\n",
      "evaluation/Returns Max                                   0.983864\n",
      "evaluation/Returns Min                                  -3.70963\n",
      "evaluation/Actions Mean                                  0.00187879\n",
      "evaluation/Actions Std                                   0.0728665\n",
      "evaluation/Actions Max                                   0.688405\n",
      "evaluation/Actions Min                                  -0.745729\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.06541\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0685799\n",
      "evaluation/env_infos/final/reward_dist Std               0.134646\n",
      "evaluation/env_infos/final/reward_dist Max               0.545118\n",
      "evaluation/env_infos/final/reward_dist Min               2.97538e-33\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00566398\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00932576\n",
      "evaluation/env_infos/initial/reward_dist Max             0.038538\n",
      "evaluation/env_infos/initial/reward_dist Min             9.12586e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.165822\n",
      "evaluation/env_infos/reward_dist Std                     0.245469\n",
      "evaluation/env_infos/reward_dist Max                     0.999444\n",
      "evaluation/env_infos/reward_dist Min                     2.97538e-33\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0367164\n",
      "evaluation/env_infos/final/reward_energy Std             0.0464634\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00170104\n",
      "evaluation/env_infos/final/reward_energy Min            -0.273491\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.231512\n",
      "evaluation/env_infos/initial/reward_energy Std           0.230245\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0159139\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.868661\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0609396\n",
      "evaluation/env_infos/reward_energy Std                   0.0831413\n",
      "evaluation/env_infos/reward_energy Max                  -0.00146658\n",
      "evaluation/env_infos/reward_energy Min                  -0.868661\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00256867\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.264259\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.492451\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.618989\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00120672\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0114807\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0344202\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0372864\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00540467\n",
      "evaluation/env_infos/end_effector_loc Std                0.16763\n",
      "evaluation/env_infos/end_effector_loc Max                0.492451\n",
      "evaluation/env_infos/end_effector_loc Min               -0.618989\n",
      "time/data storing (s)                                    0.00302765\n",
      "time/evaluation sampling (s)                             0.985354\n",
      "time/exploration sampling (s)                            0.126233\n",
      "time/logging (s)                                         0.0192381\n",
      "time/saving (s)                                          0.0270467\n",
      "time/training (s)                                       48.6291\n",
      "time/epoch (s)                                          49.79\n",
      "time/total (s)                                        3448.66\n",
      "Epoch                                                   69\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:23:15.934962 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 70 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00067761\n",
      "trainer/QF2 Loss                                         0.00082893\n",
      "trainer/Policy Loss                                      3.28411\n",
      "trainer/Q1 Predictions Mean                             -1.26548\n",
      "trainer/Q1 Predictions Std                               0.881988\n",
      "trainer/Q1 Predictions Max                               0.863735\n",
      "trainer/Q1 Predictions Min                              -3.31856\n",
      "trainer/Q2 Predictions Mean                             -1.26307\n",
      "trainer/Q2 Predictions Std                               0.881034\n",
      "trainer/Q2 Predictions Max                               0.858544\n",
      "trainer/Q2 Predictions Min                              -3.33858\n",
      "trainer/Q Targets Mean                                  -1.25928\n",
      "trainer/Q Targets Std                                    0.881933\n",
      "trainer/Q Targets Max                                    0.847508\n",
      "trainer/Q Targets Min                                   -3.3003\n",
      "trainer/Log Pis Mean                                     2.02841\n",
      "trainer/Log Pis Std                                      1.47298\n",
      "trainer/Log Pis Max                                      4.61068\n",
      "trainer/Log Pis Min                                     -4.23207\n",
      "trainer/Policy mu Mean                                   0.00976759\n",
      "trainer/Policy mu Std                                    0.308773\n",
      "trainer/Policy mu Max                                    1.36709\n",
      "trainer/Policy mu Min                                   -1.80422\n",
      "trainer/Policy log std Mean                             -2.34348\n",
      "trainer/Policy log std Std                               0.623164\n",
      "trainer/Policy log std Max                              -0.155634\n",
      "trainer/Policy log std Min                              -3.25313\n",
      "trainer/Alpha                                            0.0226011\n",
      "trainer/Alpha Loss                                       0.107683\n",
      "exploration/num steps total                           8100\n",
      "exploration/num paths total                            405\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.107869\n",
      "exploration/Rewards Std                                  0.0516748\n",
      "exploration/Rewards Max                                  0.0359467\n",
      "exploration/Rewards Min                                 -0.302525\n",
      "exploration/Returns Mean                                -2.15739\n",
      "exploration/Returns Std                                  0.385669\n",
      "exploration/Returns Max                                 -1.70303\n",
      "exploration/Returns Min                                 -2.62353\n",
      "exploration/Actions Mean                                 0.00210449\n",
      "exploration/Actions Std                                  0.136825\n",
      "exploration/Actions Max                                  0.501863\n",
      "exploration/Actions Min                                 -0.935936\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.15739\n",
      "exploration/env_infos/final/reward_dist Mean             0.000709981\n",
      "exploration/env_infos/final/reward_dist Std              0.00141995\n",
      "exploration/env_infos/final/reward_dist Max              0.00354988\n",
      "exploration/env_infos/final/reward_dist Min              4.1737e-26\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0219133\n",
      "exploration/env_infos/initial/reward_dist Std            0.0271101\n",
      "exploration/env_infos/initial/reward_dist Max            0.0690864\n",
      "exploration/env_infos/initial/reward_dist Min            1.308e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0990189\n",
      "exploration/env_infos/reward_dist Std                    0.233166\n",
      "exploration/env_infos/reward_dist Max                    0.983897\n",
      "exploration/env_infos/reward_dist Min                    4.1737e-26\n",
      "exploration/env_infos/final/reward_energy Mean          -0.148502\n",
      "exploration/env_infos/final/reward_energy Std            0.0489454\n",
      "exploration/env_infos/final/reward_energy Max           -0.0998077\n",
      "exploration/env_infos/final/reward_energy Min           -0.232283\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.329278\n",
      "exploration/env_infos/initial/reward_energy Std          0.371611\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0427611\n",
      "exploration/env_infos/initial/reward_energy Min         -0.985159\n",
      "exploration/env_infos/reward_energy Mean                -0.144381\n",
      "exploration/env_infos/reward_energy Std                  0.12886\n",
      "exploration/env_infos/reward_energy Max                 -0.013158\n",
      "exploration/env_infos/reward_energy Min                 -0.985159\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0634185\n",
      "exploration/env_infos/final/end_effector_loc Std         0.266352\n",
      "exploration/env_infos/final/end_effector_loc Max         0.546713\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.245968\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00354208\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0171931\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0250931\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0467968\n",
      "exploration/env_infos/end_effector_loc Mean              0.0236789\n",
      "exploration/env_infos/end_effector_loc Std               0.193137\n",
      "exploration/env_infos/end_effector_loc Max               0.551667\n",
      "exploration/env_infos/end_effector_loc Min              -0.417148\n",
      "evaluation/num steps total                           71000\n",
      "evaluation/num paths total                            3550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.048161\n",
      "evaluation/Rewards Std                                   0.076957\n",
      "evaluation/Rewards Max                                   0.16054\n",
      "evaluation/Rewards Min                                  -0.373781\n",
      "evaluation/Returns Mean                                 -0.96322\n",
      "evaluation/Returns Std                                   1.16668\n",
      "evaluation/Returns Max                                   1.15382\n",
      "evaluation/Returns Min                                  -3.08357\n",
      "evaluation/Actions Mean                                  0.0029595\n",
      "evaluation/Actions Std                                   0.0792346\n",
      "evaluation/Actions Max                                   0.494885\n",
      "evaluation/Actions Min                                  -0.986349\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.96322\n",
      "evaluation/env_infos/final/reward_dist Mean              0.29648\n",
      "evaluation/env_infos/final/reward_dist Std               0.327964\n",
      "evaluation/env_infos/final/reward_dist Max               0.999525\n",
      "evaluation/env_infos/final/reward_dist Min               2.12241e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0108816\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0197919\n",
      "evaluation/env_infos/initial/reward_dist Max             0.105143\n",
      "evaluation/env_infos/initial/reward_dist Min             1.77161e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.235278\n",
      "evaluation/env_infos/reward_dist Std                     0.296738\n",
      "evaluation/env_infos/reward_dist Max                     0.999525\n",
      "evaluation/env_infos/reward_dist Min                     2.12241e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0539763\n",
      "evaluation/env_infos/final/reward_energy Std             0.084284\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0037007\n",
      "evaluation/env_infos/final/reward_energy Min            -0.518926\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.237599\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255794\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0132959\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.21304\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0644339\n",
      "evaluation/env_infos/reward_energy Std                   0.0917715\n",
      "evaluation/env_infos/reward_energy Max                  -0.000902422\n",
      "evaluation/env_infos/reward_energy Min                  -1.21304\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00756622\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.253989\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.682794\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.488399\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00165384\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122319\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0247442\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0493174\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00263564\n",
      "evaluation/env_infos/end_effector_loc Std                0.167649\n",
      "evaluation/env_infos/end_effector_loc Max                0.682794\n",
      "evaluation/env_infos/end_effector_loc Min               -0.488399\n",
      "time/data storing (s)                                    0.00302955\n",
      "time/evaluation sampling (s)                             0.992106\n",
      "time/exploration sampling (s)                            0.12606\n",
      "time/logging (s)                                         0.0234389\n",
      "time/saving (s)                                          0.0293689\n",
      "time/training (s)                                       48.6712\n",
      "time/epoch (s)                                          49.8452\n",
      "time/total (s)                                        3499.37\n",
      "Epoch                                                   70\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:24:06.020678 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 71 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000933849\r\n",
      "trainer/QF2 Loss                                         0.000966241\r\n",
      "trainer/Policy Loss                                      2.91713\r\n",
      "trainer/Q1 Predictions Mean                             -1.07943\r\n",
      "trainer/Q1 Predictions Std                               0.866178\r\n",
      "trainer/Q1 Predictions Max                               1.22607\r\n",
      "trainer/Q1 Predictions Min                              -3.43242\r\n",
      "trainer/Q2 Predictions Mean                             -1.07468\r\n",
      "trainer/Q2 Predictions Std                               0.864412\r\n",
      "trainer/Q2 Predictions Max                               1.21117\r\n",
      "trainer/Q2 Predictions Min                              -3.44985\r\n",
      "trainer/Q Targets Mean                                  -1.0712\r\n",
      "trainer/Q Targets Std                                    0.864888\r\n",
      "trainer/Q Targets Max                                    1.18584\r\n",
      "trainer/Q Targets Min                                   -3.47762\r\n",
      "trainer/Log Pis Mean                                     1.83648\r\n",
      "trainer/Log Pis Std                                      1.27768\r\n",
      "trainer/Log Pis Max                                      4.11516\r\n",
      "trainer/Log Pis Min                                     -2.48852\r\n",
      "trainer/Policy mu Mean                                   0.0303272\r\n",
      "trainer/Policy mu Std                                    0.295716\r\n",
      "trainer/Policy mu Max                                    1.64069\r\n",
      "trainer/Policy mu Min                                   -1.30455\r\n",
      "trainer/Policy log std Mean                             -2.23488\r\n",
      "trainer/Policy log std Std                               0.608597\r\n",
      "trainer/Policy log std Max                              -0.319722\r\n",
      "trainer/Policy log std Min                              -3.23815\r\n",
      "trainer/Alpha                                            0.0226957\r\n",
      "trainer/Alpha Loss                                      -0.61891\r\n",
      "exploration/num steps total                           8200\r\n",
      "exploration/num paths total                            410\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0808401\r\n",
      "exploration/Rewards Std                                  0.0718923\r\n",
      "exploration/Rewards Max                                  0.0353937\r\n",
      "exploration/Rewards Min                                 -0.594055\r\n",
      "exploration/Returns Mean                                -1.6168\r\n",
      "exploration/Returns Std                                  0.512901\r\n",
      "exploration/Returns Max                                 -1.06121\r\n",
      "exploration/Returns Min                                 -2.54159\r\n",
      "exploration/Actions Mean                                 0.00823882\r\n",
      "exploration/Actions Std                                  0.133173\r\n",
      "exploration/Actions Max                                  0.578739\r\n",
      "exploration/Actions Min                                 -0.773362\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.6168\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.164572\r\n",
      "exploration/env_infos/final/reward_dist Std              0.202397\r\n",
      "exploration/env_infos/final/reward_dist Max              0.44223\r\n",
      "exploration/env_infos/final/reward_dist Min              1.97065e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0120119\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0199204\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0515621\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000344107\r\n",
      "exploration/env_infos/reward_dist Mean                   0.152894\r\n",
      "exploration/env_infos/reward_dist Std                    0.239948\r\n",
      "exploration/env_infos/reward_dist Max                    0.934106\r\n",
      "exploration/env_infos/reward_dist Min                    7.55207e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.124072\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0615268\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0430175\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.229763\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.404912\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.316674\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0300679\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.814621\r\n",
      "exploration/env_infos/reward_energy Mean                -0.128832\r\n",
      "exploration/env_infos/reward_energy Std                  0.137869\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0180296\r\n",
      "exploration/env_infos/reward_energy Min                 -0.814621\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0361425\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.24588\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.512964\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.434238\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00278875\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0179588\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0222173\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0386681\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00388596\r\n",
      "exploration/env_infos/end_effector_loc Std               0.181597\r\n",
      "exploration/env_infos/end_effector_loc Max               0.512964\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.469832\r\n",
      "evaluation/num steps total                           72000\r\n",
      "evaluation/num paths total                            3600\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0512251\r\n",
      "evaluation/Rewards Std                                   0.079413\r\n",
      "evaluation/Rewards Max                                   0.149412\r\n",
      "evaluation/Rewards Min                                  -0.694786\r\n",
      "evaluation/Returns Mean                                 -1.0245\r\n",
      "evaluation/Returns Std                                   1.13961\r\n",
      "evaluation/Returns Max                                   2.11206\r\n",
      "evaluation/Returns Min                                  -5.17624\r\n",
      "evaluation/Actions Mean                                  0.00355475\r\n",
      "evaluation/Actions Std                                   0.0797162\r\n",
      "evaluation/Actions Max                                   0.661872\r\n",
      "evaluation/Actions Min                                  -0.867439\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.0245\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.233588\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.320058\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.988518\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.61387e-97\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00712585\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0106051\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0424511\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.23273e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.21517\r\n",
      "evaluation/env_infos/reward_dist Std                     0.282242\r\n",
      "evaluation/env_infos/reward_dist Max                     1\r\n",
      "evaluation/env_infos/reward_dist Min                     8.61387e-97\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0755723\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.139757\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0040871\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.754334\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.224537\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.224555\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00397889\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.906472\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.064667\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0924814\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000855171\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.906472\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0406184\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.249537\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.414779\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000912248\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0111902\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.025133\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0433719\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.02153\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.153493\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.414779\r\n",
      "time/data storing (s)                                    0.00288802\r\n",
      "time/evaluation sampling (s)                             0.983816\r\n",
      "time/exploration sampling (s)                            0.125545\r\n",
      "time/logging (s)                                         0.0205728\r\n",
      "time/saving (s)                                          0.0294247\r\n",
      "time/training (s)                                       47.9402\r\n",
      "time/epoch (s)                                          49.1024\r\n",
      "time/total (s)                                        3549.45\r\n",
      "Epoch                                                   71\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:24:56.187837 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 72 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000909551\n",
      "trainer/QF2 Loss                                         0.000760931\n",
      "trainer/Policy Loss                                      3.45846\n",
      "trainer/Q1 Predictions Mean                             -1.25861\n",
      "trainer/Q1 Predictions Std                               0.863\n",
      "trainer/Q1 Predictions Max                               0.897449\n",
      "trainer/Q1 Predictions Min                              -3.47107\n",
      "trainer/Q2 Predictions Mean                             -1.25147\n",
      "trainer/Q2 Predictions Std                               0.863416\n",
      "trainer/Q2 Predictions Max                               0.854898\n",
      "trainer/Q2 Predictions Min                              -3.47101\n",
      "trainer/Q Targets Mean                                  -1.24906\n",
      "trainer/Q Targets Std                                    0.861997\n",
      "trainer/Q Targets Max                                    0.873771\n",
      "trainer/Q Targets Min                                   -3.52495\n",
      "trainer/Log Pis Mean                                     2.20937\n",
      "trainer/Log Pis Std                                      1.28884\n",
      "trainer/Log Pis Max                                      4.69985\n",
      "trainer/Log Pis Min                                     -2.17544\n",
      "trainer/Policy mu Mean                                   0.00859512\n",
      "trainer/Policy mu Std                                    0.231722\n",
      "trainer/Policy mu Max                                    1.17068\n",
      "trainer/Policy mu Min                                   -1.55767\n",
      "trainer/Policy log std Mean                             -2.42016\n",
      "trainer/Policy log std Std                               0.550277\n",
      "trainer/Policy log std Max                              -0.70136\n",
      "trainer/Policy log std Min                              -3.42124\n",
      "trainer/Alpha                                            0.0230616\n",
      "trainer/Alpha Loss                                       0.78942\n",
      "exploration/num steps total                           8300\n",
      "exploration/num paths total                            415\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.109283\n",
      "exploration/Rewards Std                                  0.110154\n",
      "exploration/Rewards Max                                  0.0683411\n",
      "exploration/Rewards Min                                 -0.688974\n",
      "exploration/Returns Mean                                -2.18565\n",
      "exploration/Returns Std                                  1.63198\n",
      "exploration/Returns Max                                 -0.429391\n",
      "exploration/Returns Min                                 -5.07478\n",
      "exploration/Actions Mean                                -1.79031e-05\n",
      "exploration/Actions Std                                  0.163321\n",
      "exploration/Actions Max                                  0.650414\n",
      "exploration/Actions Min                                 -0.449145\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.18565\n",
      "exploration/env_infos/final/reward_dist Mean             0.0512322\n",
      "exploration/env_infos/final/reward_dist Std              0.0655489\n",
      "exploration/env_infos/final/reward_dist Max              0.158111\n",
      "exploration/env_infos/final/reward_dist Min              2.38494e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0106987\n",
      "exploration/env_infos/initial/reward_dist Std            0.0113032\n",
      "exploration/env_infos/initial/reward_dist Max            0.0265981\n",
      "exploration/env_infos/initial/reward_dist Min            3.0235e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0773017\n",
      "exploration/env_infos/reward_dist Std                    0.164347\n",
      "exploration/env_infos/reward_dist Max                    0.936217\n",
      "exploration/env_infos/reward_dist Min                    8.95082e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.176516\n",
      "exploration/env_infos/final/reward_energy Std            0.0696335\n",
      "exploration/env_infos/final/reward_energy Max           -0.113575\n",
      "exploration/env_infos/final/reward_energy Min           -0.311935\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.322216\n",
      "exploration/env_infos/initial/reward_energy Std          0.187518\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0427308\n",
      "exploration/env_infos/initial/reward_energy Min         -0.532375\n",
      "exploration/env_infos/reward_energy Mean                -0.195703\n",
      "exploration/env_infos/reward_energy Std                  0.12267\n",
      "exploration/env_infos/reward_energy Max                 -0.0159297\n",
      "exploration/env_infos/reward_energy Min                 -0.685912\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0154771\n",
      "exploration/env_infos/final/end_effector_loc Std         0.263539\n",
      "exploration/env_infos/final/end_effector_loc Max         0.481457\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.445834\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00401514\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0125543\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0232033\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0224572\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00756241\n",
      "exploration/env_infos/end_effector_loc Std               0.200361\n",
      "exploration/env_infos/end_effector_loc Max               0.556836\n",
      "exploration/env_infos/end_effector_loc Min              -0.450517\n",
      "evaluation/num steps total                           73000\n",
      "evaluation/num paths total                            3650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0567901\n",
      "evaluation/Rewards Std                                   0.0760603\n",
      "evaluation/Rewards Max                                   0.129603\n",
      "evaluation/Rewards Min                                  -0.469098\n",
      "evaluation/Returns Mean                                 -1.1358\n",
      "evaluation/Returns Std                                   1.09119\n",
      "evaluation/Returns Max                                   0.678061\n",
      "evaluation/Returns Min                                  -4.64711\n",
      "evaluation/Actions Mean                                  0.00297403\n",
      "evaluation/Actions Std                                   0.0818852\n",
      "evaluation/Actions Max                                   0.692129\n",
      "evaluation/Actions Min                                  -0.798767\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.1358\n",
      "evaluation/env_infos/final/reward_dist Mean              0.165829\n",
      "evaluation/env_infos/final/reward_dist Std               0.242373\n",
      "evaluation/env_infos/final/reward_dist Max               0.910344\n",
      "evaluation/env_infos/final/reward_dist Min               4.32495e-100\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0073267\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0108329\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0364556\n",
      "evaluation/env_infos/initial/reward_dist Min             1.71578e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.205232\n",
      "evaluation/env_infos/reward_dist Std                     0.269432\n",
      "evaluation/env_infos/reward_dist Max                     0.997623\n",
      "evaluation/env_infos/reward_dist Min                     4.32495e-100\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.056135\n",
      "evaluation/env_infos/final/reward_energy Std             0.118099\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00218629\n",
      "evaluation/env_infos/final/reward_energy Min            -0.772063\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.225882\n",
      "evaluation/env_infos/initial/reward_energy Std           0.228103\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00681566\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.848953\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0632312\n",
      "evaluation/env_infos/reward_energy Std                   0.0971075\n",
      "evaluation/env_infos/reward_energy Max                  -0.00117835\n",
      "evaluation/env_infos/reward_energy Min                  -0.848953\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.054149\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.260944\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.6707\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0036676\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107408\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0346064\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0399384\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0395369\n",
      "evaluation/env_infos/end_effector_loc Std                0.168158\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.6707\n",
      "time/data storing (s)                                    0.00311781\n",
      "time/evaluation sampling (s)                             0.947244\n",
      "time/exploration sampling (s)                            0.119744\n",
      "time/logging (s)                                         0.0199005\n",
      "time/saving (s)                                          0.0281707\n",
      "time/training (s)                                       48.084\n",
      "time/epoch (s)                                          49.2022\n",
      "time/total (s)                                        3599.61\n",
      "Epoch                                                   72\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:25:46.845365 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 73 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000566648\n",
      "trainer/QF2 Loss                                         0.000790792\n",
      "trainer/Policy Loss                                      2.92605\n",
      "trainer/Q1 Predictions Mean                             -1.04742\n",
      "trainer/Q1 Predictions Std                               0.865891\n",
      "trainer/Q1 Predictions Max                               1.22881\n",
      "trainer/Q1 Predictions Min                              -3.77677\n",
      "trainer/Q2 Predictions Mean                             -1.05444\n",
      "trainer/Q2 Predictions Std                               0.867626\n",
      "trainer/Q2 Predictions Max                               1.23928\n",
      "trainer/Q2 Predictions Min                              -3.77882\n",
      "trainer/Q Targets Mean                                  -1.04559\n",
      "trainer/Q Targets Std                                    0.860448\n",
      "trainer/Q Targets Max                                    1.20028\n",
      "trainer/Q Targets Min                                   -3.74542\n",
      "trainer/Log Pis Mean                                     1.8762\n",
      "trainer/Log Pis Std                                      1.37929\n",
      "trainer/Log Pis Max                                      4.2\n",
      "trainer/Log Pis Min                                     -3.73588\n",
      "trainer/Policy mu Mean                                   0.0413386\n",
      "trainer/Policy mu Std                                    0.300251\n",
      "trainer/Policy mu Max                                    1.70749\n",
      "trainer/Policy mu Min                                   -1.31965\n",
      "trainer/Policy log std Mean                             -2.2799\n",
      "trainer/Policy log std Std                               0.611822\n",
      "trainer/Policy log std Max                              -0.188869\n",
      "trainer/Policy log std Min                              -3.3546\n",
      "trainer/Alpha                                            0.0222377\n",
      "trainer/Alpha Loss                                      -0.471162\n",
      "exploration/num steps total                           8400\n",
      "exploration/num paths total                            420\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0911932\n",
      "exploration/Rewards Std                                  0.0782359\n",
      "exploration/Rewards Max                                  0.117461\n",
      "exploration/Rewards Min                                 -0.379602\n",
      "exploration/Returns Mean                                -1.82386\n",
      "exploration/Returns Std                                  0.942118\n",
      "exploration/Returns Max                                 -0.260485\n",
      "exploration/Returns Min                                 -2.77934\n",
      "exploration/Actions Mean                                -0.00272344\n",
      "exploration/Actions Std                                  0.0987086\n",
      "exploration/Actions Max                                  0.388581\n",
      "exploration/Actions Min                                 -0.328086\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.82386\n",
      "exploration/env_infos/final/reward_dist Mean             0.0789832\n",
      "exploration/env_infos/final/reward_dist Std              0.106286\n",
      "exploration/env_infos/final/reward_dist Max              0.27457\n",
      "exploration/env_infos/final/reward_dist Min              3.05045e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000150886\n",
      "exploration/env_infos/initial/reward_dist Std            0.00010756\n",
      "exploration/env_infos/initial/reward_dist Max            0.000309439\n",
      "exploration/env_infos/initial/reward_dist Min            1.31151e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.133266\n",
      "exploration/env_infos/reward_dist Std                    0.22121\n",
      "exploration/env_infos/reward_dist Max                    0.992348\n",
      "exploration/env_infos/reward_dist Min                    1.31151e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.129938\n",
      "exploration/env_infos/final/reward_energy Std            0.0880274\n",
      "exploration/env_infos/final/reward_energy Max           -0.0183847\n",
      "exploration/env_infos/final/reward_energy Min           -0.245748\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.212073\n",
      "exploration/env_infos/initial/reward_energy Std          0.110676\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0296867\n",
      "exploration/env_infos/initial/reward_energy Min         -0.365458\n",
      "exploration/env_infos/reward_energy Mean                -0.113404\n",
      "exploration/env_infos/reward_energy Std                  0.0814929\n",
      "exploration/env_infos/reward_energy Max                 -0.00195488\n",
      "exploration/env_infos/reward_energy Min                 -0.43976\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.027224\n",
      "exploration/env_infos/final/end_effector_loc Std         0.230915\n",
      "exploration/env_infos/final/end_effector_loc Max         0.408961\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.408729\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00055035\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00843963\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0164053\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00804759\n",
      "exploration/env_infos/end_effector_loc Mean              0.0278061\n",
      "exploration/env_infos/end_effector_loc Std               0.160744\n",
      "exploration/env_infos/end_effector_loc Max               0.408961\n",
      "exploration/env_infos/end_effector_loc Min              -0.408729\n",
      "evaluation/num steps total                           74000\n",
      "evaluation/num paths total                            3700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0461172\n",
      "evaluation/Rewards Std                                   0.0795324\n",
      "evaluation/Rewards Max                                   0.179078\n",
      "evaluation/Rewards Min                                  -0.459704\n",
      "evaluation/Returns Mean                                 -0.922344\n",
      "evaluation/Returns Std                                   1.20038\n",
      "evaluation/Returns Max                                   2.49339\n",
      "evaluation/Returns Min                                  -3.22894\n",
      "evaluation/Actions Mean                                 -0.00243599\n",
      "evaluation/Actions Std                                   0.0729711\n",
      "evaluation/Actions Max                                   0.599325\n",
      "evaluation/Actions Min                                  -0.821327\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.922344\n",
      "evaluation/env_infos/final/reward_dist Mean              0.141079\n",
      "evaluation/env_infos/final/reward_dist Std               0.187026\n",
      "evaluation/env_infos/final/reward_dist Max               0.761096\n",
      "evaluation/env_infos/final/reward_dist Min               3.85985e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00777009\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0226754\n",
      "evaluation/env_infos/initial/reward_dist Max             0.130532\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97142e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.15891\n",
      "evaluation/env_infos/reward_dist Std                     0.235335\n",
      "evaluation/env_infos/reward_dist Max                     0.996795\n",
      "evaluation/env_infos/reward_dist Min                     3.85985e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.034279\n",
      "evaluation/env_infos/final/reward_energy Std             0.0302135\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00208216\n",
      "evaluation/env_infos/final/reward_energy Min            -0.119497\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.247821\n",
      "evaluation/env_infos/initial/reward_energy Std           0.26772\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0184779\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.03769\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0577981\n",
      "evaluation/env_infos/reward_energy Std                   0.0855618\n",
      "evaluation/env_infos/reward_energy Max                  -0.00174582\n",
      "evaluation/env_infos/reward_energy Min                  -1.03769\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0699772\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.232147\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.582343\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.676979\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00401759\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122564\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0299662\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0410663\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0394361\n",
      "evaluation/env_infos/end_effector_loc Std                0.151999\n",
      "evaluation/env_infos/end_effector_loc Max                0.582343\n",
      "evaluation/env_infos/end_effector_loc Min               -0.676979\n",
      "time/data storing (s)                                    0.00296172\n",
      "time/evaluation sampling (s)                             0.931534\n",
      "time/exploration sampling (s)                            0.120119\n",
      "time/logging (s)                                         0.0254115\n",
      "time/saving (s)                                          0.031027\n",
      "time/training (s)                                       48.6218\n",
      "time/epoch (s)                                          49.7328\n",
      "time/total (s)                                        3650.27\n",
      "Epoch                                                   73\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:26:39.423571 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 74 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000568868\n",
      "trainer/QF2 Loss                                         0.000566029\n",
      "trainer/Policy Loss                                      3.06203\n",
      "trainer/Q1 Predictions Mean                             -1.13993\n",
      "trainer/Q1 Predictions Std                               0.822353\n",
      "trainer/Q1 Predictions Max                               0.853925\n",
      "trainer/Q1 Predictions Min                              -3.6003\n",
      "trainer/Q2 Predictions Mean                             -1.14077\n",
      "trainer/Q2 Predictions Std                               0.821583\n",
      "trainer/Q2 Predictions Max                               0.848635\n",
      "trainer/Q2 Predictions Min                              -3.58567\n",
      "trainer/Q Targets Mean                                  -1.14386\n",
      "trainer/Q Targets Std                                    0.822877\n",
      "trainer/Q Targets Max                                    0.872227\n",
      "trainer/Q Targets Min                                   -3.59972\n",
      "trainer/Log Pis Mean                                     1.92213\n",
      "trainer/Log Pis Std                                      1.30932\n",
      "trainer/Log Pis Max                                      4.41955\n",
      "trainer/Log Pis Min                                     -2.5893\n",
      "trainer/Policy mu Mean                                   0.0255634\n",
      "trainer/Policy mu Std                                    0.274383\n",
      "trainer/Policy mu Max                                    1.68296\n",
      "trainer/Policy mu Min                                   -1.36368\n",
      "trainer/Policy log std Mean                             -2.29775\n",
      "trainer/Policy log std Std                               0.574466\n",
      "trainer/Policy log std Max                              -0.00674558\n",
      "trainer/Policy log std Min                              -3.21397\n",
      "trainer/Alpha                                            0.0240624\n",
      "trainer/Alpha Loss                                      -0.29023\n",
      "exploration/num steps total                           8500\n",
      "exploration/num paths total                            425\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0767073\n",
      "exploration/Rewards Std                                  0.051329\n",
      "exploration/Rewards Max                                  0.0556786\n",
      "exploration/Rewards Min                                 -0.22084\n",
      "exploration/Returns Mean                                -1.53415\n",
      "exploration/Returns Std                                  0.68672\n",
      "exploration/Returns Max                                 -0.287155\n",
      "exploration/Returns Min                                 -2.25788\n",
      "exploration/Actions Mean                                 0.0105182\n",
      "exploration/Actions Std                                  0.0777301\n",
      "exploration/Actions Max                                  0.22853\n",
      "exploration/Actions Min                                 -0.226932\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.53415\n",
      "exploration/env_infos/final/reward_dist Mean             0.188062\n",
      "exploration/env_infos/final/reward_dist Std              0.340725\n",
      "exploration/env_infos/final/reward_dist Max              0.868174\n",
      "exploration/env_infos/final/reward_dist Min              3.90563e-15\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00814117\n",
      "exploration/env_infos/initial/reward_dist Std            0.00963569\n",
      "exploration/env_infos/initial/reward_dist Max            0.0220829\n",
      "exploration/env_infos/initial/reward_dist Min            1.69474e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.167786\n",
      "exploration/env_infos/reward_dist Std                    0.247747\n",
      "exploration/env_infos/reward_dist Max                    0.983608\n",
      "exploration/env_infos/reward_dist Min                    3.90563e-15\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0947197\n",
      "exploration/env_infos/final/reward_energy Std            0.0452672\n",
      "exploration/env_infos/final/reward_energy Max           -0.0295305\n",
      "exploration/env_infos/final/reward_energy Min           -0.160418\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.113964\n",
      "exploration/env_infos/initial/reward_energy Std          0.053806\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0580438\n",
      "exploration/env_infos/initial/reward_energy Min         -0.17813\n",
      "exploration/env_infos/reward_energy Mean                -0.0968238\n",
      "exploration/env_infos/reward_energy Std                  0.0541327\n",
      "exploration/env_infos/reward_energy Max                 -0.00919411\n",
      "exploration/env_infos/reward_energy Min                 -0.228613\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0937939\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275244\n",
      "exploration/env_infos/final/end_effector_loc Max         0.702439\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.326922\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00190791\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00402661\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00724638\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00735162\n",
      "exploration/env_infos/end_effector_loc Mean              0.0319783\n",
      "exploration/env_infos/end_effector_loc Std               0.152127\n",
      "exploration/env_infos/end_effector_loc Max               0.702439\n",
      "exploration/env_infos/end_effector_loc Min              -0.326922\n",
      "evaluation/num steps total                           75000\n",
      "evaluation/num paths total                            3750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0382309\n",
      "evaluation/Rewards Std                                   0.0651618\n",
      "evaluation/Rewards Max                                   0.15911\n",
      "evaluation/Rewards Min                                  -0.436894\n",
      "evaluation/Returns Mean                                 -0.764619\n",
      "evaluation/Returns Std                                   0.919025\n",
      "evaluation/Returns Max                                   1.37177\n",
      "evaluation/Returns Min                                  -2.54442\n",
      "evaluation/Actions Mean                                  0.00235383\n",
      "evaluation/Actions Std                                   0.0746204\n",
      "evaluation/Actions Max                                   0.616858\n",
      "evaluation/Actions Min                                  -0.757315\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.764619\n",
      "evaluation/env_infos/final/reward_dist Mean              0.210313\n",
      "evaluation/env_infos/final/reward_dist Std               0.276512\n",
      "evaluation/env_infos/final/reward_dist Max               0.905114\n",
      "evaluation/env_infos/final/reward_dist Min               1.18453e-08\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00928486\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165834\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0752171\n",
      "evaluation/env_infos/initial/reward_dist Min             9.89679e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.210817\n",
      "evaluation/env_infos/reward_dist Std                     0.260714\n",
      "evaluation/env_infos/reward_dist Max                     0.99045\n",
      "evaluation/env_infos/reward_dist Min                     1.18453e-08\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0487056\n",
      "evaluation/env_infos/final/reward_energy Std             0.0405916\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00350038\n",
      "evaluation/env_infos/final/reward_energy Min            -0.152252\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.235426\n",
      "evaluation/env_infos/initial/reward_energy Std           0.224121\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00763535\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.996583\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0662758\n",
      "evaluation/env_infos/reward_energy Std                   0.0821888\n",
      "evaluation/env_infos/reward_energy Max                  -0.000972626\n",
      "evaluation/env_infos/reward_energy Min                  -0.996583\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0065735\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.219285\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.42146\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.548808\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000209474\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0114902\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0308429\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0378657\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00121764\n",
      "evaluation/env_infos/end_effector_loc Std                0.148723\n",
      "evaluation/env_infos/end_effector_loc Max                0.42146\n",
      "evaluation/env_infos/end_effector_loc Min               -0.548808\n",
      "time/data storing (s)                                    0.00310561\n",
      "time/evaluation sampling (s)                             1.57959\n",
      "time/exploration sampling (s)                            0.128033\n",
      "time/logging (s)                                         0.0193974\n",
      "time/saving (s)                                          0.0272632\n",
      "time/training (s)                                       49.7453\n",
      "time/epoch (s)                                          51.5027\n",
      "time/total (s)                                        3702.84\n",
      "Epoch                                                   74\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:27:35.010192 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 75 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00109703\n",
      "trainer/QF2 Loss                                         0.000609169\n",
      "trainer/Policy Loss                                      3.14343\n",
      "trainer/Q1 Predictions Mean                             -1.14385\n",
      "trainer/Q1 Predictions Std                               0.768\n",
      "trainer/Q1 Predictions Max                               0.365679\n",
      "trainer/Q1 Predictions Min                              -3.70483\n",
      "trainer/Q2 Predictions Mean                             -1.16125\n",
      "trainer/Q2 Predictions Std                               0.767302\n",
      "trainer/Q2 Predictions Max                               0.354781\n",
      "trainer/Q2 Predictions Min                              -3.72756\n",
      "trainer/Q Targets Mean                                  -1.15883\n",
      "trainer/Q Targets Std                                    0.771834\n",
      "trainer/Q Targets Max                                    0.354594\n",
      "trainer/Q Targets Min                                   -3.7169\n",
      "trainer/Log Pis Mean                                     1.98808\n",
      "trainer/Log Pis Std                                      1.31029\n",
      "trainer/Log Pis Max                                      4.10618\n",
      "trainer/Log Pis Min                                     -3.15809\n",
      "trainer/Policy mu Mean                                   0.041226\n",
      "trainer/Policy mu Std                                    0.284205\n",
      "trainer/Policy mu Max                                    2.04668\n",
      "trainer/Policy mu Min                                   -1.55748\n",
      "trainer/Policy log std Mean                             -2.36995\n",
      "trainer/Policy log std Std                               0.548574\n",
      "trainer/Policy log std Max                              -0.190122\n",
      "trainer/Policy log std Min                              -3.15162\n",
      "trainer/Alpha                                            0.0243954\n",
      "trainer/Alpha Loss                                      -0.0442638\n",
      "exploration/num steps total                           8600\n",
      "exploration/num paths total                            430\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0840368\n",
      "exploration/Rewards Std                                  0.0913895\n",
      "exploration/Rewards Max                                  0.160845\n",
      "exploration/Rewards Min                                 -0.345285\n",
      "exploration/Returns Mean                                -1.68074\n",
      "exploration/Returns Std                                  1.32874\n",
      "exploration/Returns Max                                  0.847767\n",
      "exploration/Returns Min                                 -3.05087\n",
      "exploration/Actions Mean                                -0.0022916\n",
      "exploration/Actions Std                                  0.11744\n",
      "exploration/Actions Max                                  0.419474\n",
      "exploration/Actions Min                                 -0.783506\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.68074\n",
      "exploration/env_infos/final/reward_dist Mean             0.0331423\n",
      "exploration/env_infos/final/reward_dist Std              0.0393288\n",
      "exploration/env_infos/final/reward_dist Max              0.0899947\n",
      "exploration/env_infos/final/reward_dist Min              2.43245e-22\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00425004\n",
      "exploration/env_infos/initial/reward_dist Std            0.00508209\n",
      "exploration/env_infos/initial/reward_dist Max            0.0131382\n",
      "exploration/env_infos/initial/reward_dist Min            1.63794e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.178369\n",
      "exploration/env_infos/reward_dist Std                    0.268962\n",
      "exploration/env_infos/reward_dist Max                    0.99865\n",
      "exploration/env_infos/reward_dist Min                    2.43245e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.134306\n",
      "exploration/env_infos/final/reward_energy Std            0.0501627\n",
      "exploration/env_infos/final/reward_energy Max           -0.0656202\n",
      "exploration/env_infos/final/reward_energy Min           -0.216808\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.280322\n",
      "exploration/env_infos/initial/reward_energy Std          0.331166\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0647969\n",
      "exploration/env_infos/initial/reward_energy Min         -0.937381\n",
      "exploration/env_infos/reward_energy Mean                -0.121262\n",
      "exploration/env_infos/reward_energy Std                  0.113535\n",
      "exploration/env_infos/reward_energy Max                 -0.0136057\n",
      "exploration/env_infos/reward_energy Min                 -0.937381\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0499793\n",
      "exploration/env_infos/final/end_effector_loc Std         0.34763\n",
      "exploration/env_infos/final/end_effector_loc Max         0.747365\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.557843\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00705919\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0136192\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00576906\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0391753\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0256254\n",
      "exploration/env_infos/end_effector_loc Std               0.189447\n",
      "exploration/env_infos/end_effector_loc Max               0.747365\n",
      "exploration/env_infos/end_effector_loc Min              -0.557843\n",
      "evaluation/num steps total                           76000\n",
      "evaluation/num paths total                            3800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0405435\n",
      "evaluation/Rewards Std                                   0.0707536\n",
      "evaluation/Rewards Max                                   0.153829\n",
      "evaluation/Rewards Min                                  -0.388672\n",
      "evaluation/Returns Mean                                 -0.810871\n",
      "evaluation/Returns Std                                   1.05435\n",
      "evaluation/Returns Max                                   1.8556\n",
      "evaluation/Returns Min                                  -3.28564\n",
      "evaluation/Actions Mean                                 -0.00171409\n",
      "evaluation/Actions Std                                   0.0537755\n",
      "evaluation/Actions Max                                   0.501258\n",
      "evaluation/Actions Min                                  -0.538252\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.810871\n",
      "evaluation/env_infos/final/reward_dist Mean              0.215551\n",
      "evaluation/env_infos/final/reward_dist Std               0.298207\n",
      "evaluation/env_infos/final/reward_dist Max               0.940517\n",
      "evaluation/env_infos/final/reward_dist Min               3.53455e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00948148\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0148197\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0562403\n",
      "evaluation/env_infos/initial/reward_dist Min             3.31646e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.212013\n",
      "evaluation/env_infos/reward_dist Std                     0.278361\n",
      "evaluation/env_infos/reward_dist Max                     0.998574\n",
      "evaluation/env_infos/reward_dist Min                     3.53455e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.050862\n",
      "evaluation/env_infos/final/reward_energy Std             0.0749639\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0019198\n",
      "evaluation/env_infos/final/reward_energy Min            -0.490521\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.168348\n",
      "evaluation/env_infos/initial/reward_energy Std           0.136164\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0188881\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.5636\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0505823\n",
      "evaluation/env_infos/reward_energy Std                   0.0568412\n",
      "evaluation/env_infos/reward_energy Max                  -0.000631781\n",
      "evaluation/env_infos/reward_energy Min                  -0.5636\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0422619\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.219297\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.526796\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.477871\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000140212\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00765391\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0250629\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0269126\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0182229\n",
      "evaluation/env_infos/end_effector_loc Std                0.141081\n",
      "evaluation/env_infos/end_effector_loc Max                0.526796\n",
      "evaluation/env_infos/end_effector_loc Min               -0.477871\n",
      "time/data storing (s)                                    0.00308916\n",
      "time/evaluation sampling (s)                             1.18316\n",
      "time/exploration sampling (s)                            0.132621\n",
      "time/logging (s)                                         0.0204407\n",
      "time/saving (s)                                          0.0315581\n",
      "time/training (s)                                       53.1797\n",
      "time/epoch (s)                                          54.5506\n",
      "time/total (s)                                        3758.43\n",
      "Epoch                                                   75\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:28:28.492167 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 76 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000699217\r\n",
      "trainer/QF2 Loss                                         0.000724117\r\n",
      "trainer/Policy Loss                                      3.29609\r\n",
      "trainer/Q1 Predictions Mean                             -1.23935\r\n",
      "trainer/Q1 Predictions Std                               0.85254\r\n",
      "trainer/Q1 Predictions Max                               0.582245\r\n",
      "trainer/Q1 Predictions Min                              -3.36701\r\n",
      "trainer/Q2 Predictions Mean                             -1.25216\r\n",
      "trainer/Q2 Predictions Std                               0.856632\r\n",
      "trainer/Q2 Predictions Max                               0.552655\r\n",
      "trainer/Q2 Predictions Min                              -3.38267\r\n",
      "trainer/Q Targets Mean                                  -1.24821\r\n",
      "trainer/Q Targets Std                                    0.857338\r\n",
      "trainer/Q Targets Max                                    0.545001\r\n",
      "trainer/Q Targets Min                                   -3.39643\r\n",
      "trainer/Log Pis Mean                                     2.05215\r\n",
      "trainer/Log Pis Std                                      1.41557\r\n",
      "trainer/Log Pis Max                                      4.62379\r\n",
      "trainer/Log Pis Min                                     -4.26523\r\n",
      "trainer/Policy mu Mean                                   0.0338589\r\n",
      "trainer/Policy mu Std                                    0.293676\r\n",
      "trainer/Policy mu Max                                    1.99545\r\n",
      "trainer/Policy mu Min                                   -1.66\r\n",
      "trainer/Policy log std Mean                             -2.32103\r\n",
      "trainer/Policy log std Std                               0.624684\r\n",
      "trainer/Policy log std Max                              -0.414215\r\n",
      "trainer/Policy log std Min                              -3.30564\r\n",
      "trainer/Alpha                                            0.0240141\r\n",
      "trainer/Alpha Loss                                       0.194536\r\n",
      "exploration/num steps total                           8700\r\n",
      "exploration/num paths total                            435\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0706069\r\n",
      "exploration/Rewards Std                                  0.0788181\r\n",
      "exploration/Rewards Max                                  0.110892\r\n",
      "exploration/Rewards Min                                 -0.23281\r\n",
      "exploration/Returns Mean                                -1.41214\r\n",
      "exploration/Returns Std                                  1.10277\r\n",
      "exploration/Returns Max                                  0.668489\r\n",
      "exploration/Returns Min                                 -2.38336\r\n",
      "exploration/Actions Mean                                 0.00412315\r\n",
      "exploration/Actions Std                                  0.0893249\r\n",
      "exploration/Actions Max                                  0.24145\r\n",
      "exploration/Actions Min                                 -0.27714\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.41214\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.226044\r\n",
      "exploration/env_infos/final/reward_dist Std              0.313037\r\n",
      "exploration/env_infos/final/reward_dist Max              0.840062\r\n",
      "exploration/env_infos/final/reward_dist Min              0.00120318\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00437537\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00825455\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0208673\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.81296e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.211182\r\n",
      "exploration/env_infos/reward_dist Std                    0.2539\r\n",
      "exploration/env_infos/reward_dist Max                    0.950794\r\n",
      "exploration/env_infos/reward_dist Min                    1.81296e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.181793\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0267401\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.145712\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.210818\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.13447\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.074452\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0531016\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.254803\r\n",
      "exploration/env_infos/reward_energy Mean                -0.108633\r\n",
      "exploration/env_infos/reward_energy Std                  0.0647361\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0143442\r\n",
      "exploration/env_infos/reward_energy Min                 -0.387866\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0169698\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.234812\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.466491\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.233426\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000682583\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00539127\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0120725\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00807097\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00419939\r\n",
      "exploration/env_infos/end_effector_loc Std               0.145484\r\n",
      "exploration/env_infos/end_effector_loc Max               0.466491\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.233426\r\n",
      "evaluation/num steps total                           77000\r\n",
      "evaluation/num paths total                            3850\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0459767\r\n",
      "evaluation/Rewards Std                                   0.0657171\r\n",
      "evaluation/Rewards Max                                   0.112751\r\n",
      "evaluation/Rewards Min                                  -0.474306\r\n",
      "evaluation/Returns Mean                                 -0.919534\r\n",
      "evaluation/Returns Std                                   0.918455\r\n",
      "evaluation/Returns Max                                   0.768208\r\n",
      "evaluation/Returns Min                                  -3.58232\r\n",
      "evaluation/Actions Mean                                 -0.0012804\r\n",
      "evaluation/Actions Std                                   0.0665048\r\n",
      "evaluation/Actions Max                                   0.472785\r\n",
      "evaluation/Actions Min                                  -0.673214\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.919534\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.151071\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.222921\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.732301\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.34969e-14\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00692078\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165045\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.104163\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0071e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.174642\r\n",
      "evaluation/env_infos/reward_dist Std                     0.258021\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996708\r\n",
      "evaluation/env_infos/reward_dist Min                     4.34969e-14\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0371784\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0359974\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00222343\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.165411\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.19597\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.192388\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0299668\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.810293\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0572625\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0746328\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00107476\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.810293\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0387727\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.245579\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.415993\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.630214\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0017826\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00954433\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0236392\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0336607\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0241056\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.158245\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.415993\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.630214\r\n",
      "time/data storing (s)                                    0.00308064\r\n",
      "time/evaluation sampling (s)                             1.04852\r\n",
      "time/exploration sampling (s)                            0.142672\r\n",
      "time/logging (s)                                         0.0196173\r\n",
      "time/saving (s)                                          0.0290098\r\n",
      "time/training (s)                                       51.1101\r\n",
      "time/epoch (s)                                          52.353\r\n",
      "time/total (s)                                        3811.9\r\n",
      "Epoch                                                   76\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:29:20.805547 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 77 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000839078\n",
      "trainer/QF2 Loss                                         0.000786215\n",
      "trainer/Policy Loss                                      3.00661\n",
      "trainer/Q1 Predictions Mean                             -1.02398\n",
      "trainer/Q1 Predictions Std                               0.799134\n",
      "trainer/Q1 Predictions Max                               0.4346\n",
      "trainer/Q1 Predictions Min                              -3.49235\n",
      "trainer/Q2 Predictions Mean                             -1.02236\n",
      "trainer/Q2 Predictions Std                               0.795281\n",
      "trainer/Q2 Predictions Max                               0.420096\n",
      "trainer/Q2 Predictions Min                              -3.49057\n",
      "trainer/Q Targets Mean                                  -1.01386\n",
      "trainer/Q Targets Std                                    0.795108\n",
      "trainer/Q Targets Max                                    0.440746\n",
      "trainer/Q Targets Min                                   -3.4655\n",
      "trainer/Log Pis Mean                                     1.98253\n",
      "trainer/Log Pis Std                                      1.32034\n",
      "trainer/Log Pis Max                                      4.11892\n",
      "trainer/Log Pis Min                                     -2.77286\n",
      "trainer/Policy mu Mean                                   0.0152095\n",
      "trainer/Policy mu Std                                    0.193015\n",
      "trainer/Policy mu Max                                    1.3738\n",
      "trainer/Policy mu Min                                   -1.21425\n",
      "trainer/Policy log std Mean                             -2.36466\n",
      "trainer/Policy log std Std                               0.540161\n",
      "trainer/Policy log std Max                               0.159771\n",
      "trainer/Policy log std Min                              -3.19223\n",
      "trainer/Alpha                                            0.024106\n",
      "trainer/Alpha Loss                                      -0.0651017\n",
      "exploration/num steps total                           8800\n",
      "exploration/num paths total                            440\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0572293\n",
      "exploration/Rewards Std                                  0.0767772\n",
      "exploration/Rewards Max                                  0.110654\n",
      "exploration/Rewards Min                                 -0.27328\n",
      "exploration/Returns Mean                                -1.14459\n",
      "exploration/Returns Std                                  1.2199\n",
      "exploration/Returns Max                                  0.964196\n",
      "exploration/Returns Min                                 -2.55233\n",
      "exploration/Actions Mean                                 0.00590121\n",
      "exploration/Actions Std                                  0.200684\n",
      "exploration/Actions Max                                  0.581985\n",
      "exploration/Actions Min                                 -0.914668\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.14459\n",
      "exploration/env_infos/final/reward_dist Mean             0.00433651\n",
      "exploration/env_infos/final/reward_dist Std              0.0046994\n",
      "exploration/env_infos/final/reward_dist Max              0.0128666\n",
      "exploration/env_infos/final/reward_dist Min              1.22444e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000898927\n",
      "exploration/env_infos/initial/reward_dist Std            0.000829877\n",
      "exploration/env_infos/initial/reward_dist Max            0.00219669\n",
      "exploration/env_infos/initial/reward_dist Min            4.14857e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.113666\n",
      "exploration/env_infos/reward_dist Std                    0.180575\n",
      "exploration/env_infos/reward_dist Max                    0.895583\n",
      "exploration/env_infos/reward_dist Min                    1.87166e-09\n",
      "exploration/env_infos/final/reward_energy Mean          -0.141727\n",
      "exploration/env_infos/final/reward_energy Std            0.0803665\n",
      "exploration/env_infos/final/reward_energy Max           -0.0576639\n",
      "exploration/env_infos/final/reward_energy Min           -0.264657\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.38672\n",
      "exploration/env_infos/initial/reward_energy Std          0.285667\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0853373\n",
      "exploration/env_infos/initial/reward_energy Min         -0.728782\n",
      "exploration/env_infos/reward_energy Mean                -0.216293\n",
      "exploration/env_infos/reward_energy Std                  0.183942\n",
      "exploration/env_infos/reward_energy Max                 -0.00853861\n",
      "exploration/env_infos/reward_energy Min                 -1.00752\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0346721\n",
      "exploration/env_infos/final/end_effector_loc Std         0.251208\n",
      "exploration/env_infos/final/end_effector_loc Max         0.305851\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.531605\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00412069\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0164914\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0287606\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0355473\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0271409\n",
      "exploration/env_infos/end_effector_loc Std               0.184134\n",
      "exploration/env_infos/end_effector_loc Max               0.305851\n",
      "exploration/env_infos/end_effector_loc Min              -0.531605\n",
      "evaluation/num steps total                           78000\n",
      "evaluation/num paths total                            3900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0546061\n",
      "evaluation/Rewards Std                                   0.0783644\n",
      "evaluation/Rewards Max                                   0.153748\n",
      "evaluation/Rewards Min                                  -0.565301\n",
      "evaluation/Returns Mean                                 -1.09212\n",
      "evaluation/Returns Std                                   1.07089\n",
      "evaluation/Returns Max                                   1.73441\n",
      "evaluation/Returns Min                                  -4.19288\n",
      "evaluation/Actions Mean                                  0.00436934\n",
      "evaluation/Actions Std                                   0.0662377\n",
      "evaluation/Actions Max                                   0.3883\n",
      "evaluation/Actions Min                                  -0.558574\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.09212\n",
      "evaluation/env_infos/final/reward_dist Mean              0.139764\n",
      "evaluation/env_infos/final/reward_dist Std               0.252627\n",
      "evaluation/env_infos/final/reward_dist Max               0.954868\n",
      "evaluation/env_infos/final/reward_dist Min               3.87648e-39\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00779978\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0180695\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0933218\n",
      "evaluation/env_infos/initial/reward_dist Min             9.96933e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.158876\n",
      "evaluation/env_infos/reward_dist Std                     0.253578\n",
      "evaluation/env_infos/reward_dist Max                     0.995081\n",
      "evaluation/env_infos/reward_dist Min                     1.18593e-39\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0523407\n",
      "evaluation/env_infos/final/reward_energy Std             0.0610357\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00225912\n",
      "evaluation/env_infos/final/reward_energy Min            -0.349962\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.167176\n",
      "evaluation/env_infos/initial/reward_energy Std           0.165676\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0110628\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.680294\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0606432\n",
      "evaluation/env_infos/reward_energy Std                   0.0716621\n",
      "evaluation/env_infos/reward_energy Max                  -0.000703361\n",
      "evaluation/env_infos/reward_energy Min                  -0.680294\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0104652\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.277402\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.713604\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00111212\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00824672\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.019415\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0279287\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00482812\n",
      "evaluation/env_infos/end_effector_loc Std                0.171087\n",
      "evaluation/env_infos/end_effector_loc Max                0.713604\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.0029806\n",
      "time/evaluation sampling (s)                             0.968922\n",
      "time/exploration sampling (s)                            0.126479\n",
      "time/logging (s)                                         0.0215762\n",
      "time/saving (s)                                          0.0296224\n",
      "time/training (s)                                       50.1517\n",
      "time/epoch (s)                                          51.3013\n",
      "time/total (s)                                        3864.22\n",
      "Epoch                                                   77\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:30:13.577452 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 78 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000702262\n",
      "trainer/QF2 Loss                                         0.00057913\n",
      "trainer/Policy Loss                                      3.18069\n",
      "trainer/Q1 Predictions Mean                             -1.15093\n",
      "trainer/Q1 Predictions Std                               0.819447\n",
      "trainer/Q1 Predictions Max                               0.535822\n",
      "trainer/Q1 Predictions Min                              -3.45505\n",
      "trainer/Q2 Predictions Mean                             -1.15526\n",
      "trainer/Q2 Predictions Std                               0.818222\n",
      "trainer/Q2 Predictions Max                               0.53946\n",
      "trainer/Q2 Predictions Min                              -3.45605\n",
      "trainer/Q Targets Mean                                  -1.15542\n",
      "trainer/Q Targets Std                                    0.823061\n",
      "trainer/Q Targets Max                                    0.517948\n",
      "trainer/Q Targets Min                                   -3.49687\n",
      "trainer/Log Pis Mean                                     2.03592\n",
      "trainer/Log Pis Std                                      1.25414\n",
      "trainer/Log Pis Max                                      4.27327\n",
      "trainer/Log Pis Min                                     -3.78618\n",
      "trainer/Policy mu Mean                                   0.0466617\n",
      "trainer/Policy mu Std                                    0.291193\n",
      "trainer/Policy mu Max                                    2.19756\n",
      "trainer/Policy mu Min                                   -1.83475\n",
      "trainer/Policy log std Mean                             -2.33277\n",
      "trainer/Policy log std Std                               0.549802\n",
      "trainer/Policy log std Max                              -0.148577\n",
      "trainer/Policy log std Min                              -3.18623\n",
      "trainer/Alpha                                            0.024881\n",
      "trainer/Alpha Loss                                       0.132717\n",
      "exploration/num steps total                           8900\n",
      "exploration/num paths total                            445\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0776395\n",
      "exploration/Rewards Std                                  0.0922157\n",
      "exploration/Rewards Max                                  0.139795\n",
      "exploration/Rewards Min                                 -0.236431\n",
      "exploration/Returns Mean                                -1.55279\n",
      "exploration/Returns Std                                  1.58656\n",
      "exploration/Returns Max                                  0.358998\n",
      "exploration/Returns Min                                 -3.31004\n",
      "exploration/Actions Mean                                 0.00371972\n",
      "exploration/Actions Std                                  0.204237\n",
      "exploration/Actions Max                                  0.652673\n",
      "exploration/Actions Min                                 -0.936933\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.55279\n",
      "exploration/env_infos/final/reward_dist Mean             0.180429\n",
      "exploration/env_infos/final/reward_dist Std              0.307033\n",
      "exploration/env_infos/final/reward_dist Max              0.793662\n",
      "exploration/env_infos/final/reward_dist Min              0.000534932\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00302532\n",
      "exploration/env_infos/initial/reward_dist Std            0.00410415\n",
      "exploration/env_infos/initial/reward_dist Max            0.0109659\n",
      "exploration/env_infos/initial/reward_dist Min            1.45446e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.166083\n",
      "exploration/env_infos/reward_dist Std                    0.264226\n",
      "exploration/env_infos/reward_dist Max                    0.964446\n",
      "exploration/env_infos/reward_dist Min                    7.11653e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.102476\n",
      "exploration/env_infos/final/reward_energy Std            0.0584363\n",
      "exploration/env_infos/final/reward_energy Max           -0.0197578\n",
      "exploration/env_infos/final/reward_energy Min           -0.19336\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.436664\n",
      "exploration/env_infos/initial/reward_energy Std          0.426903\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0606459\n",
      "exploration/env_infos/initial/reward_energy Min         -1.00639\n",
      "exploration/env_infos/reward_energy Mean                -0.209823\n",
      "exploration/env_infos/reward_energy Std                  0.198564\n",
      "exploration/env_infos/reward_energy Max                 -0.01339\n",
      "exploration/env_infos/reward_energy Min                 -1.00639\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0420519\n",
      "exploration/env_infos/final/end_effector_loc Std         0.197908\n",
      "exploration/env_infos/final/end_effector_loc Max         0.239627\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.321416\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0121834\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0178247\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00459797\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0468467\n",
      "exploration/env_infos/end_effector_loc Mean             -0.058267\n",
      "exploration/env_infos/end_effector_loc Std               0.15824\n",
      "exploration/env_infos/end_effector_loc Max               0.239627\n",
      "exploration/env_infos/end_effector_loc Min              -0.45703\n",
      "evaluation/num steps total                           79000\n",
      "evaluation/num paths total                            3950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.050001\n",
      "evaluation/Rewards Std                                   0.0708965\n",
      "evaluation/Rewards Max                                   0.137244\n",
      "evaluation/Rewards Min                                  -0.373947\n",
      "evaluation/Returns Mean                                 -1.00002\n",
      "evaluation/Returns Std                                   1.06671\n",
      "evaluation/Returns Max                                   1.70195\n",
      "evaluation/Returns Min                                  -3.74396\n",
      "evaluation/Actions Mean                                  0.00160698\n",
      "evaluation/Actions Std                                   0.0748365\n",
      "evaluation/Actions Max                                   0.498003\n",
      "evaluation/Actions Min                                  -0.70241\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.00002\n",
      "evaluation/env_infos/final/reward_dist Mean              0.197982\n",
      "evaluation/env_infos/final/reward_dist Std               0.276561\n",
      "evaluation/env_infos/final/reward_dist Max               0.981777\n",
      "evaluation/env_infos/final/reward_dist Min               8.85074e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00678391\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0105987\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0365472\n",
      "evaluation/env_infos/initial/reward_dist Min             1.44131e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.191003\n",
      "evaluation/env_infos/reward_dist Std                     0.275413\n",
      "evaluation/env_infos/reward_dist Max                     0.991446\n",
      "evaluation/env_infos/reward_dist Min                     8.85074e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0408598\n",
      "evaluation/env_infos/final/reward_energy Std             0.0498846\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00313898\n",
      "evaluation/env_infos/final/reward_energy Min            -0.260334\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.229876\n",
      "evaluation/env_infos/initial/reward_energy Std           0.233432\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0068349\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.950922\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0650972\n",
      "evaluation/env_infos/reward_energy Std                   0.0834776\n",
      "evaluation/env_infos/reward_energy Max                  -0.00113963\n",
      "evaluation/env_infos/reward_energy Min                  -0.950922\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0176422\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.212489\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.461251\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.718104\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00263548\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0112792\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0249002\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0351205\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0187645\n",
      "evaluation/env_infos/end_effector_loc Std                0.144023\n",
      "evaluation/env_infos/end_effector_loc Max                0.461251\n",
      "evaluation/env_infos/end_effector_loc Min               -0.718104\n",
      "time/data storing (s)                                    0.00289643\n",
      "time/evaluation sampling (s)                             0.945086\n",
      "time/exploration sampling (s)                            0.128266\n",
      "time/logging (s)                                         0.0200638\n",
      "time/saving (s)                                          0.0277922\n",
      "time/training (s)                                       50.6125\n",
      "time/epoch (s)                                          51.7366\n",
      "time/total (s)                                        3916.99\n",
      "Epoch                                                   78\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:31:05.077334 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 79 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000796271\n",
      "trainer/QF2 Loss                                         0.000755338\n",
      "trainer/Policy Loss                                      3.13744\n",
      "trainer/Q1 Predictions Mean                             -1.21219\n",
      "trainer/Q1 Predictions Std                               0.947012\n",
      "trainer/Q1 Predictions Max                               0.445473\n",
      "trainer/Q1 Predictions Min                              -4.00425\n",
      "trainer/Q2 Predictions Mean                             -1.21542\n",
      "trainer/Q2 Predictions Std                               0.947659\n",
      "trainer/Q2 Predictions Max                               0.439016\n",
      "trainer/Q2 Predictions Min                              -3.99707\n",
      "trainer/Q Targets Mean                                  -1.21259\n",
      "trainer/Q Targets Std                                    0.94569\n",
      "trainer/Q Targets Max                                    0.435708\n",
      "trainer/Q Targets Min                                   -3.96539\n",
      "trainer/Log Pis Mean                                     1.92253\n",
      "trainer/Log Pis Std                                      1.34187\n",
      "trainer/Log Pis Max                                      4.23194\n",
      "trainer/Log Pis Min                                     -2.98636\n",
      "trainer/Policy mu Mean                                   0.018632\n",
      "trainer/Policy mu Std                                    0.295609\n",
      "trainer/Policy mu Max                                    1.84593\n",
      "trainer/Policy mu Min                                   -1.89781\n",
      "trainer/Policy log std Mean                             -2.30487\n",
      "trainer/Policy log std Std                               0.602611\n",
      "trainer/Policy log std Max                              -0.150855\n",
      "trainer/Policy log std Min                              -3.18923\n",
      "trainer/Alpha                                            0.023201\n",
      "trainer/Alpha Loss                                      -0.291532\n",
      "exploration/num steps total                           9000\n",
      "exploration/num paths total                            450\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0474926\n",
      "exploration/Rewards Std                                  0.0917056\n",
      "exploration/Rewards Max                                  0.162367\n",
      "exploration/Rewards Min                                 -0.228052\n",
      "exploration/Returns Mean                                -0.949851\n",
      "exploration/Returns Std                                  1.64756\n",
      "exploration/Returns Max                                  2.18351\n",
      "exploration/Returns Min                                 -2.34786\n",
      "exploration/Actions Mean                                 0.00446668\n",
      "exploration/Actions Std                                  0.117659\n",
      "exploration/Actions Max                                  0.461918\n",
      "exploration/Actions Min                                 -0.511791\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.949851\n",
      "exploration/env_infos/final/reward_dist Mean             0.209684\n",
      "exploration/env_infos/final/reward_dist Std              0.375911\n",
      "exploration/env_infos/final/reward_dist Max              0.95834\n",
      "exploration/env_infos/final/reward_dist Min              1.36101e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000830026\n",
      "exploration/env_infos/initial/reward_dist Std            0.000582802\n",
      "exploration/env_infos/initial/reward_dist Max            0.00155427\n",
      "exploration/env_infos/initial/reward_dist Min            0.000134896\n",
      "exploration/env_infos/reward_dist Mean                   0.209197\n",
      "exploration/env_infos/reward_dist Std                    0.32168\n",
      "exploration/env_infos/reward_dist Max                    0.99992\n",
      "exploration/env_infos/reward_dist Min                    1.36101e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.105401\n",
      "exploration/env_infos/final/reward_energy Std            0.0727751\n",
      "exploration/env_infos/final/reward_energy Max           -0.0282466\n",
      "exploration/env_infos/final/reward_energy Min           -0.200376\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.156144\n",
      "exploration/env_infos/initial/reward_energy Std          0.0860235\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0617517\n",
      "exploration/env_infos/initial/reward_energy Min         -0.285902\n",
      "exploration/env_infos/reward_energy Mean                -0.122898\n",
      "exploration/env_infos/reward_energy Std                  0.112353\n",
      "exploration/env_infos/reward_energy Max                 -0.00229004\n",
      "exploration/env_infos/reward_energy Min                 -0.580878\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0550092\n",
      "exploration/env_infos/final/end_effector_loc Std         0.233881\n",
      "exploration/env_infos/final/end_effector_loc Max         0.53215\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.307795\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000190916\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00629998\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0112868\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0107924\n",
      "exploration/env_infos/end_effector_loc Mean              0.0261026\n",
      "exploration/env_infos/end_effector_loc Std               0.138634\n",
      "exploration/env_infos/end_effector_loc Max               0.53215\n",
      "exploration/env_infos/end_effector_loc Min              -0.307795\n",
      "evaluation/num steps total                           80000\n",
      "evaluation/num paths total                            4000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0525616\n",
      "evaluation/Rewards Std                                   0.0701012\n",
      "evaluation/Rewards Max                                   0.13698\n",
      "evaluation/Rewards Min                                  -0.367176\n",
      "evaluation/Returns Mean                                 -1.05123\n",
      "evaluation/Returns Std                                   1.0651\n",
      "evaluation/Returns Max                                   1.33051\n",
      "evaluation/Returns Min                                  -3.017\n",
      "evaluation/Actions Mean                                  0.00341609\n",
      "evaluation/Actions Std                                   0.0832639\n",
      "evaluation/Actions Max                                   0.776508\n",
      "evaluation/Actions Min                                  -0.818485\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.05123\n",
      "evaluation/env_infos/final/reward_dist Mean              0.155782\n",
      "evaluation/env_infos/final/reward_dist Std               0.25991\n",
      "evaluation/env_infos/final/reward_dist Max               0.981161\n",
      "evaluation/env_infos/final/reward_dist Min               8.88201e-46\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00572943\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0114806\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0526692\n",
      "evaluation/env_infos/initial/reward_dist Min             9.41027e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.146851\n",
      "evaluation/env_infos/reward_dist Std                     0.23456\n",
      "evaluation/env_infos/reward_dist Max                     0.992819\n",
      "evaluation/env_infos/reward_dist Min                     8.88201e-46\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0610072\n",
      "evaluation/env_infos/final/reward_energy Std             0.0681232\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000749923\n",
      "evaluation/env_infos/final/reward_energy Min            -0.424836\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.236879\n",
      "evaluation/env_infos/initial/reward_energy Std           0.235126\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0182627\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.06816\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0655236\n",
      "evaluation/env_infos/reward_energy Std                   0.097958\n",
      "evaluation/env_infos/reward_energy Max                  -0.000749923\n",
      "evaluation/env_infos/reward_energy Min                  -1.06816\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00957148\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.256363\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.954862\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.603625\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00204662\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116214\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0304915\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0409243\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0145459\n",
      "evaluation/env_infos/end_effector_loc Std                0.165964\n",
      "evaluation/env_infos/end_effector_loc Max                0.954862\n",
      "evaluation/env_infos/end_effector_loc Min               -0.603625\n",
      "time/data storing (s)                                    0.00303468\n",
      "time/evaluation sampling (s)                             1.02708\n",
      "time/exploration sampling (s)                            0.119945\n",
      "time/logging (s)                                         0.0201309\n",
      "time/saving (s)                                          0.0267632\n",
      "time/training (s)                                       49.2097\n",
      "time/epoch (s)                                          50.4067\n",
      "time/total (s)                                        3968.48\n",
      "Epoch                                                   79\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:31:57.878900 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 80 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000679713\r\n",
      "trainer/QF2 Loss                                         0.000766436\r\n",
      "trainer/Policy Loss                                      3.1449\r\n",
      "trainer/Q1 Predictions Mean                             -1.15067\r\n",
      "trainer/Q1 Predictions Std                               0.875893\r\n",
      "trainer/Q1 Predictions Max                               0.573801\r\n",
      "trainer/Q1 Predictions Min                              -3.86465\r\n",
      "trainer/Q2 Predictions Mean                             -1.15048\r\n",
      "trainer/Q2 Predictions Std                               0.875497\r\n",
      "trainer/Q2 Predictions Max                               0.579091\r\n",
      "trainer/Q2 Predictions Min                              -3.84228\r\n",
      "trainer/Q Targets Mean                                  -1.14899\r\n",
      "trainer/Q Targets Std                                    0.871182\r\n",
      "trainer/Q Targets Max                                    0.576972\r\n",
      "trainer/Q Targets Min                                   -3.82047\r\n",
      "trainer/Log Pis Mean                                     1.99743\r\n",
      "trainer/Log Pis Std                                      1.34482\r\n",
      "trainer/Log Pis Max                                      4.17201\r\n",
      "trainer/Log Pis Min                                     -2.50222\r\n",
      "trainer/Policy mu Mean                                   0.0143022\r\n",
      "trainer/Policy mu Std                                    0.265359\r\n",
      "trainer/Policy mu Max                                    1.79566\r\n",
      "trainer/Policy mu Min                                   -1.50122\r\n",
      "trainer/Policy log std Mean                             -2.35807\r\n",
      "trainer/Policy log std Std                               0.556564\r\n",
      "trainer/Policy log std Max                              -0.26023\r\n",
      "trainer/Policy log std Min                              -3.18018\r\n",
      "trainer/Alpha                                            0.023244\r\n",
      "trainer/Alpha Loss                                      -0.0096694\r\n",
      "exploration/num steps total                           9100\r\n",
      "exploration/num paths total                            455\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0730276\r\n",
      "exploration/Rewards Std                                  0.0787583\r\n",
      "exploration/Rewards Max                                  0.0986001\r\n",
      "exploration/Rewards Min                                 -0.222559\r\n",
      "exploration/Returns Mean                                -1.46055\r\n",
      "exploration/Returns Std                                  1.27688\r\n",
      "exploration/Returns Max                                  0.632581\r\n",
      "exploration/Returns Min                                 -3.16492\r\n",
      "exploration/Actions Mean                                -0.00182581\r\n",
      "exploration/Actions Std                                  0.133262\r\n",
      "exploration/Actions Max                                  0.582742\r\n",
      "exploration/Actions Min                                 -0.632351\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.46055\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0970719\r\n",
      "exploration/env_infos/final/reward_dist Std              0.1726\r\n",
      "exploration/env_infos/final/reward_dist Max              0.440554\r\n",
      "exploration/env_infos/final/reward_dist Min              1.0324e-11\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0179606\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0248005\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0633179\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.02383e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.138082\r\n",
      "exploration/env_infos/reward_dist Std                    0.202126\r\n",
      "exploration/env_infos/reward_dist Max                    0.815419\r\n",
      "exploration/env_infos/reward_dist Min                    1.0324e-11\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.125562\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0990056\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0231003\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.304492\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.291661\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.285577\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0290967\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.670648\r\n",
      "exploration/env_infos/reward_energy Mean                -0.135668\r\n",
      "exploration/env_infos/reward_energy Std                  0.130836\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00738137\r\n",
      "exploration/env_infos/reward_energy Min                 -0.670648\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00142423\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.223778\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.344043\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.459281\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00092868\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0144018\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0291371\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0316176\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00646764\r\n",
      "exploration/env_infos/end_effector_loc Std               0.133524\r\n",
      "exploration/env_infos/end_effector_loc Max               0.347011\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.459281\r\n",
      "evaluation/num steps total                           81000\r\n",
      "evaluation/num paths total                            4050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0664632\r\n",
      "evaluation/Rewards Std                                   0.0829162\r\n",
      "evaluation/Rewards Max                                   0.0952115\r\n",
      "evaluation/Rewards Min                                  -0.751546\r\n",
      "evaluation/Returns Mean                                 -1.32926\r\n",
      "evaluation/Returns Std                                   1.15436\r\n",
      "evaluation/Returns Max                                   0.812738\r\n",
      "evaluation/Returns Min                                  -4.04634\r\n",
      "evaluation/Actions Mean                                  0.00422942\r\n",
      "evaluation/Actions Std                                   0.0974431\r\n",
      "evaluation/Actions Max                                   0.751401\r\n",
      "evaluation/Actions Min                                  -0.96724\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.32926\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.206629\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.282226\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.947758\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.08859e-53\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0115702\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0250304\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.112165\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.32712e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.149051\r\n",
      "evaluation/env_infos/reward_dist Std                     0.246926\r\n",
      "evaluation/env_infos/reward_dist Max                     0.99623\r\n",
      "evaluation/env_infos/reward_dist Min                     9.08859e-53\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0554786\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.119005\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00231332\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.838819\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.294981\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.308288\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00978314\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.09223\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0732416\r\n",
      "evaluation/env_infos/reward_energy Std                   0.116883\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00064512\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.09223\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0244347\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.281825\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.66502\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0051289\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0141867\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0211887\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.048362\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.032526\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.175653\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.66502\r\n",
      "time/data storing (s)                                    0.00306501\r\n",
      "time/evaluation sampling (s)                             1.13164\r\n",
      "time/exploration sampling (s)                            0.124464\r\n",
      "time/logging (s)                                         0.0202948\r\n",
      "time/saving (s)                                          0.0288522\r\n",
      "time/training (s)                                       50.3623\r\n",
      "time/epoch (s)                                          51.6706\r\n",
      "time/total (s)                                        4021.28\r\n",
      "Epoch                                                   80\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:32:50.326417 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 81 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000501914\n",
      "trainer/QF2 Loss                                         0.000812817\n",
      "trainer/Policy Loss                                      3.27369\n",
      "trainer/Q1 Predictions Mean                             -1.18551\n",
      "trainer/Q1 Predictions Std                               0.89019\n",
      "trainer/Q1 Predictions Max                               0.300216\n",
      "trainer/Q1 Predictions Min                              -3.56582\n",
      "trainer/Q2 Predictions Mean                             -1.18698\n",
      "trainer/Q2 Predictions Std                               0.889841\n",
      "trainer/Q2 Predictions Max                               0.270419\n",
      "trainer/Q2 Predictions Min                              -3.57703\n",
      "trainer/Q Targets Mean                                  -1.18013\n",
      "trainer/Q Targets Std                                    0.892682\n",
      "trainer/Q Targets Max                                    0.274576\n",
      "trainer/Q Targets Min                                   -3.5356\n",
      "trainer/Log Pis Mean                                     2.0906\n",
      "trainer/Log Pis Std                                      1.40047\n",
      "trainer/Log Pis Max                                      4.41417\n",
      "trainer/Log Pis Min                                     -3.86845\n",
      "trainer/Policy mu Mean                                   0.0195433\n",
      "trainer/Policy mu Std                                    0.262279\n",
      "trainer/Policy mu Max                                    1.68268\n",
      "trainer/Policy mu Min                                   -1.74498\n",
      "trainer/Policy log std Mean                             -2.40118\n",
      "trainer/Policy log std Std                               0.587813\n",
      "trainer/Policy log std Max                              -0.0769494\n",
      "trainer/Policy log std Min                              -3.2624\n",
      "trainer/Alpha                                            0.023733\n",
      "trainer/Alpha Loss                                       0.338905\n",
      "exploration/num steps total                           9200\n",
      "exploration/num paths total                            460\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0937772\n",
      "exploration/Rewards Std                                  0.0677251\n",
      "exploration/Rewards Max                                  0.121206\n",
      "exploration/Rewards Min                                 -0.332113\n",
      "exploration/Returns Mean                                -1.87554\n",
      "exploration/Returns Std                                  0.765732\n",
      "exploration/Returns Max                                 -0.679487\n",
      "exploration/Returns Min                                 -2.98573\n",
      "exploration/Actions Mean                                 0.000957297\n",
      "exploration/Actions Std                                  0.103705\n",
      "exploration/Actions Max                                  0.365997\n",
      "exploration/Actions Min                                 -0.539242\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.87554\n",
      "exploration/env_infos/final/reward_dist Mean             0.165378\n",
      "exploration/env_infos/final/reward_dist Std              0.323246\n",
      "exploration/env_infos/final/reward_dist Max              0.811792\n",
      "exploration/env_infos/final/reward_dist Min              2.70561e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0104887\n",
      "exploration/env_infos/initial/reward_dist Std            0.0130221\n",
      "exploration/env_infos/initial/reward_dist Max            0.032101\n",
      "exploration/env_infos/initial/reward_dist Min            9.66414e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.136261\n",
      "exploration/env_infos/reward_dist Std                    0.25204\n",
      "exploration/env_infos/reward_dist Max                    0.963598\n",
      "exploration/env_infos/reward_dist Min                    2.70561e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.130591\n",
      "exploration/env_infos/final/reward_energy Std            0.0764548\n",
      "exploration/env_infos/final/reward_energy Max           -0.0331466\n",
      "exploration/env_infos/final/reward_energy Min           -0.266098\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.183905\n",
      "exploration/env_infos/initial/reward_energy Std          0.181686\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0421181\n",
      "exploration/env_infos/initial/reward_energy Min         -0.542123\n",
      "exploration/env_infos/reward_energy Mean                -0.112348\n",
      "exploration/env_infos/reward_energy Std                  0.0942817\n",
      "exploration/env_infos/reward_energy Max                 -0.00555508\n",
      "exploration/env_infos/reward_energy Min                 -0.542123\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0360206\n",
      "exploration/env_infos/final/end_effector_loc Std         0.237847\n",
      "exploration/env_infos/final/end_effector_loc Max         0.314664\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.347579\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00416952\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0081335\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00487872\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0269621\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0326138\n",
      "exploration/env_infos/end_effector_loc Std               0.128067\n",
      "exploration/env_infos/end_effector_loc Max               0.314664\n",
      "exploration/env_infos/end_effector_loc Min              -0.347579\n",
      "evaluation/num steps total                           82000\n",
      "evaluation/num paths total                            4100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0548885\n",
      "evaluation/Rewards Std                                   0.100443\n",
      "evaluation/Rewards Max                                   0.15758\n",
      "evaluation/Rewards Min                                  -0.760129\n",
      "evaluation/Returns Mean                                 -1.09777\n",
      "evaluation/Returns Std                                   1.4825\n",
      "evaluation/Returns Max                                   1.63514\n",
      "evaluation/Returns Min                                  -5.67063\n",
      "evaluation/Actions Mean                                  0.0129991\n",
      "evaluation/Actions Std                                   0.114505\n",
      "evaluation/Actions Max                                   0.868372\n",
      "evaluation/Actions Min                                  -0.65571\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.09777\n",
      "evaluation/env_infos/final/reward_dist Mean              0.161199\n",
      "evaluation/env_infos/final/reward_dist Std               0.236473\n",
      "evaluation/env_infos/final/reward_dist Max               0.940226\n",
      "evaluation/env_infos/final/reward_dist Min               1.95073e-105\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0107524\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0161723\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0715394\n",
      "evaluation/env_infos/initial/reward_dist Min             1.31279e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.17005\n",
      "evaluation/env_infos/reward_dist Std                     0.255779\n",
      "evaluation/env_infos/reward_dist Max                     0.995173\n",
      "evaluation/env_infos/reward_dist Min                     1.95073e-105\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0980509\n",
      "evaluation/env_infos/final/reward_energy Std             0.209188\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00194174\n",
      "evaluation/env_infos/final/reward_energy Min            -0.975538\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.23985\n",
      "evaluation/env_infos/initial/reward_energy Std           0.248776\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00581004\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.868034\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0836414\n",
      "evaluation/env_infos/reward_energy Std                   0.139875\n",
      "evaluation/env_infos/reward_energy Max                  -0.000939147\n",
      "evaluation/env_infos/reward_energy Min                  -0.975538\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0351502\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.302549\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.450905\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000970512\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121791\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0371899\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0327855\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00447512\n",
      "evaluation/env_infos/end_effector_loc Std                0.188927\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.518941\n",
      "time/data storing (s)                                    0.00303631\n",
      "time/evaluation sampling (s)                             0.971898\n",
      "time/exploration sampling (s)                            0.128173\n",
      "time/logging (s)                                         0.0235715\n",
      "time/saving (s)                                          0.0289428\n",
      "time/training (s)                                       50.2456\n",
      "time/epoch (s)                                          51.4012\n",
      "time/total (s)                                        4073.73\n",
      "Epoch                                                   81\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:33:44.371001 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 82 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000624508\n",
      "trainer/QF2 Loss                                         0.000726083\n",
      "trainer/Policy Loss                                      3.16976\n",
      "trainer/Q1 Predictions Mean                             -1.20486\n",
      "trainer/Q1 Predictions Std                               0.861983\n",
      "trainer/Q1 Predictions Max                               0.486261\n",
      "trainer/Q1 Predictions Min                              -3.48362\n",
      "trainer/Q2 Predictions Mean                             -1.20125\n",
      "trainer/Q2 Predictions Std                               0.863261\n",
      "trainer/Q2 Predictions Max                               0.490338\n",
      "trainer/Q2 Predictions Min                              -3.47104\n",
      "trainer/Q Targets Mean                                  -1.20341\n",
      "trainer/Q Targets Std                                    0.854031\n",
      "trainer/Q Targets Max                                    0.498528\n",
      "trainer/Q Targets Min                                   -3.48417\n",
      "trainer/Log Pis Mean                                     1.96605\n",
      "trainer/Log Pis Std                                      1.49546\n",
      "trainer/Log Pis Max                                      4.48759\n",
      "trainer/Log Pis Min                                     -4.55692\n",
      "trainer/Policy mu Mean                                   0.0412045\n",
      "trainer/Policy mu Std                                    0.238788\n",
      "trainer/Policy mu Max                                    2.06931\n",
      "trainer/Policy mu Min                                   -1.03584\n",
      "trainer/Policy log std Mean                             -2.33697\n",
      "trainer/Policy log std Std                               0.601505\n",
      "trainer/Policy log std Max                               0.133116\n",
      "trainer/Policy log std Min                              -3.18393\n",
      "trainer/Alpha                                            0.0246573\n",
      "trainer/Alpha Loss                                      -0.125703\n",
      "exploration/num steps total                           9300\n",
      "exploration/num paths total                            465\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0991734\n",
      "exploration/Rewards Std                                  0.0640419\n",
      "exploration/Rewards Max                                  0.0164013\n",
      "exploration/Rewards Min                                 -0.283169\n",
      "exploration/Returns Mean                                -1.98347\n",
      "exploration/Returns Std                                  0.675818\n",
      "exploration/Returns Max                                 -1.04208\n",
      "exploration/Returns Min                                 -2.91907\n",
      "exploration/Actions Mean                                -0.0094017\n",
      "exploration/Actions Std                                  0.160359\n",
      "exploration/Actions Max                                  0.546498\n",
      "exploration/Actions Min                                 -0.507404\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.98347\n",
      "exploration/env_infos/final/reward_dist Mean             0.0826754\n",
      "exploration/env_infos/final/reward_dist Std              0.164924\n",
      "exploration/env_infos/final/reward_dist Max              0.412522\n",
      "exploration/env_infos/final/reward_dist Min              1.13822e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000378723\n",
      "exploration/env_infos/initial/reward_dist Std            0.000609849\n",
      "exploration/env_infos/initial/reward_dist Max            0.0015895\n",
      "exploration/env_infos/initial/reward_dist Min            1.07075e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.093575\n",
      "exploration/env_infos/reward_dist Std                    0.23091\n",
      "exploration/env_infos/reward_dist Max                    0.986114\n",
      "exploration/env_infos/reward_dist Min                    1.13822e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.18314\n",
      "exploration/env_infos/final/reward_energy Std            0.0727217\n",
      "exploration/env_infos/final/reward_energy Max           -0.0807634\n",
      "exploration/env_infos/final/reward_energy Min           -0.260826\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.17704\n",
      "exploration/env_infos/initial/reward_energy Std          0.0770149\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0936821\n",
      "exploration/env_infos/initial/reward_energy Min         -0.287202\n",
      "exploration/env_infos/reward_energy Mean                -0.189661\n",
      "exploration/env_infos/reward_energy Std                  0.125042\n",
      "exploration/env_infos/reward_energy Max                 -0.0120968\n",
      "exploration/env_infos/reward_energy Min                 -0.574931\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0204503\n",
      "exploration/env_infos/final/end_effector_loc Std         0.193362\n",
      "exploration/env_infos/final/end_effector_loc Max         0.243918\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.464843\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00415736\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00541381\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0132426\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00476134\n",
      "exploration/env_infos/end_effector_loc Mean              0.0431561\n",
      "exploration/env_infos/end_effector_loc Std               0.150079\n",
      "exploration/env_infos/end_effector_loc Max               0.367726\n",
      "exploration/env_infos/end_effector_loc Min              -0.465\n",
      "evaluation/num steps total                           83000\n",
      "evaluation/num paths total                            4150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0620006\n",
      "evaluation/Rewards Std                                   0.108704\n",
      "evaluation/Rewards Max                                   0.123184\n",
      "evaluation/Rewards Min                                  -0.807144\n",
      "evaluation/Returns Mean                                 -1.24001\n",
      "evaluation/Returns Std                                   1.5477\n",
      "evaluation/Returns Max                                   1.22329\n",
      "evaluation/Returns Min                                  -8.129\n",
      "evaluation/Actions Mean                                  0.0204754\n",
      "evaluation/Actions Std                                   0.143312\n",
      "evaluation/Actions Max                                   0.98739\n",
      "evaluation/Actions Min                                  -0.988171\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.24001\n",
      "evaluation/env_infos/final/reward_dist Mean              0.152226\n",
      "evaluation/env_infos/final/reward_dist Std               0.213322\n",
      "evaluation/env_infos/final/reward_dist Max               0.748834\n",
      "evaluation/env_infos/final/reward_dist Min               8.07708e-120\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00715763\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0138692\n",
      "evaluation/env_infos/initial/reward_dist Max             0.076537\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97017e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.120622\n",
      "evaluation/env_infos/reward_dist Std                     0.200044\n",
      "evaluation/env_infos/reward_dist Max                     0.982176\n",
      "evaluation/env_infos/reward_dist Min                     2.9298e-123\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.116247\n",
      "evaluation/env_infos/final/reward_energy Std             0.232813\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00414608\n",
      "evaluation/env_infos/final/reward_energy Min            -1.33831\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.306321\n",
      "evaluation/env_infos/initial/reward_energy Std           0.32445\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.000217716\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.25535\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0984051\n",
      "evaluation/env_infos/reward_energy Std                   0.179531\n",
      "evaluation/env_infos/reward_energy Max                  -0.000217716\n",
      "evaluation/env_infos/reward_energy Min                  -1.33831\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.078785\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.30718\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.549252\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00335411\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0154151\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0328633\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0494086\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0129987\n",
      "evaluation/env_infos/end_effector_loc Std                0.205026\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.600959\n",
      "time/data storing (s)                                    0.00293472\n",
      "time/evaluation sampling (s)                             1.21773\n",
      "time/exploration sampling (s)                            0.133259\n",
      "time/logging (s)                                         0.0205646\n",
      "time/saving (s)                                          0.0287646\n",
      "time/training (s)                                       51.4936\n",
      "time/epoch (s)                                          52.8968\n",
      "time/total (s)                                        4127.77\n",
      "Epoch                                                   82\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:34:38.203568 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 83 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000802997\n",
      "trainer/QF2 Loss                                         0.000728589\n",
      "trainer/Policy Loss                                      3.12219\n",
      "trainer/Q1 Predictions Mean                             -1.07807\n",
      "trainer/Q1 Predictions Std                               0.798062\n",
      "trainer/Q1 Predictions Max                               0.625394\n",
      "trainer/Q1 Predictions Min                              -3.13844\n",
      "trainer/Q2 Predictions Mean                             -1.07892\n",
      "trainer/Q2 Predictions Std                               0.798842\n",
      "trainer/Q2 Predictions Max                               0.628263\n",
      "trainer/Q2 Predictions Min                              -3.13457\n",
      "trainer/Q Targets Mean                                  -1.08678\n",
      "trainer/Q Targets Std                                    0.799065\n",
      "trainer/Q Targets Max                                    0.637205\n",
      "trainer/Q Targets Min                                   -3.14338\n",
      "trainer/Log Pis Mean                                     2.04393\n",
      "trainer/Log Pis Std                                      1.47663\n",
      "trainer/Log Pis Max                                      4.45773\n",
      "trainer/Log Pis Min                                     -4.44094\n",
      "trainer/Policy mu Mean                                   0.0555146\n",
      "trainer/Policy mu Std                                    0.293363\n",
      "trainer/Policy mu Max                                    2.06583\n",
      "trainer/Policy mu Min                                   -1.57229\n",
      "trainer/Policy log std Mean                             -2.33591\n",
      "trainer/Policy log std Std                               0.633862\n",
      "trainer/Policy log std Max                               0.102997\n",
      "trainer/Policy log std Min                              -3.20876\n",
      "trainer/Alpha                                            0.0248268\n",
      "trainer/Alpha Loss                                       0.162348\n",
      "exploration/num steps total                           9400\n",
      "exploration/num paths total                            470\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.100622\n",
      "exploration/Rewards Std                                  0.159106\n",
      "exploration/Rewards Max                                  0.105981\n",
      "exploration/Rewards Min                                 -0.701694\n",
      "exploration/Returns Mean                                -2.01245\n",
      "exploration/Returns Std                                  2.03215\n",
      "exploration/Returns Max                                  0.832667\n",
      "exploration/Returns Min                                 -5.46242\n",
      "exploration/Actions Mean                                -0.010638\n",
      "exploration/Actions Std                                  0.20874\n",
      "exploration/Actions Max                                  0.951553\n",
      "exploration/Actions Min                                 -0.971894\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.01245\n",
      "exploration/env_infos/final/reward_dist Mean             0.32504\n",
      "exploration/env_infos/final/reward_dist Std              0.308045\n",
      "exploration/env_infos/final/reward_dist Max              0.800821\n",
      "exploration/env_infos/final/reward_dist Min              1.38855e-63\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00116555\n",
      "exploration/env_infos/initial/reward_dist Std            0.00118257\n",
      "exploration/env_infos/initial/reward_dist Max            0.0026958\n",
      "exploration/env_infos/initial/reward_dist Min            2.9348e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.236133\n",
      "exploration/env_infos/reward_dist Std                    0.313118\n",
      "exploration/env_infos/reward_dist Max                    0.97419\n",
      "exploration/env_infos/reward_dist Min                    1.38855e-63\n",
      "exploration/env_infos/final/reward_energy Mean          -0.104883\n",
      "exploration/env_infos/final/reward_energy Std            0.0513993\n",
      "exploration/env_infos/final/reward_energy Max           -0.0546693\n",
      "exploration/env_infos/final/reward_energy Min           -0.185233\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.287152\n",
      "exploration/env_infos/initial/reward_energy Std          0.218961\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0756704\n",
      "exploration/env_infos/initial/reward_energy Min         -0.564252\n",
      "exploration/env_infos/reward_energy Mean                -0.197064\n",
      "exploration/env_infos/reward_energy Std                  0.220311\n",
      "exploration/env_infos/reward_energy Max                 -0.0115961\n",
      "exploration/env_infos/reward_energy Min                 -1.25978\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.125205\n",
      "exploration/env_infos/final/end_effector_loc Std         0.392401\n",
      "exploration/env_infos/final/end_effector_loc Max         0.304062\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000831577\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.01274\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0266949\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0260856\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0435148\n",
      "exploration/env_infos/end_effector_loc Std               0.249425\n",
      "exploration/env_infos/end_effector_loc Max               0.304062\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           84000\n",
      "evaluation/num paths total                            4200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0515327\n",
      "evaluation/Rewards Std                                   0.0780406\n",
      "evaluation/Rewards Max                                   0.136748\n",
      "evaluation/Rewards Min                                  -0.307914\n",
      "evaluation/Returns Mean                                 -1.03065\n",
      "evaluation/Returns Std                                   1.21853\n",
      "evaluation/Returns Max                                   1.83176\n",
      "evaluation/Returns Min                                  -3.60723\n",
      "evaluation/Actions Mean                                  0.00411539\n",
      "evaluation/Actions Std                                   0.0866956\n",
      "evaluation/Actions Max                                   0.731663\n",
      "evaluation/Actions Min                                  -0.843167\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.03065\n",
      "evaluation/env_infos/final/reward_dist Mean              0.194708\n",
      "evaluation/env_infos/final/reward_dist Std               0.278293\n",
      "evaluation/env_infos/final/reward_dist Max               0.977219\n",
      "evaluation/env_infos/final/reward_dist Min               4.03447e-09\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0072118\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0186561\n",
      "evaluation/env_infos/initial/reward_dist Max             0.117427\n",
      "evaluation/env_infos/initial/reward_dist Min             1.05836e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.199292\n",
      "evaluation/env_infos/reward_dist Std                     0.265534\n",
      "evaluation/env_infos/reward_dist Max                     0.996633\n",
      "evaluation/env_infos/reward_dist Min                     1.5478e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0546571\n",
      "evaluation/env_infos/final/reward_energy Std             0.0667798\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00635457\n",
      "evaluation/env_infos/final/reward_energy Min            -0.389303\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.230729\n",
      "evaluation/env_infos/initial/reward_energy Std           0.223137\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00611514\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.955398\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0672942\n",
      "evaluation/env_infos/reward_energy Std                   0.102653\n",
      "evaluation/env_infos/reward_energy Max                  -0.000464299\n",
      "evaluation/env_infos/reward_energy Min                  -0.955398\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0383224\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.225179\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.479811\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.477778\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000628841\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0113308\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0285488\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0421583\n",
      "evaluation/env_infos/end_effector_loc Mean               0.012545\n",
      "evaluation/env_infos/end_effector_loc Std                0.151521\n",
      "evaluation/env_infos/end_effector_loc Max                0.552201\n",
      "evaluation/env_infos/end_effector_loc Min               -0.477778\n",
      "time/data storing (s)                                    0.00309867\n",
      "time/evaluation sampling (s)                             1.27101\n",
      "time/exploration sampling (s)                            0.146733\n",
      "time/logging (s)                                         0.0211133\n",
      "time/saving (s)                                          0.0269889\n",
      "time/training (s)                                       51.2073\n",
      "time/epoch (s)                                          52.6762\n",
      "time/total (s)                                        4181.6\n",
      "Epoch                                                   83\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:35:31.783162 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 84 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000751667\n",
      "trainer/QF2 Loss                                         0.000705779\n",
      "trainer/Policy Loss                                      3.10635\n",
      "trainer/Q1 Predictions Mean                             -1.17131\n",
      "trainer/Q1 Predictions Std                               0.903222\n",
      "trainer/Q1 Predictions Max                               0.707973\n",
      "trainer/Q1 Predictions Min                              -3.62542\n",
      "trainer/Q2 Predictions Mean                             -1.16984\n",
      "trainer/Q2 Predictions Std                               0.902014\n",
      "trainer/Q2 Predictions Max                               0.693504\n",
      "trainer/Q2 Predictions Min                              -3.61057\n",
      "trainer/Q Targets Mean                                  -1.17083\n",
      "trainer/Q Targets Std                                    0.90419\n",
      "trainer/Q Targets Max                                    0.716348\n",
      "trainer/Q Targets Min                                   -3.58481\n",
      "trainer/Log Pis Mean                                     1.93474\n",
      "trainer/Log Pis Std                                      1.43514\n",
      "trainer/Log Pis Max                                      4.2128\n",
      "trainer/Log Pis Min                                     -2.99211\n",
      "trainer/Policy mu Mean                                   0.0519202\n",
      "trainer/Policy mu Std                                    0.221896\n",
      "trainer/Policy mu Max                                    1.50276\n",
      "trainer/Policy mu Min                                   -0.561848\n",
      "trainer/Policy log std Mean                             -2.34007\n",
      "trainer/Policy log std Std                               0.603693\n",
      "trainer/Policy log std Max                              -0.269094\n",
      "trainer/Policy log std Min                              -3.16929\n",
      "trainer/Alpha                                            0.023956\n",
      "trainer/Alpha Loss                                      -0.243462\n",
      "exploration/num steps total                           9500\n",
      "exploration/num paths total                            475\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0875493\n",
      "exploration/Rewards Std                                  0.0938802\n",
      "exploration/Rewards Max                                  0.0602913\n",
      "exploration/Rewards Min                                 -0.403812\n",
      "exploration/Returns Mean                                -1.75099\n",
      "exploration/Returns Std                                  1.47369\n",
      "exploration/Returns Max                                 -0.114906\n",
      "exploration/Returns Min                                 -4.03441\n",
      "exploration/Actions Mean                                 0.0113549\n",
      "exploration/Actions Std                                  0.14819\n",
      "exploration/Actions Max                                  0.474245\n",
      "exploration/Actions Min                                 -0.531655\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.75099\n",
      "exploration/env_infos/final/reward_dist Mean             0.0894367\n",
      "exploration/env_infos/final/reward_dist Std              0.155276\n",
      "exploration/env_infos/final/reward_dist Max              0.397608\n",
      "exploration/env_infos/final/reward_dist Min              1.1131e-82\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00725961\n",
      "exploration/env_infos/initial/reward_dist Std            0.00615325\n",
      "exploration/env_infos/initial/reward_dist Max            0.0157032\n",
      "exploration/env_infos/initial/reward_dist Min            1.63028e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.165233\n",
      "exploration/env_infos/reward_dist Std                    0.252508\n",
      "exploration/env_infos/reward_dist Max                    0.963572\n",
      "exploration/env_infos/reward_dist Min                    1.1131e-82\n",
      "exploration/env_infos/final/reward_energy Mean          -0.183541\n",
      "exploration/env_infos/final/reward_energy Std            0.0763726\n",
      "exploration/env_infos/final/reward_energy Max           -0.0826769\n",
      "exploration/env_infos/final/reward_energy Min           -0.299847\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.295377\n",
      "exploration/env_infos/initial/reward_energy Std          0.169579\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0554643\n",
      "exploration/env_infos/initial/reward_energy Min         -0.480806\n",
      "exploration/env_infos/reward_energy Mean                -0.171598\n",
      "exploration/env_infos/reward_energy Std                  0.121379\n",
      "exploration/env_infos/reward_energy Max                 -0.0139685\n",
      "exploration/env_infos/reward_energy Min                 -0.590943\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.102304\n",
      "exploration/env_infos/final/end_effector_loc Std         0.448656\n",
      "exploration/env_infos/final/end_effector_loc Max         0.985917\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.439669\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00212934\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0118521\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0237122\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0197018\n",
      "exploration/env_infos/end_effector_loc Mean              0.0377609\n",
      "exploration/env_infos/end_effector_loc Std               0.247678\n",
      "exploration/env_infos/end_effector_loc Max               0.985917\n",
      "exploration/env_infos/end_effector_loc Min              -0.439669\n",
      "evaluation/num steps total                           85000\n",
      "evaluation/num paths total                            4250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0707834\n",
      "evaluation/Rewards Std                                   0.0794524\n",
      "evaluation/Rewards Max                                   0.110934\n",
      "evaluation/Rewards Min                                  -0.475107\n",
      "evaluation/Returns Mean                                 -1.41567\n",
      "evaluation/Returns Std                                   1.25408\n",
      "evaluation/Returns Max                                   0.942785\n",
      "evaluation/Returns Min                                  -6.3838\n",
      "evaluation/Actions Mean                                  0.0109766\n",
      "evaluation/Actions Std                                   0.0944548\n",
      "evaluation/Actions Max                                   0.883904\n",
      "evaluation/Actions Min                                  -0.798296\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.41567\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0441786\n",
      "evaluation/env_infos/final/reward_dist Std               0.0958876\n",
      "evaluation/env_infos/final/reward_dist Max               0.390884\n",
      "evaluation/env_infos/final/reward_dist Min               7.69454e-35\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0074676\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0150878\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0869212\n",
      "evaluation/env_infos/initial/reward_dist Min             2.20235e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.111048\n",
      "evaluation/env_infos/reward_dist Std                     0.212007\n",
      "evaluation/env_infos/reward_dist Max                     0.993214\n",
      "evaluation/env_infos/reward_dist Min                     3.1203e-100\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.066275\n",
      "evaluation/env_infos/final/reward_energy Std             0.0843609\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00353066\n",
      "evaluation/env_infos/final/reward_energy Min            -0.452562\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.166574\n",
      "evaluation/env_infos/initial/reward_energy Std           0.196738\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00902033\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.901841\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0718194\n",
      "evaluation/env_infos/reward_energy Std                   0.113694\n",
      "evaluation/env_infos/reward_energy Max                  -0.000757595\n",
      "evaluation/env_infos/reward_energy Min                  -0.984006\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.116495\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.257772\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.71681\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.5124\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       9.12571e-06\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00911406\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0386118\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0321199\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0484285\n",
      "evaluation/env_infos/end_effector_loc Std                0.17143\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.5124\n",
      "time/data storing (s)                                    0.00285648\n",
      "time/evaluation sampling (s)                             1.01317\n",
      "time/exploration sampling (s)                            0.129844\n",
      "time/logging (s)                                         0.0227733\n",
      "time/saving (s)                                          0.028127\n",
      "time/training (s)                                       51.3197\n",
      "time/epoch (s)                                          52.5165\n",
      "time/total (s)                                        4235.18\n",
      "Epoch                                                   84\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:36:23.825490 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 85 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000855532\n",
      "trainer/QF2 Loss                                         0.000963405\n",
      "trainer/Policy Loss                                      3.16513\n",
      "trainer/Q1 Predictions Mean                             -1.16825\n",
      "trainer/Q1 Predictions Std                               1.00838\n",
      "trainer/Q1 Predictions Max                               0.924561\n",
      "trainer/Q1 Predictions Min                              -4.13899\n",
      "trainer/Q2 Predictions Mean                             -1.1718\n",
      "trainer/Q2 Predictions Std                               1.00159\n",
      "trainer/Q2 Predictions Max                               0.877535\n",
      "trainer/Q2 Predictions Min                              -4.10344\n",
      "trainer/Q Targets Mean                                  -1.17555\n",
      "trainer/Q Targets Std                                    1.00967\n",
      "trainer/Q Targets Max                                    0.891863\n",
      "trainer/Q Targets Min                                   -4.11423\n",
      "trainer/Log Pis Mean                                     2.00454\n",
      "trainer/Log Pis Std                                      1.30367\n",
      "trainer/Log Pis Max                                      6.24955\n",
      "trainer/Log Pis Min                                     -3.44088\n",
      "trainer/Policy mu Mean                                   0.0513605\n",
      "trainer/Policy mu Std                                    0.331835\n",
      "trainer/Policy mu Max                                    2.26121\n",
      "trainer/Policy mu Min                                   -1.19138\n",
      "trainer/Policy log std Mean                             -2.2443\n",
      "trainer/Policy log std Std                               0.6423\n",
      "trainer/Policy log std Max                               0.0606581\n",
      "trainer/Policy log std Min                              -3.18303\n",
      "trainer/Alpha                                            0.0237196\n",
      "trainer/Alpha Loss                                       0.0169725\n",
      "exploration/num steps total                           9600\n",
      "exploration/num paths total                            480\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0940113\n",
      "exploration/Rewards Std                                  0.0877212\n",
      "exploration/Rewards Max                                  0.0834497\n",
      "exploration/Rewards Min                                 -0.327191\n",
      "exploration/Returns Mean                                -1.88023\n",
      "exploration/Returns Std                                  0.997942\n",
      "exploration/Returns Max                                 -0.899154\n",
      "exploration/Returns Min                                 -3.67311\n",
      "exploration/Actions Mean                                -0.00923733\n",
      "exploration/Actions Std                                  0.121246\n",
      "exploration/Actions Max                                  0.454258\n",
      "exploration/Actions Min                                 -0.546875\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.88023\n",
      "exploration/env_infos/final/reward_dist Mean             0.0478526\n",
      "exploration/env_infos/final/reward_dist Std              0.0953706\n",
      "exploration/env_infos/final/reward_dist Max              0.238594\n",
      "exploration/env_infos/final/reward_dist Min              3.66281e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00858526\n",
      "exploration/env_infos/initial/reward_dist Std            0.0138561\n",
      "exploration/env_infos/initial/reward_dist Max            0.036058\n",
      "exploration/env_infos/initial/reward_dist Min            1.24269e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.177428\n",
      "exploration/env_infos/reward_dist Std                    0.265888\n",
      "exploration/env_infos/reward_dist Max                    0.978666\n",
      "exploration/env_infos/reward_dist Min                    3.66281e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.244279\n",
      "exploration/env_infos/final/reward_energy Std            0.175954\n",
      "exploration/env_infos/final/reward_energy Max           -0.0237224\n",
      "exploration/env_infos/final/reward_energy Min           -0.479927\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.241581\n",
      "exploration/env_infos/initial/reward_energy Std          0.134951\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0965781\n",
      "exploration/env_infos/initial/reward_energy Min         -0.46201\n",
      "exploration/env_infos/reward_energy Mean                -0.135361\n",
      "exploration/env_infos/reward_energy Std                  0.106062\n",
      "exploration/env_infos/reward_energy Max                 -0.0184605\n",
      "exploration/env_infos/reward_energy Min                 -0.566015\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.133946\n",
      "exploration/env_infos/final/end_effector_loc Std         0.274934\n",
      "exploration/env_infos/final/end_effector_loc Max         0.295825\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.586996\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000881553\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00974368\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0227129\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0137665\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0456989\n",
      "exploration/env_infos/end_effector_loc Std               0.194054\n",
      "exploration/env_infos/end_effector_loc Max               0.319806\n",
      "exploration/env_infos/end_effector_loc Min              -0.586996\n",
      "evaluation/num steps total                           86000\n",
      "evaluation/num paths total                            4300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0568031\n",
      "evaluation/Rewards Std                                   0.0950642\n",
      "evaluation/Rewards Max                                   0.135685\n",
      "evaluation/Rewards Min                                  -0.500726\n",
      "evaluation/Returns Mean                                 -1.13606\n",
      "evaluation/Returns Std                                   1.47621\n",
      "evaluation/Returns Max                                   1.36832\n",
      "evaluation/Returns Min                                  -6.97271\n",
      "evaluation/Actions Mean                                  0.00514501\n",
      "evaluation/Actions Std                                   0.100675\n",
      "evaluation/Actions Max                                   0.777781\n",
      "evaluation/Actions Min                                  -0.627673\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.13606\n",
      "evaluation/env_infos/final/reward_dist Mean              0.103041\n",
      "evaluation/env_infos/final/reward_dist Std               0.210808\n",
      "evaluation/env_infos/final/reward_dist Max               0.967063\n",
      "evaluation/env_infos/final/reward_dist Min               2.55266e-41\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0090488\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126075\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0471988\n",
      "evaluation/env_infos/initial/reward_dist Min             2.64548e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.181395\n",
      "evaluation/env_infos/reward_dist Std                     0.256165\n",
      "evaluation/env_infos/reward_dist Max                     0.995307\n",
      "evaluation/env_infos/reward_dist Min                     2.55266e-41\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0705127\n",
      "evaluation/env_infos/final/reward_energy Std             0.0663465\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00696136\n",
      "evaluation/env_infos/final/reward_energy Min            -0.443711\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.295688\n",
      "evaluation/env_infos/initial/reward_energy Std           0.206119\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0225626\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.710746\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0864135\n",
      "evaluation/env_infos/reward_energy Std                   0.113387\n",
      "evaluation/env_infos/reward_energy Max                  -0.00316886\n",
      "evaluation/env_infos/reward_energy Min                  -0.844356\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0116608\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.300108\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.816215\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.69929\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000785667\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127192\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0289242\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0305362\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00212452\n",
      "evaluation/env_infos/end_effector_loc Std                0.198984\n",
      "evaluation/env_infos/end_effector_loc Max                0.816215\n",
      "evaluation/env_infos/end_effector_loc Min               -0.69929\n",
      "time/data storing (s)                                    0.00282452\n",
      "time/evaluation sampling (s)                             1.10168\n",
      "time/exploration sampling (s)                            0.122554\n",
      "time/logging (s)                                         0.0195618\n",
      "time/saving (s)                                          0.0294849\n",
      "time/training (s)                                       49.6159\n",
      "time/epoch (s)                                          50.8921\n",
      "time/total (s)                                        4287.22\n",
      "Epoch                                                   85\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:37:15.500305 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 86 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00081931\r\n",
      "trainer/QF2 Loss                                         0.000836693\r\n",
      "trainer/Policy Loss                                      3.03035\r\n",
      "trainer/Q1 Predictions Mean                             -1.125\r\n",
      "trainer/Q1 Predictions Std                               0.936117\r\n",
      "trainer/Q1 Predictions Max                               0.962572\r\n",
      "trainer/Q1 Predictions Min                              -3.92476\r\n",
      "trainer/Q2 Predictions Mean                             -1.12644\r\n",
      "trainer/Q2 Predictions Std                               0.939584\r\n",
      "trainer/Q2 Predictions Max                               0.991322\r\n",
      "trainer/Q2 Predictions Min                              -3.92007\r\n",
      "trainer/Q Targets Mean                                  -1.12588\r\n",
      "trainer/Q Targets Std                                    0.937968\r\n",
      "trainer/Q Targets Max                                    1.01837\r\n",
      "trainer/Q Targets Min                                   -3.9435\r\n",
      "trainer/Log Pis Mean                                     1.9083\r\n",
      "trainer/Log Pis Std                                      1.50904\r\n",
      "trainer/Log Pis Max                                      4.32252\r\n",
      "trainer/Log Pis Min                                     -4.30051\r\n",
      "trainer/Policy mu Mean                                   0.0687053\r\n",
      "trainer/Policy mu Std                                    0.350528\r\n",
      "trainer/Policy mu Max                                    2.23132\r\n",
      "trainer/Policy mu Min                                   -1.80152\r\n",
      "trainer/Policy log std Mean                             -2.25008\r\n",
      "trainer/Policy log std Std                               0.668669\r\n",
      "trainer/Policy log std Max                               0.00763177\r\n",
      "trainer/Policy log std Min                              -3.25716\r\n",
      "trainer/Alpha                                            0.0246952\r\n",
      "trainer/Alpha Loss                                      -0.339355\r\n",
      "exploration/num steps total                           9700\r\n",
      "exploration/num paths total                            485\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.156608\r\n",
      "exploration/Rewards Std                                  0.110325\r\n",
      "exploration/Rewards Max                                  0.0296039\r\n",
      "exploration/Rewards Min                                 -0.477681\r\n",
      "exploration/Returns Mean                                -3.13217\r\n",
      "exploration/Returns Std                                  1.36198\r\n",
      "exploration/Returns Max                                 -2.04634\r\n",
      "exploration/Returns Min                                 -5.73329\r\n",
      "exploration/Actions Mean                                 0.0104222\r\n",
      "exploration/Actions Std                                  0.203443\r\n",
      "exploration/Actions Max                                  0.698733\r\n",
      "exploration/Actions Min                                 -0.811316\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -3.13217\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.107084\r\n",
      "exploration/env_infos/final/reward_dist Std              0.194842\r\n",
      "exploration/env_infos/final/reward_dist Max              0.495542\r\n",
      "exploration/env_infos/final/reward_dist Min              4.65517e-38\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00777185\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0118895\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0312708\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.91535e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.103352\r\n",
      "exploration/env_infos/reward_dist Std                    0.236001\r\n",
      "exploration/env_infos/reward_dist Max                    0.948472\r\n",
      "exploration/env_infos/reward_dist Min                    4.65517e-38\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.260993\r\n",
      "exploration/env_infos/final/reward_energy Std            0.166338\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0816403\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.553158\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.319056\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.152971\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0723563\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.50494\r\n",
      "exploration/env_infos/reward_energy Mean                -0.223699\r\n",
      "exploration/env_infos/reward_energy Std                  0.181533\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00675088\r\n",
      "exploration/env_infos/reward_energy Min                 -0.886753\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.225965\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.297271\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.602847\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.290822\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00702379\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0103519\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0252356\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00408863\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.142583\r\n",
      "exploration/env_infos/end_effector_loc Std               0.197046\r\n",
      "exploration/env_infos/end_effector_loc Max               0.602847\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.290822\r\n",
      "evaluation/num steps total                           87000\r\n",
      "evaluation/num paths total                            4350\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0453707\r\n",
      "evaluation/Rewards Std                                   0.0742961\r\n",
      "evaluation/Rewards Max                                   0.153327\r\n",
      "evaluation/Rewards Min                                  -0.512343\r\n",
      "evaluation/Returns Mean                                 -0.907414\r\n",
      "evaluation/Returns Std                                   1.11827\r\n",
      "evaluation/Returns Max                                   1.85325\r\n",
      "evaluation/Returns Min                                  -2.95858\r\n",
      "evaluation/Actions Mean                                  0.0069006\r\n",
      "evaluation/Actions Std                                   0.0809212\r\n",
      "evaluation/Actions Max                                   0.673162\r\n",
      "evaluation/Actions Min                                  -0.797242\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.907414\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.170603\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.258682\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.936887\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.71062e-48\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00608185\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0122879\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0648421\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.44074e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.206823\r\n",
      "evaluation/env_infos/reward_dist Std                     0.285879\r\n",
      "evaluation/env_infos/reward_dist Max                     0.99968\r\n",
      "evaluation/env_infos/reward_dist Min                     2.71062e-48\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0763778\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.140963\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00133194\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.772978\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.240526\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.216141\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00680561\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.05712\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0683702\r\n",
      "evaluation/env_infos/reward_energy Std                   0.092289\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00089725\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.05712\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0712399\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.246841\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.976319\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.50621\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000429362\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0114249\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.023658\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0398621\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0285739\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.162503\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.976319\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.50621\r\n",
      "time/data storing (s)                                    0.00304001\r\n",
      "time/evaluation sampling (s)                             0.994811\r\n",
      "time/exploration sampling (s)                            0.124561\r\n",
      "time/logging (s)                                         0.0197373\r\n",
      "time/saving (s)                                          0.0278067\r\n",
      "time/training (s)                                       49.3938\r\n",
      "time/epoch (s)                                          50.5638\r\n",
      "time/total (s)                                        4338.89\r\n",
      "Epoch                                                   86\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:38:06.713764 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 87 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000967391\r\n",
      "trainer/QF2 Loss                                         0.000740711\r\n",
      "trainer/Policy Loss                                      3.10805\r\n",
      "trainer/Q1 Predictions Mean                             -1.09719\r\n",
      "trainer/Q1 Predictions Std                               0.898056\r\n",
      "trainer/Q1 Predictions Max                               1.07411\r\n",
      "trainer/Q1 Predictions Min                              -3.83963\r\n",
      "trainer/Q2 Predictions Mean                             -1.09717\r\n",
      "trainer/Q2 Predictions Std                               0.895508\r\n",
      "trainer/Q2 Predictions Max                               1.04275\r\n",
      "trainer/Q2 Predictions Min                              -3.82989\r\n",
      "trainer/Q Targets Mean                                  -1.09238\r\n",
      "trainer/Q Targets Std                                    0.894355\r\n",
      "trainer/Q Targets Max                                    1.06338\r\n",
      "trainer/Q Targets Min                                   -3.83702\r\n",
      "trainer/Log Pis Mean                                     2.01512\r\n",
      "trainer/Log Pis Std                                      1.28843\r\n",
      "trainer/Log Pis Max                                      4.52608\r\n",
      "trainer/Log Pis Min                                     -2.25408\r\n",
      "trainer/Policy mu Mean                                   0.0118531\r\n",
      "trainer/Policy mu Std                                    0.341171\r\n",
      "trainer/Policy mu Max                                    2.08294\r\n",
      "trainer/Policy mu Min                                   -1.97468\r\n",
      "trainer/Policy log std Mean                             -2.26922\r\n",
      "trainer/Policy log std Std                               0.592561\r\n",
      "trainer/Policy log std Max                              -0.0713712\r\n",
      "trainer/Policy log std Min                              -3.24393\r\n",
      "trainer/Alpha                                            0.0245906\r\n",
      "trainer/Alpha Loss                                       0.0560372\r\n",
      "exploration/num steps total                           9800\r\n",
      "exploration/num paths total                            490\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0348848\r\n",
      "exploration/Rewards Std                                  0.0751926\r\n",
      "exploration/Rewards Max                                  0.119248\r\n",
      "exploration/Rewards Min                                 -0.182206\r\n",
      "exploration/Returns Mean                                -0.697696\r\n",
      "exploration/Returns Std                                  0.987555\r\n",
      "exploration/Returns Max                                  0.731453\r\n",
      "exploration/Returns Min                                 -1.96429\r\n",
      "exploration/Actions Mean                                 0.00423574\r\n",
      "exploration/Actions Std                                  0.153079\r\n",
      "exploration/Actions Max                                  0.551274\r\n",
      "exploration/Actions Min                                 -0.551642\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.697696\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.137535\r\n",
      "exploration/env_infos/final/reward_dist Std              0.154808\r\n",
      "exploration/env_infos/final/reward_dist Max              0.382346\r\n",
      "exploration/env_infos/final/reward_dist Min              1.24052e-07\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000601893\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000720201\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00189981\r\n",
      "exploration/env_infos/initial/reward_dist Min            7.73048e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.192923\r\n",
      "exploration/env_infos/reward_dist Std                    0.240579\r\n",
      "exploration/env_infos/reward_dist Max                    0.905998\r\n",
      "exploration/env_infos/reward_dist Min                    1.24052e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.12808\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0328142\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.08518\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.167604\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.305314\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.185088\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0502204\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.526035\r\n",
      "exploration/env_infos/reward_energy Mean                -0.175031\r\n",
      "exploration/env_infos/reward_energy Std                  0.127539\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0066996\r\n",
      "exploration/env_infos/reward_energy Min                 -0.66289\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00151801\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.245833\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.260676\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.515441\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00348638\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0121321\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0252777\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00882859\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00734038\r\n",
      "exploration/env_infos/end_effector_loc Std               0.164489\r\n",
      "exploration/env_infos/end_effector_loc Max               0.261854\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.515441\r\n",
      "evaluation/num steps total                           88000\r\n",
      "evaluation/num paths total                            4400\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.044048\r\n",
      "evaluation/Rewards Std                                   0.0780151\r\n",
      "evaluation/Rewards Max                                   0.10574\r\n",
      "evaluation/Rewards Min                                  -0.667833\r\n",
      "evaluation/Returns Mean                                 -0.88096\r\n",
      "evaluation/Returns Std                                   1.25006\r\n",
      "evaluation/Returns Max                                   1.12782\r\n",
      "evaluation/Returns Min                                  -4.8447\r\n",
      "evaluation/Actions Mean                                  0.00322819\r\n",
      "evaluation/Actions Std                                   0.0753415\r\n",
      "evaluation/Actions Max                                   0.608989\r\n",
      "evaluation/Actions Min                                  -0.763478\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.88096\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.20437\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.281357\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.993613\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.60494e-32\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00625269\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00972274\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0331669\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.58418e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.174688\r\n",
      "evaluation/env_infos/reward_dist Std                     0.244664\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993613\r\n",
      "evaluation/env_infos/reward_dist Min                     4.60494e-32\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.044362\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0413011\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00322027\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.190079\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.222912\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.213851\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0125951\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.02969\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.063092\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0859821\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00104715\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.02969\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0707715\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.244738\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.698589\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.488588\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00167996\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107914\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0304494\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0381739\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0376834\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/end_effector_loc Std                0.154626\n",
      "evaluation/env_infos/end_effector_loc Max                0.698589\n",
      "evaluation/env_infos/end_effector_loc Min               -0.488588\n",
      "time/data storing (s)                                    0.00314935\n",
      "time/evaluation sampling (s)                             0.977253\n",
      "time/exploration sampling (s)                            0.129601\n",
      "time/logging (s)                                         0.0202238\n",
      "time/saving (s)                                          0.028472\n",
      "time/training (s)                                       48.9125\n",
      "time/epoch (s)                                          50.0712\n",
      "time/total (s)                                        4390.1\n",
      "Epoch                                                   87\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 11:38:58.231733 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 88 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000713256\n",
      "trainer/QF2 Loss                                         0.000946556\n",
      "trainer/Policy Loss                                      2.854\n",
      "trainer/Q1 Predictions Mean                             -1.0402\n",
      "trainer/Q1 Predictions Std                               0.95404\n",
      "trainer/Q1 Predictions Max                               1.12228\n",
      "trainer/Q1 Predictions Min                              -3.69971\n",
      "trainer/Q2 Predictions Mean                             -1.03471\n",
      "trainer/Q2 Predictions Std                               0.95488\n",
      "trainer/Q2 Predictions Max                               1.11517\n",
      "trainer/Q2 Predictions Min                              -3.68875\n",
      "trainer/Q Targets Mean                                  -1.04437\n",
      "trainer/Q Targets Std                                    0.957498\n",
      "trainer/Q Targets Max                                    1.15716\n",
      "trainer/Q Targets Min                                   -3.75101\n",
      "trainer/Log Pis Mean                                     1.82518\n",
      "trainer/Log Pis Std                                      1.5307\n",
      "trainer/Log Pis Max                                      5.01542\n",
      "trainer/Log Pis Min                                     -3.70905\n",
      "trainer/Policy mu Mean                                   0.0473441\n",
      "trainer/Policy mu Std                                    0.407791\n",
      "trainer/Policy mu Max                                    2.20331\n",
      "trainer/Policy mu Min                                   -2.30514\n",
      "trainer/Policy log std Mean                             -2.20436\n",
      "trainer/Policy log std Std                               0.671218\n",
      "trainer/Policy log std Max                              -0.0405939\n",
      "trainer/Policy log std Min                              -3.3227\n",
      "trainer/Alpha                                            0.022816\n",
      "trainer/Alpha Loss                                      -0.660891\n",
      "exploration/num steps total                           9900\n",
      "exploration/num paths total                            495\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117967\n",
      "exploration/Rewards Std                                  0.0972992\n",
      "exploration/Rewards Max                                  0.0613822\n",
      "exploration/Rewards Min                                 -0.719589\n",
      "exploration/Returns Mean                                -2.35934\n",
      "exploration/Returns Std                                  1.22194\n",
      "exploration/Returns Max                                 -0.00465625\n",
      "exploration/Returns Min                                 -3.42246\n",
      "exploration/Actions Mean                                -0.00316595\n",
      "exploration/Actions Std                                  0.151526\n",
      "exploration/Actions Max                                  0.397889\n",
      "exploration/Actions Min                                 -0.900657\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.35934\n",
      "exploration/env_infos/final/reward_dist Mean             0.0628594\n",
      "exploration/env_infos/final/reward_dist Std              0.058378\n",
      "exploration/env_infos/final/reward_dist Max              0.15015\n",
      "exploration/env_infos/final/reward_dist Min              4.10023e-20\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00260435\n",
      "exploration/env_infos/initial/reward_dist Std            0.00458815\n",
      "exploration/env_infos/initial/reward_dist Max            0.0117664\n",
      "exploration/env_infos/initial/reward_dist Min            1.06785e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.124466\n",
      "exploration/env_infos/reward_dist Std                    0.168553\n",
      "exploration/env_infos/reward_dist Max                    0.740134\n",
      "exploration/env_infos/reward_dist Min                    4.10023e-20\n",
      "exploration/env_infos/final/reward_energy Mean          -0.15136\n",
      "exploration/env_infos/final/reward_energy Std            0.0512278\n",
      "exploration/env_infos/final/reward_energy Max           -0.0827304\n",
      "exploration/env_infos/final/reward_energy Min           -0.225133\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.392085\n",
      "exploration/env_infos/initial/reward_energy Std          0.427589\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0630191\n",
      "exploration/env_infos/initial/reward_energy Min         -1.22093\n",
      "exploration/env_infos/reward_energy Mean                -0.155471\n",
      "exploration/env_infos/reward_energy Std                  0.147543\n",
      "exploration/env_infos/reward_energy Max                 -0.0175455\n",
      "exploration/env_infos/reward_energy Min                 -1.22093\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.108977\n",
      "exploration/env_infos/final/end_effector_loc Std         0.307836\n",
      "exploration/env_infos/final/end_effector_loc Max         0.44749\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.594712\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0115781\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0169308\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00295757\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0450329\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0840313\n",
      "exploration/env_infos/end_effector_loc Std               0.174826\n",
      "exploration/env_infos/end_effector_loc Max               0.44749\n",
      "exploration/env_infos/end_effector_loc Min              -0.594712\n",
      "evaluation/num steps total                           89000\n",
      "evaluation/num paths total                            4450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0585888\n",
      "evaluation/Rewards Std                                   0.0703203\n",
      "evaluation/Rewards Max                                   0.127061\n",
      "evaluation/Rewards Min                                  -0.45308\n",
      "evaluation/Returns Mean                                 -1.17178\n",
      "evaluation/Returns Std                                   1.04895\n",
      "evaluation/Returns Max                                   1.83083\n",
      "evaluation/Returns Min                                  -3.29048\n",
      "evaluation/Actions Mean                                  0.00175714\n",
      "evaluation/Actions Std                                   0.0733458\n",
      "evaluation/Actions Max                                   0.668374\n",
      "evaluation/Actions Min                                  -0.448036\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.17178\n",
      "evaluation/env_infos/final/reward_dist Mean              0.173446\n",
      "evaluation/env_infos/final/reward_dist Std               0.266338\n",
      "evaluation/env_infos/final/reward_dist Max               0.999222\n",
      "evaluation/env_infos/final/reward_dist Min               3.16517e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00684491\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126979\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0512853\n",
      "evaluation/env_infos/initial/reward_dist Min             9.39962e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.153595\n",
      "evaluation/env_infos/reward_dist Std                     0.243147\n",
      "evaluation/env_infos/reward_dist Max                     0.999222\n",
      "evaluation/env_infos/reward_dist Min                     3.16517e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0876882\n",
      "evaluation/env_infos/final/reward_energy Std             0.11302\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00258433\n",
      "evaluation/env_infos/final/reward_energy Min            -0.630141\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.232505\n",
      "evaluation/env_infos/initial/reward_energy Std           0.213519\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00914598\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.725212\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0619488\n",
      "evaluation/env_infos/reward_energy Std                   0.0832329\n",
      "evaluation/env_infos/reward_energy Max                  -0.000496725\n",
      "evaluation/env_infos/reward_energy Min                  -0.725212\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0474509\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.252252\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.531435\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.620855\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00325979\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010674\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0334187\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0224018\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0292928\n",
      "evaluation/env_infos/end_effector_loc Std                0.158389\n",
      "evaluation/env_infos/end_effector_loc Max                0.531435\n",
      "evaluation/env_infos/end_effector_loc Min               -0.620855\n",
      "time/data storing (s)                                    0.00364018\n",
      "time/evaluation sampling (s)                             1.08489\n",
      "time/exploration sampling (s)                            0.130115\n",
      "time/logging (s)                                         0.0192907\n",
      "time/saving (s)                                          0.0272917\n",
      "time/training (s)                                       49.1073\n",
      "time/epoch (s)                                          50.3725\n",
      "time/total (s)                                        4441.61\n",
      "Epoch                                                   88\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:39:49.719206 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 89 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000661742\r\n",
      "trainer/QF2 Loss                                         0.000607599\r\n",
      "trainer/Policy Loss                                      3.14147\r\n",
      "trainer/Q1 Predictions Mean                             -1.11789\r\n",
      "trainer/Q1 Predictions Std                               0.877175\r\n",
      "trainer/Q1 Predictions Max                               1.19313\r\n",
      "trainer/Q1 Predictions Min                              -3.03903\r\n",
      "trainer/Q2 Predictions Mean                             -1.11696\r\n",
      "trainer/Q2 Predictions Std                               0.878045\r\n",
      "trainer/Q2 Predictions Max                               1.19455\r\n",
      "trainer/Q2 Predictions Min                              -3.04007\r\n",
      "trainer/Q Targets Mean                                  -1.12013\r\n",
      "trainer/Q Targets Std                                    0.882249\r\n",
      "trainer/Q Targets Max                                    1.20659\r\n",
      "trainer/Q Targets Min                                   -3.06234\r\n",
      "trainer/Log Pis Mean                                     2.03584\r\n",
      "trainer/Log Pis Std                                      1.33571\r\n",
      "trainer/Log Pis Max                                      4.46727\r\n",
      "trainer/Log Pis Min                                     -2.39148\r\n",
      "trainer/Policy mu Mean                                   0.0127224\r\n",
      "trainer/Policy mu Std                                    0.411846\r\n",
      "trainer/Policy mu Max                                    2.15832\r\n",
      "trainer/Policy mu Min                                   -2.48651\r\n",
      "trainer/Policy log std Mean                             -2.29916\r\n",
      "trainer/Policy log std Std                               0.636434\r\n",
      "trainer/Policy log std Max                              -0.265467\r\n",
      "trainer/Policy log std Min                              -3.27702\r\n",
      "trainer/Alpha                                            0.0228393\r\n",
      "trainer/Alpha Loss                                       0.135478\r\n",
      "exploration/num steps total                          10000\r\n",
      "exploration/num paths total                            500\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0561464\r\n",
      "exploration/Rewards Std                                  0.107047\r\n",
      "exploration/Rewards Max                                  0.15085\r\n",
      "exploration/Rewards Min                                 -0.300006\r\n",
      "exploration/Returns Mean                                -1.12293\r\n",
      "exploration/Returns Std                                  1.91971\r\n",
      "exploration/Returns Max                                  2.27685\r\n",
      "exploration/Returns Min                                 -3.14705\r\n",
      "exploration/Actions Mean                                -0.00717381\r\n",
      "exploration/Actions Std                                  0.124177\r\n",
      "exploration/Actions Max                                  0.43078\r\n",
      "exploration/Actions Min                                 -0.661325\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.12293\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0169231\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0334529\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0838278\r\n",
      "exploration/env_infos/final/reward_dist Min              1.07735e-17\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103564\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.015862\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0412912\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.06136e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.145638\r\n",
      "exploration/env_infos/reward_dist Std                    0.290544\r\n",
      "exploration/env_infos/reward_dist Max                    0.953906\r\n",
      "exploration/env_infos/reward_dist Min                    1.07735e-17\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.122873\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0596587\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.059025\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.212737\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.283125\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.217481\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0480824\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.661942\r\n",
      "exploration/env_infos/reward_energy Mean                -0.132783\r\n",
      "exploration/env_infos/reward_energy Std                  0.115375\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00545208\r\n",
      "exploration/env_infos/reward_energy Min                 -0.782788\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0390409\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.26013\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.548649\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.347223\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00457411\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0117643\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00857293\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0330662\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0162528\r\n",
      "exploration/env_infos/end_effector_loc Std               0.153439\r\n",
      "exploration/env_infos/end_effector_loc Max               0.548649\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.347223\r\n",
      "evaluation/num steps total                           90000\r\n",
      "evaluation/num paths total                            4500\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0394548\r\n",
      "evaluation/Rewards Std                                   0.0750897\r\n",
      "evaluation/Rewards Max                                   0.169366\r\n",
      "evaluation/Rewards Min                                  -0.551534\r\n",
      "evaluation/Returns Mean                                 -0.789095\r\n",
      "evaluation/Returns Std                                   1.19663\r\n",
      "evaluation/Returns Max                                   1.68729\r\n",
      "evaluation/Returns Min                                  -2.82151\r\n",
      "evaluation/Actions Mean                                  0.00187652\r\n",
      "evaluation/Actions Std                                   0.0733587\r\n",
      "evaluation/Actions Max                                   0.840842\r\n",
      "evaluation/Actions Min                                  -0.720185\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.789095\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.221816\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.281053\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.912149\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.19618e-12\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00697503\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117158\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0662914\r\n",
      "evaluation/env_infos/initial/reward_dist Min             4.62636e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.213293\r\n",
      "evaluation/env_infos/reward_dist Std                     0.287505\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996289\r\n",
      "evaluation/env_infos/reward_dist Min                     2.19618e-12\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0533888\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0544848\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0078962\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.234725\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.242182\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.220842\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00525646\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.909047\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0628743\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0825642\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000219416\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.909047\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0358187\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23201\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.486271\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.559394\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00122375\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0115231\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0420421\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0360092\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0185866\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.157884\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.486271\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.559394\r\n",
      "time/data storing (s)                                    0.00298995\r\n",
      "time/evaluation sampling (s)                             0.94986\r\n",
      "time/exploration sampling (s)                            0.220384\r\n",
      "time/logging (s)                                         0.0189447\r\n",
      "time/saving (s)                                          0.0282875\r\n",
      "time/training (s)                                       49.1325\r\n",
      "time/epoch (s)                                          50.3529\r\n",
      "time/total (s)                                        4493.1\r\n",
      "Epoch                                                   89\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:40:40.688407 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 90 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000640028\r\n",
      "trainer/QF2 Loss                                         0.00113842\r\n",
      "trainer/Policy Loss                                      2.83636\r\n",
      "trainer/Q1 Predictions Mean                             -0.984102\r\n",
      "trainer/Q1 Predictions Std                               0.858803\r\n",
      "trainer/Q1 Predictions Max                               0.801073\r\n",
      "trainer/Q1 Predictions Min                              -2.88309\r\n",
      "trainer/Q2 Predictions Mean                             -0.974686\r\n",
      "trainer/Q2 Predictions Std                               0.847579\r\n",
      "trainer/Q2 Predictions Max                               0.757766\r\n",
      "trainer/Q2 Predictions Min                              -2.8244\r\n",
      "trainer/Q Targets Mean                                  -0.987131\r\n",
      "trainer/Q Targets Std                                    0.859702\r\n",
      "trainer/Q Targets Max                                    0.828441\r\n",
      "trainer/Q Targets Min                                   -2.92039\r\n",
      "trainer/Log Pis Mean                                     1.85798\r\n",
      "trainer/Log Pis Std                                      1.42351\r\n",
      "trainer/Log Pis Max                                      4.24953\r\n",
      "trainer/Log Pis Min                                     -4.17863\r\n",
      "trainer/Policy mu Mean                                  -0.0211141\r\n",
      "trainer/Policy mu Std                                    0.307442\r\n",
      "trainer/Policy mu Max                                    2.31925\r\n",
      "trainer/Policy mu Min                                   -1.68744\r\n",
      "trainer/Policy log std Mean                             -2.27155\r\n",
      "trainer/Policy log std Std                               0.598817\r\n",
      "trainer/Policy log std Max                              -0.0942147\r\n",
      "trainer/Policy log std Min                              -3.14763\r\n",
      "trainer/Alpha                                            0.0239412\r\n",
      "trainer/Alpha Loss                                      -0.529912\r\n",
      "exploration/num steps total                          10100\r\n",
      "exploration/num paths total                            505\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.106559\r\n",
      "exploration/Rewards Std                                  0.211285\r\n",
      "exploration/Rewards Max                                  0.1533\r\n",
      "exploration/Rewards Min                                 -0.963324\r\n",
      "exploration/Returns Mean                                -2.13119\r\n",
      "exploration/Returns Std                                  3.08466\r\n",
      "exploration/Returns Max                                  1.3905\r\n",
      "exploration/Returns Min                                 -7.73321\r\n",
      "exploration/Actions Mean                                 0.0245213\r\n",
      "exploration/Actions Std                                  0.170214\r\n",
      "exploration/Actions Max                                  0.60834\r\n",
      "exploration/Actions Min                                 -0.722439\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.13119\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.240022\r\n",
      "exploration/env_infos/final/reward_dist Std              0.362824\r\n",
      "exploration/env_infos/final/reward_dist Max              0.948812\r\n",
      "exploration/env_infos/final/reward_dist Min              4.3996e-59\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00914037\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0139875\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0368588\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.49888e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.235362\r\n",
      "exploration/env_infos/reward_dist Std                    0.279659\r\n",
      "exploration/env_infos/reward_dist Max                    0.979063\r\n",
      "exploration/env_infos/reward_dist Min                    7.70115e-61\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.180492\r\n",
      "exploration/env_infos/final/reward_energy Std            0.119609\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0493929\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.401154\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.47752\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.283066\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0116296\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.791655\r\n",
      "exploration/env_infos/reward_energy Mean                -0.18824\r\n",
      "exploration/env_infos/reward_energy Std                  0.153992\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0110388\r\n",
      "exploration/env_infos/reward_energy Min                 -0.791655\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.205558\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.313639\r\n",
      "exploration/env_infos/final/end_effector_loc Max         1\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.0995369\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00377049\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0192606\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0285209\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0361219\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0861278\r\n",
      "exploration/env_infos/end_effector_loc Std               0.216575\r\n",
      "exploration/env_infos/end_effector_loc Max               1\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.208321\r\n",
      "evaluation/num steps total                           91000\r\n",
      "evaluation/num paths total                            4550\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0375306\r\n",
      "evaluation/Rewards Std                                   0.0779658\r\n",
      "evaluation/Rewards Max                                   0.162847\r\n",
      "evaluation/Rewards Min                                  -0.38362\r\n",
      "evaluation/Returns Mean                                 -0.750612\r\n",
      "evaluation/Returns Std                                   1.24582\r\n",
      "evaluation/Returns Max                                   2.71314\r\n",
      "evaluation/Returns Min                                  -2.97882\r\n",
      "evaluation/Actions Mean                                  0.00120107\r\n",
      "evaluation/Actions Std                                   0.0802581\r\n",
      "evaluation/Actions Max                                   0.66231\r\n",
      "evaluation/Actions Min                                  -0.935465\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.750612\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.247381\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.317342\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.939502\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.18724e-13\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0101378\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0179332\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0896233\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.68309e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.240723\r\n",
      "evaluation/env_infos/reward_dist Std                     0.302109\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996951\r\n",
      "evaluation/env_infos/reward_dist Min                     1.18724e-13\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0598011\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0655014\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0053453\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.301835\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.279602\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.248965\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0106258\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.988779\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0660358\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0923303\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000516442\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.988779\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0069542\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23478\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.57791\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.577618\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000544904\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0132251\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0331155\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0467733\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.000728956\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.156388\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.57791\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.577618\r\n",
      "time/data storing (s)                                    0.0028797\r\n",
      "time/evaluation sampling (s)                             0.94833\r\n",
      "time/exploration sampling (s)                            0.119104\r\n",
      "time/logging (s)                                         0.0196386\r\n",
      "time/saving (s)                                          0.0273992\r\n",
      "time/training (s)                                       48.7729\r\n",
      "time/epoch (s)                                          49.8903\r\n",
      "time/total (s)                                        4544.07\r\n",
      "Epoch                                                   90\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:41:32.818036 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 91 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00123428\n",
      "trainer/QF2 Loss                                         0.00085233\n",
      "trainer/Policy Loss                                      2.77402\n",
      "trainer/Q1 Predictions Mean                             -0.906798\n",
      "trainer/Q1 Predictions Std                               0.890154\n",
      "trainer/Q1 Predictions Max                               1.06924\n",
      "trainer/Q1 Predictions Min                              -3.41769\n",
      "trainer/Q2 Predictions Mean                             -0.903045\n",
      "trainer/Q2 Predictions Std                               0.892602\n",
      "trainer/Q2 Predictions Max                               1.09302\n",
      "trainer/Q2 Predictions Min                              -3.37066\n",
      "trainer/Q Targets Mean                                  -0.90397\n",
      "trainer/Q Targets Std                                    0.89054\n",
      "trainer/Q Targets Max                                    1.09966\n",
      "trainer/Q Targets Min                                   -3.38674\n",
      "trainer/Log Pis Mean                                     1.87763\n",
      "trainer/Log Pis Std                                      1.52987\n",
      "trainer/Log Pis Max                                      9.41667\n",
      "trainer/Log Pis Min                                     -6.19847\n",
      "trainer/Policy mu Mean                                   0.0032631\n",
      "trainer/Policy mu Std                                    0.341327\n",
      "trainer/Policy mu Max                                    2.13775\n",
      "trainer/Policy mu Min                                   -1.54728\n",
      "trainer/Policy log std Mean                             -2.23084\n",
      "trainer/Policy log std Std                               0.649008\n",
      "trainer/Policy log std Max                               1.11785\n",
      "trainer/Policy log std Min                              -3.22023\n",
      "trainer/Alpha                                            0.0232713\n",
      "trainer/Alpha Loss                                      -0.460019\n",
      "exploration/num steps total                          10200\n",
      "exploration/num paths total                            510\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0790683\n",
      "exploration/Rewards Std                                  0.0506442\n",
      "exploration/Rewards Max                                 -0.00658312\n",
      "exploration/Rewards Min                                 -0.261524\n",
      "exploration/Returns Mean                                -1.58137\n",
      "exploration/Returns Std                                  0.478011\n",
      "exploration/Returns Max                                 -0.897225\n",
      "exploration/Returns Min                                 -2.15701\n",
      "exploration/Actions Mean                                -0.00417638\n",
      "exploration/Actions Std                                  0.133492\n",
      "exploration/Actions Max                                  0.45813\n",
      "exploration/Actions Min                                 -0.880313\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.58137\n",
      "exploration/env_infos/final/reward_dist Mean             0.117311\n",
      "exploration/env_infos/final/reward_dist Std              0.187085\n",
      "exploration/env_infos/final/reward_dist Max              0.487298\n",
      "exploration/env_infos/final/reward_dist Min              1.2911e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00467156\n",
      "exploration/env_infos/initial/reward_dist Std            0.00505045\n",
      "exploration/env_infos/initial/reward_dist Max            0.0114577\n",
      "exploration/env_infos/initial/reward_dist Min            1.49405e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0451016\n",
      "exploration/env_infos/reward_dist Std                    0.0803597\n",
      "exploration/env_infos/reward_dist Max                    0.487298\n",
      "exploration/env_infos/reward_dist Min                    1.2911e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.116114\n",
      "exploration/env_infos/final/reward_energy Std            0.0544181\n",
      "exploration/env_infos/final/reward_energy Max           -0.0143198\n",
      "exploration/env_infos/final/reward_energy Min           -0.161671\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.295228\n",
      "exploration/env_infos/initial/reward_energy Std          0.327942\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0518224\n",
      "exploration/env_infos/initial/reward_energy Min         -0.932794\n",
      "exploration/env_infos/reward_energy Mean                -0.134848\n",
      "exploration/env_infos/reward_energy Std                  0.132255\n",
      "exploration/env_infos/reward_energy Max                 -0.008866\n",
      "exploration/env_infos/reward_energy Min                 -0.932794\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.12443\n",
      "exploration/env_infos/final/end_effector_loc Std         0.137382\n",
      "exploration/env_infos/final/end_effector_loc Max         0.11283\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.340223\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00459428\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0149089\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0139562\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0440156\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0770302\n",
      "exploration/env_infos/end_effector_loc Std               0.131847\n",
      "exploration/env_infos/end_effector_loc Max               0.124984\n",
      "exploration/env_infos/end_effector_loc Min              -0.442337\n",
      "evaluation/num steps total                           92000\n",
      "evaluation/num paths total                            4600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0562163\n",
      "evaluation/Rewards Std                                   0.0951759\n",
      "evaluation/Rewards Max                                   0.151251\n",
      "evaluation/Rewards Min                                  -0.563385\n",
      "evaluation/Returns Mean                                 -1.12433\n",
      "evaluation/Returns Std                                   1.51144\n",
      "evaluation/Returns Max                                   1.95285\n",
      "evaluation/Returns Min                                  -4.04166\n",
      "evaluation/Actions Mean                                 -0.00244832\n",
      "evaluation/Actions Std                                   0.103738\n",
      "evaluation/Actions Max                                   0.913266\n",
      "evaluation/Actions Min                                  -0.946562\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.12433\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0929317\n",
      "evaluation/env_infos/final/reward_dist Std               0.204088\n",
      "evaluation/env_infos/final/reward_dist Max               0.990081\n",
      "evaluation/env_infos/final/reward_dist Min               1.09003e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705997\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0140316\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0866816\n",
      "evaluation/env_infos/initial/reward_dist Min             2.25059e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.199076\n",
      "evaluation/env_infos/reward_dist Std                     0.281397\n",
      "evaluation/env_infos/reward_dist Max                     0.990081\n",
      "evaluation/env_infos/reward_dist Min                     1.09003e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.063027\n",
      "evaluation/env_infos/final/reward_energy Std             0.0678059\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00729689\n",
      "evaluation/env_infos/final/reward_energy Min            -0.296924\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.386898\n",
      "evaluation/env_infos/initial/reward_energy Std           0.257223\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00744011\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.07643\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0887136\n",
      "evaluation/env_infos/reward_energy Std                   0.116897\n",
      "evaluation/env_infos/reward_energy Max                  -0.000436344\n",
      "evaluation/env_infos/reward_energy Min                  -1.07643\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.020869\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.278568\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.65079\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.599551\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00065268\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0164131\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0456633\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0473281\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0100973\n",
      "evaluation/env_infos/end_effector_loc Std                0.190104\n",
      "evaluation/env_infos/end_effector_loc Max                0.65079\n",
      "evaluation/env_infos/end_effector_loc Min               -0.599551\n",
      "time/data storing (s)                                    0.0032028\n",
      "time/evaluation sampling (s)                             1.01234\n",
      "time/exploration sampling (s)                            0.126626\n",
      "time/logging (s)                                         0.0217783\n",
      "time/saving (s)                                          0.0292397\n",
      "time/training (s)                                       49.7921\n",
      "time/epoch (s)                                          50.9852\n",
      "time/total (s)                                        4596.2\n",
      "Epoch                                                   91\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:42:24.140497 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 92 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000881785\n",
      "trainer/QF2 Loss                                         0.000763298\n",
      "trainer/Policy Loss                                      3.13001\n",
      "trainer/Q1 Predictions Mean                             -1.04225\n",
      "trainer/Q1 Predictions Std                               0.92153\n",
      "trainer/Q1 Predictions Max                               1.11958\n",
      "trainer/Q1 Predictions Min                              -3.44071\n",
      "trainer/Q2 Predictions Mean                             -1.03701\n",
      "trainer/Q2 Predictions Std                               0.923219\n",
      "trainer/Q2 Predictions Max                               1.11549\n",
      "trainer/Q2 Predictions Min                              -3.45125\n",
      "trainer/Q Targets Mean                                  -1.03499\n",
      "trainer/Q Targets Std                                    0.930849\n",
      "trainer/Q Targets Max                                    1.13908\n",
      "trainer/Q Targets Min                                   -3.41586\n",
      "trainer/Log Pis Mean                                     2.09011\n",
      "trainer/Log Pis Std                                      1.33522\n",
      "trainer/Log Pis Max                                      4.28459\n",
      "trainer/Log Pis Min                                     -2.80409\n",
      "trainer/Policy mu Mean                                   0.0103411\n",
      "trainer/Policy mu Std                                    0.287976\n",
      "trainer/Policy mu Max                                    1.80597\n",
      "trainer/Policy mu Min                                   -1.43025\n",
      "trainer/Policy log std Mean                             -2.35636\n",
      "trainer/Policy log std Std                               0.597603\n",
      "trainer/Policy log std Max                              -0.468701\n",
      "trainer/Policy log std Min                              -3.19407\n",
      "trainer/Alpha                                            0.0229874\n",
      "trainer/Alpha Loss                                       0.34003\n",
      "exploration/num steps total                          10300\n",
      "exploration/num paths total                            515\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.070474\n",
      "exploration/Rewards Std                                  0.0577322\n",
      "exploration/Rewards Max                                  0.0465409\n",
      "exploration/Rewards Min                                 -0.249857\n",
      "exploration/Returns Mean                                -1.40948\n",
      "exploration/Returns Std                                  0.645698\n",
      "exploration/Returns Max                                 -0.88944\n",
      "exploration/Returns Min                                 -2.65808\n",
      "exploration/Actions Mean                                -0.00174995\n",
      "exploration/Actions Std                                  0.0904617\n",
      "exploration/Actions Max                                  0.27762\n",
      "exploration/Actions Min                                 -0.328585\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.40948\n",
      "exploration/env_infos/final/reward_dist Mean             0.272191\n",
      "exploration/env_infos/final/reward_dist Std              0.327954\n",
      "exploration/env_infos/final/reward_dist Max              0.778744\n",
      "exploration/env_infos/final/reward_dist Min              0.00208809\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00869101\n",
      "exploration/env_infos/initial/reward_dist Std            0.0128147\n",
      "exploration/env_infos/initial/reward_dist Max            0.0337604\n",
      "exploration/env_infos/initial/reward_dist Min            5.45648e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.153023\n",
      "exploration/env_infos/reward_dist Std                    0.228222\n",
      "exploration/env_infos/reward_dist Max                    0.946448\n",
      "exploration/env_infos/reward_dist Min                    5.45648e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.174964\n",
      "exploration/env_infos/final/reward_energy Std            0.133364\n",
      "exploration/env_infos/final/reward_energy Max           -0.0463026\n",
      "exploration/env_infos/final/reward_energy Min           -0.428768\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.16898\n",
      "exploration/env_infos/initial/reward_energy Std          0.108237\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0724433\n",
      "exploration/env_infos/initial/reward_energy Min         -0.316997\n",
      "exploration/env_infos/reward_energy Mean                -0.103157\n",
      "exploration/env_infos/reward_energy Std                  0.0757057\n",
      "exploration/env_infos/reward_energy Max                 -0.0190504\n",
      "exploration/env_infos/reward_energy Min                 -0.428768\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00691004\n",
      "exploration/env_infos/final/end_effector_loc Std         0.186741\n",
      "exploration/env_infos/final/end_effector_loc Max         0.256686\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.274634\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00187988\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00684127\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0129462\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00914399\n",
      "exploration/env_infos/end_effector_loc Mean              0.0185676\n",
      "exploration/env_infos/end_effector_loc Std               0.122167\n",
      "exploration/env_infos/end_effector_loc Max               0.303053\n",
      "exploration/env_infos/end_effector_loc Min              -0.274634\n",
      "evaluation/num steps total                           93000\n",
      "evaluation/num paths total                            4650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0471932\n",
      "evaluation/Rewards Std                                   0.0850357\n",
      "evaluation/Rewards Max                                   0.144433\n",
      "evaluation/Rewards Min                                  -0.635974\n",
      "evaluation/Returns Mean                                 -0.943864\n",
      "evaluation/Returns Std                                   1.21566\n",
      "evaluation/Returns Max                                   1.71129\n",
      "evaluation/Returns Min                                  -4.3987\n",
      "evaluation/Actions Mean                                 -0.00117127\n",
      "evaluation/Actions Std                                   0.0931722\n",
      "evaluation/Actions Max                                   0.833919\n",
      "evaluation/Actions Min                                  -0.872828\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.943864\n",
      "evaluation/env_infos/final/reward_dist Mean              0.213278\n",
      "evaluation/env_infos/final/reward_dist Std               0.303675\n",
      "evaluation/env_infos/final/reward_dist Max               0.969478\n",
      "evaluation/env_infos/final/reward_dist Min               9.06188e-28\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0141376\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0292777\n",
      "evaluation/env_infos/initial/reward_dist Max             0.14953\n",
      "evaluation/env_infos/initial/reward_dist Min             2.87403e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.20467\n",
      "evaluation/env_infos/reward_dist Std                     0.283211\n",
      "evaluation/env_infos/reward_dist Max                     0.996765\n",
      "evaluation/env_infos/reward_dist Min                     9.06188e-28\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0422569\n",
      "evaluation/env_infos/final/reward_energy Std             0.0367209\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000947462\n",
      "evaluation/env_infos/final/reward_energy Min            -0.149969\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.324137\n",
      "evaluation/env_infos/initial/reward_energy Std           0.301542\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00288932\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.20717\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0737012\n",
      "evaluation/env_infos/reward_energy Std                   0.109238\n",
      "evaluation/env_infos/reward_energy Max                  -0.000906137\n",
      "evaluation/env_infos/reward_energy Min                  -1.20717\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00574169\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.26625\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.692232\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.771108\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000364004\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0156479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.041696\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0436414\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00187669\n",
      "evaluation/env_infos/end_effector_loc Std                0.179506\n",
      "evaluation/env_infos/end_effector_loc Max                0.692232\n",
      "evaluation/env_infos/end_effector_loc Min               -0.771108\n",
      "time/data storing (s)                                    0.00292084\n",
      "time/evaluation sampling (s)                             0.990382\n",
      "time/exploration sampling (s)                            0.124531\n",
      "time/logging (s)                                         0.0194273\n",
      "time/saving (s)                                          0.0281375\n",
      "time/training (s)                                       48.9824\n",
      "time/epoch (s)                                          50.1478\n",
      "time/total (s)                                        4647.51\n",
      "Epoch                                                   92\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:43:15.761360 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 93 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000552864\n",
      "trainer/QF2 Loss                                         0.000750813\n",
      "trainer/Policy Loss                                      3.05719\n",
      "trainer/Q1 Predictions Mean                             -1.03226\n",
      "trainer/Q1 Predictions Std                               0.927364\n",
      "trainer/Q1 Predictions Max                               1.24941\n",
      "trainer/Q1 Predictions Min                              -3.33057\n",
      "trainer/Q2 Predictions Mean                             -1.03732\n",
      "trainer/Q2 Predictions Std                               0.92654\n",
      "trainer/Q2 Predictions Max                               1.2585\n",
      "trainer/Q2 Predictions Min                              -3.32075\n",
      "trainer/Q Targets Mean                                  -1.02803\n",
      "trainer/Q Targets Std                                    0.927139\n",
      "trainer/Q Targets Max                                    1.27629\n",
      "trainer/Q Targets Min                                   -3.34463\n",
      "trainer/Log Pis Mean                                     2.03596\n",
      "trainer/Log Pis Std                                      1.29991\n",
      "trainer/Log Pis Max                                      4.26897\n",
      "trainer/Log Pis Min                                     -3.0998\n",
      "trainer/Policy mu Mean                                  -0.00950598\n",
      "trainer/Policy mu Std                                    0.351936\n",
      "trainer/Policy mu Max                                    2.28394\n",
      "trainer/Policy mu Min                                   -1.84048\n",
      "trainer/Policy log std Mean                             -2.29362\n",
      "trainer/Policy log std Std                               0.646054\n",
      "trainer/Policy log std Max                              -0.123959\n",
      "trainer/Policy log std Min                              -3.19659\n",
      "trainer/Alpha                                            0.0244861\n",
      "trainer/Alpha Loss                                       0.133436\n",
      "exploration/num steps total                          10400\n",
      "exploration/num paths total                            520\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0474226\n",
      "exploration/Rewards Std                                  0.0751239\n",
      "exploration/Rewards Max                                  0.0642233\n",
      "exploration/Rewards Min                                 -0.232944\n",
      "exploration/Returns Mean                                -0.948451\n",
      "exploration/Returns Std                                  1.09127\n",
      "exploration/Returns Max                                  0.162239\n",
      "exploration/Returns Min                                 -2.98206\n",
      "exploration/Actions Mean                                -0.000756202\n",
      "exploration/Actions Std                                  0.143882\n",
      "exploration/Actions Max                                  0.557134\n",
      "exploration/Actions Min                                 -0.768123\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.948451\n",
      "exploration/env_infos/final/reward_dist Mean             0.279471\n",
      "exploration/env_infos/final/reward_dist Std              0.198886\n",
      "exploration/env_infos/final/reward_dist Max              0.579468\n",
      "exploration/env_infos/final/reward_dist Min              1.35962e-09\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00176598\n",
      "exploration/env_infos/initial/reward_dist Std            0.0029237\n",
      "exploration/env_infos/initial/reward_dist Max            0.00758152\n",
      "exploration/env_infos/initial/reward_dist Min            6.02475e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.313747\n",
      "exploration/env_infos/reward_dist Std                    0.331051\n",
      "exploration/env_infos/reward_dist Max                    0.994138\n",
      "exploration/env_infos/reward_dist Min                    1.35962e-09\n",
      "exploration/env_infos/final/reward_energy Mean          -0.186314\n",
      "exploration/env_infos/final/reward_energy Std            0.107127\n",
      "exploration/env_infos/final/reward_energy Max           -0.0519045\n",
      "exploration/env_infos/final/reward_energy Min           -0.378223\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.273843\n",
      "exploration/env_infos/initial/reward_energy Std          0.272559\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0638855\n",
      "exploration/env_infos/initial/reward_energy Min         -0.77913\n",
      "exploration/env_infos/reward_energy Mean                -0.154393\n",
      "exploration/env_infos/reward_energy Std                  0.132545\n",
      "exploration/env_infos/reward_energy Max                 -0.00433585\n",
      "exploration/env_infos/reward_energy Min                 -0.77913\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0196306\n",
      "exploration/env_infos/final/end_effector_loc Std         0.209946\n",
      "exploration/env_infos/final/end_effector_loc Max         0.279164\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.281417\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0018457\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0135348\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0160664\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0384062\n",
      "exploration/env_infos/end_effector_loc Mean              0.0166589\n",
      "exploration/env_infos/end_effector_loc Std               0.140372\n",
      "exploration/env_infos/end_effector_loc Max               0.296906\n",
      "exploration/env_infos/end_effector_loc Min              -0.281417\n",
      "evaluation/num steps total                           94000\n",
      "evaluation/num paths total                            4700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.041777\n",
      "evaluation/Rewards Std                                   0.0771634\n",
      "evaluation/Rewards Max                                   0.142204\n",
      "evaluation/Rewards Min                                  -0.439577\n",
      "evaluation/Returns Mean                                 -0.83554\n",
      "evaluation/Returns Std                                   1.14921\n",
      "evaluation/Returns Max                                   1.20556\n",
      "evaluation/Returns Min                                  -3.84012\n",
      "evaluation/Actions Mean                                  0.000223204\n",
      "evaluation/Actions Std                                   0.0739557\n",
      "evaluation/Actions Max                                   0.662821\n",
      "evaluation/Actions Min                                  -0.750245\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.83554\n",
      "evaluation/env_infos/final/reward_dist Mean              0.243994\n",
      "evaluation/env_infos/final/reward_dist Std               0.300124\n",
      "evaluation/env_infos/final/reward_dist Max               0.948336\n",
      "evaluation/env_infos/final/reward_dist Min               6.78386e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00569732\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0100974\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0407956\n",
      "evaluation/env_infos/initial/reward_dist Min             1.26062e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.197057\n",
      "evaluation/env_infos/reward_dist Std                     0.285154\n",
      "evaluation/env_infos/reward_dist Max                     0.993238\n",
      "evaluation/env_infos/reward_dist Min                     6.78386e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0403232\n",
      "evaluation/env_infos/final/reward_energy Std             0.03679\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00255966\n",
      "evaluation/env_infos/final/reward_energy Min            -0.172251\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.21221\n",
      "evaluation/env_infos/initial/reward_energy Std           0.214899\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.014303\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.75515\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0571961\n",
      "evaluation/env_infos/reward_energy Std                   0.0875648\n",
      "evaluation/env_infos/reward_energy Max                  -0.000656482\n",
      "evaluation/env_infos/reward_energy Min                  -0.812033\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0063101\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.243653\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.700578\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.44369\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000201977\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010676\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0301827\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0375122\n",
      "evaluation/env_infos/end_effector_loc Mean               0.000471489\n",
      "evaluation/env_infos/end_effector_loc Std                0.154048\n",
      "evaluation/env_infos/end_effector_loc Max                0.700578\n",
      "evaluation/env_infos/end_effector_loc Min               -0.44369\n",
      "time/data storing (s)                                    0.00327324\n",
      "time/evaluation sampling (s)                             0.944376\n",
      "time/exploration sampling (s)                            0.133575\n",
      "time/logging (s)                                         0.0201781\n",
      "time/saving (s)                                          0.0274239\n",
      "time/training (s)                                       49.2946\n",
      "time/epoch (s)                                          50.4234\n",
      "time/total (s)                                        4699.13\n",
      "Epoch                                                   93\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:44:07.471348 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 94 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000688004\n",
      "trainer/QF2 Loss                                         0.000717312\n",
      "trainer/Policy Loss                                      3.06034\n",
      "trainer/Q1 Predictions Mean                             -0.982229\n",
      "trainer/Q1 Predictions Std                               0.874711\n",
      "trainer/Q1 Predictions Max                               1.2721\n",
      "trainer/Q1 Predictions Min                              -3.23544\n",
      "trainer/Q2 Predictions Mean                             -0.973025\n",
      "trainer/Q2 Predictions Std                               0.871957\n",
      "trainer/Q2 Predictions Max                               1.28832\n",
      "trainer/Q2 Predictions Min                              -3.29083\n",
      "trainer/Q Targets Mean                                  -0.974871\n",
      "trainer/Q Targets Std                                    0.872212\n",
      "trainer/Q Targets Max                                    1.26761\n",
      "trainer/Q Targets Min                                   -3.25212\n",
      "trainer/Log Pis Mean                                     2.08728\n",
      "trainer/Log Pis Std                                      1.50328\n",
      "trainer/Log Pis Max                                      4.72616\n",
      "trainer/Log Pis Min                                     -4.13195\n",
      "trainer/Policy mu Mean                                   0.0239979\n",
      "trainer/Policy mu Std                                    0.342394\n",
      "trainer/Policy mu Max                                    2.52072\n",
      "trainer/Policy mu Min                                   -2.29813\n",
      "trainer/Policy log std Mean                             -2.39093\n",
      "trainer/Policy log std Std                               0.687371\n",
      "trainer/Policy log std Max                              -0.0266174\n",
      "trainer/Policy log std Min                              -3.29567\n",
      "trainer/Alpha                                            0.0238707\n",
      "trainer/Alpha Loss                                       0.325977\n",
      "exploration/num steps total                          10500\n",
      "exploration/num paths total                            525\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0745564\n",
      "exploration/Rewards Std                                  0.0731778\n",
      "exploration/Rewards Max                                  0.0799721\n",
      "exploration/Rewards Min                                 -0.332349\n",
      "exploration/Returns Mean                                -1.49113\n",
      "exploration/Returns Std                                  0.672955\n",
      "exploration/Returns Max                                 -0.819372\n",
      "exploration/Returns Min                                 -2.76734\n",
      "exploration/Actions Mean                                -0.00314385\n",
      "exploration/Actions Std                                  0.166792\n",
      "exploration/Actions Max                                  0.669795\n",
      "exploration/Actions Min                                 -0.663214\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.49113\n",
      "exploration/env_infos/final/reward_dist Mean             0.118917\n",
      "exploration/env_infos/final/reward_dist Std              0.205142\n",
      "exploration/env_infos/final/reward_dist Max              0.527486\n",
      "exploration/env_infos/final/reward_dist Min              3.49265e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00799545\n",
      "exploration/env_infos/initial/reward_dist Std            0.00992791\n",
      "exploration/env_infos/initial/reward_dist Max            0.0273758\n",
      "exploration/env_infos/initial/reward_dist Min            0.000600852\n",
      "exploration/env_infos/reward_dist Mean                   0.269861\n",
      "exploration/env_infos/reward_dist Std                    0.304526\n",
      "exploration/env_infos/reward_dist Max                    0.999244\n",
      "exploration/env_infos/reward_dist Min                    3.49265e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.110215\n",
      "exploration/env_infos/final/reward_energy Std            0.0768455\n",
      "exploration/env_infos/final/reward_energy Max           -0.0276589\n",
      "exploration/env_infos/final/reward_energy Min           -0.248298\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.382988\n",
      "exploration/env_infos/initial/reward_energy Std          0.269855\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0916288\n",
      "exploration/env_infos/initial/reward_energy Min         -0.721265\n",
      "exploration/env_infos/reward_energy Mean                -0.162365\n",
      "exploration/env_infos/reward_energy Std                  0.171163\n",
      "exploration/env_infos/reward_energy Max                 -0.0109869\n",
      "exploration/env_infos/reward_energy Min                 -0.851149\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0351003\n",
      "exploration/env_infos/final/end_effector_loc Std         0.252962\n",
      "exploration/env_infos/final/end_effector_loc Max         0.411933\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.369028\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000762092\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0165468\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0334898\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0331607\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0133455\n",
      "exploration/env_infos/end_effector_loc Std               0.168502\n",
      "exploration/env_infos/end_effector_loc Max               0.411933\n",
      "exploration/env_infos/end_effector_loc Min              -0.369028\n",
      "evaluation/num steps total                           95000\n",
      "evaluation/num paths total                            4750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0551627\n",
      "evaluation/Rewards Std                                   0.0801043\n",
      "evaluation/Rewards Max                                   0.151601\n",
      "evaluation/Rewards Min                                  -0.481389\n",
      "evaluation/Returns Mean                                 -1.10325\n",
      "evaluation/Returns Std                                   1.14044\n",
      "evaluation/Returns Max                                   1.56293\n",
      "evaluation/Returns Min                                  -3.84451\n",
      "evaluation/Actions Mean                                  0.00349118\n",
      "evaluation/Actions Std                                   0.0815461\n",
      "evaluation/Actions Max                                   0.59823\n",
      "evaluation/Actions Min                                  -0.820185\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.10325\n",
      "evaluation/env_infos/final/reward_dist Mean              0.136655\n",
      "evaluation/env_infos/final/reward_dist Std               0.241591\n",
      "evaluation/env_infos/final/reward_dist Max               0.787572\n",
      "evaluation/env_infos/final/reward_dist Min               6.50086e-66\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00696124\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0129431\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0651122\n",
      "evaluation/env_infos/initial/reward_dist Min             2.6294e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.170711\n",
      "evaluation/env_infos/reward_dist Std                     0.271852\n",
      "evaluation/env_infos/reward_dist Max                     0.999358\n",
      "evaluation/env_infos/reward_dist Min                     6.50086e-66\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0459941\n",
      "evaluation/env_infos/final/reward_energy Std             0.0810337\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00257424\n",
      "evaluation/env_infos/final/reward_energy Min            -0.572593\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.255375\n",
      "evaluation/env_infos/initial/reward_energy Std           0.241591\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.000679267\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.01518\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0648959\n",
      "evaluation/env_infos/reward_energy Std                   0.095459\n",
      "evaluation/env_infos/reward_energy Max                  -0.00066192\n",
      "evaluation/env_infos/reward_energy Min                  -1.01518\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0190783\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.280253\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.558225\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00185366\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122899\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0299115\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0410092\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00182413\n",
      "evaluation/env_infos/end_effector_loc Std                0.175094\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.558225\n",
      "time/data storing (s)                                    0.00556827\n",
      "time/evaluation sampling (s)                             0.954993\n",
      "time/exploration sampling (s)                            0.131897\n",
      "time/logging (s)                                         0.0202891\n",
      "time/saving (s)                                          0.0273594\n",
      "time/training (s)                                       49.4755\n",
      "time/epoch (s)                                          50.6157\n",
      "time/total (s)                                        4750.84\n",
      "Epoch                                                   94\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:44:59.746889 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 95 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000758044\r\n",
      "trainer/QF2 Loss                                         0.00111567\r\n",
      "trainer/Policy Loss                                      2.8698\r\n",
      "trainer/Q1 Predictions Mean                             -0.940152\r\n",
      "trainer/Q1 Predictions Std                               0.885712\r\n",
      "trainer/Q1 Predictions Max                               1.19003\r\n",
      "trainer/Q1 Predictions Min                              -3.26018\r\n",
      "trainer/Q2 Predictions Mean                             -0.937716\r\n",
      "trainer/Q2 Predictions Std                               0.89208\r\n",
      "trainer/Q2 Predictions Max                               1.20477\r\n",
      "trainer/Q2 Predictions Min                              -3.26447\r\n",
      "trainer/Q Targets Mean                                  -0.943889\r\n",
      "trainer/Q Targets Std                                    0.892841\r\n",
      "trainer/Q Targets Max                                    1.17051\r\n",
      "trainer/Q Targets Min                                   -3.26833\r\n",
      "trainer/Log Pis Mean                                     1.93788\r\n",
      "trainer/Log Pis Std                                      1.44957\r\n",
      "trainer/Log Pis Max                                      4.32512\r\n",
      "trainer/Log Pis Min                                     -4.86589\r\n",
      "trainer/Policy mu Mean                                   0.00882974\r\n",
      "trainer/Policy mu Std                                    0.324555\r\n",
      "trainer/Policy mu Max                                    2.94644\r\n",
      "trainer/Policy mu Min                                   -1.5805\r\n",
      "trainer/Policy log std Mean                             -2.29702\r\n",
      "trainer/Policy log std Std                               0.636705\r\n",
      "trainer/Policy log std Max                               0.21114\r\n",
      "trainer/Policy log std Min                              -3.34425\r\n",
      "trainer/Alpha                                            0.0245923\r\n",
      "trainer/Alpha Loss                                      -0.230166\r\n",
      "exploration/num steps total                          10600\r\n",
      "exploration/num paths total                            530\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0627204\r\n",
      "exploration/Rewards Std                                  0.105086\r\n",
      "exploration/Rewards Max                                  0.13097\r\n",
      "exploration/Rewards Min                                 -0.345744\r\n",
      "exploration/Returns Mean                                -1.25441\r\n",
      "exploration/Returns Std                                  1.6589\r\n",
      "exploration/Returns Max                                  1.1709\r\n",
      "exploration/Returns Min                                 -3.80937\r\n",
      "exploration/Actions Mean                                -0.00101617\r\n",
      "exploration/Actions Std                                  0.212225\r\n",
      "exploration/Actions Max                                  0.834595\r\n",
      "exploration/Actions Min                                 -0.786222\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.25441\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.267103\r\n",
      "exploration/env_infos/final/reward_dist Std              0.248473\r\n",
      "exploration/env_infos/final/reward_dist Max              0.721079\r\n",
      "exploration/env_infos/final/reward_dist Min              0.000264099\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0123459\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0112297\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0291787\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.1233e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.224949\r\n",
      "exploration/env_infos/reward_dist Std                    0.277695\r\n",
      "exploration/env_infos/reward_dist Max                    0.994677\r\n",
      "exploration/env_infos/reward_dist Min                    2.1233e-05\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.17428\r\n",
      "exploration/env_infos/final/reward_energy Std            0.107365\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0334332\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.335858\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.380144\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.15034\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.121184\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.537139\r\n",
      "exploration/env_infos/reward_energy Mean                -0.235651\r\n",
      "exploration/env_infos/reward_energy Std                  0.185875\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0220533\r\n",
      "exploration/env_infos/reward_energy Min                 -0.85963\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0543693\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.228366\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.453216\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.31111\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00406145\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0138706\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0217428\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0130313\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0610448\r\n",
      "exploration/env_infos/end_effector_loc Std               0.170153\r\n",
      "exploration/env_infos/end_effector_loc Max               0.477645\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.31111\r\n",
      "evaluation/num steps total                           96000\r\n",
      "evaluation/num paths total                            4800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0584013\r\n",
      "evaluation/Rewards Std                                   0.0963796\r\n",
      "evaluation/Rewards Max                                   0.185632\r\n",
      "evaluation/Rewards Min                                  -1.05927\r\n",
      "evaluation/Returns Mean                                 -1.16803\r\n",
      "evaluation/Returns Std                                   1.39482\r\n",
      "evaluation/Returns Max                                   2.27492\r\n",
      "evaluation/Returns Min                                  -5.34732\r\n",
      "evaluation/Actions Mean                                  0.00361708\r\n",
      "evaluation/Actions Std                                   0.0877271\r\n",
      "evaluation/Actions Max                                   0.827899\r\n",
      "evaluation/Actions Min                                  -0.77107\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.16803\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.121693\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.200953\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.809957\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.32103e-76\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00487069\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00696911\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0228596\r\n",
      "evaluation/env_infos/initial/reward_dist Min             3.31288e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.190191\r\n",
      "evaluation/env_infos/reward_dist Std                     0.27434\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997352\r\n",
      "evaluation/env_infos/reward_dist Min                     1.32103e-76\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0513059\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.114378\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00146481\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.814317\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.263047\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.231751\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0151685\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.977458\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.06431\r\n",
      "evaluation/env_infos/reward_energy Std                   0.106219\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00146481\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.16607\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0224897\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.305299\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.889316\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000159913\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123936\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0312294\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0385535\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00742768\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.182973\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.889316\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00287587\r\n",
      "time/evaluation sampling (s)                             1.28538\r\n",
      "time/exploration sampling (s)                            0.127847\r\n",
      "time/logging (s)                                         0.0208127\r\n",
      "time/saving (s)                                          0.0269602\r\n",
      "time/training (s)                                       49.4971\r\n",
      "time/epoch (s)                                          50.961\r\n",
      "time/total (s)                                        4803.11\r\n",
      "Epoch                                                   95\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:45:52.006662 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 96 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000856302\n",
      "trainer/QF2 Loss                                         0.00066757\n",
      "trainer/Policy Loss                                      3.04197\n",
      "trainer/Q1 Predictions Mean                             -0.914488\n",
      "trainer/Q1 Predictions Std                               0.89333\n",
      "trainer/Q1 Predictions Max                               1.21323\n",
      "trainer/Q1 Predictions Min                              -3.05841\n",
      "trainer/Q2 Predictions Mean                             -0.922874\n",
      "trainer/Q2 Predictions Std                               0.894385\n",
      "trainer/Q2 Predictions Max                               1.17264\n",
      "trainer/Q2 Predictions Min                              -3.08942\n",
      "trainer/Q Targets Mean                                  -0.92274\n",
      "trainer/Q Targets Std                                    0.896049\n",
      "trainer/Q Targets Max                                    1.18823\n",
      "trainer/Q Targets Min                                   -3.14664\n",
      "trainer/Log Pis Mean                                     2.1362\n",
      "trainer/Log Pis Std                                      1.43315\n",
      "trainer/Log Pis Max                                      4.63741\n",
      "trainer/Log Pis Min                                     -4.19714\n",
      "trainer/Policy mu Mean                                   0.000297535\n",
      "trainer/Policy mu Std                                    0.280818\n",
      "trainer/Policy mu Max                                    2.38247\n",
      "trainer/Policy mu Min                                   -1.83082\n",
      "trainer/Policy log std Mean                             -2.38514\n",
      "trainer/Policy log std Std                               0.584461\n",
      "trainer/Policy log std Max                              -0.67978\n",
      "trainer/Policy log std Min                              -3.30892\n",
      "trainer/Alpha                                            0.0237794\n",
      "trainer/Alpha Loss                                       0.50932\n",
      "exploration/num steps total                          10700\n",
      "exploration/num paths total                            535\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0418833\n",
      "exploration/Rewards Std                                  0.0852786\n",
      "exploration/Rewards Max                                  0.0890087\n",
      "exploration/Rewards Min                                 -0.379224\n",
      "exploration/Returns Mean                                -0.837666\n",
      "exploration/Returns Std                                  1.36166\n",
      "exploration/Returns Max                                  0.510004\n",
      "exploration/Returns Min                                 -2.82186\n",
      "exploration/Actions Mean                                 0.00518576\n",
      "exploration/Actions Std                                  0.120675\n",
      "exploration/Actions Max                                  0.456777\n",
      "exploration/Actions Min                                 -0.609062\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.837666\n",
      "exploration/env_infos/final/reward_dist Mean             0.207554\n",
      "exploration/env_infos/final/reward_dist Std              0.320219\n",
      "exploration/env_infos/final/reward_dist Max              0.829304\n",
      "exploration/env_infos/final/reward_dist Min              0.000199841\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0133204\n",
      "exploration/env_infos/initial/reward_dist Std            0.0165284\n",
      "exploration/env_infos/initial/reward_dist Max            0.0458353\n",
      "exploration/env_infos/initial/reward_dist Min            0.0029938\n",
      "exploration/env_infos/reward_dist Mean                   0.213549\n",
      "exploration/env_infos/reward_dist Std                    0.269068\n",
      "exploration/env_infos/reward_dist Max                    0.959488\n",
      "exploration/env_infos/reward_dist Min                    0.000199841\n",
      "exploration/env_infos/final/reward_energy Mean          -0.141519\n",
      "exploration/env_infos/final/reward_energy Std            0.108185\n",
      "exploration/env_infos/final/reward_energy Max           -0.0165354\n",
      "exploration/env_infos/final/reward_energy Min           -0.323003\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.35154\n",
      "exploration/env_infos/initial/reward_energy Std          0.177474\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0829441\n",
      "exploration/env_infos/initial/reward_energy Min         -0.61416\n",
      "exploration/env_infos/reward_energy Mean                -0.131261\n",
      "exploration/env_infos/reward_energy Std                  0.109312\n",
      "exploration/env_infos/reward_energy Max                 -0.00862862\n",
      "exploration/env_infos/reward_energy Min                 -0.61416\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0545678\n",
      "exploration/env_infos/final/end_effector_loc Std         0.146088\n",
      "exploration/env_infos/final/end_effector_loc Max         0.263877\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.152186\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00190476\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.013792\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0228388\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0304531\n",
      "exploration/env_infos/end_effector_loc Mean              0.0201514\n",
      "exploration/env_infos/end_effector_loc Std               0.124326\n",
      "exploration/env_infos/end_effector_loc Max               0.263877\n",
      "exploration/env_infos/end_effector_loc Min              -0.24456\n",
      "evaluation/num steps total                           97000\n",
      "evaluation/num paths total                            4850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0304878\n",
      "evaluation/Rewards Std                                   0.073711\n",
      "evaluation/Rewards Max                                   0.158272\n",
      "evaluation/Rewards Min                                  -0.30184\n",
      "evaluation/Returns Mean                                 -0.609756\n",
      "evaluation/Returns Std                                   1.1213\n",
      "evaluation/Returns Max                                   2.30432\n",
      "evaluation/Returns Min                                  -2.77697\n",
      "evaluation/Actions Mean                                  0.00240792\n",
      "evaluation/Actions Std                                   0.0757135\n",
      "evaluation/Actions Max                                   0.569313\n",
      "evaluation/Actions Min                                  -0.833519\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.609756\n",
      "evaluation/env_infos/final/reward_dist Mean              0.141798\n",
      "evaluation/env_infos/final/reward_dist Std               0.234228\n",
      "evaluation/env_infos/final/reward_dist Max               0.86428\n",
      "evaluation/env_infos/final/reward_dist Min               4.64787e-34\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0122134\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0214538\n",
      "evaluation/env_infos/initial/reward_dist Max             0.112471\n",
      "evaluation/env_infos/initial/reward_dist Min             2.29793e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.197493\n",
      "evaluation/env_infos/reward_dist Std                     0.267954\n",
      "evaluation/env_infos/reward_dist Max                     0.997187\n",
      "evaluation/env_infos/reward_dist Min                     4.64787e-34\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.037745\n",
      "evaluation/env_infos/final/reward_energy Std             0.0335232\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00965169\n",
      "evaluation/env_infos/final/reward_energy Min            -0.190099\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.266814\n",
      "evaluation/env_infos/initial/reward_energy Std           0.211506\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.013866\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.944848\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0633629\n",
      "evaluation/env_infos/reward_energy Std                   0.0863817\n",
      "evaluation/env_infos/reward_energy Max                  -0.000306703\n",
      "evaluation/env_infos/reward_energy Min                  -0.944848\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0334023\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.25405\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.680096\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.740994\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000306194\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0120338\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0284656\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0416759\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0140236\n",
      "evaluation/env_infos/end_effector_loc Std                0.164969\n",
      "evaluation/env_infos/end_effector_loc Max                0.680096\n",
      "evaluation/env_infos/end_effector_loc Min               -0.740994\n",
      "time/data storing (s)                                    0.00291565\n",
      "time/evaluation sampling (s)                             0.987612\n",
      "time/exploration sampling (s)                            0.12452\n",
      "time/logging (s)                                         0.0183376\n",
      "time/saving (s)                                          0.02668\n",
      "time/training (s)                                       49.8597\n",
      "time/epoch (s)                                          51.0197\n",
      "time/total (s)                                        4855.37\n",
      "Epoch                                                   96\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:46:43.865828 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 97 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000648741\n",
      "trainer/QF2 Loss                                         0.000595328\n",
      "trainer/Policy Loss                                      2.7864\n",
      "trainer/Q1 Predictions Mean                             -0.879615\n",
      "trainer/Q1 Predictions Std                               0.879684\n",
      "trainer/Q1 Predictions Max                               1.37787\n",
      "trainer/Q1 Predictions Min                              -3.29309\n",
      "trainer/Q2 Predictions Mean                             -0.88585\n",
      "trainer/Q2 Predictions Std                               0.873978\n",
      "trainer/Q2 Predictions Max                               1.36005\n",
      "trainer/Q2 Predictions Min                              -3.19472\n",
      "trainer/Q Targets Mean                                  -0.88556\n",
      "trainer/Q Targets Std                                    0.87643\n",
      "trainer/Q Targets Max                                    1.3235\n",
      "trainer/Q Targets Min                                   -3.1628\n",
      "trainer/Log Pis Mean                                     1.91316\n",
      "trainer/Log Pis Std                                      1.41629\n",
      "trainer/Log Pis Max                                      4.52847\n",
      "trainer/Log Pis Min                                     -2.21281\n",
      "trainer/Policy mu Mean                                   0.00313378\n",
      "trainer/Policy mu Std                                    0.326168\n",
      "trainer/Policy mu Max                                    1.96702\n",
      "trainer/Policy mu Min                                   -2.16815\n",
      "trainer/Policy log std Mean                             -2.28222\n",
      "trainer/Policy log std Std                               0.624367\n",
      "trainer/Policy log std Max                              -0.279602\n",
      "trainer/Policy log std Min                              -3.30772\n",
      "trainer/Alpha                                            0.0231315\n",
      "trainer/Alpha Loss                                      -0.327062\n",
      "exploration/num steps total                          10800\n",
      "exploration/num paths total                            540\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0781071\n",
      "exploration/Rewards Std                                  0.0709478\n",
      "exploration/Rewards Max                                  0.100983\n",
      "exploration/Rewards Min                                 -0.284782\n",
      "exploration/Returns Mean                                -1.56214\n",
      "exploration/Returns Std                                  0.550719\n",
      "exploration/Returns Max                                 -0.734329\n",
      "exploration/Returns Min                                 -2.39623\n",
      "exploration/Actions Mean                                 0.0132154\n",
      "exploration/Actions Std                                  0.137956\n",
      "exploration/Actions Max                                  0.471749\n",
      "exploration/Actions Min                                 -0.605127\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.56214\n",
      "exploration/env_infos/final/reward_dist Mean             0.00683041\n",
      "exploration/env_infos/final/reward_dist Std              0.0125174\n",
      "exploration/env_infos/final/reward_dist Max              0.031852\n",
      "exploration/env_infos/final/reward_dist Min              1.56785e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00742935\n",
      "exploration/env_infos/initial/reward_dist Std            0.00694507\n",
      "exploration/env_infos/initial/reward_dist Max            0.0185226\n",
      "exploration/env_infos/initial/reward_dist Min            5.02147e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.133221\n",
      "exploration/env_infos/reward_dist Std                    0.168488\n",
      "exploration/env_infos/reward_dist Max                    0.585569\n",
      "exploration/env_infos/reward_dist Min                    1.56785e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.149151\n",
      "exploration/env_infos/final/reward_energy Std            0.0601879\n",
      "exploration/env_infos/final/reward_energy Max           -0.0913021\n",
      "exploration/env_infos/final/reward_energy Min           -0.245859\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.355656\n",
      "exploration/env_infos/initial/reward_energy Std          0.142056\n",
      "exploration/env_infos/initial/reward_energy Max         -0.144299\n",
      "exploration/env_infos/initial/reward_energy Min         -0.500478\n",
      "exploration/env_infos/reward_energy Mean                -0.153926\n",
      "exploration/env_infos/reward_energy Std                  0.121324\n",
      "exploration/env_infos/reward_energy Max                 -0.00888746\n",
      "exploration/env_infos/reward_energy Min                 -0.62065\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.155983\n",
      "exploration/env_infos/final/end_effector_loc Std         0.22867\n",
      "exploration/env_infos/final/end_effector_loc Max         0.372841\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.39571\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00550856\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0123691\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0235874\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0161442\n",
      "exploration/env_infos/end_effector_loc Mean              0.0653447\n",
      "exploration/env_infos/end_effector_loc Std               0.160035\n",
      "exploration/env_infos/end_effector_loc Max               0.372841\n",
      "exploration/env_infos/end_effector_loc Min              -0.396622\n",
      "evaluation/num steps total                           98000\n",
      "evaluation/num paths total                            4900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0579093\n",
      "evaluation/Rewards Std                                   0.0724965\n",
      "evaluation/Rewards Max                                   0.149408\n",
      "evaluation/Rewards Min                                  -0.38742\n",
      "evaluation/Returns Mean                                 -1.15819\n",
      "evaluation/Returns Std                                   1.09349\n",
      "evaluation/Returns Max                                   1.4309\n",
      "evaluation/Returns Min                                  -4.1537\n",
      "evaluation/Actions Mean                                  0.00200762\n",
      "evaluation/Actions Std                                   0.0787153\n",
      "evaluation/Actions Max                                   0.584002\n",
      "evaluation/Actions Min                                  -0.696077\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.15819\n",
      "evaluation/env_infos/final/reward_dist Mean              0.105575\n",
      "evaluation/env_infos/final/reward_dist Std               0.221256\n",
      "evaluation/env_infos/final/reward_dist Max               0.891359\n",
      "evaluation/env_infos/final/reward_dist Min               2.12517e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00739259\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0123257\n",
      "evaluation/env_infos/initial/reward_dist Max             0.048806\n",
      "evaluation/env_infos/initial/reward_dist Min             1.20632e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.191279\n",
      "evaluation/env_infos/reward_dist Std                     0.278368\n",
      "evaluation/env_infos/reward_dist Max                     0.995895\n",
      "evaluation/env_infos/reward_dist Min                     2.12517e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0471072\n",
      "evaluation/env_infos/final/reward_energy Std             0.0496696\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00468534\n",
      "evaluation/env_infos/final/reward_energy Min            -0.308491\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.273471\n",
      "evaluation/env_infos/initial/reward_energy Std           0.208606\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0277949\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.908615\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0689311\n",
      "evaluation/env_infos/reward_energy Std                   0.0874572\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178582\n",
      "evaluation/env_infos/reward_energy Min                  -0.908615\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0222133\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.287276\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.724901\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.693056\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000456067\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.012152\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0292001\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0348038\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00675316\n",
      "evaluation/env_infos/end_effector_loc Std                0.187713\n",
      "evaluation/env_infos/end_effector_loc Max                0.724901\n",
      "evaluation/env_infos/end_effector_loc Min               -0.693056\n",
      "time/data storing (s)                                    0.00314444\n",
      "time/evaluation sampling (s)                             0.99411\n",
      "time/exploration sampling (s)                            0.118527\n",
      "time/logging (s)                                         0.0188777\n",
      "time/saving (s)                                          0.0269748\n",
      "time/training (s)                                       49.4968\n",
      "time/epoch (s)                                          50.6584\n",
      "time/total (s)                                        4907.22\n",
      "Epoch                                                   97\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:47:37.424992 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 98 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000979636\r\n",
      "trainer/QF2 Loss                                         0.00145386\r\n",
      "trainer/Policy Loss                                      2.72805\r\n",
      "trainer/Q1 Predictions Mean                             -0.879703\r\n",
      "trainer/Q1 Predictions Std                               0.897893\r\n",
      "trainer/Q1 Predictions Max                               1.43377\r\n",
      "trainer/Q1 Predictions Min                              -3.2412\r\n",
      "trainer/Q2 Predictions Mean                             -0.855599\r\n",
      "trainer/Q2 Predictions Std                               0.888868\r\n",
      "trainer/Q2 Predictions Max                               1.42271\r\n",
      "trainer/Q2 Predictions Min                              -3.21501\r\n",
      "trainer/Q Targets Mean                                  -0.876059\r\n",
      "trainer/Q Targets Std                                    0.894003\r\n",
      "trainer/Q Targets Max                                    1.39547\r\n",
      "trainer/Q Targets Min                                   -3.23343\r\n",
      "trainer/Log Pis Mean                                     1.86358\r\n",
      "trainer/Log Pis Std                                      1.45628\r\n",
      "trainer/Log Pis Max                                      4.58112\r\n",
      "trainer/Log Pis Min                                     -4.68844\r\n",
      "trainer/Policy mu Mean                                   0.0348339\r\n",
      "trainer/Policy mu Std                                    0.310976\r\n",
      "trainer/Policy mu Max                                    2.67831\r\n",
      "trainer/Policy mu Min                                   -1.41446\r\n",
      "trainer/Policy log std Mean                             -2.31506\r\n",
      "trainer/Policy log std Std                               0.600367\r\n",
      "trainer/Policy log std Max                              -0.0463334\r\n",
      "trainer/Policy log std Min                              -3.26277\r\n",
      "trainer/Alpha                                            0.0226434\r\n",
      "trainer/Alpha Loss                                      -0.516654\r\n",
      "exploration/num steps total                          10900\r\n",
      "exploration/num paths total                            545\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0579444\r\n",
      "exploration/Rewards Std                                  0.100761\r\n",
      "exploration/Rewards Max                                  0.107177\r\n",
      "exploration/Rewards Min                                 -0.445149\r\n",
      "exploration/Returns Mean                                -1.15889\r\n",
      "exploration/Returns Std                                  1.56214\r\n",
      "exploration/Returns Max                                  0.804083\r\n",
      "exploration/Returns Min                                 -3.3982\r\n",
      "exploration/Actions Mean                                -0.000897653\r\n",
      "exploration/Actions Std                                  0.197299\r\n",
      "exploration/Actions Max                                  0.554897\r\n",
      "exploration/Actions Min                                 -0.5579\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.15889\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.349541\r\n",
      "exploration/env_infos/final/reward_dist Std              0.304936\r\n",
      "exploration/env_infos/final/reward_dist Max              0.82714\r\n",
      "exploration/env_infos/final/reward_dist Min              0.0011835\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00429136\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00784501\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0199671\r\n",
      "exploration/env_infos/initial/reward_dist Min            9.34723e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.209032\r\n",
      "exploration/env_infos/reward_dist Std                    0.21722\r\n",
      "exploration/env_infos/reward_dist Max                    0.82714\r\n",
      "exploration/env_infos/reward_dist Min                    9.34723e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.242154\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0764205\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.124625\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.314558\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.341317\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.188436\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.120804\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.632633\r\n",
      "exploration/env_infos/reward_energy Mean                -0.233495\r\n",
      "exploration/env_infos/reward_energy Std                  0.15276\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0151134\r\n",
      "exploration/env_infos/reward_energy Min                 -0.645274\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0357071\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.210238\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.336163\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.275738\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000565689\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0137727\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0277095\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0223589\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.029667\r\n",
      "exploration/env_infos/end_effector_loc Std               0.16221\r\n",
      "exploration/env_infos/end_effector_loc Max               0.351197\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.3309\r\n",
      "evaluation/num steps total                           99000\r\n",
      "evaluation/num paths total                            4950\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.042962\r\n",
      "evaluation/Rewards Std                                   0.071765\r\n",
      "evaluation/Rewards Max                                   0.169266\r\n",
      "evaluation/Rewards Min                                  -0.355064\r\n",
      "evaluation/Returns Mean                                 -0.859239\r\n",
      "evaluation/Returns Std                                   1.04224\r\n",
      "evaluation/Returns Max                                   1.67012\r\n",
      "evaluation/Returns Min                                  -2.91675\r\n",
      "evaluation/Actions Mean                                  0.00209217\r\n",
      "evaluation/Actions Std                                   0.0767482\r\n",
      "evaluation/Actions Max                                   0.707976\r\n",
      "evaluation/Actions Min                                  -0.680147\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.859239\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.145953\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.25892\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.957263\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.09889e-19\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00595851\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0112068\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0520305\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.02967e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.179038\r\n",
      "evaluation/env_infos/reward_dist Std                     0.272862\r\n",
      "evaluation/env_infos/reward_dist Max                     0.985024\r\n",
      "evaluation/env_infos/reward_dist Min                     3.09889e-19\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0514498\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0404716\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00476627\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.183021\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.279691\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.213724\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0179589\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.780758\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0671767\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0853031\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000705802\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.780758\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0457971\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.251701\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.619819\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.525132\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00176154\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123198\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0353988\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0340073\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0227388\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.168486\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.619819\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.525132\r\n",
      "time/data storing (s)                                    0.00304988\r\n",
      "time/evaluation sampling (s)                             0.988384\r\n",
      "time/exploration sampling (s)                            0.152148\r\n",
      "time/logging (s)                                         0.019048\r\n",
      "time/saving (s)                                          0.0303942\r\n",
      "time/training (s)                                       51.0956\r\n",
      "time/epoch (s)                                          52.2887\r\n",
      "time/total (s)                                        4960.78\r\n",
      "Epoch                                                   98\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:48:34.386517 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 99 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000759883\n",
      "trainer/QF2 Loss                                          0.000806125\n",
      "trainer/Policy Loss                                       2.87161\n",
      "trainer/Q1 Predictions Mean                              -0.899691\n",
      "trainer/Q1 Predictions Std                                0.815776\n",
      "trainer/Q1 Predictions Max                                1.26907\n",
      "trainer/Q1 Predictions Min                               -2.85473\n",
      "trainer/Q2 Predictions Mean                              -0.894161\n",
      "trainer/Q2 Predictions Std                                0.815116\n",
      "trainer/Q2 Predictions Max                                1.28765\n",
      "trainer/Q2 Predictions Min                               -2.85111\n",
      "trainer/Q Targets Mean                                   -0.895059\n",
      "trainer/Q Targets Std                                     0.814831\n",
      "trainer/Q Targets Max                                     1.2531\n",
      "trainer/Q Targets Min                                    -2.80889\n",
      "trainer/Log Pis Mean                                      1.99265\n",
      "trainer/Log Pis Std                                       1.27234\n",
      "trainer/Log Pis Max                                       4.41099\n",
      "trainer/Log Pis Min                                      -1.37371\n",
      "trainer/Policy mu Mean                                   -0.0114338\n",
      "trainer/Policy mu Std                                     0.335486\n",
      "trainer/Policy mu Max                                     1.93696\n",
      "trainer/Policy mu Min                                    -2.46527\n",
      "trainer/Policy log std Mean                              -2.29367\n",
      "trainer/Policy log std Std                                0.581983\n",
      "trainer/Policy log std Max                               -0.357106\n",
      "trainer/Policy log std Min                               -3.18399\n",
      "trainer/Alpha                                             0.0234676\n",
      "trainer/Alpha Loss                                       -0.0275885\n",
      "exploration/num steps total                           11000\n",
      "exploration/num paths total                             550\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.084779\n",
      "exploration/Rewards Std                                   0.0708489\n",
      "exploration/Rewards Max                                   0.114574\n",
      "exploration/Rewards Min                                  -0.21966\n",
      "exploration/Returns Mean                                 -1.69558\n",
      "exploration/Returns Std                                   1.11728\n",
      "exploration/Returns Max                                   0.419051\n",
      "exploration/Returns Min                                  -2.83494\n",
      "exploration/Actions Mean                                  0.00309165\n",
      "exploration/Actions Std                                   0.149925\n",
      "exploration/Actions Max                                   0.514514\n",
      "exploration/Actions Min                                  -0.800542\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.69558\n",
      "exploration/env_infos/final/reward_dist Mean              0.289366\n",
      "exploration/env_infos/final/reward_dist Std               0.359168\n",
      "exploration/env_infos/final/reward_dist Max               0.817577\n",
      "exploration/env_infos/final/reward_dist Min               1.10764e-15\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00136419\n",
      "exploration/env_infos/initial/reward_dist Std             0.00237832\n",
      "exploration/env_infos/initial/reward_dist Max             0.00609587\n",
      "exploration/env_infos/initial/reward_dist Min             6.72222e-06\n",
      "exploration/env_infos/reward_dist Mean                    0.0600753\n",
      "exploration/env_infos/reward_dist Std                     0.154825\n",
      "exploration/env_infos/reward_dist Max                     0.817577\n",
      "exploration/env_infos/reward_dist Min                     1.10764e-15\n",
      "exploration/env_infos/final/reward_energy Mean           -0.141545\n",
      "exploration/env_infos/final/reward_energy Std             0.106899\n",
      "exploration/env_infos/final/reward_energy Max            -0.0178928\n",
      "exploration/env_infos/final/reward_energy Min            -0.340662\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.188919\n",
      "exploration/env_infos/initial/reward_energy Std           0.118401\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0357281\n",
      "exploration/env_infos/initial/reward_energy Min          -0.347112\n",
      "exploration/env_infos/reward_energy Mean                 -0.15483\n",
      "exploration/env_infos/reward_energy Std                   0.14492\n",
      "exploration/env_infos/reward_energy Max                  -0.0112202\n",
      "exploration/env_infos/reward_energy Min                  -0.829585\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.104239\n",
      "exploration/env_infos/final/end_effector_loc Std          0.27996\n",
      "exploration/env_infos/final/end_effector_loc Max          0.622515\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.166346\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.000658925\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00785508\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0151565\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0163496\n",
      "exploration/env_infos/end_effector_loc Mean               0.0488755\n",
      "exploration/env_infos/end_effector_loc Std                0.177835\n",
      "exploration/env_infos/end_effector_loc Max                0.622515\n",
      "exploration/env_infos/end_effector_loc Min               -0.166346\n",
      "evaluation/num steps total                           100000\n",
      "evaluation/num paths total                             5000\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0555603\n",
      "evaluation/Rewards Std                                    0.0788821\n",
      "evaluation/Rewards Max                                    0.148223\n",
      "evaluation/Rewards Min                                   -0.313124\n",
      "evaluation/Returns Mean                                  -1.11121\n",
      "evaluation/Returns Std                                    1.25351\n",
      "evaluation/Returns Max                                    2.11246\n",
      "evaluation/Returns Min                                   -3.47184\n",
      "evaluation/Actions Mean                                  -0.004054\n",
      "evaluation/Actions Std                                    0.0673975\n",
      "evaluation/Actions Max                                    0.498848\n",
      "evaluation/Actions Min                                   -0.46377\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -1.11121\n",
      "evaluation/env_infos/final/reward_dist Mean               0.15208\n",
      "evaluation/env_infos/final/reward_dist Std                0.220148\n",
      "evaluation/env_infos/final/reward_dist Max                0.778266\n",
      "evaluation/env_infos/final/reward_dist Min                5.51273e-45\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00657164\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0129755\n",
      "evaluation/env_infos/initial/reward_dist Max              0.071424\n",
      "evaluation/env_infos/initial/reward_dist Min              1.89152e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.1936\n",
      "evaluation/env_infos/reward_dist Std                      0.281209\n",
      "evaluation/env_infos/reward_dist Max                      0.986701\n",
      "evaluation/env_infos/reward_dist Min                      7.94104e-47\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.048012\n",
      "evaluation/env_infos/final/reward_energy Std              0.0507591\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00544975\n",
      "evaluation/env_infos/final/reward_energy Min             -0.250308\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.20114\n",
      "evaluation/env_infos/initial/reward_energy Std            0.164995\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.00614818\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.569343\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0603391\n",
      "evaluation/env_infos/reward_energy Std                    0.0740062\n",
      "evaluation/env_infos/reward_energy Max                   -0.000848076\n",
      "evaluation/env_infos/reward_energy Min                   -0.569343\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0113357\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.271391\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.506994\n",
      "evaluation/env_infos/final/end_effector_loc Min          -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000662461\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.009174\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0249424\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0231885\n",
      "evaluation/env_infos/end_effector_loc Mean                0.000605874\n",
      "evaluation/env_infos/end_effector_loc Std                 0.17629\n",
      "evaluation/env_infos/end_effector_loc Max                 0.506994\n",
      "evaluation/env_infos/end_effector_loc Min                -1\n",
      "time/data storing (s)                                     0.00288734\n",
      "time/evaluation sampling (s)                              1.15627\n",
      "time/exploration sampling (s)                             0.126535\n",
      "time/logging (s)                                          0.0208459\n",
      "time/saving (s)                                           0.0283143\n",
      "time/training (s)                                        54.3833\n",
      "time/epoch (s)                                           55.7182\n",
      "time/total (s)                                         5017.74\n",
      "Epoch                                                    99\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[18990]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1b762778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1b7e3740). One of the two will be used. Which one is undefined.\n",
      "objc[18990]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1b762700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1b7e3768). One of the two will be used. Which one is undefined.\n",
      "objc[18990]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1b7627a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1b7e37b8). One of the two will be used. Which one is undefined.\n",
      "objc[18990]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1b762818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1b7e3830). One of the two will be used. Which one is undefined.\n",
      "using sparse reward if specified\n",
      "2021-05-25 11:48:41.577000 PDT | Variant:\n",
      "2021-05-25 11:48:41.577616 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"pointreacherobs\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 20,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 20,\n",
      "    \"num_train_loops_per_epoch\": 5\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.97,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 2000,\n",
      "    \"latent_dim\": 6,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 1,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"sparse_reward\": null,\n",
      "    \"fixed_ratio\": null\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"horizon\": 20\n",
      "  }\n",
      "}\n",
      "pointreacher\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "2021-05-25 11:49:15.531827 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 0 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.672348\n",
      "trainer/QF2 Loss                                        0.67117\n",
      "trainer/Policy Loss                                    -1.37884\n",
      "trainer/Q1 Predictions Mean                            -0.00121823\n",
      "trainer/Q1 Predictions Std                              0.000552746\n",
      "trainer/Q1 Predictions Max                              0.000254711\n",
      "trainer/Q1 Predictions Min                             -0.00227255\n",
      "trainer/Q2 Predictions Mean                            -8.03623e-05\n",
      "trainer/Q2 Predictions Std                              0.000632807\n",
      "trainer/Q2 Predictions Max                              0.00120733\n",
      "trainer/Q2 Predictions Min                             -0.00179445\n",
      "trainer/Q Targets Mean                                  0.537571\n",
      "trainer/Q Targets Std                                   0.618124\n",
      "trainer/Q Targets Max                                   1.71732\n",
      "trainer/Q Targets Min                                  -1.51786\n",
      "trainer/Log Pis Mean                                   -1.38017\n",
      "trainer/Log Pis Std                                     0.289098\n",
      "trainer/Log Pis Max                                    -0.574903\n",
      "trainer/Log Pis Min                                    -2.60542\n",
      "trainer/Policy mu Mean                                  0.000985406\n",
      "trainer/Policy mu Std                                   0.000596871\n",
      "trainer/Policy mu Max                                   0.00221213\n",
      "trainer/Policy mu Min                                   1.31949e-05\n",
      "trainer/Policy log std Mean                             6.00163e-05\n",
      "trainer/Policy log std Std                              0.000636428\n",
      "trainer/Policy log std Max                              0.000963059\n",
      "trainer/Policy log std Min                             -0.00131503\n",
      "trainer/Alpha                                           0.997005\n",
      "trainer/Alpha Loss                                     -0\n",
      "exploration/num steps total                          1100\n",
      "exploration/num paths total                            55\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.779334\n",
      "exploration/Rewards Std                                 0.360297\n",
      "exploration/Rewards Max                                -0.0619928\n",
      "exploration/Rewards Min                                -1.39334\n",
      "exploration/Returns Mean                              -15.5867\n",
      "exploration/Returns Std                                 5.11475\n",
      "exploration/Returns Max                                -5.88001\n",
      "exploration/Returns Min                               -19.5088\n",
      "exploration/Actions Mean                                0.0672882\n",
      "exploration/Actions Std                                 0.585152\n",
      "exploration/Actions Max                                 0.981492\n",
      "exploration/Actions Min                                -0.953663\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -15.5867\n",
      "exploration/env_infos/final/reward_dist Mean            3.5514e-52\n",
      "exploration/env_infos/final/reward_dist Std             7.1028e-52\n",
      "exploration/env_infos/final/reward_dist Max             1.7757e-51\n",
      "exploration/env_infos/final/reward_dist Min             5.23713e-100\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00110881\n",
      "exploration/env_infos/initial/reward_dist Std           0.000799436\n",
      "exploration/env_infos/initial/reward_dist Max           0.00197989\n",
      "exploration/env_infos/initial/reward_dist Min           8.24155e-06\n",
      "exploration/env_infos/reward_dist Mean                  8.78519e-05\n",
      "exploration/env_infos/reward_dist Std                   0.000349362\n",
      "exploration/env_infos/reward_dist Max                   0.00197989\n",
      "exploration/env_infos/reward_dist Min                   5.23713e-100\n",
      "exploration/env_infos/final/reward_energy Mean         -0.78875\n",
      "exploration/env_infos/final/reward_energy Std           0.300753\n",
      "exploration/env_infos/final/reward_energy Max          -0.191105\n",
      "exploration/env_infos/final/reward_energy Min          -1.0009\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.890991\n",
      "exploration/env_infos/initial/reward_energy Std         0.138347\n",
      "exploration/env_infos/initial/reward_energy Max        -0.722018\n",
      "exploration/env_infos/initial/reward_energy Min        -1.11969\n",
      "exploration/env_infos/reward_energy Mean               -0.784903\n",
      "exploration/env_infos/reward_energy Std                 0.278904\n",
      "exploration/env_infos/reward_energy Max                -0.158037\n",
      "exploration/env_infos/reward_energy Min                -1.26039\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.442852\n",
      "exploration/env_infos/final/end_effector_loc Std        0.662506\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.0222619\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.022818\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0474395\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0351271\n",
      "exploration/env_infos/end_effector_loc Mean             0.286389\n",
      "exploration/env_infos/end_effector_loc Std              0.487203\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           1000\n",
      "evaluation/num paths total                             50\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0724238\n",
      "evaluation/Rewards Std                                  0.0456905\n",
      "evaluation/Rewards Max                                  0.0273882\n",
      "evaluation/Rewards Min                                 -0.145049\n",
      "evaluation/Returns Mean                                -1.44848\n",
      "evaluation/Returns Std                                  0.912568\n",
      "evaluation/Returns Max                                  0.437927\n",
      "evaluation/Returns Min                                 -2.87246\n",
      "evaluation/Actions Mean                                 0.000980846\n",
      "evaluation/Actions Std                                  0.000595694\n",
      "evaluation/Actions Max                                  0.00203524\n",
      "evaluation/Actions Min                                  0.000153851\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.44848\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00345347\n",
      "evaluation/env_infos/final/reward_dist Std              0.00716752\n",
      "evaluation/env_infos/final/reward_dist Max              0.0294951\n",
      "evaluation/env_infos/final/reward_dist Min              3.19905e-07\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00264807\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00510559\n",
      "evaluation/env_infos/initial/reward_dist Max            0.020763\n",
      "evaluation/env_infos/initial/reward_dist Min            1.16923e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.00287612\n",
      "evaluation/env_infos/reward_dist Std                    0.00567184\n",
      "evaluation/env_infos/reward_dist Max                    0.0294951\n",
      "evaluation/env_infos/reward_dist Min                    3.19905e-07\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.00164662\n",
      "evaluation/env_infos/final/reward_energy Std            0.000274058\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0011215\n",
      "evaluation/env_infos/final/reward_energy Min           -0.00206738\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.00155927\n",
      "evaluation/env_infos/initial/reward_energy Std          0.000240541\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00113108\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.00193048\n",
      "evaluation/env_infos/reward_energy Mean                -0.00160215\n",
      "evaluation/env_infos/reward_energy Std                  0.000258701\n",
      "evaluation/env_infos/reward_energy Max                 -0.0011215\n",
      "evaluation/env_infos/reward_energy Min                 -0.00206738\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.010226\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.0061497\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.0203188\n",
      "evaluation/env_infos/final/end_effector_loc Min         0.00180004\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      4.80138e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std       2.83926e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Max       9.60617e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Min       7.69257e-06\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00373622\n",
      "evaluation/env_infos/end_effector_loc Std               0.00432289\n",
      "evaluation/env_infos/end_effector_loc Max               0.0203188\n",
      "evaluation/env_infos/end_effector_loc Min               7.69257e-06\n",
      "time/data storing (s)                                   0.00621381\n",
      "time/evaluation sampling (s)                            1.17551\n",
      "time/exploration sampling (s)                           0.137434\n",
      "time/logging (s)                                        0.0204966\n",
      "time/saving (s)                                         0.0732453\n",
      "time/training (s)                                      31.1826\n",
      "time/epoch (s)                                         32.5955\n",
      "time/total (s)                                         37.2896\n",
      "Epoch                                                   0\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:49:56.045638 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 1 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.0158935\n",
      "trainer/QF2 Loss                                        0.018351\n",
      "trainer/Policy Loss                                    -0.853415\n",
      "trainer/Q1 Predictions Mean                            -0.450416\n",
      "trainer/Q1 Predictions Std                              0.639989\n",
      "trainer/Q1 Predictions Max                              0.662978\n",
      "trainer/Q1 Predictions Min                             -2.21416\n",
      "trainer/Q2 Predictions Mean                            -0.365557\n",
      "trainer/Q2 Predictions Std                              0.637782\n",
      "trainer/Q2 Predictions Max                              0.74346\n",
      "trainer/Q2 Predictions Min                             -2.22241\n",
      "trainer/Q Targets Mean                                 -0.394795\n",
      "trainer/Q Targets Std                                   0.674326\n",
      "trainer/Q Targets Max                                   0.865267\n",
      "trainer/Q Targets Min                                  -2.36967\n",
      "trainer/Log Pis Mean                                   -1.21348\n",
      "trainer/Log Pis Std                                     0.438561\n",
      "trainer/Log Pis Max                                    -0.254476\n",
      "trainer/Log Pis Min                                    -3.41673\n",
      "trainer/Policy mu Mean                                  0.0157817\n",
      "trainer/Policy mu Std                                   0.0356279\n",
      "trainer/Policy mu Max                                   0.0823842\n",
      "trainer/Policy mu Min                                  -0.0756194\n",
      "trainer/Policy log std Mean                            -0.464191\n",
      "trainer/Policy log std Std                              0.159663\n",
      "trainer/Policy log std Max                             -0.171675\n",
      "trainer/Policy log std Min                             -0.87282\n",
      "trainer/Alpha                                           0.225871\n",
      "trainer/Alpha Loss                                     -4.77171\n",
      "exploration/num steps total                          1200\n",
      "exploration/num paths total                            60\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.472323\n",
      "exploration/Rewards Std                                 0.320149\n",
      "exploration/Rewards Max                                -0.0845086\n",
      "exploration/Rewards Min                                -1.49455\n",
      "exploration/Returns Mean                               -9.44645\n",
      "exploration/Returns Std                                 4.19507\n",
      "exploration/Returns Max                                -5.82318\n",
      "exploration/Returns Min                               -17.4822\n",
      "exploration/Actions Mean                                0.0315322\n",
      "exploration/Actions Std                                 0.453184\n",
      "exploration/Actions Max                                 0.942808\n",
      "exploration/Actions Min                                -0.895441\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -9.44645\n",
      "exploration/env_infos/final/reward_dist Mean            4.27435e-44\n",
      "exploration/env_infos/final/reward_dist Std             8.5487e-44\n",
      "exploration/env_infos/final/reward_dist Max             2.13717e-43\n",
      "exploration/env_infos/final/reward_dist Min             4.3175e-109\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00930873\n",
      "exploration/env_infos/initial/reward_dist Std           0.0130394\n",
      "exploration/env_infos/initial/reward_dist Max           0.033955\n",
      "exploration/env_infos/initial/reward_dist Min           2.55492e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0409551\n",
      "exploration/env_infos/reward_dist Std                   0.13018\n",
      "exploration/env_infos/reward_dist Max                   0.827212\n",
      "exploration/env_infos/reward_dist Min                   4.3175e-109\n",
      "exploration/env_infos/final/reward_energy Mean         -0.564381\n",
      "exploration/env_infos/final/reward_energy Std           0.153437\n",
      "exploration/env_infos/final/reward_energy Max          -0.410481\n",
      "exploration/env_infos/final/reward_energy Min          -0.81602\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.547129\n",
      "exploration/env_infos/initial/reward_energy Std         0.164637\n",
      "exploration/env_infos/initial/reward_energy Max        -0.381345\n",
      "exploration/env_infos/initial/reward_energy Min        -0.839745\n",
      "exploration/env_infos/reward_energy Mean               -0.587652\n",
      "exploration/env_infos/reward_energy Std                 0.259625\n",
      "exploration/env_infos/reward_energy Max                -0.0608045\n",
      "exploration/env_infos/reward_energy Min                -1.21176\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.217326\n",
      "exploration/env_infos/final/end_effector_loc Std        0.808911\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.000712911\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0201882\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0293443\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0400074\n",
      "exploration/env_infos/end_effector_loc Mean             0.102351\n",
      "exploration/env_infos/end_effector_loc Std              0.511242\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           2000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.121711\n",
      "evaluation/Rewards Std                                  0.0947938\n",
      "evaluation/Rewards Max                                  0.0820133\n",
      "evaluation/Rewards Min                                 -0.674623\n",
      "evaluation/Returns Mean                                -2.43422\n",
      "evaluation/Returns Std                                  1.38812\n",
      "evaluation/Returns Max                                  0.0802905\n",
      "evaluation/Returns Min                                 -5.80616\n",
      "evaluation/Actions Mean                                 0.0243281\n",
      "evaluation/Actions Std                                  0.032682\n",
      "evaluation/Actions Max                                  0.0833411\n",
      "evaluation/Actions Min                                 -0.0667044\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.43422\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0126902\n",
      "evaluation/env_infos/final/reward_dist Std              0.0516867\n",
      "evaluation/env_infos/final/reward_dist Max              0.338091\n",
      "evaluation/env_infos/final/reward_dist Min              1.01821e-83\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00374265\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00663119\n",
      "evaluation/env_infos/initial/reward_dist Max            0.025391\n",
      "evaluation/env_infos/initial/reward_dist Min            8.27134e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0285517\n",
      "evaluation/env_infos/reward_dist Std                    0.0829103\n",
      "evaluation/env_infos/reward_dist Max                    0.633927\n",
      "evaluation/env_infos/reward_dist Min                    1.01821e-83\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.058981\n",
      "evaluation/env_infos/final/reward_energy Std            0.0203501\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0265055\n",
      "evaluation/env_infos/final/reward_energy Min           -0.0952714\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.0502729\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0188656\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0234923\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.0862124\n",
      "evaluation/env_infos/reward_energy Mean                -0.0541446\n",
      "evaluation/env_infos/reward_energy Std                  0.0197054\n",
      "evaluation/env_infos/reward_energy Max                 -0.0234923\n",
      "evaluation/env_infos/reward_energy Min                 -0.0952714\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.236419\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.341325\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.867469\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.628864\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000954822\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00164086\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.00411694\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00333522\n",
      "evaluation/env_infos/end_effector_loc Mean              0.083256\n",
      "evaluation/env_infos/end_effector_loc Std               0.179578\n",
      "evaluation/env_infos/end_effector_loc Max               0.867469\n",
      "evaluation/env_infos/end_effector_loc Min              -0.628864\n",
      "time/data storing (s)                                   0.00616514\n",
      "time/evaluation sampling (s)                            1.09697\n",
      "time/exploration sampling (s)                           0.127657\n",
      "time/logging (s)                                        0.0210535\n",
      "time/saving (s)                                         0.0318642\n",
      "time/training (s)                                      39.0353\n",
      "time/epoch (s)                                         40.319\n",
      "time/total (s)                                         77.8032\n",
      "Epoch                                                   1\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:50:39.244329 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 2 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                   2000\r\n",
      "trainer/QF1 Loss                                        0.0152706\r\n",
      "trainer/QF2 Loss                                        0.0163452\r\n",
      "trainer/Policy Loss                                     1.00282\r\n",
      "trainer/Q1 Predictions Mean                            -1.28647\r\n",
      "trainer/Q1 Predictions Std                              1.02599\r\n",
      "trainer/Q1 Predictions Max                              0.476811\r\n",
      "trainer/Q1 Predictions Min                             -4.35677\r\n",
      "trainer/Q2 Predictions Mean                            -1.25823\r\n",
      "trainer/Q2 Predictions Std                              1.04016\r\n",
      "trainer/Q2 Predictions Max                              0.512977\r\n",
      "trainer/Q2 Predictions Min                             -4.29007\r\n",
      "trainer/Q Targets Mean                                 -1.2498\r\n",
      "trainer/Q Targets Std                                   1.04695\r\n",
      "trainer/Q Targets Max                                   0.463098\r\n",
      "trainer/Q Targets Min                                  -4.41931\r\n",
      "trainer/Log Pis Mean                                   -0.0670519\r\n",
      "trainer/Log Pis Std                                     1.02024\r\n",
      "trainer/Log Pis Max                                     2.39869\r\n",
      "trainer/Log Pis Min                                    -4.59829\r\n",
      "trainer/Policy mu Mean                                  0.0569354\r\n",
      "trainer/Policy mu Std                                   0.322808\r\n",
      "trainer/Policy mu Max                                   1.64144\r\n",
      "trainer/Policy mu Min                                  -1.02321\r\n",
      "trainer/Policy log std Mean                            -1.15017\r\n",
      "trainer/Policy log std Std                              0.404949\r\n",
      "trainer/Policy log std Max                             -0.283445\r\n",
      "trainer/Policy log std Min                             -2.06719\r\n",
      "trainer/Alpha                                           0.0621381\r\n",
      "trainer/Alpha Loss                                     -5.73845\r\n",
      "exploration/num steps total                          1300\r\n",
      "exploration/num paths total                            65\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.286799\r\n",
      "exploration/Rewards Std                                 0.138236\r\n",
      "exploration/Rewards Max                                -0.0648243\r\n",
      "exploration/Rewards Min                                -0.552259\r\n",
      "exploration/Returns Mean                               -5.73598\r\n",
      "exploration/Returns Std                                 1.56042\r\n",
      "exploration/Returns Max                                -3.90122\r\n",
      "exploration/Returns Min                                -8.45994\r\n",
      "exploration/Actions Mean                                0.0188313\r\n",
      "exploration/Actions Std                                 0.251142\r\n",
      "exploration/Actions Max                                 0.80495\r\n",
      "exploration/Actions Min                                -0.72136\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -5.73598\r\n",
      "exploration/env_infos/final/reward_dist Mean            4.80611e-14\r\n",
      "exploration/env_infos/final/reward_dist Std             9.61223e-14\r\n",
      "exploration/env_infos/final/reward_dist Max             2.40306e-13\r\n",
      "exploration/env_infos/final/reward_dist Min             2.00722e-90\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000189909\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.000358006\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.000905668\r\n",
      "exploration/env_infos/initial/reward_dist Min           5.30858e-07\r\n",
      "exploration/env_infos/reward_dist Mean                  0.00105752\r\n",
      "exploration/env_infos/reward_dist Std                   0.00522925\r\n",
      "exploration/env_infos/reward_dist Max                   0.04493\r\n",
      "exploration/env_infos/reward_dist Min                   2.25678e-91\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.194224\r\n",
      "exploration/env_infos/final/reward_energy Std           0.0974876\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.0574928\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.304506\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.486962\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.274816\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.184341\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.980845\r\n",
      "exploration/env_infos/reward_energy Mean               -0.290606\r\n",
      "exploration/env_infos/reward_energy Std                 0.205916\r\n",
      "exploration/env_infos/reward_energy Max                -0.0250696\r\n",
      "exploration/env_infos/reward_energy Min                -0.986171\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.385392\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.539764\r\n",
      "exploration/env_infos/final/end_effector_loc Max        1\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00474241\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0191919\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0360032\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0182805\r\n",
      "exploration/env_infos/end_effector_loc Mean             0.221574\r\n",
      "exploration/env_infos/end_effector_loc Std              0.382089\r\n",
      "exploration/env_infos/end_effector_loc Max              1\r\n",
      "exploration/env_infos/end_effector_loc Min             -1\r\n",
      "evaluation/num steps total                           3000\r\n",
      "evaluation/num paths total                            150\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.229423\r\n",
      "evaluation/Rewards Std                                  0.220588\r\n",
      "evaluation/Rewards Max                                  0.119587\r\n",
      "evaluation/Rewards Min                                 -1.02478\r\n",
      "evaluation/Returns Mean                                -4.58847\r\n",
      "evaluation/Returns Std                                  3.15404\r\n",
      "evaluation/Returns Max                                 -0.0429861\r\n",
      "evaluation/Returns Min                                -12.1634\r\n",
      "evaluation/Actions Mean                                 0.017984\r\n",
      "evaluation/Actions Std                                  0.141246\r\n",
      "evaluation/Actions Max                                  0.76732\r\n",
      "evaluation/Actions Min                                 -0.474665\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -4.58847\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0116957\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.0633309\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.428023\r\n",
      "evaluation/env_infos/final/reward_dist Min              3.03315e-157\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00694228\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0150435\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0892187\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.11902e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0404275\r\n",
      "evaluation/env_infos/reward_dist Std                    0.138081\r\n",
      "evaluation/env_infos/reward_dist Max                    0.993975\r\n",
      "evaluation/env_infos/reward_dist Min                    1.78698e-162\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.184112\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.141627\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0182393\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.621349\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.182862\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.169334\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00709136\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.775226\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.159095\r\n",
      "evaluation/env_infos/reward_energy Std                  0.123435\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00483205\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.775226\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.197605\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.608407\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00346167\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00810293\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.038366\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00774605\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.132323\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.397563\r\n",
      "evaluation/env_infos/end_effector_loc Max               1\r\n",
      "evaluation/env_infos/end_effector_loc Min              -1\r\n",
      "time/data storing (s)                                   0.00640571\r\n",
      "time/evaluation sampling (s)                            1.10215\r\n",
      "time/exploration sampling (s)                           0.12414\r\n",
      "time/logging (s)                                        0.0200475\r\n",
      "time/saving (s)                                         0.0271609\r\n",
      "time/training (s)                                      41.8603\r\n",
      "time/epoch (s)                                         43.1402\r\n",
      "time/total (s)                                        121\r\n",
      "Epoch                                                   2\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:51:24.166264 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 3 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   2000\r\n",
      "trainer/QF1 Loss                                        0.00743615\r\n",
      "trainer/QF2 Loss                                        0.0145448\r\n",
      "trainer/Policy Loss                                     2.66085\r\n",
      "trainer/Q1 Predictions Mean                            -1.9713\r\n",
      "trainer/Q1 Predictions Std                              1.34341\r\n",
      "trainer/Q1 Predictions Max                              0.23657\r\n",
      "trainer/Q1 Predictions Min                             -6.3588\r\n",
      "trainer/Q2 Predictions Mean                            -1.92974\r\n",
      "trainer/Q2 Predictions Std                              1.33664\r\n",
      "trainer/Q2 Predictions Max                              0.309348\r\n",
      "trainer/Q2 Predictions Min                             -6.32205\r\n",
      "trainer/Q Targets Mean                                 -1.97046\r\n",
      "trainer/Q Targets Std                                   1.3545\r\n",
      "trainer/Q Targets Max                                   0.301583\r\n",
      "trainer/Q Targets Min                                  -6.32522\r\n",
      "trainer/Log Pis Mean                                    0.952188\r\n",
      "trainer/Log Pis Std                                     1.23678\r\n",
      "trainer/Log Pis Max                                     5.21669\r\n",
      "trainer/Log Pis Min                                    -2.80981\r\n",
      "trainer/Policy mu Mean                                  0.0489619\r\n",
      "trainer/Policy mu Std                                   0.521926\r\n",
      "trainer/Policy mu Max                                   2.55613\r\n",
      "trainer/Policy mu Min                                  -1.72504\r\n",
      "trainer/Policy log std Mean                            -1.64454\r\n",
      "trainer/Policy log std Std                              0.508096\r\n",
      "trainer/Policy log std Max                             -0.0916855\r\n",
      "trainer/Policy log std Min                             -2.88881\r\n",
      "trainer/Alpha                                           0.0250817\r\n",
      "trainer/Alpha Loss                                     -3.86045\r\n",
      "exploration/num steps total                          1400\r\n",
      "exploration/num paths total                            70\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.157539\r\n",
      "exploration/Rewards Std                                 0.133584\r\n",
      "exploration/Rewards Max                                 0.100013\r\n",
      "exploration/Rewards Min                                -0.604888\r\n",
      "exploration/Returns Mean                               -3.15078\r\n",
      "exploration/Returns Std                                 1.56085\r\n",
      "exploration/Returns Max                                -0.469782\r\n",
      "exploration/Returns Min                                -5.30904\r\n",
      "exploration/Actions Mean                                0.00587114\r\n",
      "exploration/Actions Std                                 0.195478\r\n",
      "exploration/Actions Max                                 0.557101\r\n",
      "exploration/Actions Min                                -0.700095\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -3.15078\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.0211113\r\n",
      "exploration/env_infos/final/reward_dist Std             0.0406629\r\n",
      "exploration/env_infos/final/reward_dist Max             0.1024\r\n",
      "exploration/env_infos/final/reward_dist Min             2.53362e-91\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00675354\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.0118536\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0304343\r\n",
      "exploration/env_infos/initial/reward_dist Min           0.000253309\r\n",
      "exploration/env_infos/reward_dist Mean                  0.0884899\r\n",
      "exploration/env_infos/reward_dist Std                   0.186906\r\n",
      "exploration/env_infos/reward_dist Max                   0.899566\r\n",
      "exploration/env_infos/reward_dist Min                   2.53362e-91\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.379046\r\n",
      "exploration/env_infos/final/reward_energy Std           0.119973\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.222019\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.588564\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.279663\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.13841\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.101393\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.504465\r\n",
      "exploration/env_infos/reward_energy Mean               -0.231289\r\n",
      "exploration/env_infos/reward_energy Std                 0.151651\r\n",
      "exploration/env_infos/reward_energy Max                -0.0281224\r\n",
      "exploration/env_infos/reward_energy Min                -0.704296\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.193287\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.530781\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.991886\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.577216\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00575624\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00941153\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0230343\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0103115\r\n",
      "exploration/env_infos/end_effector_loc Mean             0.124367\r\n",
      "exploration/env_infos/end_effector_loc Std              0.316305\r\n",
      "exploration/env_infos/end_effector_loc Max              1\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.577216\r\n",
      "evaluation/num steps total                           4000\r\n",
      "evaluation/num paths total                            200\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.131483\r\n",
      "evaluation/Rewards Std                                  0.132743\r\n",
      "evaluation/Rewards Max                                  0.13564\r\n",
      "evaluation/Rewards Min                                 -0.777741\r\n",
      "evaluation/Returns Mean                                -2.62966\r\n",
      "evaluation/Returns Std                                  1.70769\r\n",
      "evaluation/Returns Max                                  0.0746871\r\n",
      "evaluation/Returns Min                                 -7.79977\r\n",
      "evaluation/Actions Mean                                -0.0120567\r\n",
      "evaluation/Actions Std                                  0.132851\r\n",
      "evaluation/Actions Max                                  0.63865\r\n",
      "evaluation/Actions Min                                 -0.563438\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -2.62966\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0579879\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.170225\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.863926\r\n",
      "evaluation/env_infos/final/reward_dist Min              7.2263e-61\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00417104\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00688119\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0308881\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.4858e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0857776\r\n",
      "evaluation/env_infos/reward_dist Std                    0.191196\r\n",
      "evaluation/env_infos/reward_dist Max                    0.990913\r\n",
      "evaluation/env_infos/reward_dist Min                    7.2263e-61\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.159823\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.142738\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00632063\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.630265\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.196552\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.155489\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0107947\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.711108\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.133333\r\n",
      "evaluation/env_infos/reward_energy Std                  0.13346\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00197174\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.711108\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.0356862\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.458586\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00276349\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00841874\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0319325\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0106757\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0507872\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.313227\r\n",
      "evaluation/env_infos/end_effector_loc Max               1\r\n",
      "evaluation/env_infos/end_effector_loc Min              -1\r\n",
      "time/data storing (s)                                   0.00585583\r\n",
      "time/evaluation sampling (s)                            0.980161\r\n",
      "time/exploration sampling (s)                           0.11636\r\n",
      "time/logging (s)                                        0.0208298\r\n",
      "time/saving (s)                                         0.0280159\r\n",
      "time/training (s)                                      43.7022\r\n",
      "time/epoch (s)                                         44.8534\r\n",
      "time/total (s)                                        165.922\r\n",
      "Epoch                                                   3\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:52:09.394135 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 4 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00900255\n",
      "trainer/QF2 Loss                                        0.00792023\n",
      "trainer/Policy Loss                                     3.79236\n",
      "trainer/Q1 Predictions Mean                            -2.29525\n",
      "trainer/Q1 Predictions Std                              1.36468\n",
      "trainer/Q1 Predictions Max                              0.155333\n",
      "trainer/Q1 Predictions Min                             -7.41158\n",
      "trainer/Q2 Predictions Mean                            -2.19783\n",
      "trainer/Q2 Predictions Std                              1.33976\n",
      "trainer/Q2 Predictions Max                              0.234088\n",
      "trainer/Q2 Predictions Min                             -7.40944\n",
      "trainer/Q Targets Mean                                 -2.24644\n",
      "trainer/Q Targets Std                                   1.34892\n",
      "trainer/Q Targets Max                                   0.163697\n",
      "trainer/Q Targets Min                                  -7.36343\n",
      "trainer/Log Pis Mean                                    1.79361\n",
      "trainer/Log Pis Std                                     1.46633\n",
      "trainer/Log Pis Max                                    10.4119\n",
      "trainer/Log Pis Min                                    -5.86206\n",
      "trainer/Policy mu Mean                                  0.0139832\n",
      "trainer/Policy mu Std                                   0.686709\n",
      "trainer/Policy mu Max                                   3.57824\n",
      "trainer/Policy mu Min                                  -3.14863\n",
      "trainer/Policy log std Mean                            -1.93881\n",
      "trainer/Policy log std Std                              0.585949\n",
      "trainer/Policy log std Max                             -0.253617\n",
      "trainer/Policy log std Min                             -2.80068\n",
      "trainer/Alpha                                           0.0158476\n",
      "trainer/Alpha Loss                                     -0.855331\n",
      "exploration/num steps total                          1500\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.110888\n",
      "exploration/Rewards Std                                 0.0696856\n",
      "exploration/Rewards Max                                 0.0394911\n",
      "exploration/Rewards Min                                -0.289838\n",
      "exploration/Returns Mean                               -2.21776\n",
      "exploration/Returns Std                                 0.751434\n",
      "exploration/Returns Max                                -1.27306\n",
      "exploration/Returns Min                                -3.22543\n",
      "exploration/Actions Mean                               -0.0102886\n",
      "exploration/Actions Std                                 0.0864903\n",
      "exploration/Actions Max                                 0.268437\n",
      "exploration/Actions Min                                -0.273123\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.21776\n",
      "exploration/env_infos/final/reward_dist Mean            0.0212517\n",
      "exploration/env_infos/final/reward_dist Std             0.0343024\n",
      "exploration/env_infos/final/reward_dist Max             0.0884713\n",
      "exploration/env_infos/final/reward_dist Min             9.1168e-15\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000384663\n",
      "exploration/env_infos/initial/reward_dist Std           0.000681671\n",
      "exploration/env_infos/initial/reward_dist Max           0.00174597\n",
      "exploration/env_infos/initial/reward_dist Min           5.76326e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.130253\n",
      "exploration/env_infos/reward_dist Std                   0.252659\n",
      "exploration/env_infos/reward_dist Max                   0.989593\n",
      "exploration/env_infos/reward_dist Min                   9.1168e-15\n",
      "exploration/env_infos/final/reward_energy Mean         -0.126808\n",
      "exploration/env_infos/final/reward_energy Std           0.0924511\n",
      "exploration/env_infos/final/reward_energy Max          -0.0648077\n",
      "exploration/env_infos/final/reward_energy Min          -0.309886\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.108618\n",
      "exploration/env_infos/initial/reward_energy Std         0.0458186\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0366528\n",
      "exploration/env_infos/initial/reward_energy Min        -0.179025\n",
      "exploration/env_infos/reward_energy Mean               -0.10721\n",
      "exploration/env_infos/reward_energy Std                 0.0606534\n",
      "exploration/env_infos/reward_energy Max                -0.00262183\n",
      "exploration/env_infos/reward_energy Min                -0.309886\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0494958\n",
      "exploration/env_infos/final/end_effector_loc Std        0.277684\n",
      "exploration/env_infos/final/end_effector_loc Max        0.372061\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.428366\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.000262898\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00415963\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00792012\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.00479534\n",
      "exploration/env_infos/end_effector_loc Mean             0.00126472\n",
      "exploration/env_infos/end_effector_loc Std              0.155724\n",
      "exploration/env_infos/end_effector_loc Max              0.372061\n",
      "exploration/env_infos/end_effector_loc Min             -0.428366\n",
      "evaluation/num steps total                           5000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.149924\n",
      "evaluation/Rewards Std                                  0.148945\n",
      "evaluation/Rewards Max                                  0.129102\n",
      "evaluation/Rewards Min                                 -1.06267\n",
      "evaluation/Returns Mean                                -2.99849\n",
      "evaluation/Returns Std                                  1.93408\n",
      "evaluation/Returns Max                                 -0.171615\n",
      "evaluation/Returns Min                                 -9.61362\n",
      "evaluation/Actions Mean                                -0.00934618\n",
      "evaluation/Actions Std                                  0.121605\n",
      "evaluation/Actions Max                                  0.828549\n",
      "evaluation/Actions Min                                 -0.811231\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.99849\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0398358\n",
      "evaluation/env_infos/final/reward_dist Std              0.165901\n",
      "evaluation/env_infos/final/reward_dist Max              0.929791\n",
      "evaluation/env_infos/final/reward_dist Min              8.6986e-113\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.0049856\n",
      "evaluation/env_infos/initial/reward_dist Std            0.008458\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0321189\n",
      "evaluation/env_infos/initial/reward_dist Min            9.01533e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0844899\n",
      "evaluation/env_infos/reward_dist Std                    0.196414\n",
      "evaluation/env_infos/reward_dist Max                    0.996238\n",
      "evaluation/env_infos/reward_dist Min                    8.6986e-113\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.15929\n",
      "evaluation/env_infos/final/reward_energy Std            0.126319\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00916499\n",
      "evaluation/env_infos/final/reward_energy Min           -0.51637\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.271121\n",
      "evaluation/env_infos/initial/reward_energy Std          0.240372\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0183368\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.921877\n",
      "evaluation/env_infos/reward_energy Mean                -0.129561\n",
      "evaluation/env_infos/reward_energy Std                  0.113862\n",
      "evaluation/env_infos/reward_energy Max                 -0.00470734\n",
      "evaluation/env_infos/reward_energy Min                 -0.921877\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0239422\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.501794\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00262822\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0125379\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0414274\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0405615\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0222596\n",
      "evaluation/env_infos/end_effector_loc Std               0.326799\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00623227\n",
      "time/evaluation sampling (s)                            1.14809\n",
      "time/exploration sampling (s)                           0.123522\n",
      "time/logging (s)                                        0.0214727\n",
      "time/saving (s)                                         0.0259325\n",
      "time/training (s)                                      43.8123\n",
      "time/epoch (s)                                         45.1376\n",
      "time/total (s)                                        211.15\n",
      "Epoch                                                   4\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:52:56.385889 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 5 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00567169\n",
      "trainer/QF2 Loss                                        0.0122032\n",
      "trainer/Policy Loss                                     4.39703\n",
      "trainer/Q1 Predictions Mean                            -2.47431\n",
      "trainer/Q1 Predictions Std                              1.53245\n",
      "trainer/Q1 Predictions Max                              0.158491\n",
      "trainer/Q1 Predictions Min                             -8.17439\n",
      "trainer/Q2 Predictions Mean                            -2.51741\n",
      "trainer/Q2 Predictions Std                              1.53965\n",
      "trainer/Q2 Predictions Max                              0.067403\n",
      "trainer/Q2 Predictions Min                             -8.12406\n",
      "trainer/Q Targets Mean                                 -2.48117\n",
      "trainer/Q Targets Std                                   1.53311\n",
      "trainer/Q Targets Max                                   0.0825881\n",
      "trainer/Q Targets Min                                  -8.08491\n",
      "trainer/Log Pis Mean                                    2.15063\n",
      "trainer/Log Pis Std                                     1.38034\n",
      "trainer/Log Pis Max                                     8.85853\n",
      "trainer/Log Pis Min                                    -2.79126\n",
      "trainer/Policy mu Mean                                  0.0549099\n",
      "trainer/Policy mu Std                                   0.637185\n",
      "trainer/Policy mu Max                                   3.22923\n",
      "trainer/Policy mu Min                                  -2.26771\n",
      "trainer/Policy log std Mean                            -2.12076\n",
      "trainer/Policy log std Std                              0.614233\n",
      "trainer/Policy log std Max                             -0.237867\n",
      "trainer/Policy log std Min                             -3.07506\n",
      "trainer/Alpha                                           0.0144181\n",
      "trainer/Alpha Loss                                      0.63856\n",
      "exploration/num steps total                          1600\n",
      "exploration/num paths total                            80\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.130305\n",
      "exploration/Rewards Std                                 0.0792142\n",
      "exploration/Rewards Max                                 0.0260418\n",
      "exploration/Rewards Min                                -0.309706\n",
      "exploration/Returns Mean                               -2.6061\n",
      "exploration/Returns Std                                 1.2684\n",
      "exploration/Returns Max                                -1.07159\n",
      "exploration/Returns Min                                -4.505\n",
      "exploration/Actions Mean                               -0.0140483\n",
      "exploration/Actions Std                                 0.205331\n",
      "exploration/Actions Max                                 0.754753\n",
      "exploration/Actions Min                                -0.662271\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.6061\n",
      "exploration/env_infos/final/reward_dist Mean            3.65199e-05\n",
      "exploration/env_infos/final/reward_dist Std             4.54696e-05\n",
      "exploration/env_infos/final/reward_dist Max             0.000107332\n",
      "exploration/env_infos/final/reward_dist Min             4.2556e-86\n",
      "exploration/env_infos/initial/reward_dist Mean          0.015449\n",
      "exploration/env_infos/initial/reward_dist Std           0.019222\n",
      "exploration/env_infos/initial/reward_dist Max           0.0525612\n",
      "exploration/env_infos/initial/reward_dist Min           2.19166e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0751058\n",
      "exploration/env_infos/reward_dist Std                   0.185626\n",
      "exploration/env_infos/reward_dist Max                   0.907345\n",
      "exploration/env_infos/reward_dist Min                   4.2556e-86\n",
      "exploration/env_infos/final/reward_energy Mean         -0.283911\n",
      "exploration/env_infos/final/reward_energy Std           0.0609173\n",
      "exploration/env_infos/final/reward_energy Max          -0.194715\n",
      "exploration/env_infos/final/reward_energy Min          -0.363324\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.354308\n",
      "exploration/env_infos/initial/reward_energy Std         0.206757\n",
      "exploration/env_infos/initial/reward_energy Max        -0.151482\n",
      "exploration/env_infos/initial/reward_energy Min        -0.720095\n",
      "exploration/env_infos/reward_energy Mean               -0.238556\n",
      "exploration/env_infos/reward_energy Std                 0.166755\n",
      "exploration/env_infos/reward_energy Max                -0.019065\n",
      "exploration/env_infos/reward_energy Min                -0.90766\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.143686\n",
      "exploration/env_infos/final/end_effector_loc Std        0.392308\n",
      "exploration/env_infos/final/end_effector_loc Max        0.414712\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.835596\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.000419141\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0144975\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0355738\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0208871\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0312969\n",
      "exploration/env_infos/end_effector_loc Std              0.27247\n",
      "exploration/env_infos/end_effector_loc Max              0.45454\n",
      "exploration/env_infos/end_effector_loc Min             -0.835596\n",
      "evaluation/num steps total                           6000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.118701\n",
      "evaluation/Rewards Std                                  0.126882\n",
      "evaluation/Rewards Max                                  0.121322\n",
      "evaluation/Rewards Min                                 -0.778763\n",
      "evaluation/Returns Mean                                -2.37402\n",
      "evaluation/Returns Std                                  1.81265\n",
      "evaluation/Returns Max                                 -0.0820162\n",
      "evaluation/Returns Min                                 -7.85345\n",
      "evaluation/Actions Mean                                -0.00571539\n",
      "evaluation/Actions Std                                  0.0785998\n",
      "evaluation/Actions Max                                  0.541266\n",
      "evaluation/Actions Min                                 -0.449876\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.37402\n",
      "evaluation/env_infos/final/reward_dist Mean             0.052454\n",
      "evaluation/env_infos/final/reward_dist Std              0.134443\n",
      "evaluation/env_infos/final/reward_dist Max              0.65142\n",
      "evaluation/env_infos/final/reward_dist Min              1.44847e-68\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00742581\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0107273\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0522268\n",
      "evaluation/env_infos/initial/reward_dist Min            1.13364e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0885291\n",
      "evaluation/env_infos/reward_dist Std                    0.186356\n",
      "evaluation/env_infos/reward_dist Max                    0.998247\n",
      "evaluation/env_infos/reward_dist Min                    1.44847e-68\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0744104\n",
      "evaluation/env_infos/final/reward_energy Std            0.0639856\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00553929\n",
      "evaluation/env_infos/final/reward_energy Min           -0.310044\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.185687\n",
      "evaluation/env_infos/initial/reward_energy Std          0.171221\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0204229\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.667717\n",
      "evaluation/env_infos/reward_energy Mean                -0.0807099\n",
      "evaluation/env_infos/reward_energy Std                  0.0768576\n",
      "evaluation/env_infos/reward_energy Max                 -0.00371764\n",
      "evaluation/env_infos/reward_energy Min                 -0.667717\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0262751\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.42951\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00209328\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0086812\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0270633\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0224938\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00496048\n",
      "evaluation/env_infos/end_effector_loc Std               0.265103\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00583848\n",
      "time/evaluation sampling (s)                            0.996723\n",
      "time/exploration sampling (s)                           0.130271\n",
      "time/logging (s)                                        0.0192504\n",
      "time/saving (s)                                         0.0273568\n",
      "time/training (s)                                      45.7176\n",
      "time/epoch (s)                                         46.897\n",
      "time/total (s)                                        258.139\n",
      "Epoch                                                   5\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:53:42.316920 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 6 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00633279\n",
      "trainer/QF2 Loss                                        0.00661089\n",
      "trainer/Policy Loss                                     4.54233\n",
      "trainer/Q1 Predictions Mean                            -2.59909\n",
      "trainer/Q1 Predictions Std                              1.8217\n",
      "trainer/Q1 Predictions Max                             -0.0421654\n",
      "trainer/Q1 Predictions Min                            -10.3377\n",
      "trainer/Q2 Predictions Mean                            -2.56751\n",
      "trainer/Q2 Predictions Std                              1.80243\n",
      "trainer/Q2 Predictions Max                             -0.0236893\n",
      "trainer/Q2 Predictions Min                            -10.2991\n",
      "trainer/Q Targets Mean                                 -2.58349\n",
      "trainer/Q Targets Std                                   1.81972\n",
      "trainer/Q Targets Max                                  -0.0724789\n",
      "trainer/Q Targets Min                                 -10.3058\n",
      "trainer/Log Pis Mean                                    2.15655\n",
      "trainer/Log Pis Std                                     1.6873\n",
      "trainer/Log Pis Max                                    10.0157\n",
      "trainer/Log Pis Min                                    -2.76369\n",
      "trainer/Policy mu Mean                                 -0.00182546\n",
      "trainer/Policy mu Std                                   0.830559\n",
      "trainer/Policy mu Max                                   3.61494\n",
      "trainer/Policy mu Min                                  -3.16031\n",
      "trainer/Policy log std Mean                            -2.08212\n",
      "trainer/Policy log std Std                              0.709946\n",
      "trainer/Policy log std Max                             -0.0453137\n",
      "trainer/Policy log std Min                             -3.09719\n",
      "trainer/Alpha                                           0.0150135\n",
      "trainer/Alpha Loss                                      0.657339\n",
      "exploration/num steps total                          1700\n",
      "exploration/num paths total                            85\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.1943\n",
      "exploration/Rewards Std                                 0.151057\n",
      "exploration/Rewards Max                                 0.107483\n",
      "exploration/Rewards Min                                -0.79885\n",
      "exploration/Returns Mean                               -3.886\n",
      "exploration/Returns Std                                 2.36637\n",
      "exploration/Returns Max                                -0.93731\n",
      "exploration/Returns Min                                -8.14266\n",
      "exploration/Actions Mean                               -0.0135752\n",
      "exploration/Actions Std                                 0.134729\n",
      "exploration/Actions Max                                 0.359998\n",
      "exploration/Actions Min                                -0.541396\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -3.886\n",
      "exploration/env_infos/final/reward_dist Mean            3.07272e-11\n",
      "exploration/env_infos/final/reward_dist Std             6.14545e-11\n",
      "exploration/env_infos/final/reward_dist Max             1.53636e-10\n",
      "exploration/env_infos/final/reward_dist Min             1.13219e-75\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00113706\n",
      "exploration/env_infos/initial/reward_dist Std           0.0011281\n",
      "exploration/env_infos/initial/reward_dist Max           0.00302697\n",
      "exploration/env_infos/initial/reward_dist Min           1.98745e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0305783\n",
      "exploration/env_infos/reward_dist Std                   0.120119\n",
      "exploration/env_infos/reward_dist Max                   0.740737\n",
      "exploration/env_infos/reward_dist Min                   1.13219e-75\n",
      "exploration/env_infos/final/reward_energy Mean         -0.170091\n",
      "exploration/env_infos/final/reward_energy Std           0.12207\n",
      "exploration/env_infos/final/reward_energy Max          -0.0640755\n",
      "exploration/env_infos/final/reward_energy Min          -0.40762\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.22864\n",
      "exploration/env_infos/initial/reward_energy Std         0.145261\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0525122\n",
      "exploration/env_infos/initial/reward_energy Min        -0.489391\n",
      "exploration/env_infos/reward_energy Mean               -0.155159\n",
      "exploration/env_infos/reward_energy Std                 0.112241\n",
      "exploration/env_infos/reward_energy Max                -0.0121795\n",
      "exploration/env_infos/reward_energy Min                -0.542801\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.195889\n",
      "exploration/env_infos/final/end_effector_loc Std        0.477377\n",
      "exploration/env_infos/final/end_effector_loc Max        0.394653\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00479282\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00829158\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0116731\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0205173\n",
      "exploration/env_infos/end_effector_loc Mean            -0.109625\n",
      "exploration/env_infos/end_effector_loc Std              0.280183\n",
      "exploration/env_infos/end_effector_loc Max              0.394653\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           7000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0792104\n",
      "evaluation/Rewards Std                                  0.0736306\n",
      "evaluation/Rewards Max                                  0.102641\n",
      "evaluation/Rewards Min                                 -0.407788\n",
      "evaluation/Returns Mean                                -1.58421\n",
      "evaluation/Returns Std                                  1.05238\n",
      "evaluation/Returns Max                                  0.0967754\n",
      "evaluation/Returns Min                                 -4.2172\n",
      "evaluation/Actions Mean                                -0.00535401\n",
      "evaluation/Actions Std                                  0.0743881\n",
      "evaluation/Actions Max                                  0.525742\n",
      "evaluation/Actions Min                                 -0.738899\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.58421\n",
      "evaluation/env_infos/final/reward_dist Mean             0.162834\n",
      "evaluation/env_infos/final/reward_dist Std              0.271171\n",
      "evaluation/env_infos/final/reward_dist Max              0.931191\n",
      "evaluation/env_infos/final/reward_dist Min              6.76781e-54\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00672518\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0118008\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0442445\n",
      "evaluation/env_infos/initial/reward_dist Min            3.0342e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.147249\n",
      "evaluation/env_infos/reward_dist Std                    0.252514\n",
      "evaluation/env_infos/reward_dist Max                    0.999118\n",
      "evaluation/env_infos/reward_dist Min                    6.76781e-54\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0668445\n",
      "evaluation/env_infos/final/reward_energy Std            0.0429974\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0111899\n",
      "evaluation/env_infos/final/reward_energy Min           -0.244395\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.172766\n",
      "evaluation/env_infos/initial/reward_energy Std          0.193862\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00824561\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.769268\n",
      "evaluation/env_infos/reward_energy Mean                -0.0728739\n",
      "evaluation/env_infos/reward_energy Std                  0.076249\n",
      "evaluation/env_infos/reward_energy Max                 -0.000649051\n",
      "evaluation/env_infos/reward_energy Min                 -0.769268\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0831516\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.288532\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.332522\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean     -0.000258487\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00917724\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0262871\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0369449\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0316448\n",
      "evaluation/env_infos/end_effector_loc Std               0.184745\n",
      "evaluation/env_infos/end_effector_loc Max               0.332522\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00589812\n",
      "time/evaluation sampling (s)                            0.903825\n",
      "time/exploration sampling (s)                           0.119275\n",
      "time/logging (s)                                        0.0199945\n",
      "time/saving (s)                                         0.0272606\n",
      "time/training (s)                                      44.7508\n",
      "time/epoch (s)                                         45.827\n",
      "time/total (s)                                        304.07\n",
      "Epoch                                                   6\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:54:28.701213 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 7 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   2000\r\n",
      "trainer/QF1 Loss                                        0.00681199\r\n",
      "trainer/QF2 Loss                                        0.0050364\r\n",
      "trainer/Policy Loss                                     4.57428\r\n",
      "trainer/Q1 Predictions Mean                            -2.73911\r\n",
      "trainer/Q1 Predictions Std                              1.71652\r\n",
      "trainer/Q1 Predictions Max                              0.0181174\r\n",
      "trainer/Q1 Predictions Min                             -9.72226\r\n",
      "trainer/Q2 Predictions Mean                            -2.71108\r\n",
      "trainer/Q2 Predictions Std                              1.68856\r\n",
      "trainer/Q2 Predictions Max                              0.0174201\r\n",
      "trainer/Q2 Predictions Min                             -9.39219\r\n",
      "trainer/Q Targets Mean                                 -2.72035\r\n",
      "trainer/Q Targets Std                                   1.70165\r\n",
      "trainer/Q Targets Max                                   0.0317613\r\n",
      "trainer/Q Targets Min                                  -9.53117\r\n",
      "trainer/Log Pis Mean                                    2.06398\r\n",
      "trainer/Log Pis Std                                     1.49321\r\n",
      "trainer/Log Pis Max                                     8.84349\r\n",
      "trainer/Log Pis Min                                    -2.54917\r\n",
      "trainer/Policy mu Mean                                  0.0559917\r\n",
      "trainer/Policy mu Std                                   0.794381\r\n",
      "trainer/Policy mu Max                                   3.87972\r\n",
      "trainer/Policy mu Min                                  -2.85911\r\n",
      "trainer/Policy log std Mean                            -2.055\r\n",
      "trainer/Policy log std Std                              0.716259\r\n",
      "trainer/Policy log std Max                              0.395696\r\n",
      "trainer/Policy log std Min                             -3.2741\r\n",
      "trainer/Alpha                                           0.0164657\r\n",
      "trainer/Alpha Loss                                      0.262764\r\n",
      "exploration/num steps total                          1800\r\n",
      "exploration/num paths total                            90\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.151768\r\n",
      "exploration/Rewards Std                                 0.056797\r\n",
      "exploration/Rewards Max                                -0.039229\r\n",
      "exploration/Rewards Min                                -0.339839\r\n",
      "exploration/Returns Mean                               -3.03535\r\n",
      "exploration/Returns Std                                 0.581125\r\n",
      "exploration/Returns Max                                -2.21891\r\n",
      "exploration/Returns Min                                -3.8612\r\n",
      "exploration/Actions Mean                               -0.000202929\r\n",
      "exploration/Actions Std                                 0.114183\r\n",
      "exploration/Actions Max                                 0.655157\r\n",
      "exploration/Actions Min                                -0.47409\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -3.03535\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.024186\r\n",
      "exploration/env_infos/final/reward_dist Std             0.046795\r\n",
      "exploration/env_infos/final/reward_dist Max             0.117759\r\n",
      "exploration/env_infos/final/reward_dist Min             9.49089e-20\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00187196\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.0021543\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.00582026\r\n",
      "exploration/env_infos/initial/reward_dist Min           0.000199299\r\n",
      "exploration/env_infos/reward_dist Mean                  0.0505343\r\n",
      "exploration/env_infos/reward_dist Std                   0.128366\r\n",
      "exploration/env_infos/reward_dist Max                   0.775154\r\n",
      "exploration/env_infos/reward_dist Min                   9.49089e-20\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.138562\r\n",
      "exploration/env_infos/final/reward_energy Std           0.0539767\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.0724274\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.224472\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.17544\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.185066\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0431646\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.542937\r\n",
      "exploration/env_infos/reward_energy Mean               -0.122463\r\n",
      "exploration/env_infos/reward_energy Std                 0.105255\r\n",
      "exploration/env_infos/reward_energy Max                -0.0225659\r\n",
      "exploration/env_infos/reward_energy Min                -0.667024\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.0906072\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.253651\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.379516\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.409797\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00114376\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00894301\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0227838\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0147598\r\n",
      "exploration/env_infos/end_effector_loc Mean             0.0573177\r\n",
      "exploration/env_infos/end_effector_loc Std              0.174468\r\n",
      "exploration/env_infos/end_effector_loc Max              0.479329\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.409797\r\n",
      "evaluation/num steps total                           8000\r\n",
      "evaluation/num paths total                            400\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.0976331\r\n",
      "evaluation/Rewards Std                                  0.0820227\r\n",
      "evaluation/Rewards Max                                  0.126871\r\n",
      "evaluation/Rewards Min                                 -0.629454\r\n",
      "evaluation/Returns Mean                                -1.95266\r\n",
      "evaluation/Returns Std                                  1.25017\r\n",
      "evaluation/Returns Max                                  0.473641\r\n",
      "evaluation/Returns Min                                 -4.29378\r\n",
      "evaluation/Actions Mean                                -0.00699374\r\n",
      "evaluation/Actions Std                                  0.062958\r\n",
      "evaluation/Actions Max                                  0.53756\r\n",
      "evaluation/Actions Min                                 -0.317085\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -1.95266\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0256312\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.0708405\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.374638\r\n",
      "evaluation/env_infos/final/reward_dist Min              7.751e-61\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00519144\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00919349\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0353432\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.3794e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0691148\r\n",
      "evaluation/env_infos/reward_dist Std                    0.164294\r\n",
      "evaluation/env_infos/reward_dist Max                    0.995254\r\n",
      "evaluation/env_infos/reward_dist Min                    7.751e-61\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0777651\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.0384106\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00736555\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.21387\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.159041\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.118669\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0232223\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.548265\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.0723762\r\n",
      "evaluation/env_infos/reward_energy Std                  0.0527914\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00281106\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.548265\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.00271826\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.323918\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.864556\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.643009\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.0032554\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00621471\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.026878\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0109632\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0289922\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.18374\r\n",
      "evaluation/env_infos/end_effector_loc Max               0.864556\r\n",
      "evaluation/env_infos/end_effector_loc Min              -0.643009\r\n",
      "time/data storing (s)                                   0.00645136\r\n",
      "time/evaluation sampling (s)                            1.07677\r\n",
      "time/exploration sampling (s)                           0.132517\r\n",
      "time/logging (s)                                        0.018259\r\n",
      "time/saving (s)                                         0.0271046\r\n",
      "time/training (s)                                      45.0058\r\n",
      "time/epoch (s)                                         46.2669\r\n",
      "time/total (s)                                        350.452\r\n",
      "Epoch                                                   7\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:55:17.635194 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 8 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   2000\r\n",
      "trainer/QF1 Loss                                        0.00994483\r\n",
      "trainer/QF2 Loss                                        0.00419931\r\n",
      "trainer/Policy Loss                                     4.35377\r\n",
      "trainer/Q1 Predictions Mean                            -2.58492\r\n",
      "trainer/Q1 Predictions Std                              1.73543\r\n",
      "trainer/Q1 Predictions Max                              0.0519852\r\n",
      "trainer/Q1 Predictions Min                             -9.48846\r\n",
      "trainer/Q2 Predictions Mean                            -2.61422\r\n",
      "trainer/Q2 Predictions Std                              1.72955\r\n",
      "trainer/Q2 Predictions Max                             -0.00511848\r\n",
      "trainer/Q2 Predictions Min                             -9.60227\r\n",
      "trainer/Q Targets Mean                                 -2.58832\r\n",
      "trainer/Q Targets Std                                   1.72475\r\n",
      "trainer/Q Targets Max                                   0.0104756\r\n",
      "trainer/Q Targets Min                                  -9.57905\r\n",
      "trainer/Log Pis Mean                                    1.96961\r\n",
      "trainer/Log Pis Std                                     1.3962\r\n",
      "trainer/Log Pis Max                                     6.36898\r\n",
      "trainer/Log Pis Min                                    -2.26972\r\n",
      "trainer/Policy mu Mean                                 -0.0878941\r\n",
      "trainer/Policy mu Std                                   0.534441\r\n",
      "trainer/Policy mu Max                                   3.22847\r\n",
      "trainer/Policy mu Min                                  -2.42123\r\n",
      "trainer/Policy log std Mean                            -2.18484\r\n",
      "trainer/Policy log std Std                              0.632337\r\n",
      "trainer/Policy log std Max                             -0.0397819\r\n",
      "trainer/Policy log std Min                             -3.29879\r\n",
      "trainer/Alpha                                           0.0169114\r\n",
      "trainer/Alpha Loss                                     -0.123994\r\n",
      "exploration/num steps total                          1900\r\n",
      "exploration/num paths total                            95\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.106803\r\n",
      "exploration/Rewards Std                                 0.0609244\r\n",
      "exploration/Rewards Max                                -0.0105959\r\n",
      "exploration/Rewards Min                                -0.348127\r\n",
      "exploration/Returns Mean                               -2.13606\r\n",
      "exploration/Returns Std                                 0.539058\r\n",
      "exploration/Returns Max                                -1.51124\r\n",
      "exploration/Returns Min                                -3.04686\r\n",
      "exploration/Actions Mean                               -0.0133414\r\n",
      "exploration/Actions Std                                 0.12327\r\n",
      "exploration/Actions Max                                 0.431672\r\n",
      "exploration/Actions Min                                -0.470231\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -2.13606\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.000471733\r\n",
      "exploration/env_infos/final/reward_dist Std             0.000845769\r\n",
      "exploration/env_infos/final/reward_dist Max             0.00215597\r\n",
      "exploration/env_infos/final/reward_dist Min             6.65339e-12\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00374005\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.00684395\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0174226\r\n",
      "exploration/env_infos/initial/reward_dist Min           4.00697e-05\r\n",
      "exploration/env_infos/reward_dist Mean                  0.00685885\r\n",
      "exploration/env_infos/reward_dist Std                   0.0161518\r\n",
      "exploration/env_infos/reward_dist Max                   0.0849804\r\n",
      "exploration/env_infos/reward_dist Min                   6.65339e-12\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.180666\r\n",
      "exploration/env_infos/final/reward_energy Std           0.08952\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.0993884\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.293473\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.157402\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.0959166\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.00479835\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.303022\r\n",
      "exploration/env_infos/reward_energy Mean               -0.135994\r\n",
      "exploration/env_infos/reward_energy Std                 0.110691\r\n",
      "exploration/env_infos/reward_energy Max                -0.00479835\r\n",
      "exploration/env_infos/reward_energy Min                -0.60868\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0754738\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.221463\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.376744\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.322015\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00162107\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00631199\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0140107\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.00810763\r\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0195363\r\n",
      "exploration/env_infos/end_effector_loc Std              0.118954\r\n",
      "exploration/env_infos/end_effector_loc Max              0.376744\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.322015\r\n",
      "evaluation/num steps total                           9000\r\n",
      "evaluation/num paths total                            450\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.0816801\r\n",
      "evaluation/Rewards Std                                  0.0749066\r\n",
      "evaluation/Rewards Max                                  0.152933\r\n",
      "evaluation/Rewards Min                                 -0.401618\r\n",
      "evaluation/Returns Mean                                -1.6336\r\n",
      "evaluation/Returns Std                                  1.26406\r\n",
      "evaluation/Returns Max                                  0.863669\r\n",
      "evaluation/Returns Min                                 -4.98829\r\n",
      "evaluation/Actions Mean                                -0.00633825\r\n",
      "evaluation/Actions Std                                  0.0486527\r\n",
      "evaluation/Actions Max                                  0.484544\r\n",
      "evaluation/Actions Min                                 -0.260106\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -1.6336\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.105194\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.18329\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.751818\r\n",
      "evaluation/env_infos/final/reward_dist Min              2.94776e-25\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00270468\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00647802\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0317762\r\n",
      "evaluation/env_infos/initial/reward_dist Min            9.9632e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0711579\r\n",
      "evaluation/env_infos/reward_dist Std                    0.155203\r\n",
      "evaluation/env_infos/reward_dist Max                    0.982466\r\n",
      "evaluation/env_infos/reward_dist Min                    2.94776e-25\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0535455\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.0336623\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00245482\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.173874\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.119997\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.108016\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00577529\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.52548\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.0537148\r\n",
      "evaluation/env_infos/reward_energy Std                  0.043923\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.000876132\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.52548\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0176926\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.218109\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.43119\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.572256\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00250782\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00512781\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0242272\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00714639\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.01236\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.129452\r\n",
      "evaluation/env_infos/end_effector_loc Max               0.43119\r\n",
      "evaluation/env_infos/end_effector_loc Min              -0.572256\r\n",
      "time/data storing (s)                                   0.00662246\r\n",
      "time/evaluation sampling (s)                            0.973708\r\n",
      "time/exploration sampling (s)                           0.125896\r\n",
      "time/logging (s)                                        0.0198462\r\n",
      "time/saving (s)                                         0.027549\r\n",
      "time/training (s)                                      47.6558\r\n",
      "time/epoch (s)                                         48.8094\r\n",
      "time/total (s)                                        399.387\r\n",
      "Epoch                                                   8\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:56:05.252162 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 9 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00570543\r\n",
      "trainer/QF2 Loss                                         0.0038679\r\n",
      "trainer/Policy Loss                                      4.13852\r\n",
      "trainer/Q1 Predictions Mean                             -2.14128\r\n",
      "trainer/Q1 Predictions Std                               1.41236\r\n",
      "trainer/Q1 Predictions Max                              -0.0427018\r\n",
      "trainer/Q1 Predictions Min                              -7.35702\r\n",
      "trainer/Q2 Predictions Mean                             -2.1649\r\n",
      "trainer/Q2 Predictions Std                               1.42104\r\n",
      "trainer/Q2 Predictions Max                              -0.0447829\r\n",
      "trainer/Q2 Predictions Min                              -7.18682\r\n",
      "trainer/Q Targets Mean                                  -2.15307\r\n",
      "trainer/Q Targets Std                                    1.41476\r\n",
      "trainer/Q Targets Max                                   -0.010761\r\n",
      "trainer/Q Targets Min                                   -7.31529\r\n",
      "trainer/Log Pis Mean                                     2.12815\r\n",
      "trainer/Log Pis Std                                      1.43175\r\n",
      "trainer/Log Pis Max                                      6.80139\r\n",
      "trainer/Log Pis Min                                     -4.51095\r\n",
      "trainer/Policy mu Mean                                  -0.0668423\r\n",
      "trainer/Policy mu Std                                    0.485394\r\n",
      "trainer/Policy mu Max                                    2.75377\r\n",
      "trainer/Policy mu Min                                   -2.80817\r\n",
      "trainer/Policy log std Mean                             -2.32726\r\n",
      "trainer/Policy log std Std                               0.617717\r\n",
      "trainer/Policy log std Max                               0.108484\r\n",
      "trainer/Policy log std Min                              -3.33017\r\n",
      "trainer/Alpha                                            0.0174049\r\n",
      "trainer/Alpha Loss                                       0.519197\r\n",
      "exploration/num steps total                           2000\r\n",
      "exploration/num paths total                            100\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.112829\r\n",
      "exploration/Rewards Std                                  0.0548481\r\n",
      "exploration/Rewards Max                                 -0.00911166\r\n",
      "exploration/Rewards Min                                 -0.264598\r\n",
      "exploration/Returns Mean                                -2.25657\r\n",
      "exploration/Returns Std                                  0.741188\r\n",
      "exploration/Returns Max                                 -1.09547\r\n",
      "exploration/Returns Min                                 -3.31923\r\n",
      "exploration/Actions Mean                                -0.011143\r\n",
      "exploration/Actions Std                                  0.156428\r\n",
      "exploration/Actions Max                                  0.664053\r\n",
      "exploration/Actions Min                                 -0.723389\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.25657\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0296076\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0451877\r\n",
      "exploration/env_infos/final/reward_dist Max              0.116985\r\n",
      "exploration/env_infos/final/reward_dist Min              9.08166e-20\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00153842\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00187138\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0052305\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.00016646\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0813231\r\n",
      "exploration/env_infos/reward_dist Std                    0.220469\r\n",
      "exploration/env_infos/reward_dist Max                    0.934222\r\n",
      "exploration/env_infos/reward_dist Min                    9.08166e-20\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.126982\r\n",
      "exploration/env_infos/final/reward_energy Std            0.076099\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0393261\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.238642\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.281439\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.246975\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0656877\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.67177\r\n",
      "exploration/env_infos/reward_energy Mean                -0.159663\r\n",
      "exploration/env_infos/reward_energy Std                  0.153934\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0131872\r\n",
      "exploration/env_infos/reward_energy Min                 -0.770136\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0358385\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.231671\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.274591\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.445407\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0051081\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0122132\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0332027\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00529771\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00960494\r\n",
      "exploration/env_infos/end_effector_loc Std               0.138768\r\n",
      "exploration/env_infos/end_effector_loc Max               0.274591\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.445407\r\n",
      "evaluation/num steps total                           10000\r\n",
      "evaluation/num paths total                             500\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.101219\r\n",
      "evaluation/Rewards Std                                   0.0766764\r\n",
      "evaluation/Rewards Max                                   0.160141\r\n",
      "evaluation/Rewards Min                                  -0.43223\r\n",
      "evaluation/Returns Mean                                 -2.02438\r\n",
      "evaluation/Returns Std                                   0.940026\r\n",
      "evaluation/Returns Max                                   0.800924\r\n",
      "evaluation/Returns Min                                  -4.48966\r\n",
      "evaluation/Actions Mean                                 -0.0159762\r\n",
      "evaluation/Actions Std                                   0.0635802\r\n",
      "evaluation/Actions Max                                   0.538648\r\n",
      "evaluation/Actions Min                                  -0.73726\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.02438\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.00350949\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.0184681\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.122934\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.09351e-41\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00895051\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126317\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0482769\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.49159e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0545728\r\n",
      "evaluation/env_infos/reward_dist Std                     0.156991\r\n",
      "evaluation/env_infos/reward_dist Max                     0.985992\r\n",
      "evaluation/env_infos/reward_dist Min                     6.09351e-41\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0616133\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0371605\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0129536\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.176696\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.161233\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.161781\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00627345\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.892844\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0699691\r\n",
      "evaluation/env_infos/reward_energy Std                   0.060825\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000186554\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.892844\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.14569\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.316395\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.590499\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.8195\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       1.70531e-05\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00807535\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0269324\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.036863\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.045655\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.198681\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.590499\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.8195\r\n",
      "time/data storing (s)                                    0.00591018\r\n",
      "time/evaluation sampling (s)                             1.09492\r\n",
      "time/exploration sampling (s)                            0.123204\r\n",
      "time/logging (s)                                         0.0203079\r\n",
      "time/saving (s)                                          0.0299642\r\n",
      "time/training (s)                                       46.2004\r\n",
      "time/epoch (s)                                          47.4747\r\n",
      "time/total (s)                                         447.004\r\n",
      "Epoch                                                    9\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:56:53.967325 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 10 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00275141\n",
      "trainer/QF2 Loss                                         0.00233624\n",
      "trainer/Policy Loss                                      3.86375\n",
      "trainer/Q1 Predictions Mean                             -2.05558\n",
      "trainer/Q1 Predictions Std                               1.50876\n",
      "trainer/Q1 Predictions Max                               0.00440224\n",
      "trainer/Q1 Predictions Min                              -8.77215\n",
      "trainer/Q2 Predictions Mean                             -2.04684\n",
      "trainer/Q2 Predictions Std                               1.50415\n",
      "trainer/Q2 Predictions Max                               0.0158683\n",
      "trainer/Q2 Predictions Min                              -8.63884\n",
      "trainer/Q Targets Mean                                  -2.05325\n",
      "trainer/Q Targets Std                                    1.51062\n",
      "trainer/Q Targets Max                                    0.0489913\n",
      "trainer/Q Targets Min                                   -8.68765\n",
      "trainer/Log Pis Mean                                     1.95703\n",
      "trainer/Log Pis Std                                      1.42759\n",
      "trainer/Log Pis Max                                      7.95479\n",
      "trainer/Log Pis Min                                     -2.57103\n",
      "trainer/Policy mu Mean                                  -0.0398943\n",
      "trainer/Policy mu Std                                    0.530987\n",
      "trainer/Policy mu Max                                    3.23659\n",
      "trainer/Policy mu Min                                   -3.07985\n",
      "trainer/Policy log std Mean                             -2.16581\n",
      "trainer/Policy log std Std                               0.633642\n",
      "trainer/Policy log std Max                              -0.330272\n",
      "trainer/Policy log std Min                              -3.27722\n",
      "trainer/Alpha                                            0.0184243\n",
      "trainer/Alpha Loss                                      -0.171646\n",
      "exploration/num steps total                           2100\n",
      "exploration/num paths total                            105\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.135718\n",
      "exploration/Rewards Std                                  0.0651935\n",
      "exploration/Rewards Max                                 -0.000324864\n",
      "exploration/Rewards Min                                 -0.34746\n",
      "exploration/Returns Mean                                -2.71436\n",
      "exploration/Returns Std                                  0.96212\n",
      "exploration/Returns Max                                 -1.76606\n",
      "exploration/Returns Min                                 -4.4496\n",
      "exploration/Actions Mean                                -0.00734716\n",
      "exploration/Actions Std                                  0.153289\n",
      "exploration/Actions Max                                  0.409148\n",
      "exploration/Actions Min                                 -0.645\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.71436\n",
      "exploration/env_infos/final/reward_dist Mean             0.101659\n",
      "exploration/env_infos/final/reward_dist Std              0.127008\n",
      "exploration/env_infos/final/reward_dist Max              0.299808\n",
      "exploration/env_infos/final/reward_dist Min              5.06065e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00107079\n",
      "exploration/env_infos/initial/reward_dist Std            0.00117183\n",
      "exploration/env_infos/initial/reward_dist Max            0.00330836\n",
      "exploration/env_infos/initial/reward_dist Min            5.26536e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.168609\n",
      "exploration/env_infos/reward_dist Std                    0.268213\n",
      "exploration/env_infos/reward_dist Max                    0.992729\n",
      "exploration/env_infos/reward_dist Min                    2.74707e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.156667\n",
      "exploration/env_infos/final/reward_energy Std            0.0277529\n",
      "exploration/env_infos/final/reward_energy Max           -0.120024\n",
      "exploration/env_infos/final/reward_energy Min           -0.19781\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.178458\n",
      "exploration/env_infos/initial/reward_energy Std          0.108537\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0664656\n",
      "exploration/env_infos/initial/reward_energy Min         -0.369194\n",
      "exploration/env_infos/reward_energy Mean                -0.17214\n",
      "exploration/env_infos/reward_energy Std                  0.132176\n",
      "exploration/env_infos/reward_energy Max                 -0.0144251\n",
      "exploration/env_infos/reward_energy Min                 -0.690698\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00573991\n",
      "exploration/env_infos/final/end_effector_loc Std         0.230338\n",
      "exploration/env_infos/final/end_effector_loc Max         0.537435\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.371753\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00323686\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00663756\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0160918\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00990321\n",
      "exploration/env_infos/end_effector_loc Mean              0.0445197\n",
      "exploration/env_infos/end_effector_loc Std               0.184137\n",
      "exploration/env_infos/end_effector_loc Max               0.551264\n",
      "exploration/env_infos/end_effector_loc Min              -0.371753\n",
      "evaluation/num steps total                           11000\n",
      "evaluation/num paths total                             550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0799058\n",
      "evaluation/Rewards Std                                   0.0726554\n",
      "evaluation/Rewards Max                                   0.115023\n",
      "evaluation/Rewards Min                                  -0.368163\n",
      "evaluation/Returns Mean                                 -1.59812\n",
      "evaluation/Returns Std                                   1.09957\n",
      "evaluation/Returns Max                                   0.645098\n",
      "evaluation/Returns Min                                  -4.40284\n",
      "evaluation/Actions Mean                                 -0.00821679\n",
      "evaluation/Actions Std                                   0.0867123\n",
      "evaluation/Actions Max                                   0.68083\n",
      "evaluation/Actions Min                                  -0.699399\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.59812\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0404476\n",
      "evaluation/env_infos/final/reward_dist Std               0.106274\n",
      "evaluation/env_infos/final/reward_dist Max               0.441244\n",
      "evaluation/env_infos/final/reward_dist Min               1.36793e-64\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00503769\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00993891\n",
      "evaluation/env_infos/initial/reward_dist Max             0.045878\n",
      "evaluation/env_infos/initial/reward_dist Min             1.58702e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0798276\n",
      "evaluation/env_infos/reward_dist Std                     0.182767\n",
      "evaluation/env_infos/reward_dist Max                     0.997547\n",
      "evaluation/env_infos/reward_dist Min                     1.11471e-64\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0892516\n",
      "evaluation/env_infos/final/reward_energy Std             0.058326\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00289591\n",
      "evaluation/env_infos/final/reward_energy Min            -0.256876\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.173628\n",
      "evaluation/env_infos/initial/reward_energy Std           0.195496\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.010487\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.877439\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0806106\n",
      "evaluation/env_infos/reward_energy Std                   0.0931397\n",
      "evaluation/env_infos/reward_energy Max                  -0.000882694\n",
      "evaluation/env_infos/reward_energy Min                  -0.877439\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0506597\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.304094\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.587053\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000760987\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0092129\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0340415\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0276754\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.000761707\n",
      "evaluation/env_infos/end_effector_loc Std                0.199944\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.587053\n",
      "time/data storing (s)                                    0.00626315\n",
      "time/evaluation sampling (s)                             1.34677\n",
      "time/exploration sampling (s)                            0.149854\n",
      "time/logging (s)                                         0.0183086\n",
      "time/saving (s)                                          0.0259565\n",
      "time/training (s)                                       46.9968\n",
      "time/epoch (s)                                          48.5439\n",
      "time/total (s)                                         495.717\n",
      "Epoch                                                   10\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:57:42.038544 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 11 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00366848\n",
      "trainer/QF2 Loss                                         0.00334031\n",
      "trainer/Policy Loss                                      3.62385\n",
      "trainer/Q1 Predictions Mean                             -1.61424\n",
      "trainer/Q1 Predictions Std                               0.926969\n",
      "trainer/Q1 Predictions Max                              -0.0116021\n",
      "trainer/Q1 Predictions Min                              -4.71361\n",
      "trainer/Q2 Predictions Mean                             -1.63588\n",
      "trainer/Q2 Predictions Std                               0.929444\n",
      "trainer/Q2 Predictions Max                              -0.069615\n",
      "trainer/Q2 Predictions Min                              -4.73076\n",
      "trainer/Q Targets Mean                                  -1.65231\n",
      "trainer/Q Targets Std                                    0.931405\n",
      "trainer/Q Targets Max                                   -0.0383528\n",
      "trainer/Q Targets Min                                   -4.78057\n",
      "trainer/Log Pis Mean                                     2.0443\n",
      "trainer/Log Pis Std                                      1.38234\n",
      "trainer/Log Pis Max                                      6.58164\n",
      "trainer/Log Pis Min                                     -3.66613\n",
      "trainer/Policy mu Mean                                  -0.0466545\n",
      "trainer/Policy mu Std                                    0.463625\n",
      "trainer/Policy mu Max                                    2.33464\n",
      "trainer/Policy mu Min                                   -2.32429\n",
      "trainer/Policy log std Mean                             -2.24442\n",
      "trainer/Policy log std Std                               0.66171\n",
      "trainer/Policy log std Max                              -0.120501\n",
      "trainer/Policy log std Min                              -3.23926\n",
      "trainer/Alpha                                            0.0188339\n",
      "trainer/Alpha Loss                                       0.175949\n",
      "exploration/num steps total                           2200\n",
      "exploration/num paths total                            110\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.132709\n",
      "exploration/Rewards Std                                  0.0760235\n",
      "exploration/Rewards Max                                  0.0584705\n",
      "exploration/Rewards Min                                 -0.342048\n",
      "exploration/Returns Mean                                -2.65418\n",
      "exploration/Returns Std                                  0.632381\n",
      "exploration/Returns Max                                 -1.72654\n",
      "exploration/Returns Min                                 -3.47055\n",
      "exploration/Actions Mean                                -0.0175046\n",
      "exploration/Actions Std                                  0.10759\n",
      "exploration/Actions Max                                  0.427881\n",
      "exploration/Actions Min                                 -0.307822\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.65418\n",
      "exploration/env_infos/final/reward_dist Mean             0.000222043\n",
      "exploration/env_infos/final/reward_dist Std              0.000444086\n",
      "exploration/env_infos/final/reward_dist Max              0.00111021\n",
      "exploration/env_infos/final/reward_dist Min              1.63818e-25\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000620251\n",
      "exploration/env_infos/initial/reward_dist Std            0.00113197\n",
      "exploration/env_infos/initial/reward_dist Max            0.00288239\n",
      "exploration/env_infos/initial/reward_dist Min            4.90761e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0237388\n",
      "exploration/env_infos/reward_dist Std                    0.0778706\n",
      "exploration/env_infos/reward_dist Max                    0.50168\n",
      "exploration/env_infos/reward_dist Min                    1.63818e-25\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135957\n",
      "exploration/env_infos/final/reward_energy Std            0.0913599\n",
      "exploration/env_infos/final/reward_energy Max           -0.0586493\n",
      "exploration/env_infos/final/reward_energy Min           -0.313338\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.186909\n",
      "exploration/env_infos/initial/reward_energy Std          0.0374193\n",
      "exploration/env_infos/initial/reward_energy Max         -0.130316\n",
      "exploration/env_infos/initial/reward_energy Min         -0.24\n",
      "exploration/env_infos/reward_energy Mean                -0.122963\n",
      "exploration/env_infos/reward_energy Std                  0.0929749\n",
      "exploration/env_infos/reward_energy Max                 -0.00114141\n",
      "exploration/env_infos/reward_energy Min                 -0.454508\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.222504\n",
      "exploration/env_infos/final/end_effector_loc Std         0.351814\n",
      "exploration/env_infos/final/end_effector_loc Max         0.403262\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.674855\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00245695\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00627554\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00836874\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00934002\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0859449\n",
      "exploration/env_infos/end_effector_loc Std               0.208746\n",
      "exploration/env_infos/end_effector_loc Max               0.403262\n",
      "exploration/env_infos/end_effector_loc Min              -0.674855\n",
      "evaluation/num steps total                           12000\n",
      "evaluation/num paths total                             600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0925927\n",
      "evaluation/Rewards Std                                   0.0980895\n",
      "evaluation/Rewards Max                                   0.131017\n",
      "evaluation/Rewards Min                                  -0.778961\n",
      "evaluation/Returns Mean                                 -1.85185\n",
      "evaluation/Returns Std                                   1.12701\n",
      "evaluation/Returns Max                                   0.837028\n",
      "evaluation/Returns Min                                  -4.55695\n",
      "evaluation/Actions Mean                                 -0.0142128\n",
      "evaluation/Actions Std                                   0.0659355\n",
      "evaluation/Actions Max                                   0.778425\n",
      "evaluation/Actions Min                                  -0.866108\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.85185\n",
      "evaluation/env_infos/final/reward_dist Mean              0.00835731\n",
      "evaluation/env_infos/final/reward_dist Std               0.0457097\n",
      "evaluation/env_infos/final/reward_dist Max               0.326953\n",
      "evaluation/env_infos/final/reward_dist Min               6.31363e-67\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00552242\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111337\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0567216\n",
      "evaluation/env_infos/initial/reward_dist Min             1.00926e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0752299\n",
      "evaluation/env_infos/reward_dist Std                     0.166338\n",
      "evaluation/env_infos/reward_dist Max                     0.963448\n",
      "evaluation/env_infos/reward_dist Min                     6.31363e-67\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.060563\n",
      "evaluation/env_infos/final/reward_energy Std             0.0351525\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0119427\n",
      "evaluation/env_infos/final/reward_energy Min            -0.197346\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.177346\n",
      "evaluation/env_infos/initial/reward_energy Std           0.196418\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00582724\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.16451\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0647304\n",
      "evaluation/env_infos/reward_energy Std                   0.070064\n",
      "evaluation/env_infos/reward_energy Max                  -0.00111532\n",
      "evaluation/env_infos/reward_energy Min                  -1.16451\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.165976\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.366543\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.97527\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.893346\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0015304\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00923024\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0389212\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0433054\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0669858\n",
      "evaluation/env_infos/end_effector_loc Std                0.22251\n",
      "evaluation/env_infos/end_effector_loc Max                0.97527\n",
      "evaluation/env_infos/end_effector_loc Min               -0.913898\n",
      "time/data storing (s)                                    0.00607391\n",
      "time/evaluation sampling (s)                             0.98911\n",
      "time/exploration sampling (s)                            0.126382\n",
      "time/logging (s)                                         0.0201478\n",
      "time/saving (s)                                          0.0286661\n",
      "time/training (s)                                       46.7412\n",
      "time/epoch (s)                                          47.9116\n",
      "time/total (s)                                         543.789\n",
      "Epoch                                                   11\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:58:30.140414 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 12 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00250185\r\n",
      "trainer/QF2 Loss                                         0.00245036\r\n",
      "trainer/Policy Loss                                      3.53172\r\n",
      "trainer/Q1 Predictions Mean                             -1.63861\r\n",
      "trainer/Q1 Predictions Std                               0.978148\r\n",
      "trainer/Q1 Predictions Max                               0.018596\r\n",
      "trainer/Q1 Predictions Min                              -4.5449\r\n",
      "trainer/Q2 Predictions Mean                             -1.65775\r\n",
      "trainer/Q2 Predictions Std                               0.983787\r\n",
      "trainer/Q2 Predictions Max                               0.00572579\r\n",
      "trainer/Q2 Predictions Min                              -4.72638\r\n",
      "trainer/Q Targets Mean                                  -1.63617\r\n",
      "trainer/Q Targets Std                                    0.991383\r\n",
      "trainer/Q Targets Max                                    0.0232618\r\n",
      "trainer/Q Targets Min                                   -4.72126\r\n",
      "trainer/Log Pis Mean                                     1.91835\r\n",
      "trainer/Log Pis Std                                      1.22256\r\n",
      "trainer/Log Pis Max                                      4.3025\r\n",
      "trainer/Log Pis Min                                     -2.38101\r\n",
      "trainer/Policy mu Mean                                  -0.00719113\r\n",
      "trainer/Policy mu Std                                    0.432368\r\n",
      "trainer/Policy mu Max                                    2.44071\r\n",
      "trainer/Policy mu Min                                   -2.07361\r\n",
      "trainer/Policy log std Mean                             -2.18339\r\n",
      "trainer/Policy log std Std                               0.648827\r\n",
      "trainer/Policy log std Max                              -0.290477\r\n",
      "trainer/Policy log std Min                              -3.17749\r\n",
      "trainer/Alpha                                            0.0206643\r\n",
      "trainer/Alpha Loss                                      -0.316761\r\n",
      "exploration/num steps total                           2300\r\n",
      "exploration/num paths total                            115\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.122056\r\n",
      "exploration/Rewards Std                                  0.0509369\r\n",
      "exploration/Rewards Max                                 -0.0182913\r\n",
      "exploration/Rewards Min                                 -0.277372\r\n",
      "exploration/Returns Mean                                -2.44111\r\n",
      "exploration/Returns Std                                  0.597463\r\n",
      "exploration/Returns Max                                 -1.76044\r\n",
      "exploration/Returns Min                                 -3.31742\r\n",
      "exploration/Actions Mean                                -0.00590489\r\n",
      "exploration/Actions Std                                  0.0759155\r\n",
      "exploration/Actions Max                                  0.191585\r\n",
      "exploration/Actions Min                                 -0.227421\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.44111\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0616053\r\n",
      "exploration/env_infos/final/reward_dist Std              0.109231\r\n",
      "exploration/env_infos/final/reward_dist Max              0.279107\r\n",
      "exploration/env_infos/final/reward_dist Min              7.89324e-14\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00139932\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00240292\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00616464\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.86494e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0879937\r\n",
      "exploration/env_infos/reward_dist Std                    0.16292\r\n",
      "exploration/env_infos/reward_dist Max                    0.683931\r\n",
      "exploration/env_infos/reward_dist Min                    7.89324e-14\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0709055\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0198201\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0367124\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.0899354\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.134594\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0604049\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0515494\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.229082\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0950897\r\n",
      "exploration/env_infos/reward_energy Std                  0.0505372\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00785275\r\n",
      "exploration/env_infos/reward_energy Min                 -0.258165\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.028851\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.235563\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.286523\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.364299\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00190716\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0048547\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0085407\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00763239\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00127603\r\n",
      "exploration/env_infos/end_effector_loc Std               0.133346\r\n",
      "exploration/env_infos/end_effector_loc Max               0.286523\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.364299\r\n",
      "evaluation/num steps total                           13000\r\n",
      "evaluation/num paths total                             650\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0920834\r\n",
      "evaluation/Rewards Std                                   0.109941\r\n",
      "evaluation/Rewards Max                                   0.140065\r\n",
      "evaluation/Rewards Min                                  -0.856727\r\n",
      "evaluation/Returns Mean                                 -1.84167\r\n",
      "evaluation/Returns Std                                   1.45906\r\n",
      "evaluation/Returns Max                                   0.826059\r\n",
      "evaluation/Returns Min                                  -6.02818\r\n",
      "evaluation/Actions Mean                                 -0.0087239\r\n",
      "evaluation/Actions Std                                   0.0769694\r\n",
      "evaluation/Actions Max                                   0.987338\r\n",
      "evaluation/Actions Min                                  -0.558298\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.84167\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0692281\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.184134\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.951861\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.83709e-77\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.010355\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0178232\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0759857\r\n",
      "evaluation/env_infos/initial/reward_dist Min             8.01056e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0729793\r\n",
      "evaluation/env_infos/reward_dist Std                     0.169926\r\n",
      "evaluation/env_infos/reward_dist Max                     0.989288\r\n",
      "evaluation/env_infos/reward_dist Min                     2.94199e-77\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0830241\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0832029\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00773428\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.460519\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.200531\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.236028\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00774446\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.06212\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0696662\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0845423\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00276939\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.06212\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0601074\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.361944\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.877023\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000923082\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010911\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0493669\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0279149\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00911576\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.218\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.877023\r\n",
      "time/data storing (s)                                    0.00598071\r\n",
      "time/evaluation sampling (s)                             1.04782\r\n",
      "time/exploration sampling (s)                            0.132728\r\n",
      "time/logging (s)                                         0.0218943\r\n",
      "time/saving (s)                                          0.0273118\r\n",
      "time/training (s)                                       46.684\r\n",
      "time/epoch (s)                                          47.9197\r\n",
      "time/total (s)                                         591.892\r\n",
      "Epoch                                                   12\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:59:15.694623 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 13 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0012955\n",
      "trainer/QF2 Loss                                         0.00228723\n",
      "trainer/Policy Loss                                      3.61876\n",
      "trainer/Q1 Predictions Mean                             -1.7005\n",
      "trainer/Q1 Predictions Std                               1.01655\n",
      "trainer/Q1 Predictions Max                               0.14465\n",
      "trainer/Q1 Predictions Min                              -4.4941\n",
      "trainer/Q2 Predictions Mean                             -1.6974\n",
      "trainer/Q2 Predictions Std                               1.01252\n",
      "trainer/Q2 Predictions Max                               0.170289\n",
      "trainer/Q2 Predictions Min                              -4.45331\n",
      "trainer/Q Targets Mean                                  -1.68377\n",
      "trainer/Q Targets Std                                    1.01025\n",
      "trainer/Q Targets Max                                    0.144828\n",
      "trainer/Q Targets Min                                   -4.45422\n",
      "trainer/Log Pis Mean                                     1.94956\n",
      "trainer/Log Pis Std                                      1.2712\n",
      "trainer/Log Pis Max                                      4.27738\n",
      "trainer/Log Pis Min                                     -2.89218\n",
      "trainer/Policy mu Mean                                   0.0254888\n",
      "trainer/Policy mu Std                                    0.358754\n",
      "trainer/Policy mu Max                                    2.94945\n",
      "trainer/Policy mu Min                                   -1.92481\n",
      "trainer/Policy log std Mean                             -2.29035\n",
      "trainer/Policy log std Std                               0.570859\n",
      "trainer/Policy log std Max                              -0.158431\n",
      "trainer/Policy log std Min                              -3.17641\n",
      "trainer/Alpha                                            0.0216763\n",
      "trainer/Alpha Loss                                      -0.193248\n",
      "exploration/num steps total                           2400\n",
      "exploration/num paths total                            120\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.106439\n",
      "exploration/Rewards Std                                  0.0852174\n",
      "exploration/Rewards Max                                  0.0463165\n",
      "exploration/Rewards Min                                 -0.313761\n",
      "exploration/Returns Mean                                -2.12877\n",
      "exploration/Returns Std                                  1.3658\n",
      "exploration/Returns Max                                 -0.0863483\n",
      "exploration/Returns Min                                 -3.88847\n",
      "exploration/Actions Mean                                -0.0257116\n",
      "exploration/Actions Std                                  0.108557\n",
      "exploration/Actions Max                                  0.265853\n",
      "exploration/Actions Min                                 -0.344811\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.12877\n",
      "exploration/env_infos/final/reward_dist Mean             0.00109224\n",
      "exploration/env_infos/final/reward_dist Std              0.00218447\n",
      "exploration/env_infos/final/reward_dist Max              0.00546118\n",
      "exploration/env_infos/final/reward_dist Min              3.75861e-37\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000726822\n",
      "exploration/env_infos/initial/reward_dist Std            0.00136319\n",
      "exploration/env_infos/initial/reward_dist Max            0.00345033\n",
      "exploration/env_infos/initial/reward_dist Min            3.42271e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0387978\n",
      "exploration/env_infos/reward_dist Std                    0.0841243\n",
      "exploration/env_infos/reward_dist Max                    0.357431\n",
      "exploration/env_infos/reward_dist Min                    3.75861e-37\n",
      "exploration/env_infos/final/reward_energy Mean          -0.128203\n",
      "exploration/env_infos/final/reward_energy Std            0.0969465\n",
      "exploration/env_infos/final/reward_energy Max           -0.0127765\n",
      "exploration/env_infos/final/reward_energy Min           -0.271541\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.153828\n",
      "exploration/env_infos/initial/reward_energy Std          0.0605926\n",
      "exploration/env_infos/initial/reward_energy Max         -0.087363\n",
      "exploration/env_infos/initial/reward_energy Min         -0.262014\n",
      "exploration/env_infos/reward_energy Mean                -0.141965\n",
      "exploration/env_infos/reward_energy Std                  0.0688281\n",
      "exploration/env_infos/reward_energy Max                 -0.0127765\n",
      "exploration/env_infos/reward_energy Min                 -0.362064\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.174265\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275938\n",
      "exploration/env_infos/final/end_effector_loc Max         0.121613\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.850144\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00100584\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00575816\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00732828\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0100697\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0492843\n",
      "exploration/env_infos/end_effector_loc Std               0.160273\n",
      "exploration/env_infos/end_effector_loc Max               0.140062\n",
      "exploration/env_infos/end_effector_loc Min              -0.850144\n",
      "evaluation/num steps total                           14000\n",
      "evaluation/num paths total                             700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.075538\n",
      "evaluation/Rewards Std                                   0.085953\n",
      "evaluation/Rewards Max                                   0.119086\n",
      "evaluation/Rewards Min                                  -0.711455\n",
      "evaluation/Returns Mean                                 -1.51076\n",
      "evaluation/Returns Std                                   1.18248\n",
      "evaluation/Returns Max                                   1.21209\n",
      "evaluation/Returns Min                                  -4.98247\n",
      "evaluation/Actions Mean                                 -0.00611581\n",
      "evaluation/Actions Std                                   0.0489828\n",
      "evaluation/Actions Max                                   0.633947\n",
      "evaluation/Actions Min                                  -0.38649\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.51076\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0690326\n",
      "evaluation/env_infos/final/reward_dist Std               0.153619\n",
      "evaluation/env_infos/final/reward_dist Max               0.621887\n",
      "evaluation/env_infos/final/reward_dist Min               5.68517e-61\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00561521\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117103\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0611377\n",
      "evaluation/env_infos/initial/reward_dist Min             1.72567e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0739372\n",
      "evaluation/env_infos/reward_dist Std                     0.166291\n",
      "evaluation/env_infos/reward_dist Max                     0.996378\n",
      "evaluation/env_infos/reward_dist Min                     5.68517e-61\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0475937\n",
      "evaluation/env_infos/final/reward_energy Std             0.0376235\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0015326\n",
      "evaluation/env_infos/final/reward_energy Min            -0.197719\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.13271\n",
      "evaluation/env_infos/initial/reward_energy Std           0.13167\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0116795\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.704346\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0478596\n",
      "evaluation/env_infos/reward_energy Std                   0.0508222\n",
      "evaluation/env_infos/reward_energy Max                  -0.0015326\n",
      "evaluation/env_infos/reward_energy Min                  -0.704346\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.036182\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.302191\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.850356\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.747504\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00011247\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00660858\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0316973\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0193245\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00905184\n",
      "evaluation/env_infos/end_effector_loc Std                0.175257\n",
      "evaluation/env_infos/end_effector_loc Max                0.850356\n",
      "evaluation/env_infos/end_effector_loc Min               -0.76347\n",
      "time/data storing (s)                                    0.00597621\n",
      "time/evaluation sampling (s)                             1.05476\n",
      "time/exploration sampling (s)                            0.120857\n",
      "time/logging (s)                                         0.0206122\n",
      "time/saving (s)                                          0.0275598\n",
      "time/training (s)                                       44.1305\n",
      "time/epoch (s)                                          45.3603\n",
      "time/total (s)                                         637.444\n",
      "Epoch                                                   13\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:00:04.757501 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 14 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00181051\r\n",
      "trainer/QF2 Loss                                         0.00182463\r\n",
      "trainer/Policy Loss                                      3.22203\r\n",
      "trainer/Q1 Predictions Mean                             -1.41327\r\n",
      "trainer/Q1 Predictions Std                               0.929847\r\n",
      "trainer/Q1 Predictions Max                               0.392673\r\n",
      "trainer/Q1 Predictions Min                              -4.37261\r\n",
      "trainer/Q2 Predictions Mean                             -1.42522\r\n",
      "trainer/Q2 Predictions Std                               0.933988\r\n",
      "trainer/Q2 Predictions Max                               0.358654\r\n",
      "trainer/Q2 Predictions Min                              -4.38777\r\n",
      "trainer/Q Targets Mean                                  -1.43195\r\n",
      "trainer/Q Targets Std                                    0.928018\r\n",
      "trainer/Q Targets Max                                    0.38298\r\n",
      "trainer/Q Targets Min                                   -4.3368\r\n",
      "trainer/Log Pis Mean                                     1.81635\r\n",
      "trainer/Log Pis Std                                      1.17275\r\n",
      "trainer/Log Pis Max                                      4.80277\r\n",
      "trainer/Log Pis Min                                     -4.33275\r\n",
      "trainer/Policy mu Mean                                   0.0180556\r\n",
      "trainer/Policy mu Std                                    0.383494\r\n",
      "trainer/Policy mu Max                                    2.78075\r\n",
      "trainer/Policy mu Min                                   -2.87214\r\n",
      "trainer/Policy log std Mean                             -2.20339\r\n",
      "trainer/Policy log std Std                               0.561963\r\n",
      "trainer/Policy log std Max                               0.188872\r\n",
      "trainer/Policy log std Min                              -3.06758\r\n",
      "trainer/Alpha                                            0.0227824\r\n",
      "trainer/Alpha Loss                                      -0.69449\r\n",
      "exploration/num steps total                           2500\r\n",
      "exploration/num paths total                            125\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0757075\r\n",
      "exploration/Rewards Std                                  0.076615\r\n",
      "exploration/Rewards Max                                  0.0949481\r\n",
      "exploration/Rewards Min                                 -0.291941\r\n",
      "exploration/Returns Mean                                -1.51415\r\n",
      "exploration/Returns Std                                  1.00194\r\n",
      "exploration/Returns Max                                  0.267395\r\n",
      "exploration/Returns Min                                 -2.52804\r\n",
      "exploration/Actions Mean                                -0.0050435\r\n",
      "exploration/Actions Std                                  0.116531\r\n",
      "exploration/Actions Max                                  0.45589\r\n",
      "exploration/Actions Min                                 -0.51704\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.51415\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0937383\r\n",
      "exploration/env_infos/final/reward_dist Std              0.151445\r\n",
      "exploration/env_infos/final/reward_dist Max              0.394497\r\n",
      "exploration/env_infos/final/reward_dist Min              1.46855e-15\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000808648\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000899637\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0024144\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.49588e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.114047\r\n",
      "exploration/env_infos/reward_dist Std                    0.183324\r\n",
      "exploration/env_infos/reward_dist Max                    0.917515\r\n",
      "exploration/env_infos/reward_dist Min                    1.46855e-15\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.119847\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0494116\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0568998\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.169972\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.273707\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.227588\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0366449\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.689323\r\n",
      "exploration/env_infos/reward_energy Mean                -0.134522\r\n",
      "exploration/env_infos/reward_energy Std                  0.0954656\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00270826\r\n",
      "exploration/env_infos/reward_energy Min                 -0.689323\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.114727\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.304558\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.439828\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.539369\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00131355\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0125166\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0227945\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.025852\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0415503\r\n",
      "exploration/env_infos/end_effector_loc Std               0.187017\r\n",
      "exploration/env_infos/end_effector_loc Max               0.439828\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.539369\r\n",
      "evaluation/num steps total                           15000\r\n",
      "evaluation/num paths total                             750\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0595792\r\n",
      "evaluation/Rewards Std                                   0.086126\r\n",
      "evaluation/Rewards Max                                   0.146024\r\n",
      "evaluation/Rewards Min                                  -0.674654\r\n",
      "evaluation/Returns Mean                                 -1.19158\r\n",
      "evaluation/Returns Std                                   1.35183\r\n",
      "evaluation/Returns Max                                   1.80243\r\n",
      "evaluation/Returns Min                                  -5.7267\r\n",
      "evaluation/Actions Mean                                 -0.00577141\r\n",
      "evaluation/Actions Std                                   0.0614444\r\n",
      "evaluation/Actions Max                                   0.614999\r\n",
      "evaluation/Actions Min                                  -0.48992\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.19158\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.108049\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.193935\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.749327\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.23041e-53\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00463693\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00950946\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.037191\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.18329e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.134875\r\n",
      "evaluation/env_infos/reward_dist Std                     0.249295\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993754\r\n",
      "evaluation/env_infos/reward_dist Min                     4.23041e-53\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0648689\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0516182\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0104374\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.285619\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.180814\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.159509\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0111088\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.786285\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0589785\r\n",
      "evaluation/env_infos/reward_energy Std                   0.064335\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000889358\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.786285\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.024599\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.292994\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.81883\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.639178\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00194982\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00829874\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0307499\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.024496\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0269428\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.175939\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.81883\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.639178\r\n",
      "time/data storing (s)                                    0.00587525\r\n",
      "time/evaluation sampling (s)                             0.994243\r\n",
      "time/exploration sampling (s)                            0.130059\r\n",
      "time/logging (s)                                         0.0196424\r\n",
      "time/saving (s)                                          0.0276784\r\n",
      "time/training (s)                                       47.6923\r\n",
      "time/epoch (s)                                          48.8698\r\n",
      "time/total (s)                                         686.505\r\n",
      "Epoch                                                   14\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:00:51.452506 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 15 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00277326\n",
      "trainer/QF2 Loss                                         0.000970414\n",
      "trainer/Policy Loss                                      3.57136\n",
      "trainer/Q1 Predictions Mean                             -1.66031\n",
      "trainer/Q1 Predictions Std                               0.956826\n",
      "trainer/Q1 Predictions Max                               0.521594\n",
      "trainer/Q1 Predictions Min                              -4.53476\n",
      "trainer/Q2 Predictions Mean                             -1.64403\n",
      "trainer/Q2 Predictions Std                               0.956267\n",
      "trainer/Q2 Predictions Max                               0.566101\n",
      "trainer/Q2 Predictions Min                              -4.57008\n",
      "trainer/Q Targets Mean                                  -1.64067\n",
      "trainer/Q Targets Std                                    0.955151\n",
      "trainer/Q Targets Max                                    0.528125\n",
      "trainer/Q Targets Min                                   -4.59541\n",
      "trainer/Log Pis Mean                                     1.9558\n",
      "trainer/Log Pis Std                                      1.5056\n",
      "trainer/Log Pis Max                                      6.02999\n",
      "trainer/Log Pis Min                                     -3.13218\n",
      "trainer/Policy mu Mean                                  -0.00800923\n",
      "trainer/Policy mu Std                                    0.524707\n",
      "trainer/Policy mu Max                                    2.54125\n",
      "trainer/Policy mu Min                                   -2.21569\n",
      "trainer/Policy log std Mean                             -2.21315\n",
      "trainer/Policy log std Std                               0.681987\n",
      "trainer/Policy log std Max                              -0.383481\n",
      "trainer/Policy log std Min                              -3.29557\n",
      "trainer/Alpha                                            0.0209531\n",
      "trainer/Alpha Loss                                      -0.170863\n",
      "exploration/num steps total                           2600\n",
      "exploration/num paths total                            130\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.128907\n",
      "exploration/Rewards Std                                  0.100879\n",
      "exploration/Rewards Max                                  0.0614824\n",
      "exploration/Rewards Min                                 -0.518588\n",
      "exploration/Returns Mean                                -2.57815\n",
      "exploration/Returns Std                                  1.31873\n",
      "exploration/Returns Max                                 -1.53506\n",
      "exploration/Returns Min                                 -5.18085\n",
      "exploration/Actions Mean                                -0.0138985\n",
      "exploration/Actions Std                                  0.0993334\n",
      "exploration/Actions Max                                  0.190473\n",
      "exploration/Actions Min                                 -0.325474\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.57815\n",
      "exploration/env_infos/final/reward_dist Mean             0.0313652\n",
      "exploration/env_infos/final/reward_dist Std              0.0578837\n",
      "exploration/env_infos/final/reward_dist Max              0.146876\n",
      "exploration/env_infos/final/reward_dist Min              2.61663e-46\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00441858\n",
      "exploration/env_infos/initial/reward_dist Std            0.00572628\n",
      "exploration/env_infos/initial/reward_dist Max            0.014922\n",
      "exploration/env_infos/initial/reward_dist Min            7.50913e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0292307\n",
      "exploration/env_infos/reward_dist Std                    0.0389047\n",
      "exploration/env_infos/reward_dist Max                    0.146876\n",
      "exploration/env_infos/reward_dist Min                    2.61663e-46\n",
      "exploration/env_infos/final/reward_energy Mean          -0.134446\n",
      "exploration/env_infos/final/reward_energy Std            0.0963201\n",
      "exploration/env_infos/final/reward_energy Max           -0.047824\n",
      "exploration/env_infos/final/reward_energy Min           -0.308935\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.234741\n",
      "exploration/env_infos/initial/reward_energy Std          0.0736362\n",
      "exploration/env_infos/initial/reward_energy Max         -0.14076\n",
      "exploration/env_infos/initial/reward_energy Min         -0.327565\n",
      "exploration/env_infos/reward_energy Mean                -0.119621\n",
      "exploration/env_infos/reward_energy Std                  0.0762331\n",
      "exploration/env_infos/reward_energy Max                 -0.0098298\n",
      "exploration/env_infos/reward_energy Min                 -0.327565\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.293016\n",
      "exploration/env_infos/final/end_effector_loc Std         0.289738\n",
      "exploration/env_infos/final/end_effector_loc Max         0.0540421\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.766223\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00659816\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00566757\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00184742\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0162737\n",
      "exploration/env_infos/end_effector_loc Mean             -0.148447\n",
      "exploration/env_infos/end_effector_loc Std               0.189907\n",
      "exploration/env_infos/end_effector_loc Max               0.0540421\n",
      "exploration/env_infos/end_effector_loc Min              -0.766223\n",
      "evaluation/num steps total                           16000\n",
      "evaluation/num paths total                             800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0605825\n",
      "evaluation/Rewards Std                                   0.0755916\n",
      "evaluation/Rewards Max                                   0.125512\n",
      "evaluation/Rewards Min                                  -0.458727\n",
      "evaluation/Returns Mean                                 -1.21165\n",
      "evaluation/Returns Std                                   1.06017\n",
      "evaluation/Returns Max                                   0.548262\n",
      "evaluation/Returns Min                                  -4.05787\n",
      "evaluation/Actions Mean                                 -0.00963009\n",
      "evaluation/Actions Std                                   0.0577809\n",
      "evaluation/Actions Max                                   0.629255\n",
      "evaluation/Actions Min                                  -0.443908\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.21165\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0977195\n",
      "evaluation/env_infos/final/reward_dist Std               0.193678\n",
      "evaluation/env_infos/final/reward_dist Max               0.836427\n",
      "evaluation/env_infos/final/reward_dist Min               3.29192e-57\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00525647\n",
      "evaluation/env_infos/initial/reward_dist Std             0.010511\n",
      "evaluation/env_infos/initial/reward_dist Max             0.046556\n",
      "evaluation/env_infos/initial/reward_dist Min             9.47634e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.140093\n",
      "evaluation/env_infos/reward_dist Std                     0.236228\n",
      "evaluation/env_infos/reward_dist Max                     0.99714\n",
      "evaluation/env_infos/reward_dist Min                     3.29192e-57\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0484118\n",
      "evaluation/env_infos/final/reward_energy Std             0.0307158\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0031536\n",
      "evaluation/env_infos/final/reward_energy Min            -0.126922\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.190791\n",
      "evaluation/env_infos/initial/reward_energy Std           0.162611\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0187188\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.706021\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0543842\n",
      "evaluation/env_infos/reward_energy Std                   0.0624908\n",
      "evaluation/env_infos/reward_energy Max                  -0.00086461\n",
      "evaluation/env_infos/reward_energy Min                  -0.706021\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0889162\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.300137\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.568703\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.910919\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000696054\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00883572\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0314628\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0221954\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0336897\n",
      "evaluation/env_infos/end_effector_loc Std                0.181575\n",
      "evaluation/env_infos/end_effector_loc Max                0.568703\n",
      "evaluation/env_infos/end_effector_loc Min               -0.910919\n",
      "time/data storing (s)                                    0.00605415\n",
      "time/evaluation sampling (s)                             1.03343\n",
      "time/exploration sampling (s)                            0.129127\n",
      "time/logging (s)                                         0.0195848\n",
      "time/saving (s)                                          0.0289864\n",
      "time/training (s)                                       45.2538\n",
      "time/epoch (s)                                          46.471\n",
      "time/total (s)                                         733.2\n",
      "Epoch                                                   15\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:01:37.101412 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 16 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00123056\r\n",
      "trainer/QF2 Loss                                         0.00269707\r\n",
      "trainer/Policy Loss                                      3.52899\r\n",
      "trainer/Q1 Predictions Mean                             -1.49893\r\n",
      "trainer/Q1 Predictions Std                               0.865558\r\n",
      "trainer/Q1 Predictions Max                               0.475852\r\n",
      "trainer/Q1 Predictions Min                              -3.8082\r\n",
      "trainer/Q2 Predictions Mean                             -1.50323\r\n",
      "trainer/Q2 Predictions Std                               0.863113\r\n",
      "trainer/Q2 Predictions Max                               0.44836\r\n",
      "trainer/Q2 Predictions Min                              -3.785\r\n",
      "trainer/Q Targets Mean                                  -1.5\r\n",
      "trainer/Q Targets Std                                    0.865332\r\n",
      "trainer/Q Targets Max                                    0.51668\r\n",
      "trainer/Q Targets Min                                   -3.78273\r\n",
      "trainer/Log Pis Mean                                     2.04227\r\n",
      "trainer/Log Pis Std                                      1.29832\r\n",
      "trainer/Log Pis Max                                      5.18898\r\n",
      "trainer/Log Pis Min                                     -3.19504\r\n",
      "trainer/Policy mu Mean                                   0.00604618\r\n",
      "trainer/Policy mu Std                                    0.347756\r\n",
      "trainer/Policy mu Max                                    2.11813\r\n",
      "trainer/Policy mu Min                                   -2.34388\r\n",
      "trainer/Policy log std Mean                             -2.34092\r\n",
      "trainer/Policy log std Std                               0.566149\r\n",
      "trainer/Policy log std Max                              -0.312728\r\n",
      "trainer/Policy log std Min                              -3.1446\r\n",
      "trainer/Alpha                                            0.0211978\r\n",
      "trainer/Alpha Loss                                       0.16291\r\n",
      "exploration/num steps total                           2700\r\n",
      "exploration/num paths total                            135\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.083206\r\n",
      "exploration/Rewards Std                                  0.0545607\r\n",
      "exploration/Rewards Max                                  0.0277943\r\n",
      "exploration/Rewards Min                                 -0.259958\r\n",
      "exploration/Returns Mean                                -1.66412\r\n",
      "exploration/Returns Std                                  0.586489\r\n",
      "exploration/Returns Max                                 -0.961497\r\n",
      "exploration/Returns Min                                 -2.39466\r\n",
      "exploration/Actions Mean                                -0.00835734\r\n",
      "exploration/Actions Std                                  0.0995884\r\n",
      "exploration/Actions Max                                  0.265459\r\n",
      "exploration/Actions Min                                 -0.324954\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.66412\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0267341\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0431758\r\n",
      "exploration/env_infos/final/reward_dist Max              0.11253\r\n",
      "exploration/env_infos/final/reward_dist Min              1.01719e-10\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00978989\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0131884\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0335263\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.2084e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.152842\r\n",
      "exploration/env_infos/reward_dist Std                    0.260857\r\n",
      "exploration/env_infos/reward_dist Max                    0.9982\r\n",
      "exploration/env_infos/reward_dist Min                    1.01719e-10\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.121068\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0679377\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0564694\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.237966\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.155952\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0650332\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0538064\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.22722\r\n",
      "exploration/env_infos/reward_energy Mean                -0.121617\r\n",
      "exploration/env_infos/reward_energy Std                  0.072004\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0104296\r\n",
      "exploration/env_infos/reward_energy Min                 -0.357868\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00739384\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.26803\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.389754\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.544665\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00192915\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00565389\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0110147\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0070626\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0245908\r\n",
      "exploration/env_infos/end_effector_loc Std               0.154516\r\n",
      "exploration/env_infos/end_effector_loc Max               0.404808\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.544665\r\n",
      "evaluation/num steps total                           17000\r\n",
      "evaluation/num paths total                             850\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.064096\r\n",
      "evaluation/Rewards Std                                   0.0837111\r\n",
      "evaluation/Rewards Max                                   0.131871\r\n",
      "evaluation/Rewards Min                                  -0.526999\r\n",
      "evaluation/Returns Mean                                 -1.28192\r\n",
      "evaluation/Returns Std                                   1.23631\r\n",
      "evaluation/Returns Max                                   1.46437\r\n",
      "evaluation/Returns Min                                  -5.21812\r\n",
      "evaluation/Actions Mean                                 -0.00530405\r\n",
      "evaluation/Actions Std                                   0.0517836\r\n",
      "evaluation/Actions Max                                   0.526352\r\n",
      "evaluation/Actions Min                                  -0.540149\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.28192\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.111104\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.196236\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.876803\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.86751e-38\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00378181\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00618214\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0268315\r\n",
      "evaluation/env_infos/initial/reward_dist Min             9.96477e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.133917\r\n",
      "evaluation/env_infos/reward_dist Std                     0.232853\r\n",
      "evaluation/env_infos/reward_dist Max                     0.992322\r\n",
      "evaluation/env_infos/reward_dist Min                     6.86751e-38\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0418091\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0276087\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000492808\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.12755\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.157054\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.14499\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.010563\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.754193\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0463584\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0571861\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000349595\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.754193\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0532069\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.297945\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.643744\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.729811\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000857952\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00750825\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0263176\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0270074\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0226866\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.181689\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.643744\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.729811\r\n",
      "time/data storing (s)                                    0.00607866\r\n",
      "time/evaluation sampling (s)                             1.07669\r\n",
      "time/exploration sampling (s)                            0.122511\r\n",
      "time/logging (s)                                         0.0188583\r\n",
      "time/saving (s)                                          0.0268283\r\n",
      "time/training (s)                                       44.1625\r\n",
      "time/epoch (s)                                          45.4134\r\n",
      "time/total (s)                                         778.846\r\n",
      "Epoch                                                   16\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:02:23.731561 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 17 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00118488\n",
      "trainer/QF2 Loss                                         0.0019496\n",
      "trainer/Policy Loss                                      3.48351\n",
      "trainer/Q1 Predictions Mean                             -1.53569\n",
      "trainer/Q1 Predictions Std                               0.897245\n",
      "trainer/Q1 Predictions Max                               0.619683\n",
      "trainer/Q1 Predictions Min                              -3.59456\n",
      "trainer/Q2 Predictions Mean                             -1.53977\n",
      "trainer/Q2 Predictions Std                               0.89658\n",
      "trainer/Q2 Predictions Max                               0.665325\n",
      "trainer/Q2 Predictions Min                              -3.6545\n",
      "trainer/Q Targets Mean                                  -1.54036\n",
      "trainer/Q Targets Std                                    0.898592\n",
      "trainer/Q Targets Max                                    0.576275\n",
      "trainer/Q Targets Min                                   -3.64088\n",
      "trainer/Log Pis Mean                                     1.95723\n",
      "trainer/Log Pis Std                                      1.33026\n",
      "trainer/Log Pis Max                                      5.38398\n",
      "trainer/Log Pis Min                                     -2.69093\n",
      "trainer/Policy mu Mean                                   0.0248645\n",
      "trainer/Policy mu Std                                    0.321117\n",
      "trainer/Policy mu Max                                    2.2345\n",
      "trainer/Policy mu Min                                   -2.18881\n",
      "trainer/Policy log std Mean                             -2.29733\n",
      "trainer/Policy log std Std                               0.542157\n",
      "trainer/Policy log std Max                              -0.11855\n",
      "trainer/Policy log std Min                              -3.06765\n",
      "trainer/Alpha                                            0.0223933\n",
      "trainer/Alpha Loss                                      -0.162482\n",
      "exploration/num steps total                           2800\n",
      "exploration/num paths total                            140\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.119368\n",
      "exploration/Rewards Std                                  0.0999341\n",
      "exploration/Rewards Max                                  0.0493868\n",
      "exploration/Rewards Min                                 -0.499875\n",
      "exploration/Returns Mean                                -2.38735\n",
      "exploration/Returns Std                                  1.60355\n",
      "exploration/Returns Max                                 -0.343054\n",
      "exploration/Returns Min                                 -5.08693\n",
      "exploration/Actions Mean                                 0.00176743\n",
      "exploration/Actions Std                                  0.103133\n",
      "exploration/Actions Max                                  0.423771\n",
      "exploration/Actions Min                                 -0.319901\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.38735\n",
      "exploration/env_infos/final/reward_dist Mean             0.00393639\n",
      "exploration/env_infos/final/reward_dist Std              0.00520675\n",
      "exploration/env_infos/final/reward_dist Max              0.0131264\n",
      "exploration/env_infos/final/reward_dist Min              6.09496e-26\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00644419\n",
      "exploration/env_infos/initial/reward_dist Std            0.0117976\n",
      "exploration/env_infos/initial/reward_dist Max            0.0300057\n",
      "exploration/env_infos/initial/reward_dist Min            1.30042e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.042669\n",
      "exploration/env_infos/reward_dist Std                    0.114188\n",
      "exploration/env_infos/reward_dist Max                    0.652825\n",
      "exploration/env_infos/reward_dist Min                    6.09496e-26\n",
      "exploration/env_infos/final/reward_energy Mean          -0.18793\n",
      "exploration/env_infos/final/reward_energy Std            0.0706174\n",
      "exploration/env_infos/final/reward_energy Max           -0.0738408\n",
      "exploration/env_infos/final/reward_energy Min           -0.296284\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.231646\n",
      "exploration/env_infos/initial/reward_energy Std          0.145642\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0657487\n",
      "exploration/env_infos/initial/reward_energy Min         -0.460326\n",
      "exploration/env_infos/reward_energy Mean                -0.122982\n",
      "exploration/env_infos/reward_energy Std                  0.0784511\n",
      "exploration/env_infos/reward_energy Max                 -0.0104719\n",
      "exploration/env_infos/reward_energy Min                 -0.460326\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0559756\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275753\n",
      "exploration/env_infos/final/end_effector_loc Max         0.587253\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.345684\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000746047\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00964534\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0211886\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.015995\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0395083\n",
      "exploration/env_infos/end_effector_loc Std               0.173875\n",
      "exploration/env_infos/end_effector_loc Max               0.587253\n",
      "exploration/env_infos/end_effector_loc Min              -0.345684\n",
      "evaluation/num steps total                           18000\n",
      "evaluation/num paths total                             900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0807408\n",
      "evaluation/Rewards Std                                   0.0630596\n",
      "evaluation/Rewards Max                                   0.108094\n",
      "evaluation/Rewards Min                                  -0.391963\n",
      "evaluation/Returns Mean                                 -1.61482\n",
      "evaluation/Returns Std                                   0.764863\n",
      "evaluation/Returns Max                                  -0.329045\n",
      "evaluation/Returns Min                                  -3.37887\n",
      "evaluation/Actions Mean                                 -0.00619995\n",
      "evaluation/Actions Std                                   0.0597743\n",
      "evaluation/Actions Max                                   0.538442\n",
      "evaluation/Actions Min                                  -0.42138\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.61482\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0814475\n",
      "evaluation/env_infos/final/reward_dist Std               0.217888\n",
      "evaluation/env_infos/final/reward_dist Max               0.978652\n",
      "evaluation/env_infos/final/reward_dist Min               4.68286e-27\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00763451\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0173048\n",
      "evaluation/env_infos/initial/reward_dist Max             0.102757\n",
      "evaluation/env_infos/initial/reward_dist Min             1.1386e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0784803\n",
      "evaluation/env_infos/reward_dist Std                     0.182557\n",
      "evaluation/env_infos/reward_dist Max                     0.991178\n",
      "evaluation/env_infos/reward_dist Min                     4.68286e-27\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0466677\n",
      "evaluation/env_infos/final/reward_energy Std             0.0378934\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00638429\n",
      "evaluation/env_infos/final/reward_energy Min            -0.191335\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.179533\n",
      "evaluation/env_infos/initial/reward_energy Std           0.158326\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0216958\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.633417\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0573903\n",
      "evaluation/env_infos/reward_energy Std                   0.062683\n",
      "evaluation/env_infos/reward_energy Max                  -0.00101918\n",
      "evaluation/env_infos/reward_energy Min                  -0.633417\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.118682\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.288291\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.456481\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.734072\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00119378\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00837848\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0269221\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.021069\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0555048\n",
      "evaluation/env_infos/end_effector_loc Std                0.183945\n",
      "evaluation/env_infos/end_effector_loc Max                0.456481\n",
      "evaluation/env_infos/end_effector_loc Min               -0.734072\n",
      "time/data storing (s)                                    0.00590469\n",
      "time/evaluation sampling (s)                             0.924502\n",
      "time/exploration sampling (s)                            0.119434\n",
      "time/logging (s)                                         0.0208496\n",
      "time/saving (s)                                          0.0268899\n",
      "time/training (s)                                       45.3059\n",
      "time/epoch (s)                                          46.4035\n",
      "time/total (s)                                         825.478\n",
      "Epoch                                                   17\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:03:09.597262 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 18 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00150498\n",
      "trainer/QF2 Loss                                         0.00122892\n",
      "trainer/Policy Loss                                      3.252\n",
      "trainer/Q1 Predictions Mean                             -1.35116\n",
      "trainer/Q1 Predictions Std                               0.809809\n",
      "trainer/Q1 Predictions Max                               0.502\n",
      "trainer/Q1 Predictions Min                              -3.42509\n",
      "trainer/Q2 Predictions Mean                             -1.34401\n",
      "trainer/Q2 Predictions Std                               0.812181\n",
      "trainer/Q2 Predictions Max                               0.507038\n",
      "trainer/Q2 Predictions Min                              -3.47069\n",
      "trainer/Q Targets Mean                                  -1.33686\n",
      "trainer/Q Targets Std                                    0.817907\n",
      "trainer/Q Targets Max                                    0.525133\n",
      "trainer/Q Targets Min                                   -3.48436\n",
      "trainer/Log Pis Mean                                     1.91974\n",
      "trainer/Log Pis Std                                      1.38306\n",
      "trainer/Log Pis Max                                      6.56416\n",
      "trainer/Log Pis Min                                     -3.09956\n",
      "trainer/Policy mu Mean                                   0.0322128\n",
      "trainer/Policy mu Std                                    0.33968\n",
      "trainer/Policy mu Max                                    2.35408\n",
      "trainer/Policy mu Min                                   -1.59029\n",
      "trainer/Policy log std Mean                             -2.28701\n",
      "trainer/Policy log std Std                               0.630383\n",
      "trainer/Policy log std Max                              -0.386389\n",
      "trainer/Policy log std Min                              -3.33265\n",
      "trainer/Alpha                                            0.0230253\n",
      "trainer/Alpha Loss                                      -0.302664\n",
      "exploration/num steps total                           2900\n",
      "exploration/num paths total                            145\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.159361\n",
      "exploration/Rewards Std                                  0.0903307\n",
      "exploration/Rewards Max                                 -0.0441467\n",
      "exploration/Rewards Min                                 -0.496406\n",
      "exploration/Returns Mean                                -3.18722\n",
      "exploration/Returns Std                                  0.944072\n",
      "exploration/Returns Max                                 -2.24459\n",
      "exploration/Returns Min                                 -4.8319\n",
      "exploration/Actions Mean                                -0.00371153\n",
      "exploration/Actions Std                                  0.117313\n",
      "exploration/Actions Max                                  0.847764\n",
      "exploration/Actions Min                                 -0.447572\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.18722\n",
      "exploration/env_infos/final/reward_dist Mean             0.000215734\n",
      "exploration/env_infos/final/reward_dist Std              0.000420409\n",
      "exploration/env_infos/final/reward_dist Max              0.00105637\n",
      "exploration/env_infos/final/reward_dist Min              1.5976e-36\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000315199\n",
      "exploration/env_infos/initial/reward_dist Std            0.000548964\n",
      "exploration/env_infos/initial/reward_dist Max            0.00141025\n",
      "exploration/env_infos/initial/reward_dist Min            1.15692e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0146625\n",
      "exploration/env_infos/reward_dist Std                    0.0428016\n",
      "exploration/env_infos/reward_dist Max                    0.209224\n",
      "exploration/env_infos/reward_dist Min                    1.5976e-36\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135078\n",
      "exploration/env_infos/final/reward_energy Std            0.0397578\n",
      "exploration/env_infos/final/reward_energy Max           -0.0878669\n",
      "exploration/env_infos/final/reward_energy Min           -0.196617\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.378071\n",
      "exploration/env_infos/initial/reward_energy Std          0.315776\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0110384\n",
      "exploration/env_infos/initial/reward_energy Min         -0.958658\n",
      "exploration/env_infos/reward_energy Mean                -0.120089\n",
      "exploration/env_infos/reward_energy Std                  0.11459\n",
      "exploration/env_infos/reward_energy Max                 -0.00556208\n",
      "exploration/env_infos/reward_energy Min                 -0.958658\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.019996\n",
      "exploration/env_infos/final/end_effector_loc Std         0.428426\n",
      "exploration/env_infos/final/end_effector_loc Max         0.664213\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.656201\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00216609\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0172807\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0423882\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0223786\n",
      "exploration/env_infos/end_effector_loc Mean              0.00204464\n",
      "exploration/env_infos/end_effector_loc Std               0.275093\n",
      "exploration/env_infos/end_effector_loc Max               0.664213\n",
      "exploration/env_infos/end_effector_loc Min              -0.656201\n",
      "evaluation/num steps total                           19000\n",
      "evaluation/num paths total                             950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0534568\n",
      "evaluation/Rewards Std                                   0.0706701\n",
      "evaluation/Rewards Max                                   0.154256\n",
      "evaluation/Rewards Min                                  -0.426258\n",
      "evaluation/Returns Mean                                 -1.06914\n",
      "evaluation/Returns Std                                   1.15358\n",
      "evaluation/Returns Max                                   1.62931\n",
      "evaluation/Returns Min                                  -3.45487\n",
      "evaluation/Actions Mean                                  0.00042405\n",
      "evaluation/Actions Std                                   0.0523938\n",
      "evaluation/Actions Max                                   0.63152\n",
      "evaluation/Actions Min                                  -0.36422\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.06914\n",
      "evaluation/env_infos/final/reward_dist Mean              0.122617\n",
      "evaluation/env_infos/final/reward_dist Std               0.228504\n",
      "evaluation/env_infos/final/reward_dist Max               0.852651\n",
      "evaluation/env_infos/final/reward_dist Min               1.75537e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00580764\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00999649\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0410897\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97762e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.132602\n",
      "evaluation/env_infos/reward_dist Std                     0.23402\n",
      "evaluation/env_infos/reward_dist Max                     0.998177\n",
      "evaluation/env_infos/reward_dist Min                     1.75537e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0452249\n",
      "evaluation/env_infos/final/reward_energy Std             0.0387573\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00347885\n",
      "evaluation/env_infos/final/reward_energy Min            -0.18251\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.167477\n",
      "evaluation/env_infos/initial/reward_energy Std           0.157236\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0281445\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.729022\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0489295\n",
      "evaluation/env_infos/reward_energy Std                   0.055646\n",
      "evaluation/env_infos/reward_energy Max                  -0.000942319\n",
      "evaluation/env_infos/reward_energy Min                  -0.729022\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0685221\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.21689\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.526428\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.442613\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00252572\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00771915\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.031576\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.018211\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0411444\n",
      "evaluation/env_infos/end_effector_loc Std                0.139712\n",
      "evaluation/env_infos/end_effector_loc Max                0.526428\n",
      "evaluation/env_infos/end_effector_loc Min               -0.442613\n",
      "time/data storing (s)                                    0.00636472\n",
      "time/evaluation sampling (s)                             0.954192\n",
      "time/exploration sampling (s)                            0.118211\n",
      "time/logging (s)                                         0.0192126\n",
      "time/saving (s)                                          0.0286687\n",
      "time/training (s)                                       44.4866\n",
      "time/epoch (s)                                          45.6132\n",
      "time/total (s)                                         871.341\n",
      "Epoch                                                   18\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:03:56.969130 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 19 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00161783\n",
      "trainer/QF2 Loss                                         0.00198163\n",
      "trainer/Policy Loss                                      3.43035\n",
      "trainer/Q1 Predictions Mean                             -1.50707\n",
      "trainer/Q1 Predictions Std                               0.85379\n",
      "trainer/Q1 Predictions Max                               0.466398\n",
      "trainer/Q1 Predictions Min                              -3.43156\n",
      "trainer/Q2 Predictions Mean                             -1.50864\n",
      "trainer/Q2 Predictions Std                               0.86707\n",
      "trainer/Q2 Predictions Max                               0.488732\n",
      "trainer/Q2 Predictions Min                              -3.53777\n",
      "trainer/Q Targets Mean                                  -1.5082\n",
      "trainer/Q Targets Std                                    0.855981\n",
      "trainer/Q Targets Max                                    0.4941\n",
      "trainer/Q Targets Min                                   -3.43997\n",
      "trainer/Log Pis Mean                                     1.93666\n",
      "trainer/Log Pis Std                                      1.30728\n",
      "trainer/Log Pis Max                                      3.97185\n",
      "trainer/Log Pis Min                                     -3.26538\n",
      "trainer/Policy mu Mean                                   0.00831211\n",
      "trainer/Policy mu Std                                    0.32811\n",
      "trainer/Policy mu Max                                    2.15442\n",
      "trainer/Policy mu Min                                   -1.8996\n",
      "trainer/Policy log std Mean                             -2.30068\n",
      "trainer/Policy log std Std                               0.536401\n",
      "trainer/Policy log std Max                              -0.54953\n",
      "trainer/Policy log std Min                              -3.0854\n",
      "trainer/Alpha                                            0.0236532\n",
      "trainer/Alpha Loss                                      -0.237114\n",
      "exploration/num steps total                           3000\n",
      "exploration/num paths total                            150\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0827982\n",
      "exploration/Rewards Std                                  0.0719379\n",
      "exploration/Rewards Max                                  0.0954964\n",
      "exploration/Rewards Min                                 -0.254946\n",
      "exploration/Returns Mean                                -1.65596\n",
      "exploration/Returns Std                                  0.833508\n",
      "exploration/Returns Max                                 -0.221282\n",
      "exploration/Returns Min                                 -2.67581\n",
      "exploration/Actions Mean                                 0.0106815\n",
      "exploration/Actions Std                                  0.0962861\n",
      "exploration/Actions Max                                  0.301675\n",
      "exploration/Actions Min                                 -0.282484\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.65596\n",
      "exploration/env_infos/final/reward_dist Mean             0.155098\n",
      "exploration/env_infos/final/reward_dist Std              0.240052\n",
      "exploration/env_infos/final/reward_dist Max              0.627571\n",
      "exploration/env_infos/final/reward_dist Min              3.61895e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00817356\n",
      "exploration/env_infos/initial/reward_dist Std            0.0156863\n",
      "exploration/env_infos/initial/reward_dist Max            0.0395312\n",
      "exploration/env_infos/initial/reward_dist Min            1.57767e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.180189\n",
      "exploration/env_infos/reward_dist Std                    0.277318\n",
      "exploration/env_infos/reward_dist Max                    0.977447\n",
      "exploration/env_infos/reward_dist Min                    3.61895e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.178237\n",
      "exploration/env_infos/final/reward_energy Std            0.0815347\n",
      "exploration/env_infos/final/reward_energy Max           -0.100573\n",
      "exploration/env_infos/final/reward_energy Min           -0.33541\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.136559\n",
      "exploration/env_infos/initial/reward_energy Std          0.0874879\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0442782\n",
      "exploration/env_infos/initial/reward_energy Min         -0.302058\n",
      "exploration/env_infos/reward_energy Mean                -0.117514\n",
      "exploration/env_infos/reward_energy Std                  0.0704314\n",
      "exploration/env_infos/reward_energy Max                 -0.00418087\n",
      "exploration/env_infos/reward_energy Min                 -0.355774\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0558282\n",
      "exploration/env_infos/final/end_effector_loc Std         0.256535\n",
      "exploration/env_infos/final/end_effector_loc Max         0.399526\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.327523\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -3.11039e-05\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00573385\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00902734\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.012108\n",
      "exploration/env_infos/end_effector_loc Mean              0.0169535\n",
      "exploration/env_infos/end_effector_loc Std               0.145655\n",
      "exploration/env_infos/end_effector_loc Max               0.399526\n",
      "exploration/env_infos/end_effector_loc Min              -0.327523\n",
      "evaluation/num steps total                           20000\n",
      "evaluation/num paths total                            1000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0613251\n",
      "evaluation/Rewards Std                                   0.0616856\n",
      "evaluation/Rewards Max                                   0.11689\n",
      "evaluation/Rewards Min                                  -0.258682\n",
      "evaluation/Returns Mean                                 -1.2265\n",
      "evaluation/Returns Std                                   0.995149\n",
      "evaluation/Returns Max                                   0.585285\n",
      "evaluation/Returns Min                                  -3.50533\n",
      "evaluation/Actions Mean                                 -0.00382638\n",
      "evaluation/Actions Std                                   0.0503679\n",
      "evaluation/Actions Max                                   0.563096\n",
      "evaluation/Actions Min                                  -0.353367\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.2265\n",
      "evaluation/env_infos/final/reward_dist Mean              0.230017\n",
      "evaluation/env_infos/final/reward_dist Std               0.319426\n",
      "evaluation/env_infos/final/reward_dist Max               0.939632\n",
      "evaluation/env_infos/final/reward_dist Min               6.36538e-31\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0044869\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00836858\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0343153\n",
      "evaluation/env_infos/initial/reward_dist Min             1.04295e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.141433\n",
      "evaluation/env_infos/reward_dist Std                     0.24514\n",
      "evaluation/env_infos/reward_dist Max                     0.998445\n",
      "evaluation/env_infos/reward_dist Min                     6.36538e-31\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0395917\n",
      "evaluation/env_infos/final/reward_energy Std             0.0365556\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00439756\n",
      "evaluation/env_infos/final/reward_energy Min            -0.147981\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.141413\n",
      "evaluation/env_infos/initial/reward_energy Std           0.131664\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00640207\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.640777\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0451783\n",
      "evaluation/env_infos/reward_energy Std                   0.0553359\n",
      "evaluation/env_infos/reward_energy Max                  -0.000592003\n",
      "evaluation/env_infos/reward_energy Min                  -0.640777\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0438336\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.218363\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.298367\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.611402\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000443829\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00681684\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0281548\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0176684\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0183627\n",
      "evaluation/env_infos/end_effector_loc Std                0.140574\n",
      "evaluation/env_infos/end_effector_loc Max                0.336411\n",
      "evaluation/env_infos/end_effector_loc Min               -0.611402\n",
      "time/data storing (s)                                    0.00631855\n",
      "time/evaluation sampling (s)                             0.954425\n",
      "time/exploration sampling (s)                            0.124909\n",
      "time/logging (s)                                         0.0196136\n",
      "time/saving (s)                                          0.028889\n",
      "time/training (s)                                       45.9768\n",
      "time/epoch (s)                                          47.1109\n",
      "time/total (s)                                         918.713\n",
      "Epoch                                                   19\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:04:43.832151 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 20 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000897544\n",
      "trainer/QF2 Loss                                         0.000980844\n",
      "trainer/Policy Loss                                      3.44877\n",
      "trainer/Q1 Predictions Mean                             -1.46989\n",
      "trainer/Q1 Predictions Std                               0.897256\n",
      "trainer/Q1 Predictions Max                               0.56655\n",
      "trainer/Q1 Predictions Min                              -3.8649\n",
      "trainer/Q2 Predictions Mean                             -1.46535\n",
      "trainer/Q2 Predictions Std                               0.89333\n",
      "trainer/Q2 Predictions Max                               0.584369\n",
      "trainer/Q2 Predictions Min                              -3.82116\n",
      "trainer/Q Targets Mean                                  -1.46843\n",
      "trainer/Q Targets Std                                    0.895015\n",
      "trainer/Q Targets Max                                    0.588035\n",
      "trainer/Q Targets Min                                   -3.84961\n",
      "trainer/Log Pis Mean                                     2.00088\n",
      "trainer/Log Pis Std                                      1.31252\n",
      "trainer/Log Pis Max                                      3.96472\n",
      "trainer/Log Pis Min                                     -3.13089\n",
      "trainer/Policy mu Mean                                   0.0396763\n",
      "trainer/Policy mu Std                                    0.364597\n",
      "trainer/Policy mu Max                                    2.68924\n",
      "trainer/Policy mu Min                                   -2.23711\n",
      "trainer/Policy log std Mean                             -2.3173\n",
      "trainer/Policy log std Std                               0.577136\n",
      "trainer/Policy log std Max                              -0.0299917\n",
      "trainer/Policy log std Min                              -3.11487\n",
      "trainer/Alpha                                            0.0245698\n",
      "trainer/Alpha Loss                                       0.00324391\n",
      "exploration/num steps total                           3100\n",
      "exploration/num paths total                            155\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0772978\n",
      "exploration/Rewards Std                                  0.0536013\n",
      "exploration/Rewards Max                                  0.0448934\n",
      "exploration/Rewards Min                                 -0.244306\n",
      "exploration/Returns Mean                                -1.54596\n",
      "exploration/Returns Std                                  0.68575\n",
      "exploration/Returns Max                                 -0.333234\n",
      "exploration/Returns Min                                 -2.44429\n",
      "exploration/Actions Mean                                -0.00697372\n",
      "exploration/Actions Std                                  0.0990313\n",
      "exploration/Actions Max                                  0.52794\n",
      "exploration/Actions Min                                 -0.325076\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.54596\n",
      "exploration/env_infos/final/reward_dist Mean             0.320859\n",
      "exploration/env_infos/final/reward_dist Std              0.28432\n",
      "exploration/env_infos/final/reward_dist Max              0.737238\n",
      "exploration/env_infos/final/reward_dist Min              1.61569e-29\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00464842\n",
      "exploration/env_infos/initial/reward_dist Std            0.00670226\n",
      "exploration/env_infos/initial/reward_dist Max            0.0175268\n",
      "exploration/env_infos/initial/reward_dist Min            1.11756e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.177415\n",
      "exploration/env_infos/reward_dist Std                    0.264702\n",
      "exploration/env_infos/reward_dist Max                    0.872118\n",
      "exploration/env_infos/reward_dist Min                    1.61569e-29\n",
      "exploration/env_infos/final/reward_energy Mean          -0.137386\n",
      "exploration/env_infos/final/reward_energy Std            0.0592887\n",
      "exploration/env_infos/final/reward_energy Max           -0.0558374\n",
      "exploration/env_infos/final/reward_energy Min           -0.227024\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.134332\n",
      "exploration/env_infos/initial/reward_energy Std          0.0672137\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0521952\n",
      "exploration/env_infos/initial/reward_energy Min         -0.234762\n",
      "exploration/env_infos/reward_energy Mean                -0.106843\n",
      "exploration/env_infos/reward_energy Std                  0.0910836\n",
      "exploration/env_infos/reward_energy Max                 -0.0138754\n",
      "exploration/env_infos/reward_energy Min                 -0.619996\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.106667\n",
      "exploration/env_infos/final/end_effector_loc Std         0.265129\n",
      "exploration/env_infos/final/end_effector_loc Max         0.155942\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.695171\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000240089\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00530525\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0114582\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00930831\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0404992\n",
      "exploration/env_infos/end_effector_loc Std               0.160123\n",
      "exploration/env_infos/end_effector_loc Max               0.239981\n",
      "exploration/env_infos/end_effector_loc Min              -0.695171\n",
      "evaluation/num steps total                           21000\n",
      "evaluation/num paths total                            1050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.066468\n",
      "evaluation/Rewards Std                                   0.0890923\n",
      "evaluation/Rewards Max                                   0.11175\n",
      "evaluation/Rewards Min                                  -0.582351\n",
      "evaluation/Returns Mean                                 -1.32936\n",
      "evaluation/Returns Std                                   1.44848\n",
      "evaluation/Returns Max                                   1.08794\n",
      "evaluation/Returns Min                                  -7.18042\n",
      "evaluation/Actions Mean                                 -0.00508389\n",
      "evaluation/Actions Std                                   0.0617578\n",
      "evaluation/Actions Max                                   0.435373\n",
      "evaluation/Actions Min                                  -0.313501\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.32936\n",
      "evaluation/env_infos/final/reward_dist Mean              0.145784\n",
      "evaluation/env_infos/final/reward_dist Std               0.271277\n",
      "evaluation/env_infos/final/reward_dist Max               0.984055\n",
      "evaluation/env_infos/final/reward_dist Min               1.95128e-42\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00433709\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00701358\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0346539\n",
      "evaluation/env_infos/initial/reward_dist Min             4.32e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.125927\n",
      "evaluation/env_infos/reward_dist Std                     0.233009\n",
      "evaluation/env_infos/reward_dist Max                     0.996826\n",
      "evaluation/env_infos/reward_dist Min                     1.95128e-42\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.043692\n",
      "evaluation/env_infos/final/reward_energy Std             0.0455449\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00163116\n",
      "evaluation/env_infos/final/reward_energy Min            -0.296735\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.191216\n",
      "evaluation/env_infos/initial/reward_energy Std           0.123893\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0103387\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.471108\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0598879\n",
      "evaluation/env_infos/reward_energy Std                   0.063978\n",
      "evaluation/env_infos/reward_energy Max                  -0.000404139\n",
      "evaluation/env_infos/reward_energy Min                  -0.471108\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0620879\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27902\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.640575\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.678454\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       6.88251e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00805522\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0217686\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.015675\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0199377\n",
      "evaluation/env_infos/end_effector_loc Std                0.182279\n",
      "evaluation/env_infos/end_effector_loc Max                0.65785\n",
      "evaluation/env_infos/end_effector_loc Min               -0.678454\n",
      "time/data storing (s)                                    0.00587552\n",
      "time/evaluation sampling (s)                             0.952333\n",
      "time/exploration sampling (s)                            0.121694\n",
      "time/logging (s)                                         0.0184168\n",
      "time/saving (s)                                          0.0261869\n",
      "time/training (s)                                       45.4632\n",
      "time/epoch (s)                                          46.5877\n",
      "time/total (s)                                         965.573\n",
      "Epoch                                                   20\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:05:30.476833 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 21 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00204049\r\n",
      "trainer/QF2 Loss                                         0.00309024\r\n",
      "trainer/Policy Loss                                      3.70651\r\n",
      "trainer/Q1 Predictions Mean                             -1.56299\r\n",
      "trainer/Q1 Predictions Std                               0.882018\r\n",
      "trainer/Q1 Predictions Max                               0.116234\r\n",
      "trainer/Q1 Predictions Min                              -3.96084\r\n",
      "trainer/Q2 Predictions Mean                             -1.56997\r\n",
      "trainer/Q2 Predictions Std                               0.882042\r\n",
      "trainer/Q2 Predictions Max                               0.127523\r\n",
      "trainer/Q2 Predictions Min                              -3.9961\r\n",
      "trainer/Q Targets Mean                                  -1.57284\r\n",
      "trainer/Q Targets Std                                    0.876173\r\n",
      "trainer/Q Targets Max                                    0.098966\r\n",
      "trainer/Q Targets Min                                   -3.93142\r\n",
      "trainer/Log Pis Mean                                     2.16102\r\n",
      "trainer/Log Pis Std                                      1.37808\r\n",
      "trainer/Log Pis Max                                      4.37404\r\n",
      "trainer/Log Pis Min                                     -3.51814\r\n",
      "trainer/Policy mu Mean                                   0.0526172\r\n",
      "trainer/Policy mu Std                                    0.370597\r\n",
      "trainer/Policy mu Max                                    2.2973\r\n",
      "trainer/Policy mu Min                                   -1.49954\r\n",
      "trainer/Policy log std Mean                             -2.39325\r\n",
      "trainer/Policy log std Std                               0.537453\r\n",
      "trainer/Policy log std Max                              -0.309504\r\n",
      "trainer/Policy log std Min                              -3.15483\r\n",
      "trainer/Alpha                                            0.0259946\r\n",
      "trainer/Alpha Loss                                       0.587833\r\n",
      "exploration/num steps total                           3200\r\n",
      "exploration/num paths total                            160\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.101063\r\n",
      "exploration/Rewards Std                                  0.0296336\r\n",
      "exploration/Rewards Max                                 -0.0129706\r\n",
      "exploration/Rewards Min                                 -0.187626\r\n",
      "exploration/Returns Mean                                -2.02126\r\n",
      "exploration/Returns Std                                  0.260683\r\n",
      "exploration/Returns Max                                 -1.56061\r\n",
      "exploration/Returns Min                                 -2.33853\r\n",
      "exploration/Actions Mean                                 0.00739796\r\n",
      "exploration/Actions Std                                  0.200874\r\n",
      "exploration/Actions Max                                  0.79552\r\n",
      "exploration/Actions Min                                 -0.644464\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.02126\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.222546\r\n",
      "exploration/env_infos/final/reward_dist Std              0.388021\r\n",
      "exploration/env_infos/final/reward_dist Max              0.993357\r\n",
      "exploration/env_infos/final/reward_dist Min              1.07727e-13\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00885846\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0166148\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0420765\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.17901e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.11493\r\n",
      "exploration/env_infos/reward_dist Std                    0.217072\r\n",
      "exploration/env_infos/reward_dist Max                    0.993357\r\n",
      "exploration/env_infos/reward_dist Min                    1.07727e-13\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135329\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0602911\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0534978\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.211877\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.404996\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.148783\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.239747\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.675015\r\n",
      "exploration/env_infos/reward_energy Mean                -0.23373\r\n",
      "exploration/env_infos/reward_energy Std                  0.161805\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00463098\r\n",
      "exploration/env_infos/reward_energy Min                 -0.859861\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0251057\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.284794\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.452657\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.462867\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00176203\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0151523\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0174387\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0322232\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0120921\r\n",
      "exploration/env_infos/end_effector_loc Std               0.186404\r\n",
      "exploration/env_infos/end_effector_loc Max               0.452657\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.462867\r\n",
      "evaluation/num steps total                           22000\r\n",
      "evaluation/num paths total                            1100\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0513766\r\n",
      "evaluation/Rewards Std                                   0.0727486\r\n",
      "evaluation/Rewards Max                                   0.168788\r\n",
      "evaluation/Rewards Min                                  -0.353831\r\n",
      "evaluation/Returns Mean                                 -1.02753\r\n",
      "evaluation/Returns Std                                   1.15543\r\n",
      "evaluation/Returns Max                                   0.879219\r\n",
      "evaluation/Returns Min                                  -5.1487\r\n",
      "evaluation/Actions Mean                                 -0.00238988\r\n",
      "evaluation/Actions Std                                   0.0524615\r\n",
      "evaluation/Actions Max                                   0.32518\r\n",
      "evaluation/Actions Min                                  -0.387393\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.02753\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.126988\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.223045\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.997458\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.96092e-19\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00413803\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00746633\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0308571\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.53338e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.112473\r\n",
      "evaluation/env_infos/reward_dist Std                     0.203163\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997458\r\n",
      "evaluation/env_infos/reward_dist Min                     2.96092e-19\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0475785\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0358115\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00442165\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.155142\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.1448\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.106006\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0204471\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.436365\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0529265\r\n",
      "evaluation/env_infos/reward_energy Std                   0.052102\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000448174\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.436365\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0142311\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.246159\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.575271\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.553795\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000157106\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00634276\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.016259\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0193696\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.000510152\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.156082\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.575271\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.553795\r\n",
      "time/data storing (s)                                    0.00614338\r\n",
      "time/evaluation sampling (s)                             0.962496\r\n",
      "time/exploration sampling (s)                            0.128054\r\n",
      "time/logging (s)                                         0.0194969\r\n",
      "time/saving (s)                                          0.0280033\r\n",
      "time/training (s)                                       45.2289\r\n",
      "time/epoch (s)                                          46.3731\r\n",
      "time/total (s)                                        1012.22\r\n",
      "Epoch                                                   21\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:06:18.179320 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 22 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00296181\n",
      "trainer/QF2 Loss                                         0.00213245\n",
      "trainer/Policy Loss                                      3.5267\n",
      "trainer/Q1 Predictions Mean                             -1.55285\n",
      "trainer/Q1 Predictions Std                               0.879326\n",
      "trainer/Q1 Predictions Max                               0.140949\n",
      "trainer/Q1 Predictions Min                              -3.6417\n",
      "trainer/Q2 Predictions Mean                             -1.55092\n",
      "trainer/Q2 Predictions Std                               0.879609\n",
      "trainer/Q2 Predictions Max                               0.148866\n",
      "trainer/Q2 Predictions Min                              -3.58209\n",
      "trainer/Q Targets Mean                                  -1.5581\n",
      "trainer/Q Targets Std                                    0.8879\n",
      "trainer/Q Targets Max                                    0.142767\n",
      "trainer/Q Targets Min                                   -3.75147\n",
      "trainer/Log Pis Mean                                     2.00271\n",
      "trainer/Log Pis Std                                      1.49338\n",
      "trainer/Log Pis Max                                      4.7554\n",
      "trainer/Log Pis Min                                     -3.5922\n",
      "trainer/Policy mu Mean                                   0.0691371\n",
      "trainer/Policy mu Std                                    0.335435\n",
      "trainer/Policy mu Max                                    2.40052\n",
      "trainer/Policy mu Min                                   -1.77037\n",
      "trainer/Policy log std Mean                             -2.32596\n",
      "trainer/Policy log std Std                               0.60153\n",
      "trainer/Policy log std Max                              -0.549178\n",
      "trainer/Policy log std Min                              -3.26435\n",
      "trainer/Alpha                                            0.0232098\n",
      "trainer/Alpha Loss                                       0.0101936\n",
      "exploration/num steps total                           3300\n",
      "exploration/num paths total                            165\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.084211\n",
      "exploration/Rewards Std                                  0.116139\n",
      "exploration/Rewards Max                                  0.107273\n",
      "exploration/Rewards Min                                 -0.329728\n",
      "exploration/Returns Mean                                -1.68422\n",
      "exploration/Returns Std                                  1.95324\n",
      "exploration/Returns Max                                  0.725302\n",
      "exploration/Returns Min                                 -4.31059\n",
      "exploration/Actions Mean                                 0.00504356\n",
      "exploration/Actions Std                                  0.132859\n",
      "exploration/Actions Max                                  0.487748\n",
      "exploration/Actions Min                                 -0.347111\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.68422\n",
      "exploration/env_infos/final/reward_dist Mean             0.165412\n",
      "exploration/env_infos/final/reward_dist Std              0.274803\n",
      "exploration/env_infos/final/reward_dist Max              0.707391\n",
      "exploration/env_infos/final/reward_dist Min              5.05068e-24\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00144733\n",
      "exploration/env_infos/initial/reward_dist Std            0.00213209\n",
      "exploration/env_infos/initial/reward_dist Max            0.00565559\n",
      "exploration/env_infos/initial/reward_dist Min            5.65212e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.119468\n",
      "exploration/env_infos/reward_dist Std                    0.229019\n",
      "exploration/env_infos/reward_dist Max                    0.896303\n",
      "exploration/env_infos/reward_dist Min                    5.05068e-24\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0983899\n",
      "exploration/env_infos/final/reward_energy Std            0.0494263\n",
      "exploration/env_infos/final/reward_energy Max           -0.0139971\n",
      "exploration/env_infos/final/reward_energy Min           -0.163216\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.230602\n",
      "exploration/env_infos/initial/reward_energy Std          0.117693\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0428981\n",
      "exploration/env_infos/initial/reward_energy Min         -0.374965\n",
      "exploration/env_infos/reward_energy Mean                -0.154031\n",
      "exploration/env_infos/reward_energy Std                  0.107835\n",
      "exploration/env_infos/reward_energy Max                 -0.00580927\n",
      "exploration/env_infos/reward_energy Min                 -0.5297\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0599175\n",
      "exploration/env_infos/final/end_effector_loc Std         0.2728\n",
      "exploration/env_infos/final/end_effector_loc Max         0.642195\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.371126\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000474052\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0091412\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0135315\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0149138\n",
      "exploration/env_infos/end_effector_loc Mean              0.0115807\n",
      "exploration/env_infos/end_effector_loc Std               0.171824\n",
      "exploration/env_infos/end_effector_loc Max               0.642195\n",
      "exploration/env_infos/end_effector_loc Min              -0.371126\n",
      "evaluation/num steps total                           23000\n",
      "evaluation/num paths total                            1150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0589971\n",
      "evaluation/Rewards Std                                   0.0662042\n",
      "evaluation/Rewards Max                                   0.173691\n",
      "evaluation/Rewards Min                                  -0.285179\n",
      "evaluation/Returns Mean                                 -1.17994\n",
      "evaluation/Returns Std                                   0.99045\n",
      "evaluation/Returns Max                                   0.789956\n",
      "evaluation/Returns Min                                  -3.27785\n",
      "evaluation/Actions Mean                                 -0.000106548\n",
      "evaluation/Actions Std                                   0.054774\n",
      "evaluation/Actions Max                                   0.539577\n",
      "evaluation/Actions Min                                  -0.36557\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.17994\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0639512\n",
      "evaluation/env_infos/final/reward_dist Std               0.142989\n",
      "evaluation/env_infos/final/reward_dist Max               0.735814\n",
      "evaluation/env_infos/final/reward_dist Min               1.2434e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00571345\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0102646\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0463813\n",
      "evaluation/env_infos/initial/reward_dist Min             1.38884e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0990963\n",
      "evaluation/env_infos/reward_dist Std                     0.201439\n",
      "evaluation/env_infos/reward_dist Max                     0.99025\n",
      "evaluation/env_infos/reward_dist Min                     1.2434e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0610445\n",
      "evaluation/env_infos/final/reward_energy Std             0.057435\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00347439\n",
      "evaluation/env_infos/final/reward_energy Min            -0.317079\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.142158\n",
      "evaluation/env_infos/initial/reward_energy Std           0.113287\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0099414\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.551317\n",
      "evaluation/env_infos/reward_energy Mean                 -0.055062\n",
      "evaluation/env_infos/reward_energy Std                   0.0544847\n",
      "evaluation/env_infos/reward_energy Max                  -0.00106816\n",
      "evaluation/env_infos/reward_energy Min                  -0.551317\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0205294\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.242857\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.45867\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.508343\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000655706\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00639325\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0269789\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0170125\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0143327\n",
      "evaluation/env_infos/end_effector_loc Std                0.142112\n",
      "evaluation/env_infos/end_effector_loc Max                0.45867\n",
      "evaluation/env_infos/end_effector_loc Min               -0.508343\n",
      "time/data storing (s)                                    0.006015\n",
      "time/evaluation sampling (s)                             0.926159\n",
      "time/exploration sampling (s)                            0.124119\n",
      "time/logging (s)                                         0.0199704\n",
      "time/saving (s)                                          0.0306809\n",
      "time/training (s)                                       46.2871\n",
      "time/epoch (s)                                          47.3941\n",
      "time/total (s)                                        1059.92\n",
      "Epoch                                                   22\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:07:08.529700 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 23 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00115135\n",
      "trainer/QF2 Loss                                         0.0027805\n",
      "trainer/Policy Loss                                      3.42762\n",
      "trainer/Q1 Predictions Mean                             -1.56506\n",
      "trainer/Q1 Predictions Std                               0.904277\n",
      "trainer/Q1 Predictions Max                               0.147458\n",
      "trainer/Q1 Predictions Min                              -3.66927\n",
      "trainer/Q2 Predictions Mean                             -1.55986\n",
      "trainer/Q2 Predictions Std                               0.905703\n",
      "trainer/Q2 Predictions Max                               0.147768\n",
      "trainer/Q2 Predictions Min                              -3.70043\n",
      "trainer/Q Targets Mean                                  -1.57557\n",
      "trainer/Q Targets Std                                    0.910153\n",
      "trainer/Q Targets Max                                    0.135251\n",
      "trainer/Q Targets Min                                   -3.68229\n",
      "trainer/Log Pis Mean                                     1.89491\n",
      "trainer/Log Pis Std                                      1.40336\n",
      "trainer/Log Pis Max                                      4.30214\n",
      "trainer/Log Pis Min                                     -6.89177\n",
      "trainer/Policy mu Mean                                   0.061298\n",
      "trainer/Policy mu Std                                    0.357396\n",
      "trainer/Policy mu Max                                    2.08198\n",
      "trainer/Policy mu Min                                   -1.87045\n",
      "trainer/Policy log std Mean                             -2.27027\n",
      "trainer/Policy log std Std                               0.631064\n",
      "trainer/Policy log std Max                              -0.071803\n",
      "trainer/Policy log std Min                              -3.29324\n",
      "trainer/Alpha                                            0.0213593\n",
      "trainer/Alpha Loss                                      -0.404197\n",
      "exploration/num steps total                           3400\n",
      "exploration/num paths total                            170\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0865426\n",
      "exploration/Rewards Std                                  0.114005\n",
      "exploration/Rewards Max                                  0.130129\n",
      "exploration/Rewards Min                                 -0.270445\n",
      "exploration/Returns Mean                                -1.73085\n",
      "exploration/Returns Std                                  1.98324\n",
      "exploration/Returns Max                                  0.731914\n",
      "exploration/Returns Min                                 -3.8163\n",
      "exploration/Actions Mean                                 0.00158828\n",
      "exploration/Actions Std                                  0.190727\n",
      "exploration/Actions Max                                  0.468749\n",
      "exploration/Actions Min                                 -0.615168\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.73085\n",
      "exploration/env_infos/final/reward_dist Mean             0.0373579\n",
      "exploration/env_infos/final/reward_dist Std              0.0318289\n",
      "exploration/env_infos/final/reward_dist Max              0.0805723\n",
      "exploration/env_infos/final/reward_dist Min              4.80388e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00840013\n",
      "exploration/env_infos/initial/reward_dist Std            0.0109063\n",
      "exploration/env_infos/initial/reward_dist Max            0.0268406\n",
      "exploration/env_infos/initial/reward_dist Min            2.83938e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.181077\n",
      "exploration/env_infos/reward_dist Std                    0.304643\n",
      "exploration/env_infos/reward_dist Max                    0.977205\n",
      "exploration/env_infos/reward_dist Min                    4.80388e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.218348\n",
      "exploration/env_infos/final/reward_energy Std            0.101468\n",
      "exploration/env_infos/final/reward_energy Max           -0.128902\n",
      "exploration/env_infos/final/reward_energy Min           -0.359777\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.341066\n",
      "exploration/env_infos/initial/reward_energy Std          0.0936779\n",
      "exploration/env_infos/initial/reward_energy Max         -0.212355\n",
      "exploration/env_infos/initial/reward_energy Min         -0.491588\n",
      "exploration/env_infos/reward_energy Mean                -0.222736\n",
      "exploration/env_infos/reward_energy Std                  0.152144\n",
      "exploration/env_infos/reward_energy Max                 -0.0126115\n",
      "exploration/env_infos/reward_energy Min                 -0.651009\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.064119\n",
      "exploration/env_infos/final/end_effector_loc Std         0.210376\n",
      "exploration/env_infos/final/end_effector_loc Max         0.337231\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.334866\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0092938\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00836674\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0234374\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0036429\n",
      "exploration/env_infos/end_effector_loc Mean              0.0702746\n",
      "exploration/env_infos/end_effector_loc Std               0.137809\n",
      "exploration/env_infos/end_effector_loc Max               0.337231\n",
      "exploration/env_infos/end_effector_loc Min              -0.336428\n",
      "evaluation/num steps total                           24000\n",
      "evaluation/num paths total                            1200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0528956\n",
      "evaluation/Rewards Std                                   0.0645184\n",
      "evaluation/Rewards Max                                   0.119325\n",
      "evaluation/Rewards Min                                  -0.327885\n",
      "evaluation/Returns Mean                                 -1.05791\n",
      "evaluation/Returns Std                                   0.938441\n",
      "evaluation/Returns Max                                   1.1236\n",
      "evaluation/Returns Min                                  -3.73534\n",
      "evaluation/Actions Mean                                 -0.00101301\n",
      "evaluation/Actions Std                                   0.055129\n",
      "evaluation/Actions Max                                   0.613384\n",
      "evaluation/Actions Min                                  -0.277254\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.05791\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0997395\n",
      "evaluation/env_infos/final/reward_dist Std               0.154481\n",
      "evaluation/env_infos/final/reward_dist Max               0.672443\n",
      "evaluation/env_infos/final/reward_dist Min               8.88186e-26\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00748517\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0102415\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0361432\n",
      "evaluation/env_infos/initial/reward_dist Min             1.72429e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.149298\n",
      "evaluation/env_infos/reward_dist Std                     0.23323\n",
      "evaluation/env_infos/reward_dist Max                     0.996402\n",
      "evaluation/env_infos/reward_dist Min                     8.88186e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0421956\n",
      "evaluation/env_infos/final/reward_energy Std             0.0451053\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00263871\n",
      "evaluation/env_infos/final/reward_energy Min            -0.236867\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.161849\n",
      "evaluation/env_infos/initial/reward_energy Std           0.165332\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0211589\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.771381\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0493234\n",
      "evaluation/env_infos/reward_energy Std                   0.060396\n",
      "evaluation/env_infos/reward_energy Max                  -0.000504645\n",
      "evaluation/env_infos/reward_energy Min                  -0.771381\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00811838\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.243367\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.39268\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.79027\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00217912\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00788439\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0306692\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0138627\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0151024\n",
      "evaluation/env_infos/end_effector_loc Std                0.147807\n",
      "evaluation/env_infos/end_effector_loc Max                0.39268\n",
      "evaluation/env_infos/end_effector_loc Min               -0.79027\n",
      "time/data storing (s)                                    0.0062339\n",
      "time/evaluation sampling (s)                             1.03216\n",
      "time/exploration sampling (s)                            0.121074\n",
      "time/logging (s)                                         0.0212914\n",
      "time/saving (s)                                          0.0270581\n",
      "time/training (s)                                       48.8167\n",
      "time/epoch (s)                                          50.0245\n",
      "time/total (s)                                        1110.27\n",
      "Epoch                                                   23\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:08:06.932084 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 24 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00101966\n",
      "trainer/QF2 Loss                                         0.000907167\n",
      "trainer/Policy Loss                                      3.36958\n",
      "trainer/Q1 Predictions Mean                             -1.3827\n",
      "trainer/Q1 Predictions Std                               0.84008\n",
      "trainer/Q1 Predictions Max                               0.0436106\n",
      "trainer/Q1 Predictions Min                              -3.58513\n",
      "trainer/Q2 Predictions Mean                             -1.39387\n",
      "trainer/Q2 Predictions Std                               0.85541\n",
      "trainer/Q2 Predictions Max                               0.0455982\n",
      "trainer/Q2 Predictions Min                              -3.61427\n",
      "trainer/Q Targets Mean                                  -1.38985\n",
      "trainer/Q Targets Std                                    0.85565\n",
      "trainer/Q Targets Max                                    0.0314345\n",
      "trainer/Q Targets Min                                   -3.65508\n",
      "trainer/Log Pis Mean                                     2.00658\n",
      "trainer/Log Pis Std                                      1.45038\n",
      "trainer/Log Pis Max                                      4.21204\n",
      "trainer/Log Pis Min                                     -4.38625\n",
      "trainer/Policy mu Mean                                  -0.0152967\n",
      "trainer/Policy mu Std                                    0.340893\n",
      "trainer/Policy mu Max                                    2.16236\n",
      "trainer/Policy mu Min                                   -2.18235\n",
      "trainer/Policy log std Mean                             -2.31033\n",
      "trainer/Policy log std Std                               0.617321\n",
      "trainer/Policy log std Max                              -0.547212\n",
      "trainer/Policy log std Min                              -3.20876\n",
      "trainer/Alpha                                            0.0222501\n",
      "trainer/Alpha Loss                                       0.0250247\n",
      "exploration/num steps total                           3500\n",
      "exploration/num paths total                            175\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.120915\n",
      "exploration/Rewards Std                                  0.0562137\n",
      "exploration/Rewards Max                                  0.0115819\n",
      "exploration/Rewards Min                                 -0.286442\n",
      "exploration/Returns Mean                                -2.4183\n",
      "exploration/Returns Std                                  0.917463\n",
      "exploration/Returns Max                                 -0.800855\n",
      "exploration/Returns Min                                 -3.60569\n",
      "exploration/Actions Mean                                 0.00015411\n",
      "exploration/Actions Std                                  0.0850425\n",
      "exploration/Actions Max                                  0.261787\n",
      "exploration/Actions Min                                 -0.381439\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.4183\n",
      "exploration/env_infos/final/reward_dist Mean             0.0607495\n",
      "exploration/env_infos/final/reward_dist Std              0.0590069\n",
      "exploration/env_infos/final/reward_dist Max              0.159004\n",
      "exploration/env_infos/final/reward_dist Min              3.63961e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000188128\n",
      "exploration/env_infos/initial/reward_dist Std            0.00033667\n",
      "exploration/env_infos/initial/reward_dist Max            0.000860876\n",
      "exploration/env_infos/initial/reward_dist Min            3.55106e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.030254\n",
      "exploration/env_infos/reward_dist Std                    0.0719408\n",
      "exploration/env_infos/reward_dist Max                    0.302173\n",
      "exploration/env_infos/reward_dist Min                    3.63961e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.124939\n",
      "exploration/env_infos/final/reward_energy Std            0.0502728\n",
      "exploration/env_infos/final/reward_energy Max           -0.046158\n",
      "exploration/env_infos/final/reward_energy Min           -0.171943\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0706115\n",
      "exploration/env_infos/initial/reward_energy Std          0.0364744\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0329767\n",
      "exploration/env_infos/initial/reward_energy Min         -0.128875\n",
      "exploration/env_infos/reward_energy Mean                -0.0968713\n",
      "exploration/env_infos/reward_energy Std                  0.0712772\n",
      "exploration/env_infos/reward_energy Max                 -0.00396097\n",
      "exploration/env_infos/reward_energy Min                 -0.39946\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0671781\n",
      "exploration/env_infos/final/end_effector_loc Std         0.130636\n",
      "exploration/env_infos/final/end_effector_loc Max         0.258669\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.141609\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00053758\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00275798\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00398865\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00631387\n",
      "exploration/env_infos/end_effector_loc Mean              0.0315001\n",
      "exploration/env_infos/end_effector_loc Std               0.0852649\n",
      "exploration/env_infos/end_effector_loc Max               0.258669\n",
      "exploration/env_infos/end_effector_loc Min              -0.141609\n",
      "evaluation/num steps total                           25000\n",
      "evaluation/num paths total                            1250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0477699\n",
      "evaluation/Rewards Std                                   0.0680663\n",
      "evaluation/Rewards Max                                   0.130567\n",
      "evaluation/Rewards Min                                  -0.248411\n",
      "evaluation/Returns Mean                                 -0.955399\n",
      "evaluation/Returns Std                                   1.09529\n",
      "evaluation/Returns Max                                   1.09752\n",
      "evaluation/Returns Min                                  -3.33708\n",
      "evaluation/Actions Mean                                 -0.00306304\n",
      "evaluation/Actions Std                                   0.0476677\n",
      "evaluation/Actions Max                                   0.347415\n",
      "evaluation/Actions Min                                  -0.29204\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.955399\n",
      "evaluation/env_infos/final/reward_dist Mean              0.18299\n",
      "evaluation/env_infos/final/reward_dist Std               0.258904\n",
      "evaluation/env_infos/final/reward_dist Max               0.8522\n",
      "evaluation/env_infos/final/reward_dist Min               3.80296e-15\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00456721\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00805797\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0348532\n",
      "evaluation/env_infos/initial/reward_dist Min             6.3253e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.162598\n",
      "evaluation/env_infos/reward_dist Std                     0.236136\n",
      "evaluation/env_infos/reward_dist Max                     0.999303\n",
      "evaluation/env_infos/reward_dist Min                     3.80296e-15\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0454345\n",
      "evaluation/env_infos/final/reward_energy Std             0.040406\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00272688\n",
      "evaluation/env_infos/final/reward_energy Min            -0.186397\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.129924\n",
      "evaluation/env_infos/initial/reward_energy Std           0.107884\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00508582\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.422296\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0464437\n",
      "evaluation/env_infos/reward_energy Std                   0.0490527\n",
      "evaluation/env_infos/reward_energy Max                  -0.00191862\n",
      "evaluation/env_infos/reward_energy Min                  -0.422296\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0347013\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.216086\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.421477\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.635161\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000238013\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00596591\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0173708\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.014602\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0131424\n",
      "evaluation/env_infos/end_effector_loc Std                0.133081\n",
      "evaluation/env_infos/end_effector_loc Max                0.421477\n",
      "evaluation/env_infos/end_effector_loc Min               -0.635161\n",
      "time/data storing (s)                                    0.00590094\n",
      "time/evaluation sampling (s)                             0.970361\n",
      "time/exploration sampling (s)                            0.131786\n",
      "time/logging (s)                                         0.0234659\n",
      "time/saving (s)                                          0.0295916\n",
      "time/training (s)                                       56.8348\n",
      "time/epoch (s)                                          57.9959\n",
      "time/total (s)                                        1168.68\n",
      "Epoch                                                   24\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:09:06.016520 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 25 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00158631\n",
      "trainer/QF2 Loss                                         0.00267337\n",
      "trainer/Policy Loss                                      3.51543\n",
      "trainer/Q1 Predictions Mean                             -1.61229\n",
      "trainer/Q1 Predictions Std                               0.877838\n",
      "trainer/Q1 Predictions Max                               0.222648\n",
      "trainer/Q1 Predictions Min                              -3.49534\n",
      "trainer/Q2 Predictions Mean                             -1.61337\n",
      "trainer/Q2 Predictions Std                               0.878885\n",
      "trainer/Q2 Predictions Max                               0.219926\n",
      "trainer/Q2 Predictions Min                              -3.47018\n",
      "trainer/Q Targets Mean                                  -1.61637\n",
      "trainer/Q Targets Std                                    0.885359\n",
      "trainer/Q Targets Max                                    0.218807\n",
      "trainer/Q Targets Min                                   -3.40441\n",
      "trainer/Log Pis Mean                                     1.92901\n",
      "trainer/Log Pis Std                                      1.25093\n",
      "trainer/Log Pis Max                                      4.26659\n",
      "trainer/Log Pis Min                                     -2.92524\n",
      "trainer/Policy mu Mean                                  -0.00625818\n",
      "trainer/Policy mu Std                                    0.329848\n",
      "trainer/Policy mu Max                                    1.73531\n",
      "trainer/Policy mu Min                                   -2.22944\n",
      "trainer/Policy log std Mean                             -2.27618\n",
      "trainer/Policy log std Std                               0.556591\n",
      "trainer/Policy log std Max                              -0.521209\n",
      "trainer/Policy log std Min                              -3.18397\n",
      "trainer/Alpha                                            0.0217296\n",
      "trainer/Alpha Loss                                      -0.271796\n",
      "exploration/num steps total                           3600\n",
      "exploration/num paths total                            180\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0477943\n",
      "exploration/Rewards Std                                  0.0723519\n",
      "exploration/Rewards Max                                  0.0845044\n",
      "exploration/Rewards Min                                 -0.21998\n",
      "exploration/Returns Mean                                -0.955886\n",
      "exploration/Returns Std                                  0.550725\n",
      "exploration/Returns Max                                 -0.394406\n",
      "exploration/Returns Min                                 -1.89091\n",
      "exploration/Actions Mean                                -0.00515199\n",
      "exploration/Actions Std                                  0.118295\n",
      "exploration/Actions Max                                  0.315103\n",
      "exploration/Actions Min                                 -0.292975\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.955886\n",
      "exploration/env_infos/final/reward_dist Mean             0.0998606\n",
      "exploration/env_infos/final/reward_dist Std              0.127565\n",
      "exploration/env_infos/final/reward_dist Max              0.345667\n",
      "exploration/env_infos/final/reward_dist Min              1.16603e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000283855\n",
      "exploration/env_infos/initial/reward_dist Std            0.000333428\n",
      "exploration/env_infos/initial/reward_dist Max            0.000930873\n",
      "exploration/env_infos/initial/reward_dist Min            1.44263e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.217665\n",
      "exploration/env_infos/reward_dist Std                    0.274554\n",
      "exploration/env_infos/reward_dist Max                    0.982089\n",
      "exploration/env_infos/reward_dist Min                    1.16603e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.152004\n",
      "exploration/env_infos/final/reward_energy Std            0.0569948\n",
      "exploration/env_infos/final/reward_energy Max           -0.0757899\n",
      "exploration/env_infos/final/reward_energy Min           -0.21556\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.180115\n",
      "exploration/env_infos/initial/reward_energy Std          0.111293\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0478308\n",
      "exploration/env_infos/initial/reward_energy Min         -0.317197\n",
      "exploration/env_infos/reward_energy Mean                -0.147731\n",
      "exploration/env_infos/reward_energy Std                  0.0788432\n",
      "exploration/env_infos/reward_energy Max                 -0.0105019\n",
      "exploration/env_infos/reward_energy Min                 -0.348705\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0933747\n",
      "exploration/env_infos/final/end_effector_loc Std         0.13482\n",
      "exploration/env_infos/final/end_effector_loc Max         0.282134\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.158427\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00131118\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00736989\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0157551\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.014151\n",
      "exploration/env_infos/end_effector_loc Mean              0.0693788\n",
      "exploration/env_infos/end_effector_loc Std               0.126879\n",
      "exploration/env_infos/end_effector_loc Max               0.344321\n",
      "exploration/env_infos/end_effector_loc Min              -0.18369\n",
      "evaluation/num steps total                           26000\n",
      "evaluation/num paths total                            1300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0522218\n",
      "evaluation/Rewards Std                                   0.0638551\n",
      "evaluation/Rewards Max                                   0.133847\n",
      "evaluation/Rewards Min                                  -0.325581\n",
      "evaluation/Returns Mean                                 -1.04444\n",
      "evaluation/Returns Std                                   0.965134\n",
      "evaluation/Returns Max                                   1.16098\n",
      "evaluation/Returns Min                                  -3.30275\n",
      "evaluation/Actions Mean                                  0.00154589\n",
      "evaluation/Actions Std                                   0.0430422\n",
      "evaluation/Actions Max                                   0.303963\n",
      "evaluation/Actions Min                                  -0.306416\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.04444\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0658978\n",
      "evaluation/env_infos/final/reward_dist Std               0.168616\n",
      "evaluation/env_infos/final/reward_dist Max               0.907258\n",
      "evaluation/env_infos/final/reward_dist Min               2.37445e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00451445\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00705599\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0262204\n",
      "evaluation/env_infos/initial/reward_dist Min             1.91522e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.111024\n",
      "evaluation/env_infos/reward_dist Std                     0.218484\n",
      "evaluation/env_infos/reward_dist Max                     0.993013\n",
      "evaluation/env_infos/reward_dist Min                     2.37445e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0531378\n",
      "evaluation/env_infos/final/reward_energy Std             0.0337918\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0120508\n",
      "evaluation/env_infos/final/reward_energy Min            -0.145847\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.125301\n",
      "evaluation/env_infos/initial/reward_energy Std           0.0989972\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00306618\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.382844\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0433348\n",
      "evaluation/env_infos/reward_energy Std                   0.0428034\n",
      "evaluation/env_infos/reward_energy Max                  -0.00124256\n",
      "evaluation/env_infos/reward_energy Min                  -0.382844\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.021458\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.244643\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.546189\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.471536\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000246528\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0056405\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0151982\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0153208\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0104386\n",
      "evaluation/env_infos/end_effector_loc Std                0.144891\n",
      "evaluation/env_infos/end_effector_loc Max                0.546189\n",
      "evaluation/env_infos/end_effector_loc Min               -0.471536\n",
      "time/data storing (s)                                    0.00646515\n",
      "time/evaluation sampling (s)                             3.37289\n",
      "time/exploration sampling (s)                            0.146268\n",
      "time/logging (s)                                         0.0234876\n",
      "time/saving (s)                                          0.0300112\n",
      "time/training (s)                                       54.9707\n",
      "time/epoch (s)                                          58.5499\n",
      "time/total (s)                                        1227.76\n",
      "Epoch                                                   25\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:09:57.741851 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 26 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00281357\n",
      "trainer/QF2 Loss                                         0.00193992\n",
      "trainer/Policy Loss                                      3.45541\n",
      "trainer/Q1 Predictions Mean                             -1.4035\n",
      "trainer/Q1 Predictions Std                               0.899455\n",
      "trainer/Q1 Predictions Max                               0.186174\n",
      "trainer/Q1 Predictions Min                              -3.69988\n",
      "trainer/Q2 Predictions Mean                             -1.41697\n",
      "trainer/Q2 Predictions Std                               0.912256\n",
      "trainer/Q2 Predictions Max                               0.22879\n",
      "trainer/Q2 Predictions Min                              -3.75535\n",
      "trainer/Q Targets Mean                                  -1.40688\n",
      "trainer/Q Targets Std                                    0.89604\n",
      "trainer/Q Targets Max                                    0.208038\n",
      "trainer/Q Targets Min                                   -3.69311\n",
      "trainer/Log Pis Mean                                     2.06505\n",
      "trainer/Log Pis Std                                      1.32096\n",
      "trainer/Log Pis Max                                      4.39161\n",
      "trainer/Log Pis Min                                     -2.59249\n",
      "trainer/Policy mu Mean                                  -0.0262723\n",
      "trainer/Policy mu Std                                    0.257945\n",
      "trainer/Policy mu Max                                    1.47092\n",
      "trainer/Policy mu Min                                   -1.90957\n",
      "trainer/Policy log std Mean                             -2.37267\n",
      "trainer/Policy log std Std                               0.560758\n",
      "trainer/Policy log std Max                              -0.486539\n",
      "trainer/Policy log std Min                              -3.25317\n",
      "trainer/Alpha                                            0.0228866\n",
      "trainer/Alpha Loss                                       0.245672\n",
      "exploration/num steps total                           3700\n",
      "exploration/num paths total                            185\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.110746\n",
      "exploration/Rewards Std                                  0.0611082\n",
      "exploration/Rewards Max                                 -0.00288142\n",
      "exploration/Rewards Min                                 -0.259714\n",
      "exploration/Returns Mean                                -2.21491\n",
      "exploration/Returns Std                                  0.778377\n",
      "exploration/Returns Max                                 -1.53478\n",
      "exploration/Returns Min                                 -3.44783\n",
      "exploration/Actions Mean                                -0.00872309\n",
      "exploration/Actions Std                                  0.10779\n",
      "exploration/Actions Max                                  0.347994\n",
      "exploration/Actions Min                                 -0.446452\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.21491\n",
      "exploration/env_infos/final/reward_dist Mean             0.111282\n",
      "exploration/env_infos/final/reward_dist Std              0.221239\n",
      "exploration/env_infos/final/reward_dist Max              0.553755\n",
      "exploration/env_infos/final/reward_dist Min              3.60475e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00147882\n",
      "exploration/env_infos/initial/reward_dist Std            0.00194689\n",
      "exploration/env_infos/initial/reward_dist Max            0.00490746\n",
      "exploration/env_infos/initial/reward_dist Min            7.74825e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.122819\n",
      "exploration/env_infos/reward_dist Std                    0.200081\n",
      "exploration/env_infos/reward_dist Max                    0.847192\n",
      "exploration/env_infos/reward_dist Min                    3.60475e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0849468\n",
      "exploration/env_infos/final/reward_energy Std            0.0465355\n",
      "exploration/env_infos/final/reward_energy Max           -0.0325301\n",
      "exploration/env_infos/final/reward_energy Min           -0.164864\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.151428\n",
      "exploration/env_infos/initial/reward_energy Std          0.102892\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0255414\n",
      "exploration/env_infos/initial/reward_energy Min         -0.288875\n",
      "exploration/env_infos/reward_energy Mean                -0.122558\n",
      "exploration/env_infos/reward_energy Std                  0.091483\n",
      "exploration/env_infos/reward_energy Max                 -0.00485064\n",
      "exploration/env_infos/reward_energy Min                 -0.480661\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0553979\n",
      "exploration/env_infos/final/end_effector_loc Std         0.290941\n",
      "exploration/env_infos/final/end_effector_loc Max         0.377407\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.542564\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000424514\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00645882\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0123513\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0103319\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0124346\n",
      "exploration/env_infos/end_effector_loc Std               0.178204\n",
      "exploration/env_infos/end_effector_loc Max               0.377407\n",
      "exploration/env_infos/end_effector_loc Min              -0.542564\n",
      "evaluation/num steps total                           27000\n",
      "evaluation/num paths total                            1350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0537771\n",
      "evaluation/Rewards Std                                   0.0671158\n",
      "evaluation/Rewards Max                                   0.155416\n",
      "evaluation/Rewards Min                                  -0.400143\n",
      "evaluation/Returns Mean                                 -1.07554\n",
      "evaluation/Returns Std                                   1.08731\n",
      "evaluation/Returns Max                                   2.21729\n",
      "evaluation/Returns Min                                  -3.35766\n",
      "evaluation/Actions Mean                                  0.00213927\n",
      "evaluation/Actions Std                                   0.0468149\n",
      "evaluation/Actions Max                                   0.521767\n",
      "evaluation/Actions Min                                  -0.584724\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.07554\n",
      "evaluation/env_infos/final/reward_dist Mean              0.210902\n",
      "evaluation/env_infos/final/reward_dist Std               0.28455\n",
      "evaluation/env_infos/final/reward_dist Max               0.952993\n",
      "evaluation/env_infos/final/reward_dist Min               1.92998e-32\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00517352\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00849556\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0394728\n",
      "evaluation/env_infos/initial/reward_dist Min             1.57756e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.163706\n",
      "evaluation/env_infos/reward_dist Std                     0.259254\n",
      "evaluation/env_infos/reward_dist Max                     0.988335\n",
      "evaluation/env_infos/reward_dist Min                     1.92998e-32\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0264921\n",
      "evaluation/env_infos/final/reward_energy Std             0.0300192\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00259655\n",
      "evaluation/env_infos/final/reward_energy Min            -0.171701\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.149228\n",
      "evaluation/env_infos/initial/reward_energy Std           0.145097\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0155496\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.783672\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0403244\n",
      "evaluation/env_infos/reward_energy Std                   0.0525963\n",
      "evaluation/env_infos/reward_energy Max                  -0.00104067\n",
      "evaluation/env_infos/reward_energy Min                  -0.783672\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0726844\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23333\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.638073\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.848329\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00121766\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00725742\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0260883\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0292362\n",
      "evaluation/env_infos/end_effector_loc Mean               0.039102\n",
      "evaluation/env_infos/end_effector_loc Std                0.145458\n",
      "evaluation/env_infos/end_effector_loc Max                0.638073\n",
      "evaluation/env_infos/end_effector_loc Min               -0.848329\n",
      "time/data storing (s)                                    0.00654833\n",
      "time/evaluation sampling (s)                             0.985161\n",
      "time/exploration sampling (s)                            0.143464\n",
      "time/logging (s)                                         0.0214943\n",
      "time/saving (s)                                          0.0289814\n",
      "time/training (s)                                       50.155\n",
      "time/epoch (s)                                          51.3407\n",
      "time/total (s)                                        1279.48\n",
      "Epoch                                                   26\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:10:48.313271 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 27 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00149517\n",
      "trainer/QF2 Loss                                         0.00128675\n",
      "trainer/Policy Loss                                      3.51679\n",
      "trainer/Q1 Predictions Mean                             -1.46992\n",
      "trainer/Q1 Predictions Std                               0.911845\n",
      "trainer/Q1 Predictions Max                               0.331786\n",
      "trainer/Q1 Predictions Min                              -3.69809\n",
      "trainer/Q2 Predictions Mean                             -1.46841\n",
      "trainer/Q2 Predictions Std                               0.915455\n",
      "trainer/Q2 Predictions Max                               0.352067\n",
      "trainer/Q2 Predictions Min                              -3.67409\n",
      "trainer/Q Targets Mean                                  -1.44815\n",
      "trainer/Q Targets Std                                    0.91277\n",
      "trainer/Q Targets Max                                    0.394075\n",
      "trainer/Q Targets Min                                   -3.74383\n",
      "trainer/Log Pis Mean                                     2.08005\n",
      "trainer/Log Pis Std                                      1.45196\n",
      "trainer/Log Pis Max                                      4.7279\n",
      "trainer/Log Pis Min                                     -3.92226\n",
      "trainer/Policy mu Mean                                  -0.0399967\n",
      "trainer/Policy mu Std                                    0.334598\n",
      "trainer/Policy mu Max                                    1.88919\n",
      "trainer/Policy mu Min                                   -2.12193\n",
      "trainer/Policy log std Mean                             -2.33928\n",
      "trainer/Policy log std Std                               0.640085\n",
      "trainer/Policy log std Max                              -0.310432\n",
      "trainer/Policy log std Min                              -3.36379\n",
      "trainer/Alpha                                            0.0214406\n",
      "trainer/Alpha Loss                                       0.307637\n",
      "exploration/num steps total                           3800\n",
      "exploration/num paths total                            190\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0714657\n",
      "exploration/Rewards Std                                  0.0459166\n",
      "exploration/Rewards Max                                  0.0611973\n",
      "exploration/Rewards Min                                 -0.211295\n",
      "exploration/Returns Mean                                -1.42931\n",
      "exploration/Returns Std                                  0.490158\n",
      "exploration/Returns Max                                 -0.842778\n",
      "exploration/Returns Min                                 -2.06379\n",
      "exploration/Actions Mean                                -0.00658657\n",
      "exploration/Actions Std                                  0.130849\n",
      "exploration/Actions Max                                  0.381849\n",
      "exploration/Actions Min                                 -0.404022\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.42931\n",
      "exploration/env_infos/final/reward_dist Mean             0.121058\n",
      "exploration/env_infos/final/reward_dist Std              0.225856\n",
      "exploration/env_infos/final/reward_dist Max              0.572183\n",
      "exploration/env_infos/final/reward_dist Min              1.60467e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00828228\n",
      "exploration/env_infos/initial/reward_dist Std            0.00499308\n",
      "exploration/env_infos/initial/reward_dist Max            0.0156493\n",
      "exploration/env_infos/initial/reward_dist Min            1.1238e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0771474\n",
      "exploration/env_infos/reward_dist Std                    0.108383\n",
      "exploration/env_infos/reward_dist Max                    0.572183\n",
      "exploration/env_infos/reward_dist Min                    1.60467e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.15584\n",
      "exploration/env_infos/final/reward_energy Std            0.0857617\n",
      "exploration/env_infos/final/reward_energy Max           -0.080521\n",
      "exploration/env_infos/final/reward_energy Min           -0.301497\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.194325\n",
      "exploration/env_infos/initial/reward_energy Std          0.108193\n",
      "exploration/env_infos/initial/reward_energy Max         -0.060162\n",
      "exploration/env_infos/initial/reward_energy Min         -0.363737\n",
      "exploration/env_infos/reward_energy Mean                -0.153809\n",
      "exploration/env_infos/reward_energy Std                  0.103307\n",
      "exploration/env_infos/reward_energy Max                 -0.010034\n",
      "exploration/env_infos/reward_energy Min                 -0.484408\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0376593\n",
      "exploration/env_infos/final/end_effector_loc Std         0.22252\n",
      "exploration/env_infos/final/end_effector_loc Max         0.363748\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.424205\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000764375\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00782627\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.016425\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0111287\n",
      "exploration/env_infos/end_effector_loc Mean              0.0211245\n",
      "exploration/env_infos/end_effector_loc Std               0.141391\n",
      "exploration/env_infos/end_effector_loc Max               0.363748\n",
      "exploration/env_infos/end_effector_loc Min              -0.424205\n",
      "evaluation/num steps total                           28000\n",
      "evaluation/num paths total                            1400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0465188\n",
      "evaluation/Rewards Std                                   0.064838\n",
      "evaluation/Rewards Max                                   0.149182\n",
      "evaluation/Rewards Min                                  -0.315971\n",
      "evaluation/Returns Mean                                 -0.930375\n",
      "evaluation/Returns Std                                   0.938715\n",
      "evaluation/Returns Max                                   1.82436\n",
      "evaluation/Returns Min                                  -2.34313\n",
      "evaluation/Actions Mean                                  0.00157026\n",
      "evaluation/Actions Std                                   0.0554357\n",
      "evaluation/Actions Max                                   0.395896\n",
      "evaluation/Actions Min                                  -0.480546\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.930375\n",
      "evaluation/env_infos/final/reward_dist Mean              0.144105\n",
      "evaluation/env_infos/final/reward_dist Std               0.216297\n",
      "evaluation/env_infos/final/reward_dist Max               0.978567\n",
      "evaluation/env_infos/final/reward_dist Min               4.30897e-36\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00608377\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0101884\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0460576\n",
      "evaluation/env_infos/initial/reward_dist Min             9.96735e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.154592\n",
      "evaluation/env_infos/reward_dist Std                     0.224089\n",
      "evaluation/env_infos/reward_dist Max                     0.989316\n",
      "evaluation/env_infos/reward_dist Min                     4.30897e-36\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0381194\n",
      "evaluation/env_infos/final/reward_energy Std             0.0253443\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00301004\n",
      "evaluation/env_infos/final/reward_energy Min            -0.109858\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.184462\n",
      "evaluation/env_infos/initial/reward_energy Std           0.145479\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0107161\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.506747\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0499705\n",
      "evaluation/env_infos/reward_energy Std                   0.0604493\n",
      "evaluation/env_infos/reward_energy Max                  -0.000337199\n",
      "evaluation/env_infos/reward_energy Min                  -0.506747\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0022092\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.237641\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.667794\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.890811\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000246438\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00830224\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0197948\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0240273\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00280374\n",
      "evaluation/env_infos/end_effector_loc Std                0.149061\n",
      "evaluation/env_infos/end_effector_loc Max                0.667794\n",
      "evaluation/env_infos/end_effector_loc Min               -0.890811\n",
      "time/data storing (s)                                    0.00613781\n",
      "time/evaluation sampling (s)                             1.18754\n",
      "time/exploration sampling (s)                            0.132732\n",
      "time/logging (s)                                         0.0219499\n",
      "time/saving (s)                                          0.0275108\n",
      "time/training (s)                                       48.7972\n",
      "time/epoch (s)                                          50.1731\n",
      "time/total (s)                                        1330.05\n",
      "Epoch                                                   27\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:11:37.048031 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 28 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00149126\n",
      "trainer/QF2 Loss                                         0.00110868\n",
      "trainer/Policy Loss                                      3.33954\n",
      "trainer/Q1 Predictions Mean                             -1.30032\n",
      "trainer/Q1 Predictions Std                               0.884798\n",
      "trainer/Q1 Predictions Max                               0.437295\n",
      "trainer/Q1 Predictions Min                              -3.43746\n",
      "trainer/Q2 Predictions Mean                             -1.30416\n",
      "trainer/Q2 Predictions Std                               0.881045\n",
      "trainer/Q2 Predictions Max                               0.402495\n",
      "trainer/Q2 Predictions Min                              -3.43913\n",
      "trainer/Q Targets Mean                                  -1.3033\n",
      "trainer/Q Targets Std                                    0.884399\n",
      "trainer/Q Targets Max                                    0.482364\n",
      "trainer/Q Targets Min                                   -3.49073\n",
      "trainer/Log Pis Mean                                     2.06634\n",
      "trainer/Log Pis Std                                      1.28317\n",
      "trainer/Log Pis Max                                      4.99428\n",
      "trainer/Log Pis Min                                     -5.0374\n",
      "trainer/Policy mu Mean                                  -0.00833795\n",
      "trainer/Policy mu Std                                    0.33856\n",
      "trainer/Policy mu Max                                    1.70658\n",
      "trainer/Policy mu Min                                   -1.79332\n",
      "trainer/Policy log std Mean                             -2.33009\n",
      "trainer/Policy log std Std                               0.65416\n",
      "trainer/Policy log std Max                              -0.443372\n",
      "trainer/Policy log std Min                              -3.49159\n",
      "trainer/Alpha                                            0.0197091\n",
      "trainer/Alpha Loss                                       0.260462\n",
      "exploration/num steps total                           3900\n",
      "exploration/num paths total                            195\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0624223\n",
      "exploration/Rewards Std                                  0.0662681\n",
      "exploration/Rewards Max                                  0.113572\n",
      "exploration/Rewards Min                                 -0.286321\n",
      "exploration/Returns Mean                                -1.24845\n",
      "exploration/Returns Std                                  0.747085\n",
      "exploration/Returns Max                                 -0.140806\n",
      "exploration/Returns Min                                 -2.2456\n",
      "exploration/Actions Mean                                 0.010034\n",
      "exploration/Actions Std                                  0.10883\n",
      "exploration/Actions Max                                  0.381322\n",
      "exploration/Actions Min                                 -0.3973\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.24845\n",
      "exploration/env_infos/final/reward_dist Mean             0.11192\n",
      "exploration/env_infos/final/reward_dist Std              0.0957417\n",
      "exploration/env_infos/final/reward_dist Max              0.2325\n",
      "exploration/env_infos/final/reward_dist Min              8.76281e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0159644\n",
      "exploration/env_infos/initial/reward_dist Std            0.0197279\n",
      "exploration/env_infos/initial/reward_dist Max            0.0444057\n",
      "exploration/env_infos/initial/reward_dist Min            2.31993e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.227403\n",
      "exploration/env_infos/reward_dist Std                    0.292256\n",
      "exploration/env_infos/reward_dist Max                    0.989664\n",
      "exploration/env_infos/reward_dist Min                    8.76281e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.127773\n",
      "exploration/env_infos/final/reward_energy Std            0.0748845\n",
      "exploration/env_infos/final/reward_energy Max           -0.0310444\n",
      "exploration/env_infos/final/reward_energy Min           -0.239575\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.202407\n",
      "exploration/env_infos/initial/reward_energy Std          0.0957624\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0771126\n",
      "exploration/env_infos/initial/reward_energy Min         -0.369166\n",
      "exploration/env_infos/reward_energy Mean                -0.11993\n",
      "exploration/env_infos/reward_energy Std                  0.0974981\n",
      "exploration/env_infos/reward_energy Max                 -0.00628512\n",
      "exploration/env_infos/reward_energy Min                 -0.411131\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.201351\n",
      "exploration/env_infos/final/end_effector_loc Std         0.163929\n",
      "exploration/env_infos/final/end_effector_loc Max         0.527473\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.160703\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00582067\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00536596\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0171987\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00348035\n",
      "exploration/env_infos/end_effector_loc Mean              0.106478\n",
      "exploration/env_infos/end_effector_loc Std               0.115491\n",
      "exploration/env_infos/end_effector_loc Max               0.527473\n",
      "exploration/env_infos/end_effector_loc Min              -0.160703\n",
      "evaluation/num steps total                           29000\n",
      "evaluation/num paths total                            1450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0386751\n",
      "evaluation/Rewards Std                                   0.0726434\n",
      "evaluation/Rewards Max                                   0.146765\n",
      "evaluation/Rewards Min                                  -0.315357\n",
      "evaluation/Returns Mean                                 -0.773503\n",
      "evaluation/Returns Std                                   1.16976\n",
      "evaluation/Returns Max                                   1.77575\n",
      "evaluation/Returns Min                                  -4.78987\n",
      "evaluation/Actions Mean                                  0.00216897\n",
      "evaluation/Actions Std                                   0.0609715\n",
      "evaluation/Actions Max                                   0.687285\n",
      "evaluation/Actions Min                                  -0.358177\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.773503\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0967362\n",
      "evaluation/env_infos/final/reward_dist Std               0.17698\n",
      "evaluation/env_infos/final/reward_dist Max               0.771847\n",
      "evaluation/env_infos/final/reward_dist Min               1.26886e-34\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00717228\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0123708\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0470532\n",
      "evaluation/env_infos/initial/reward_dist Min             5.49233e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.142177\n",
      "evaluation/env_infos/reward_dist Std                     0.224587\n",
      "evaluation/env_infos/reward_dist Max                     0.972756\n",
      "evaluation/env_infos/reward_dist Min                     1.26886e-34\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0601311\n",
      "evaluation/env_infos/final/reward_energy Std             0.0422587\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00575438\n",
      "evaluation/env_infos/final/reward_energy Min            -0.170427\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.197834\n",
      "evaluation/env_infos/initial/reward_energy Std           0.157259\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0143579\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.858306\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0567259\n",
      "evaluation/env_infos/reward_energy Std                   0.0650125\n",
      "evaluation/env_infos/reward_energy Max                  -0.000666125\n",
      "evaluation/env_infos/reward_energy Min                  -0.858306\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0879375\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.245279\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.672736\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.511856\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00283415\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00847367\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0343643\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0179089\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0490792\n",
      "evaluation/env_infos/end_effector_loc Std                0.157953\n",
      "evaluation/env_infos/end_effector_loc Max                0.672736\n",
      "evaluation/env_infos/end_effector_loc Min               -0.511856\n",
      "time/data storing (s)                                    0.00623608\n",
      "time/evaluation sampling (s)                             0.956143\n",
      "time/exploration sampling (s)                            0.121411\n",
      "time/logging (s)                                         0.0188281\n",
      "time/saving (s)                                          0.0270198\n",
      "time/training (s)                                       47.2327\n",
      "time/epoch (s)                                          48.3623\n",
      "time/total (s)                                        1378.78\n",
      "Epoch                                                   28\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:12:26.764358 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 29 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000737794\n",
      "trainer/QF2 Loss                                         0.000962618\n",
      "trainer/Policy Loss                                      3.30718\n",
      "trainer/Q1 Predictions Mean                             -1.31359\n",
      "trainer/Q1 Predictions Std                               0.914484\n",
      "trainer/Q1 Predictions Max                               0.180001\n",
      "trainer/Q1 Predictions Min                              -3.57987\n",
      "trainer/Q2 Predictions Mean                             -1.3262\n",
      "trainer/Q2 Predictions Std                               0.906612\n",
      "trainer/Q2 Predictions Max                               0.162486\n",
      "trainer/Q2 Predictions Min                              -3.57553\n",
      "trainer/Q Targets Mean                                  -1.31277\n",
      "trainer/Q Targets Std                                    0.907582\n",
      "trainer/Q Targets Max                                    0.13865\n",
      "trainer/Q Targets Min                                   -3.60129\n",
      "trainer/Log Pis Mean                                     2.01915\n",
      "trainer/Log Pis Std                                      1.50267\n",
      "trainer/Log Pis Max                                      4.64729\n",
      "trainer/Log Pis Min                                     -3.99946\n",
      "trainer/Policy mu Mean                                  -0.0549513\n",
      "trainer/Policy mu Std                                    0.305856\n",
      "trainer/Policy mu Max                                    1.90361\n",
      "trainer/Policy mu Min                                   -1.76062\n",
      "trainer/Policy log std Mean                             -2.31983\n",
      "trainer/Policy log std Std                               0.617152\n",
      "trainer/Policy log std Max                              -0.480606\n",
      "trainer/Policy log std Min                              -3.41705\n",
      "trainer/Alpha                                            0.0196988\n",
      "trainer/Alpha Loss                                       0.0752239\n",
      "exploration/num steps total                           4000\n",
      "exploration/num paths total                            200\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.069123\n",
      "exploration/Rewards Std                                  0.0737118\n",
      "exploration/Rewards Max                                  0.0976863\n",
      "exploration/Rewards Min                                 -0.253213\n",
      "exploration/Returns Mean                                -1.38246\n",
      "exploration/Returns Std                                  1.01684\n",
      "exploration/Returns Max                                 -0.170457\n",
      "exploration/Returns Min                                 -2.80434\n",
      "exploration/Actions Mean                                 0.00475376\n",
      "exploration/Actions Std                                  0.119379\n",
      "exploration/Actions Max                                  0.448093\n",
      "exploration/Actions Min                                 -0.345535\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.38246\n",
      "exploration/env_infos/final/reward_dist Mean             0.131263\n",
      "exploration/env_infos/final/reward_dist Std              0.262384\n",
      "exploration/env_infos/final/reward_dist Max              0.65603\n",
      "exploration/env_infos/final/reward_dist Min              6.68582e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00843955\n",
      "exploration/env_infos/initial/reward_dist Std            0.00522522\n",
      "exploration/env_infos/initial/reward_dist Max            0.0146393\n",
      "exploration/env_infos/initial/reward_dist Min            0.00150331\n",
      "exploration/env_infos/reward_dist Mean                   0.173088\n",
      "exploration/env_infos/reward_dist Std                    0.246194\n",
      "exploration/env_infos/reward_dist Max                    0.977964\n",
      "exploration/env_infos/reward_dist Min                    6.68582e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.109443\n",
      "exploration/env_infos/final/reward_energy Std            0.0905615\n",
      "exploration/env_infos/final/reward_energy Max           -0.0411406\n",
      "exploration/env_infos/final/reward_energy Min           -0.288398\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.239299\n",
      "exploration/env_infos/initial/reward_energy Std          0.111893\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0628123\n",
      "exploration/env_infos/initial/reward_energy Min         -0.362015\n",
      "exploration/env_infos/reward_energy Mean                -0.142241\n",
      "exploration/env_infos/reward_energy Std                  0.0911889\n",
      "exploration/env_infos/reward_energy Max                 -0.0128814\n",
      "exploration/env_infos/reward_energy Min                 -0.456878\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00325094\n",
      "exploration/env_infos/final/end_effector_loc Std         0.294767\n",
      "exploration/env_infos/final/end_effector_loc Max         0.648309\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.480069\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00202822\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00911682\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0145325\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0172767\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0130002\n",
      "exploration/env_infos/end_effector_loc Std               0.191353\n",
      "exploration/env_infos/end_effector_loc Max               0.648309\n",
      "exploration/env_infos/end_effector_loc Min              -0.480069\n",
      "evaluation/num steps total                           30000\n",
      "evaluation/num paths total                            1500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0478344\n",
      "evaluation/Rewards Std                                   0.0781518\n",
      "evaluation/Rewards Max                                   0.154906\n",
      "evaluation/Rewards Min                                  -0.34385\n",
      "evaluation/Returns Mean                                 -0.956688\n",
      "evaluation/Returns Std                                   1.23753\n",
      "evaluation/Returns Max                                   1.38237\n",
      "evaluation/Returns Min                                  -5.42146\n",
      "evaluation/Actions Mean                                 -5.85024e-05\n",
      "evaluation/Actions Std                                   0.0605195\n",
      "evaluation/Actions Max                                   0.585635\n",
      "evaluation/Actions Min                                  -0.571492\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.956688\n",
      "evaluation/env_infos/final/reward_dist Mean              0.174392\n",
      "evaluation/env_infos/final/reward_dist Std               0.244496\n",
      "evaluation/env_infos/final/reward_dist Max               0.925011\n",
      "evaluation/env_infos/final/reward_dist Min               1.6215e-44\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00584048\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0103673\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0423888\n",
      "evaluation/env_infos/initial/reward_dist Min             1.23617e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.165572\n",
      "evaluation/env_infos/reward_dist Std                     0.242717\n",
      "evaluation/env_infos/reward_dist Max                     0.996089\n",
      "evaluation/env_infos/reward_dist Min                     1.6215e-44\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0334933\n",
      "evaluation/env_infos/final/reward_energy Std             0.0260496\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00624763\n",
      "evaluation/env_infos/final/reward_energy Min            -0.12135\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.182071\n",
      "evaluation/env_infos/initial/reward_energy Std           0.157393\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0143253\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.59927\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0538416\n",
      "evaluation/env_infos/reward_energy Std                   0.0665305\n",
      "evaluation/env_infos/reward_energy Max                  -0.00105336\n",
      "evaluation/env_infos/reward_energy Min                  -0.59927\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0156393\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.244082\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.607835\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000664551\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.008483\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0292818\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0285746\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0076434\n",
      "evaluation/env_infos/end_effector_loc Std                0.164779\n",
      "evaluation/env_infos/end_effector_loc Max                0.607835\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00598187\n",
      "time/evaluation sampling (s)                             0.949223\n",
      "time/exploration sampling (s)                            0.13941\n",
      "time/logging (s)                                         0.0189167\n",
      "time/saving (s)                                          0.0635971\n",
      "time/training (s)                                       48.1715\n",
      "time/epoch (s)                                          49.3487\n",
      "time/total (s)                                        1428.5\n",
      "Epoch                                                   29\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:13:15.537347 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 30 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000806388\n",
      "trainer/QF2 Loss                                         0.000678222\n",
      "trainer/Policy Loss                                      3.11431\n",
      "trainer/Q1 Predictions Mean                             -1.24033\n",
      "trainer/Q1 Predictions Std                               0.86354\n",
      "trainer/Q1 Predictions Max                               0.501521\n",
      "trainer/Q1 Predictions Min                              -3.57237\n",
      "trainer/Q2 Predictions Mean                             -1.24972\n",
      "trainer/Q2 Predictions Std                               0.864981\n",
      "trainer/Q2 Predictions Max                               0.484863\n",
      "trainer/Q2 Predictions Min                              -3.55382\n",
      "trainer/Q Targets Mean                                  -1.24472\n",
      "trainer/Q Targets Std                                    0.859898\n",
      "trainer/Q Targets Max                                    0.489754\n",
      "trainer/Q Targets Min                                   -3.58597\n",
      "trainer/Log Pis Mean                                     1.89674\n",
      "trainer/Log Pis Std                                      1.49676\n",
      "trainer/Log Pis Max                                      4.30452\n",
      "trainer/Log Pis Min                                     -4.52215\n",
      "trainer/Policy mu Mean                                  -0.0399033\n",
      "trainer/Policy mu Std                                    0.346213\n",
      "trainer/Policy mu Max                                    1.80276\n",
      "trainer/Policy mu Min                                   -1.84594\n",
      "trainer/Policy log std Mean                             -2.28613\n",
      "trainer/Policy log std Std                               0.638481\n",
      "trainer/Policy log std Max                              -0.433625\n",
      "trainer/Policy log std Min                              -3.39573\n",
      "trainer/Alpha                                            0.01926\n",
      "trainer/Alpha Loss                                      -0.407837\n",
      "exploration/num steps total                           4100\n",
      "exploration/num paths total                            205\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0453252\n",
      "exploration/Rewards Std                                  0.0602498\n",
      "exploration/Rewards Max                                  0.12533\n",
      "exploration/Rewards Min                                 -0.19298\n",
      "exploration/Returns Mean                                -0.906503\n",
      "exploration/Returns Std                                  0.79581\n",
      "exploration/Returns Max                                  0.522048\n",
      "exploration/Returns Min                                 -1.62048\n",
      "exploration/Actions Mean                                -0.00388164\n",
      "exploration/Actions Std                                  0.13216\n",
      "exploration/Actions Max                                  0.885661\n",
      "exploration/Actions Min                                 -0.48715\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.906503\n",
      "exploration/env_infos/final/reward_dist Mean             0.0672868\n",
      "exploration/env_infos/final/reward_dist Std              0.0944888\n",
      "exploration/env_infos/final/reward_dist Max              0.243611\n",
      "exploration/env_infos/final/reward_dist Min              2.59687e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00663059\n",
      "exploration/env_infos/initial/reward_dist Std            0.0074863\n",
      "exploration/env_infos/initial/reward_dist Max            0.0162655\n",
      "exploration/env_infos/initial/reward_dist Min            0.000151264\n",
      "exploration/env_infos/reward_dist Mean                   0.188354\n",
      "exploration/env_infos/reward_dist Std                    0.230437\n",
      "exploration/env_infos/reward_dist Max                    0.829776\n",
      "exploration/env_infos/reward_dist Min                    2.59687e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.16471\n",
      "exploration/env_infos/final/reward_energy Std            0.0686522\n",
      "exploration/env_infos/final/reward_energy Max           -0.0769298\n",
      "exploration/env_infos/final/reward_energy Min           -0.249323\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.23283\n",
      "exploration/env_infos/initial/reward_energy Std          0.176235\n",
      "exploration/env_infos/initial/reward_energy Max         -0.048188\n",
      "exploration/env_infos/initial/reward_energy Min         -0.521752\n",
      "exploration/env_infos/reward_energy Mean                -0.125974\n",
      "exploration/env_infos/reward_energy Std                  0.138178\n",
      "exploration/env_infos/reward_energy Max                 -0.00240383\n",
      "exploration/env_infos/reward_energy Min                 -0.890503\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0533505\n",
      "exploration/env_infos/final/end_effector_loc Std         0.238039\n",
      "exploration/env_infos/final/end_effector_loc Max         0.216591\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.467336\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00178083\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0101693\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0168126\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0199474\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0322419\n",
      "exploration/env_infos/end_effector_loc Std               0.145183\n",
      "exploration/env_infos/end_effector_loc Max               0.216591\n",
      "exploration/env_infos/end_effector_loc Min              -0.467336\n",
      "evaluation/num steps total                           31000\n",
      "evaluation/num paths total                            1550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0417269\n",
      "evaluation/Rewards Std                                   0.0735391\n",
      "evaluation/Rewards Max                                   0.135855\n",
      "evaluation/Rewards Min                                  -0.359124\n",
      "evaluation/Returns Mean                                 -0.834538\n",
      "evaluation/Returns Std                                   1.07338\n",
      "evaluation/Returns Max                                   1.93759\n",
      "evaluation/Returns Min                                  -2.92244\n",
      "evaluation/Actions Mean                                 -0.00369763\n",
      "evaluation/Actions Std                                   0.0872848\n",
      "evaluation/Actions Max                                   0.648241\n",
      "evaluation/Actions Min                                  -0.640422\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.834538\n",
      "evaluation/env_infos/final/reward_dist Mean              0.234028\n",
      "evaluation/env_infos/final/reward_dist Std               0.277087\n",
      "evaluation/env_infos/final/reward_dist Max               0.968167\n",
      "evaluation/env_infos/final/reward_dist Min               3.17456e-22\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00606811\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0110053\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0465563\n",
      "evaluation/env_infos/initial/reward_dist Min             1.55093e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.219774\n",
      "evaluation/env_infos/reward_dist Std                     0.272617\n",
      "evaluation/env_infos/reward_dist Max                     0.999477\n",
      "evaluation/env_infos/reward_dist Min                     3.17456e-22\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0428928\n",
      "evaluation/env_infos/final/reward_energy Std             0.0334178\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00886486\n",
      "evaluation/env_infos/final/reward_energy Min            -0.141487\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.284414\n",
      "evaluation/env_infos/initial/reward_energy Std           0.22018\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0130913\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.778105\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0745813\n",
      "evaluation/env_infos/reward_energy Std                   0.0985001\n",
      "evaluation/env_infos/reward_energy Max                  -0.00194006\n",
      "evaluation/env_infos/reward_energy Min                  -0.778105\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0210741\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.227472\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.533442\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.658893\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0024821\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0124721\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.032412\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0320211\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.000713825\n",
      "evaluation/env_infos/end_effector_loc Std                0.162735\n",
      "evaluation/env_infos/end_effector_loc Max                0.533442\n",
      "evaluation/env_infos/end_effector_loc Min               -0.658893\n",
      "time/data storing (s)                                    0.00591682\n",
      "time/evaluation sampling (s)                             0.916772\n",
      "time/exploration sampling (s)                            0.125538\n",
      "time/logging (s)                                         0.0195633\n",
      "time/saving (s)                                          0.0272133\n",
      "time/training (s)                                       47.3048\n",
      "time/epoch (s)                                          48.3998\n",
      "time/total (s)                                        1477.27\n",
      "Epoch                                                   30\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:14:05.037172 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 31 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000884594\n",
      "trainer/QF2 Loss                                         0.00108613\n",
      "trainer/Policy Loss                                      3.33284\n",
      "trainer/Q1 Predictions Mean                             -1.25824\n",
      "trainer/Q1 Predictions Std                               0.856912\n",
      "trainer/Q1 Predictions Max                               0.389706\n",
      "trainer/Q1 Predictions Min                              -3.50737\n",
      "trainer/Q2 Predictions Mean                             -1.25326\n",
      "trainer/Q2 Predictions Std                               0.848735\n",
      "trainer/Q2 Predictions Max                               0.397129\n",
      "trainer/Q2 Predictions Min                              -3.47905\n",
      "trainer/Q Targets Mean                                  -1.26514\n",
      "trainer/Q Targets Std                                    0.858446\n",
      "trainer/Q Targets Max                                    0.407059\n",
      "trainer/Q Targets Min                                   -3.44814\n",
      "trainer/Log Pis Mean                                     2.11734\n",
      "trainer/Log Pis Std                                      1.35383\n",
      "trainer/Log Pis Max                                      5.02982\n",
      "trainer/Log Pis Min                                     -2.80121\n",
      "trainer/Policy mu Mean                                   0.00415062\n",
      "trainer/Policy mu Std                                    0.385833\n",
      "trainer/Policy mu Max                                    1.58998\n",
      "trainer/Policy mu Min                                   -1.94343\n",
      "trainer/Policy log std Mean                             -2.29805\n",
      "trainer/Policy log std Std                               0.68548\n",
      "trainer/Policy log std Max                              -0.305371\n",
      "trainer/Policy log std Min                              -3.56432\n",
      "trainer/Alpha                                            0.0198684\n",
      "trainer/Alpha Loss                                       0.460009\n",
      "exploration/num steps total                           4200\n",
      "exploration/num paths total                            210\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0734352\n",
      "exploration/Rewards Std                                  0.105338\n",
      "exploration/Rewards Max                                  0.15986\n",
      "exploration/Rewards Min                                 -0.382495\n",
      "exploration/Returns Mean                                -1.4687\n",
      "exploration/Returns Std                                  1.81362\n",
      "exploration/Returns Max                                  1.44669\n",
      "exploration/Returns Min                                 -3.38397\n",
      "exploration/Actions Mean                                 0.000547262\n",
      "exploration/Actions Std                                  0.118203\n",
      "exploration/Actions Max                                  0.49333\n",
      "exploration/Actions Min                                 -0.486355\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.4687\n",
      "exploration/env_infos/final/reward_dist Mean             0.164121\n",
      "exploration/env_infos/final/reward_dist Std              0.324499\n",
      "exploration/env_infos/final/reward_dist Max              0.813112\n",
      "exploration/env_infos/final/reward_dist Min              5.6038e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00695747\n",
      "exploration/env_infos/initial/reward_dist Std            0.00593394\n",
      "exploration/env_infos/initial/reward_dist Max            0.0146395\n",
      "exploration/env_infos/initial/reward_dist Min            1.10996e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.220305\n",
      "exploration/env_infos/reward_dist Std                    0.296604\n",
      "exploration/env_infos/reward_dist Max                    0.94611\n",
      "exploration/env_infos/reward_dist Min                    5.6038e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.173979\n",
      "exploration/env_infos/final/reward_energy Std            0.0819126\n",
      "exploration/env_infos/final/reward_energy Max           -0.0626042\n",
      "exploration/env_infos/final/reward_energy Min           -0.295022\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.271459\n",
      "exploration/env_infos/initial/reward_energy Std          0.185501\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0412249\n",
      "exploration/env_infos/initial/reward_energy Min         -0.507779\n",
      "exploration/env_infos/reward_energy Mean                -0.127927\n",
      "exploration/env_infos/reward_energy Std                  0.107606\n",
      "exploration/env_infos/reward_energy Max                 -0.00223282\n",
      "exploration/env_infos/reward_energy Min                 -0.533501\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0218679\n",
      "exploration/env_infos/final/end_effector_loc Std         0.182288\n",
      "exploration/env_infos/final/end_effector_loc Max         0.341627\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.290567\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000219988\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0116223\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0187819\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0243177\n",
      "exploration/env_infos/end_effector_loc Mean              0.00521938\n",
      "exploration/env_infos/end_effector_loc Std               0.13527\n",
      "exploration/env_infos/end_effector_loc Max               0.341627\n",
      "exploration/env_infos/end_effector_loc Min              -0.290567\n",
      "evaluation/num steps total                           32000\n",
      "evaluation/num paths total                            1600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0693876\n",
      "evaluation/Rewards Std                                   0.0735058\n",
      "evaluation/Rewards Max                                   0.132683\n",
      "evaluation/Rewards Min                                  -0.51136\n",
      "evaluation/Returns Mean                                 -1.38775\n",
      "evaluation/Returns Std                                   1.10512\n",
      "evaluation/Returns Max                                   2.10345\n",
      "evaluation/Returns Min                                  -3.7861\n",
      "evaluation/Actions Mean                                 -0.000973185\n",
      "evaluation/Actions Std                                   0.0714686\n",
      "evaluation/Actions Max                                   0.736724\n",
      "evaluation/Actions Min                                  -0.4598\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.38775\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0773632\n",
      "evaluation/env_infos/final/reward_dist Std               0.166113\n",
      "evaluation/env_infos/final/reward_dist Max               0.772974\n",
      "evaluation/env_infos/final/reward_dist Min               8.16251e-54\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00630042\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0118899\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0585457\n",
      "evaluation/env_infos/initial/reward_dist Min             2.27344e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.118735\n",
      "evaluation/env_infos/reward_dist Std                     0.206005\n",
      "evaluation/env_infos/reward_dist Max                     0.989332\n",
      "evaluation/env_infos/reward_dist Min                     8.16251e-54\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0260379\n",
      "evaluation/env_infos/final/reward_energy Std             0.0359081\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00371172\n",
      "evaluation/env_infos/final/reward_energy Min            -0.244461\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.230784\n",
      "evaluation/env_infos/initial/reward_energy Std           0.212945\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00360494\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.812973\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0569262\n",
      "evaluation/env_infos/reward_energy Std                   0.0835273\n",
      "evaluation/env_infos/reward_energy Max                  -0.000732515\n",
      "evaluation/env_infos/reward_energy Min                  -0.812973\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00531002\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.311665\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.682977\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00237355\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0108455\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0368362\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.02299\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00735527\n",
      "evaluation/env_infos/end_effector_loc Std                0.196821\n",
      "evaluation/env_infos/end_effector_loc Max                0.682977\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00623494\n",
      "time/evaluation sampling (s)                             0.922012\n",
      "time/exploration sampling (s)                            0.123553\n",
      "time/logging (s)                                         0.0200317\n",
      "time/saving (s)                                          0.0288787\n",
      "time/training (s)                                       47.9917\n",
      "time/epoch (s)                                          49.0924\n",
      "time/total (s)                                        1526.77\n",
      "Epoch                                                   31\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:14:56.644613 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 32 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00100445\r\n",
      "trainer/QF2 Loss                                         0.00147892\r\n",
      "trainer/Policy Loss                                      3.23001\r\n",
      "trainer/Q1 Predictions Mean                             -1.16532\r\n",
      "trainer/Q1 Predictions Std                               0.81824\r\n",
      "trainer/Q1 Predictions Max                               0.346757\r\n",
      "trainer/Q1 Predictions Min                              -3.38243\r\n",
      "trainer/Q2 Predictions Mean                             -1.16685\r\n",
      "trainer/Q2 Predictions Std                               0.814735\r\n",
      "trainer/Q2 Predictions Max                               0.337549\r\n",
      "trainer/Q2 Predictions Min                              -3.391\r\n",
      "trainer/Q Targets Mean                                  -1.16197\r\n",
      "trainer/Q Targets Std                                    0.811886\r\n",
      "trainer/Q Targets Max                                    0.323271\r\n",
      "trainer/Q Targets Min                                   -3.33345\r\n",
      "trainer/Log Pis Mean                                     2.10112\r\n",
      "trainer/Log Pis Std                                      1.27778\r\n",
      "trainer/Log Pis Max                                      4.82083\r\n",
      "trainer/Log Pis Min                                     -2.91114\r\n",
      "trainer/Policy mu Mean                                   0.0157166\r\n",
      "trainer/Policy mu Std                                    0.384129\r\n",
      "trainer/Policy mu Max                                    1.62763\r\n",
      "trainer/Policy mu Min                                   -2.247\r\n",
      "trainer/Policy log std Mean                             -2.32181\r\n",
      "trainer/Policy log std Std                               0.644294\r\n",
      "trainer/Policy log std Max                              -0.380948\r\n",
      "trainer/Policy log std Min                              -3.45516\r\n",
      "trainer/Alpha                                            0.0200291\r\n",
      "trainer/Alpha Loss                                       0.395436\r\n",
      "exploration/num steps total                           4300\r\n",
      "exploration/num paths total                            215\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.094197\r\n",
      "exploration/Rewards Std                                  0.0654321\r\n",
      "exploration/Rewards Max                                  0.0514576\r\n",
      "exploration/Rewards Min                                 -0.323984\r\n",
      "exploration/Returns Mean                                -1.88394\r\n",
      "exploration/Returns Std                                  0.709701\r\n",
      "exploration/Returns Max                                 -0.867758\r\n",
      "exploration/Returns Min                                 -2.97779\r\n",
      "exploration/Actions Mean                                 0.00695078\r\n",
      "exploration/Actions Std                                  0.103833\r\n",
      "exploration/Actions Max                                  0.354512\r\n",
      "exploration/Actions Min                                 -0.33994\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.88394\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.128603\r\n",
      "exploration/env_infos/final/reward_dist Std              0.164725\r\n",
      "exploration/env_infos/final/reward_dist Max              0.399021\r\n",
      "exploration/env_infos/final/reward_dist Min              7.37369e-12\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00863269\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00828279\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0202386\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.25207e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.308036\r\n",
      "exploration/env_infos/reward_dist Std                    0.343645\r\n",
      "exploration/env_infos/reward_dist Max                    0.963135\r\n",
      "exploration/env_infos/reward_dist Min                    7.37369e-12\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0665602\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0212364\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0432376\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.0965758\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.271151\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.151259\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0627075\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.458969\r\n",
      "exploration/env_infos/reward_energy Mean                -0.126208\r\n",
      "exploration/env_infos/reward_energy Std                  0.0757015\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00884883\r\n",
      "exploration/env_infos/reward_energy Min                 -0.458969\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.145077\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.22228\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.505597\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.202407\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00460953\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00996269\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0177256\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.016997\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0782557\r\n",
      "exploration/env_infos/end_effector_loc Std               0.149347\r\n",
      "exploration/env_infos/end_effector_loc Max               0.505597\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.239575\r\n",
      "evaluation/num steps total                           33000\r\n",
      "evaluation/num paths total                            1650\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0563586\r\n",
      "evaluation/Rewards Std                                   0.0716252\r\n",
      "evaluation/Rewards Max                                   0.134737\r\n",
      "evaluation/Rewards Min                                  -0.322095\r\n",
      "evaluation/Returns Mean                                 -1.12717\r\n",
      "evaluation/Returns Std                                   1.10615\r\n",
      "evaluation/Returns Max                                   1.35982\r\n",
      "evaluation/Returns Min                                  -3.76792\r\n",
      "evaluation/Actions Mean                                  0.00298158\r\n",
      "evaluation/Actions Std                                   0.0607962\r\n",
      "evaluation/Actions Max                                   0.499005\r\n",
      "evaluation/Actions Min                                  -0.549083\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.12717\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.135859\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.226408\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.916\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.78937e-20\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00454096\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00987043\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0407812\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.24736e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.165967\r\n",
      "evaluation/env_infos/reward_dist Std                     0.25265\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996011\r\n",
      "evaluation/env_infos/reward_dist Min                     2.78937e-20\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0311519\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0190405\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00127073\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0939891\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.20768\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.149295\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0154026\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.560535\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0559363\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0654314\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00042501\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.560535\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0291688\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.258787\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.610059\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.731064\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       3.28527e-05\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0090429\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0249502\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0274542\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00610758\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.16858\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.610059\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.731064\r\n",
      "time/data storing (s)                                    0.00608166\r\n",
      "time/evaluation sampling (s)                             1.0068\r\n",
      "time/exploration sampling (s)                            0.138088\r\n",
      "time/logging (s)                                         0.0186885\r\n",
      "time/saving (s)                                          0.0291984\r\n",
      "time/training (s)                                       49.9333\r\n",
      "time/epoch (s)                                          51.1322\r\n",
      "time/total (s)                                        1578.37\r\n",
      "Epoch                                                   32\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:15:46.201633 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 33 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00106044\r\n",
      "trainer/QF2 Loss                                         0.0014073\r\n",
      "trainer/Policy Loss                                      3.12047\r\n",
      "trainer/Q1 Predictions Mean                             -1.2187\r\n",
      "trainer/Q1 Predictions Std                               0.791736\r\n",
      "trainer/Q1 Predictions Max                               0.129513\r\n",
      "trainer/Q1 Predictions Min                              -3.23138\r\n",
      "trainer/Q2 Predictions Mean                             -1.21433\r\n",
      "trainer/Q2 Predictions Std                               0.790048\r\n",
      "trainer/Q2 Predictions Max                               0.129048\r\n",
      "trainer/Q2 Predictions Min                              -3.20763\r\n",
      "trainer/Q Targets Mean                                  -1.22532\r\n",
      "trainer/Q Targets Std                                    0.796831\r\n",
      "trainer/Q Targets Max                                    0.113375\r\n",
      "trainer/Q Targets Min                                   -3.21638\r\n",
      "trainer/Log Pis Mean                                     1.93625\r\n",
      "trainer/Log Pis Std                                      1.26816\r\n",
      "trainer/Log Pis Max                                      4.37243\r\n",
      "trainer/Log Pis Min                                     -1.85465\r\n",
      "trainer/Policy mu Mean                                   0.0466563\r\n",
      "trainer/Policy mu Std                                    0.445422\r\n",
      "trainer/Policy mu Max                                    1.87489\r\n",
      "trainer/Policy mu Min                                   -2.31076\r\n",
      "trainer/Policy log std Mean                             -2.20558\r\n",
      "trainer/Policy log std Std                               0.652911\r\n",
      "trainer/Policy log std Max                              -0.112452\r\n",
      "trainer/Policy log std Min                              -3.36151\r\n",
      "trainer/Alpha                                            0.0210602\r\n",
      "trainer/Alpha Loss                                      -0.246034\r\n",
      "exploration/num steps total                           4400\r\n",
      "exploration/num paths total                            220\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0936856\r\n",
      "exploration/Rewards Std                                  0.101467\r\n",
      "exploration/Rewards Max                                  0.0817833\r\n",
      "exploration/Rewards Min                                 -0.44266\r\n",
      "exploration/Returns Mean                                -1.87371\r\n",
      "exploration/Returns Std                                  1.50963\r\n",
      "exploration/Returns Max                                 -0.404006\r\n",
      "exploration/Returns Min                                 -4.76915\r\n",
      "exploration/Actions Mean                                -0.00752819\r\n",
      "exploration/Actions Std                                  0.181801\r\n",
      "exploration/Actions Max                                  0.639236\r\n",
      "exploration/Actions Min                                 -0.861012\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.87371\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0804179\r\n",
      "exploration/env_infos/final/reward_dist Std              0.159578\r\n",
      "exploration/env_infos/final/reward_dist Max              0.399567\r\n",
      "exploration/env_infos/final/reward_dist Min              9.65539e-11\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00607225\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00450389\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0137269\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000218032\r\n",
      "exploration/env_infos/reward_dist Mean                   0.123569\r\n",
      "exploration/env_infos/reward_dist Std                    0.2048\r\n",
      "exploration/env_infos/reward_dist Max                    0.81524\r\n",
      "exploration/env_infos/reward_dist Min                    9.65539e-11\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.261126\r\n",
      "exploration/env_infos/final/reward_energy Std            0.116865\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0609239\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.383953\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.424089\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.279185\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.143471\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.918443\r\n",
      "exploration/env_infos/reward_energy Mean                -0.200702\r\n",
      "exploration/env_infos/reward_energy Std                  0.161043\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0133796\r\n",
      "exploration/env_infos/reward_energy Min                 -0.918443\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00805063\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.314269\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.423402\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.582148\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00425016\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0174408\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0239209\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0430506\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00675048\r\n",
      "exploration/env_infos/end_effector_loc Std               0.224813\r\n",
      "exploration/env_infos/end_effector_loc Max               0.423402\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.582148\r\n",
      "evaluation/num steps total                           34000\r\n",
      "evaluation/num paths total                            1700\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0488928\r\n",
      "evaluation/Rewards Std                                   0.0891165\r\n",
      "evaluation/Rewards Max                                   0.143938\r\n",
      "evaluation/Rewards Min                                  -0.526458\r\n",
      "evaluation/Returns Mean                                 -0.977856\r\n",
      "evaluation/Returns Std                                   1.30123\r\n",
      "evaluation/Returns Max                                   1.92304\r\n",
      "evaluation/Returns Min                                  -4.24887\r\n",
      "evaluation/Actions Mean                                  0.00136522\r\n",
      "evaluation/Actions Std                                   0.0864364\r\n",
      "evaluation/Actions Max                                   0.786446\r\n",
      "evaluation/Actions Min                                  -0.621422\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.977856\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.139363\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.239595\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.954197\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.36532e-18\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00930598\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0150009\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0724628\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.78735e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.200337\r\n",
      "evaluation/env_infos/reward_dist Std                     0.270465\r\n",
      "evaluation/env_infos/reward_dist Max                     0.984451\r\n",
      "evaluation/env_infos/reward_dist Min                     4.36532e-18\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0373975\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0347664\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00130011\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.171276\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.344561\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.232667\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0366313\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.01857\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0726324\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0983401\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000558624\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.01857\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.070004\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.269845\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.668553\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.584951\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00422243\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0140798\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0393223\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0310711\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0404176\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.186967\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.668553\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.603298\r\n",
      "time/data storing (s)                                    0.00587633\r\n",
      "time/evaluation sampling (s)                             1.00777\r\n",
      "time/exploration sampling (s)                            0.129952\r\n",
      "time/logging (s)                                         0.0213848\r\n",
      "time/saving (s)                                          0.0281863\r\n",
      "time/training (s)                                       47.9265\r\n",
      "time/epoch (s)                                          49.1197\r\n",
      "time/total (s)                                        1627.93\r\n",
      "Epoch                                                   33\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:16:36.211615 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 34 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000947664\r\n",
      "trainer/QF2 Loss                                         0.00130401\r\n",
      "trainer/Policy Loss                                      3.3904\r\n",
      "trainer/Q1 Predictions Mean                             -1.25506\r\n",
      "trainer/Q1 Predictions Std                               0.872149\r\n",
      "trainer/Q1 Predictions Max                               0.37814\r\n",
      "trainer/Q1 Predictions Min                              -3.51131\r\n",
      "trainer/Q2 Predictions Mean                             -1.26847\r\n",
      "trainer/Q2 Predictions Std                               0.87679\r\n",
      "trainer/Q2 Predictions Max                               0.351458\r\n",
      "trainer/Q2 Predictions Min                              -3.53713\r\n",
      "trainer/Q Targets Mean                                  -1.25614\r\n",
      "trainer/Q Targets Std                                    0.872688\r\n",
      "trainer/Q Targets Max                                    0.399104\r\n",
      "trainer/Q Targets Min                                   -3.53901\r\n",
      "trainer/Log Pis Mean                                     2.16034\r\n",
      "trainer/Log Pis Std                                      1.20828\r\n",
      "trainer/Log Pis Max                                      4.54438\r\n",
      "trainer/Log Pis Min                                     -1.63546\r\n",
      "trainer/Policy mu Mean                                  -0.0144581\r\n",
      "trainer/Policy mu Std                                    0.350627\r\n",
      "trainer/Policy mu Max                                    1.66789\r\n",
      "trainer/Policy mu Min                                   -2.13915\r\n",
      "trainer/Policy log std Mean                             -2.36014\r\n",
      "trainer/Policy log std Std                               0.600207\r\n",
      "trainer/Policy log std Max                              -0.359091\r\n",
      "trainer/Policy log std Min                              -3.36773\r\n",
      "trainer/Alpha                                            0.0207443\r\n",
      "trainer/Alpha Loss                                       0.621761\r\n",
      "exploration/num steps total                           4500\r\n",
      "exploration/num paths total                            225\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0486114\r\n",
      "exploration/Rewards Std                                  0.074497\r\n",
      "exploration/Rewards Max                                  0.117918\r\n",
      "exploration/Rewards Min                                 -0.27305\r\n",
      "exploration/Returns Mean                                -0.972228\r\n",
      "exploration/Returns Std                                  0.744085\r\n",
      "exploration/Returns Max                                  0.270247\r\n",
      "exploration/Returns Min                                 -1.91976\r\n",
      "exploration/Actions Mean                                -0.0127432\r\n",
      "exploration/Actions Std                                  0.119518\r\n",
      "exploration/Actions Max                                  0.330514\r\n",
      "exploration/Actions Min                                 -0.496401\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.972228\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.179477\r\n",
      "exploration/env_infos/final/reward_dist Std              0.322229\r\n",
      "exploration/env_infos/final/reward_dist Max              0.821425\r\n",
      "exploration/env_infos/final/reward_dist Min              4.83076e-07\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000456456\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000486798\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00130943\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.50757e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.220671\r\n",
      "exploration/env_infos/reward_dist Std                    0.300425\r\n",
      "exploration/env_infos/reward_dist Max                    0.976395\r\n",
      "exploration/env_infos/reward_dist Min                    4.83076e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.159269\r\n",
      "exploration/env_infos/final/reward_energy Std            0.135029\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0177648\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.414072\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.293564\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.134112\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.169653\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.54333\r\n",
      "exploration/env_infos/reward_energy Mean                -0.142823\r\n",
      "exploration/env_infos/reward_energy Std                  0.0921707\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0112235\r\n",
      "exploration/env_infos/reward_energy Min                 -0.54333\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.14205\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.250845\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.26878\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.474988\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0031704\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0109616\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0111669\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.02482\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0663948\r\n",
      "exploration/env_infos/end_effector_loc Std               0.170885\r\n",
      "exploration/env_infos/end_effector_loc Max               0.272054\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.474988\r\n",
      "evaluation/num steps total                           35000\r\n",
      "evaluation/num paths total                            1750\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0337231\r\n",
      "evaluation/Rewards Std                                   0.074938\r\n",
      "evaluation/Rewards Max                                   0.181573\r\n",
      "evaluation/Rewards Min                                  -0.311414\r\n",
      "evaluation/Returns Mean                                 -0.674461\r\n",
      "evaluation/Returns Std                                   1.04718\r\n",
      "evaluation/Returns Max                                   2.00606\r\n",
      "evaluation/Returns Min                                  -3.21826\r\n",
      "evaluation/Actions Mean                                 -0.00444924\r\n",
      "evaluation/Actions Std                                   0.0817868\r\n",
      "evaluation/Actions Max                                   0.858753\r\n",
      "evaluation/Actions Min                                  -0.444313\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.674461\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.195676\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.285852\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.964799\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.74705e-26\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00502473\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0119267\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0570003\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.99264e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.222218\r\n",
      "evaluation/env_infos/reward_dist Std                     0.300672\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997578\r\n",
      "evaluation/env_infos/reward_dist Min                     8.74705e-26\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0321243\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0246658\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00396774\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0967522\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.316826\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.233238\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0192873\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.10255\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0672727\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0942981\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00228487\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.10255\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00514291\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.284802\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.750129\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.618433\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00348343\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134662\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0429376\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0222156\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0111962\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.187542\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.750129\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.618433\r\n",
      "time/data storing (s)                                    0.00627562\r\n",
      "time/evaluation sampling (s)                             1.13779\r\n",
      "time/exploration sampling (s)                            0.134324\r\n",
      "time/logging (s)                                         0.020059\r\n",
      "time/saving (s)                                          0.0275026\r\n",
      "time/training (s)                                       48.1795\r\n",
      "time/epoch (s)                                          49.5054\r\n",
      "time/total (s)                                        1677.94\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch                                                   34\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 12:17:25.931259 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 35 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00190679\n",
      "trainer/QF2 Loss                                         0.00268198\n",
      "trainer/Policy Loss                                      2.91858\n",
      "trainer/Q1 Predictions Mean                             -1.17971\n",
      "trainer/Q1 Predictions Std                               0.74391\n",
      "trainer/Q1 Predictions Max                               0.556153\n",
      "trainer/Q1 Predictions Min                              -3.91777\n",
      "trainer/Q2 Predictions Mean                             -1.17172\n",
      "trainer/Q2 Predictions Std                               0.749062\n",
      "trainer/Q2 Predictions Max                               0.506241\n",
      "trainer/Q2 Predictions Min                              -3.87219\n",
      "trainer/Q Targets Mean                                  -1.18017\n",
      "trainer/Q Targets Std                                    0.7481\n",
      "trainer/Q Targets Max                                    0.518286\n",
      "trainer/Q Targets Min                                   -3.87531\n",
      "trainer/Log Pis Mean                                     1.80196\n",
      "trainer/Log Pis Std                                      1.26404\n",
      "trainer/Log Pis Max                                      4.37348\n",
      "trainer/Log Pis Min                                     -3.42962\n",
      "trainer/Policy mu Mean                                   0.0471502\n",
      "trainer/Policy mu Std                                    0.540249\n",
      "trainer/Policy mu Max                                    2.42164\n",
      "trainer/Policy mu Min                                   -2.32116\n",
      "trainer/Policy log std Mean                             -2.05889\n",
      "trainer/Policy log std Std                               0.69908\n",
      "trainer/Policy log std Max                              -0.0372509\n",
      "trainer/Policy log std Min                              -3.31811\n",
      "trainer/Alpha                                            0.0205875\n",
      "trainer/Alpha Loss                                      -0.769038\n",
      "exploration/num steps total                           4600\n",
      "exploration/num paths total                            230\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.113071\n",
      "exploration/Rewards Std                                  0.0579603\n",
      "exploration/Rewards Max                                 -0.0116315\n",
      "exploration/Rewards Min                                 -0.267673\n",
      "exploration/Returns Mean                                -2.26141\n",
      "exploration/Returns Std                                  0.891842\n",
      "exploration/Returns Max                                 -1.3495\n",
      "exploration/Returns Min                                 -3.82297\n",
      "exploration/Actions Mean                                -0.00824267\n",
      "exploration/Actions Std                                  0.139306\n",
      "exploration/Actions Max                                  0.689294\n",
      "exploration/Actions Min                                 -0.445472\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.26141\n",
      "exploration/env_infos/final/reward_dist Mean             0.230588\n",
      "exploration/env_infos/final/reward_dist Std              0.310124\n",
      "exploration/env_infos/final/reward_dist Max              0.832592\n",
      "exploration/env_infos/final/reward_dist Min              0.000207061\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0011087\n",
      "exploration/env_infos/initial/reward_dist Std            0.000936904\n",
      "exploration/env_infos/initial/reward_dist Max            0.00229321\n",
      "exploration/env_infos/initial/reward_dist Min            2.13691e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.171405\n",
      "exploration/env_infos/reward_dist Std                    0.245477\n",
      "exploration/env_infos/reward_dist Max                    0.983759\n",
      "exploration/env_infos/reward_dist Min                    2.13691e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.188041\n",
      "exploration/env_infos/final/reward_energy Std            0.118131\n",
      "exploration/env_infos/final/reward_energy Max           -0.0456716\n",
      "exploration/env_infos/final/reward_energy Min           -0.385621\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.302946\n",
      "exploration/env_infos/initial/reward_energy Std          0.202659\n",
      "exploration/env_infos/initial/reward_energy Max         -0.110631\n",
      "exploration/env_infos/initial/reward_energy Min         -0.69444\n",
      "exploration/env_infos/reward_energy Mean                -0.150068\n",
      "exploration/env_infos/reward_energy Std                  0.128171\n",
      "exploration/env_infos/reward_energy Max                 -0.011233\n",
      "exploration/env_infos/reward_energy Min                 -0.69444\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0615604\n",
      "exploration/env_infos/final/end_effector_loc Std         0.192555\n",
      "exploration/env_infos/final/end_effector_loc Max         0.400334\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.205065\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00717691\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0107028\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0344647\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00515869\n",
      "exploration/env_infos/end_effector_loc Mean              0.0552967\n",
      "exploration/env_infos/end_effector_loc Std               0.126692\n",
      "exploration/env_infos/end_effector_loc Max               0.400334\n",
      "exploration/env_infos/end_effector_loc Min              -0.205065\n",
      "evaluation/num steps total                           36000\n",
      "evaluation/num paths total                            1800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.045246\n",
      "evaluation/Rewards Std                                   0.0782357\n",
      "evaluation/Rewards Max                                   0.166323\n",
      "evaluation/Rewards Min                                  -0.489615\n",
      "evaluation/Returns Mean                                 -0.904921\n",
      "evaluation/Returns Std                                   1.08932\n",
      "evaluation/Returns Max                                   2.16155\n",
      "evaluation/Returns Min                                  -3.0775\n",
      "evaluation/Actions Mean                                 -0.00325499\n",
      "evaluation/Actions Std                                   0.087349\n",
      "evaluation/Actions Max                                   0.83199\n",
      "evaluation/Actions Min                                  -0.329447\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.904921\n",
      "evaluation/env_infos/final/reward_dist Mean              0.200545\n",
      "evaluation/env_infos/final/reward_dist Std               0.272965\n",
      "evaluation/env_infos/final/reward_dist Max               0.932941\n",
      "evaluation/env_infos/final/reward_dist Min               2.26228e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00651917\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0115539\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0631858\n",
      "evaluation/env_infos/initial/reward_dist Min             1.34395e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.203665\n",
      "evaluation/env_infos/reward_dist Std                     0.268313\n",
      "evaluation/env_infos/reward_dist Max                     0.995251\n",
      "evaluation/env_infos/reward_dist Min                     2.26228e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0424279\n",
      "evaluation/env_infos/final/reward_energy Std             0.0359503\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00478242\n",
      "evaluation/env_infos/final/reward_energy Min            -0.183692\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.333312\n",
      "evaluation/env_infos/initial/reward_energy Std           0.243473\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0291867\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.985445\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0759842\n",
      "evaluation/env_infos/reward_energy Std                   0.0975054\n",
      "evaluation/env_infos/reward_energy Max                  -0.00127811\n",
      "evaluation/env_infos/reward_energy Min                  -0.985445\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.053553\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.241308\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.673544\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.344347\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00689034\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0128644\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0415995\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0164724\n",
      "evaluation/env_infos/end_effector_loc Mean               0.052966\n",
      "evaluation/env_infos/end_effector_loc Std                0.168964\n",
      "evaluation/env_infos/end_effector_loc Max                0.673544\n",
      "evaluation/env_infos/end_effector_loc Min               -0.397695\n",
      "time/data storing (s)                                    0.00610836\n",
      "time/evaluation sampling (s)                             0.939589\n",
      "time/exploration sampling (s)                            0.122766\n",
      "time/logging (s)                                         0.0194812\n",
      "time/saving (s)                                          0.027346\n",
      "time/training (s)                                       48.1452\n",
      "time/epoch (s)                                          49.2605\n",
      "time/total (s)                                        1727.66\n",
      "Epoch                                                   35\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:18:16.612031 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 36 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00190681\n",
      "trainer/QF2 Loss                                         0.00156953\n",
      "trainer/Policy Loss                                      3.19579\n",
      "trainer/Q1 Predictions Mean                             -1.20017\n",
      "trainer/Q1 Predictions Std                               0.791786\n",
      "trainer/Q1 Predictions Max                               0.113581\n",
      "trainer/Q1 Predictions Min                              -4.04767\n",
      "trainer/Q2 Predictions Mean                             -1.18771\n",
      "trainer/Q2 Predictions Std                               0.787547\n",
      "trainer/Q2 Predictions Max                               0.116903\n",
      "trainer/Q2 Predictions Min                              -4.02638\n",
      "trainer/Q Targets Mean                                  -1.17398\n",
      "trainer/Q Targets Std                                    0.785521\n",
      "trainer/Q Targets Max                                    0.090237\n",
      "trainer/Q Targets Min                                   -3.99424\n",
      "trainer/Log Pis Mean                                     2.05911\n",
      "trainer/Log Pis Std                                      1.30106\n",
      "trainer/Log Pis Max                                      5.00556\n",
      "trainer/Log Pis Min                                     -2.02847\n",
      "trainer/Policy mu Mean                                  -0.0271965\n",
      "trainer/Policy mu Std                                    0.487471\n",
      "trainer/Policy mu Max                                    1.92382\n",
      "trainer/Policy mu Min                                   -2.22314\n",
      "trainer/Policy log std Mean                             -2.23956\n",
      "trainer/Policy log std Std                               0.672561\n",
      "trainer/Policy log std Max                              -0.440405\n",
      "trainer/Policy log std Min                              -3.4474\n",
      "trainer/Alpha                                            0.0199241\n",
      "trainer/Alpha Loss                                       0.231541\n",
      "exploration/num steps total                           4700\n",
      "exploration/num paths total                            235\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.079983\n",
      "exploration/Rewards Std                                  0.0828428\n",
      "exploration/Rewards Max                                  0.0709775\n",
      "exploration/Rewards Min                                 -0.295581\n",
      "exploration/Returns Mean                                -1.59966\n",
      "exploration/Returns Std                                  0.956904\n",
      "exploration/Returns Max                                 -0.552447\n",
      "exploration/Returns Min                                 -3.13555\n",
      "exploration/Actions Mean                                -0.00851227\n",
      "exploration/Actions Std                                  0.10885\n",
      "exploration/Actions Max                                  0.37047\n",
      "exploration/Actions Min                                 -0.365713\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.59966\n",
      "exploration/env_infos/final/reward_dist Mean             0.177673\n",
      "exploration/env_infos/final/reward_dist Std              0.355197\n",
      "exploration/env_infos/final/reward_dist Max              0.888068\n",
      "exploration/env_infos/final/reward_dist Min              1.59783e-16\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0014385\n",
      "exploration/env_infos/initial/reward_dist Std            0.00203678\n",
      "exploration/env_infos/initial/reward_dist Max            0.00544889\n",
      "exploration/env_infos/initial/reward_dist Min            1.1159e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.139307\n",
      "exploration/env_infos/reward_dist Std                    0.229533\n",
      "exploration/env_infos/reward_dist Max                    0.90769\n",
      "exploration/env_infos/reward_dist Min                    1.59783e-16\n",
      "exploration/env_infos/final/reward_energy Mean          -0.134549\n",
      "exploration/env_infos/final/reward_energy Std            0.0615048\n",
      "exploration/env_infos/final/reward_energy Max           -0.05703\n",
      "exploration/env_infos/final/reward_energy Min           -0.242106\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.217989\n",
      "exploration/env_infos/initial/reward_energy Std          0.0641829\n",
      "exploration/env_infos/initial/reward_energy Max         -0.115566\n",
      "exploration/env_infos/initial/reward_energy Min         -0.30936\n",
      "exploration/env_infos/reward_energy Mean                -0.132558\n",
      "exploration/env_infos/reward_energy Std                  0.0791846\n",
      "exploration/env_infos/reward_energy Max                 -0.0114799\n",
      "exploration/env_infos/reward_energy Min                 -0.416476\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0811839\n",
      "exploration/env_infos/final/end_effector_loc Std         0.336451\n",
      "exploration/env_infos/final/end_effector_loc Max         0.292163\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.677637\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000595201\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00801213\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0129974\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0102224\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0390525\n",
      "exploration/env_infos/end_effector_loc Std               0.203154\n",
      "exploration/env_infos/end_effector_loc Max               0.303477\n",
      "exploration/env_infos/end_effector_loc Min              -0.677637\n",
      "evaluation/num steps total                           37000\n",
      "evaluation/num paths total                            1850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0506229\n",
      "evaluation/Rewards Std                                   0.0697219\n",
      "evaluation/Rewards Max                                   0.150981\n",
      "evaluation/Rewards Min                                  -0.32217\n",
      "evaluation/Returns Mean                                 -1.01246\n",
      "evaluation/Returns Std                                   1.00525\n",
      "evaluation/Returns Max                                   1.7155\n",
      "evaluation/Returns Min                                  -2.98728\n",
      "evaluation/Actions Mean                                 -0.00129044\n",
      "evaluation/Actions Std                                   0.0742394\n",
      "evaluation/Actions Max                                   0.893973\n",
      "evaluation/Actions Min                                  -0.372498\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.01246\n",
      "evaluation/env_infos/final/reward_dist Mean              0.202942\n",
      "evaluation/env_infos/final/reward_dist Std               0.273794\n",
      "evaluation/env_infos/final/reward_dist Max               0.942648\n",
      "evaluation/env_infos/final/reward_dist Min               1.3012e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00805948\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0162103\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0870908\n",
      "evaluation/env_infos/initial/reward_dist Min             1.11585e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.219177\n",
      "evaluation/env_infos/reward_dist Std                     0.293175\n",
      "evaluation/env_infos/reward_dist Max                     0.997997\n",
      "evaluation/env_infos/reward_dist Min                     1.3012e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.035176\n",
      "evaluation/env_infos/final/reward_energy Std             0.03389\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00111647\n",
      "evaluation/env_infos/final/reward_energy Min            -0.180049\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.254227\n",
      "evaluation/env_infos/initial/reward_energy Std           0.237574\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00575773\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.05462\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0618375\n",
      "evaluation/env_infos/reward_energy Std                   0.084867\n",
      "evaluation/env_infos/reward_energy Max                  -0.000642833\n",
      "evaluation/env_infos/reward_energy Min                  -1.05462\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0245184\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.258362\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.762181\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.450878\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00276324\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0119877\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0446986\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0186249\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0210745\n",
      "evaluation/env_infos/end_effector_loc Std                0.171022\n",
      "evaluation/env_infos/end_effector_loc Max                0.762181\n",
      "evaluation/env_infos/end_effector_loc Min               -0.467864\n",
      "time/data storing (s)                                    0.00607971\n",
      "time/evaluation sampling (s)                             1.11466\n",
      "time/exploration sampling (s)                            0.190079\n",
      "time/logging (s)                                         0.0204772\n",
      "time/saving (s)                                          0.0287836\n",
      "time/training (s)                                       48.8488\n",
      "time/epoch (s)                                          50.2089\n",
      "time/total (s)                                        1778.34\n",
      "Epoch                                                   36\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:19:08.889537 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 37 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0012275\n",
      "trainer/QF2 Loss                                         0.0011068\n",
      "trainer/Policy Loss                                      3.03346\n",
      "trainer/Q1 Predictions Mean                             -1.17193\n",
      "trainer/Q1 Predictions Std                               0.794355\n",
      "trainer/Q1 Predictions Max                               0.630812\n",
      "trainer/Q1 Predictions Min                              -3.2881\n",
      "trainer/Q2 Predictions Mean                             -1.15031\n",
      "trainer/Q2 Predictions Std                               0.786135\n",
      "trainer/Q2 Predictions Max                               0.62529\n",
      "trainer/Q2 Predictions Min                              -3.21871\n",
      "trainer/Q Targets Mean                                  -1.15738\n",
      "trainer/Q Targets Std                                    0.784744\n",
      "trainer/Q Targets Max                                    0.635002\n",
      "trainer/Q Targets Min                                   -3.22546\n",
      "trainer/Log Pis Mean                                     1.90624\n",
      "trainer/Log Pis Std                                      1.39768\n",
      "trainer/Log Pis Max                                      4.47403\n",
      "trainer/Log Pis Min                                     -3.2887\n",
      "trainer/Policy mu Mean                                   0.00813147\n",
      "trainer/Policy mu Std                                    0.453109\n",
      "trainer/Policy mu Max                                    2.20221\n",
      "trainer/Policy mu Min                                   -2.4272\n",
      "trainer/Policy log std Mean                             -2.21474\n",
      "trainer/Policy log std Std                               0.68483\n",
      "trainer/Policy log std Max                              -0.11016\n",
      "trainer/Policy log std Min                              -3.30421\n",
      "trainer/Alpha                                            0.0208857\n",
      "trainer/Alpha Loss                                      -0.362661\n",
      "exploration/num steps total                           4800\n",
      "exploration/num paths total                            240\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0986442\n",
      "exploration/Rewards Std                                  0.0732685\n",
      "exploration/Rewards Max                                  0.0597477\n",
      "exploration/Rewards Min                                 -0.250596\n",
      "exploration/Returns Mean                                -1.97288\n",
      "exploration/Returns Std                                  1.22987\n",
      "exploration/Returns Max                                 -0.334852\n",
      "exploration/Returns Min                                 -3.45226\n",
      "exploration/Actions Mean                                -0.0147134\n",
      "exploration/Actions Std                                  0.0811686\n",
      "exploration/Actions Max                                  0.253843\n",
      "exploration/Actions Min                                 -0.237761\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.97288\n",
      "exploration/env_infos/final/reward_dist Mean             0.0871236\n",
      "exploration/env_infos/final/reward_dist Std              0.111589\n",
      "exploration/env_infos/final/reward_dist Max              0.275197\n",
      "exploration/env_infos/final/reward_dist Min              1.38361e-29\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00938514\n",
      "exploration/env_infos/initial/reward_dist Std            0.0127285\n",
      "exploration/env_infos/initial/reward_dist Max            0.0341403\n",
      "exploration/env_infos/initial/reward_dist Min            0.000139259\n",
      "exploration/env_infos/reward_dist Mean                   0.189429\n",
      "exploration/env_infos/reward_dist Std                    0.231945\n",
      "exploration/env_infos/reward_dist Max                    0.845895\n",
      "exploration/env_infos/reward_dist Min                    1.38361e-29\n",
      "exploration/env_infos/final/reward_energy Mean          -0.104393\n",
      "exploration/env_infos/final/reward_energy Std            0.0262641\n",
      "exploration/env_infos/final/reward_energy Max           -0.0689244\n",
      "exploration/env_infos/final/reward_energy Min           -0.140386\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.149366\n",
      "exploration/env_infos/initial/reward_energy Std          0.0573045\n",
      "exploration/env_infos/initial/reward_energy Max         -0.100717\n",
      "exploration/env_infos/initial/reward_energy Min         -0.257243\n",
      "exploration/env_infos/reward_energy Mean                -0.101312\n",
      "exploration/env_infos/reward_energy Std                  0.0578408\n",
      "exploration/env_infos/reward_energy Max                 -0.0109675\n",
      "exploration/env_infos/reward_energy Min                 -0.26666\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.134053\n",
      "exploration/env_infos/final/end_effector_loc Std         0.321202\n",
      "exploration/env_infos/final/end_effector_loc Max         0.324182\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.607665\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00077419\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00560295\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0126921\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00538696\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0426176\n",
      "exploration/env_infos/end_effector_loc Std               0.192469\n",
      "exploration/env_infos/end_effector_loc Max               0.324182\n",
      "exploration/env_infos/end_effector_loc Min              -0.607665\n",
      "evaluation/num steps total                           38000\n",
      "evaluation/num paths total                            1900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0696375\n",
      "evaluation/Rewards Std                                   0.0887552\n",
      "evaluation/Rewards Max                                   0.173111\n",
      "evaluation/Rewards Min                                  -0.604474\n",
      "evaluation/Returns Mean                                 -1.39275\n",
      "evaluation/Returns Std                                   1.43434\n",
      "evaluation/Returns Max                                   2.00497\n",
      "evaluation/Returns Min                                  -5.71016\n",
      "evaluation/Actions Mean                                 -0.00339587\n",
      "evaluation/Actions Std                                   0.0704158\n",
      "evaluation/Actions Max                                   0.692833\n",
      "evaluation/Actions Min                                  -0.419993\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.39275\n",
      "evaluation/env_infos/final/reward_dist Mean              0.132139\n",
      "evaluation/env_infos/final/reward_dist Std               0.246595\n",
      "evaluation/env_infos/final/reward_dist Max               0.972497\n",
      "evaluation/env_infos/final/reward_dist Min               2.76847e-37\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00808336\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0109328\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0434286\n",
      "evaluation/env_infos/initial/reward_dist Min             1.26139e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.18475\n",
      "evaluation/env_infos/reward_dist Std                     0.271559\n",
      "evaluation/env_infos/reward_dist Max                     0.99938\n",
      "evaluation/env_infos/reward_dist Min                     2.76847e-37\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0329717\n",
      "evaluation/env_infos/final/reward_energy Std             0.0218978\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00499213\n",
      "evaluation/env_infos/final/reward_energy Min            -0.102047\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.236522\n",
      "evaluation/env_infos/initial/reward_energy Std           0.162452\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00898368\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.69796\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0631596\n",
      "evaluation/env_infos/reward_energy Std                   0.0771408\n",
      "evaluation/env_infos/reward_energy Max                  -0.000465087\n",
      "evaluation/env_infos/reward_energy Min                  -0.69796\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0328275\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.267141\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.711325\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.599732\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00265138\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00979217\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0346416\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0209997\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0308716\n",
      "evaluation/env_infos/end_effector_loc Std                0.179805\n",
      "evaluation/env_infos/end_effector_loc Max                0.711325\n",
      "evaluation/env_infos/end_effector_loc Min               -0.599732\n",
      "time/data storing (s)                                    0.00616738\n",
      "time/evaluation sampling (s)                             1.23947\n",
      "time/exploration sampling (s)                            0.126401\n",
      "time/logging (s)                                         0.0196587\n",
      "time/saving (s)                                          0.0289687\n",
      "time/training (s)                                       50.2883\n",
      "time/epoch (s)                                          51.709\n",
      "time/total (s)                                        1830.61\n",
      "Epoch                                                   37\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:20:01.998953 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 38 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00169449\r\n",
      "trainer/QF2 Loss                                         0.00117593\r\n",
      "trainer/Policy Loss                                      3.06788\r\n",
      "trainer/Q1 Predictions Mean                             -1.08602\r\n",
      "trainer/Q1 Predictions Std                               0.785696\r\n",
      "trainer/Q1 Predictions Max                               0.491625\r\n",
      "trainer/Q1 Predictions Min                              -3.55427\r\n",
      "trainer/Q2 Predictions Mean                             -1.09752\r\n",
      "trainer/Q2 Predictions Std                               0.785389\r\n",
      "trainer/Q2 Predictions Max                               0.509928\r\n",
      "trainer/Q2 Predictions Min                              -3.59127\r\n",
      "trainer/Q Targets Mean                                  -1.09306\r\n",
      "trainer/Q Targets Std                                    0.786608\r\n",
      "trainer/Q Targets Max                                    0.562517\r\n",
      "trainer/Q Targets Min                                   -3.44697\r\n",
      "trainer/Log Pis Mean                                     2.00164\r\n",
      "trainer/Log Pis Std                                      1.41608\r\n",
      "trainer/Log Pis Max                                      4.91137\r\n",
      "trainer/Log Pis Min                                     -6.32968\r\n",
      "trainer/Policy mu Mean                                  -0.0564979\r\n",
      "trainer/Policy mu Std                                    0.476844\r\n",
      "trainer/Policy mu Max                                    2.10562\r\n",
      "trainer/Policy mu Min                                   -2.4583\r\n",
      "trainer/Policy log std Mean                             -2.26717\r\n",
      "trainer/Policy log std Std                               0.686914\r\n",
      "trainer/Policy log std Max                               0.141248\r\n",
      "trainer/Policy log std Min                              -3.4371\r\n",
      "trainer/Alpha                                            0.0212059\r\n",
      "trainer/Alpha Loss                                       0.00632431\r\n",
      "exploration/num steps total                           4900\r\n",
      "exploration/num paths total                            245\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0833946\r\n",
      "exploration/Rewards Std                                  0.0985882\r\n",
      "exploration/Rewards Max                                  0.0952183\r\n",
      "exploration/Rewards Min                                 -0.35506\r\n",
      "exploration/Returns Mean                                -1.66789\r\n",
      "exploration/Returns Std                                  1.78381\r\n",
      "exploration/Returns Max                                  0.914531\r\n",
      "exploration/Returns Min                                 -4.33486\r\n",
      "exploration/Actions Mean                                 0.00283128\r\n",
      "exploration/Actions Std                                  0.122397\r\n",
      "exploration/Actions Max                                  0.523131\r\n",
      "exploration/Actions Min                                 -0.23875\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.66789\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.10154\r\n",
      "exploration/env_infos/final/reward_dist Std              0.111275\r\n",
      "exploration/env_infos/final/reward_dist Max              0.255585\r\n",
      "exploration/env_infos/final/reward_dist Min              3.62666e-27\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0005687\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000443815\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00102361\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.79755e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.127941\r\n",
      "exploration/env_infos/reward_dist Std                    0.195089\r\n",
      "exploration/env_infos/reward_dist Max                    0.794288\r\n",
      "exploration/env_infos/reward_dist Min                    3.62666e-27\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.146643\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0737586\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0455493\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.230213\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.285778\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.166149\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.117346\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.531401\r\n",
      "exploration/env_infos/reward_energy Mean                -0.146084\r\n",
      "exploration/env_infos/reward_energy Std                  0.0929393\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00766479\r\n",
      "exploration/env_infos/reward_energy Min                 -0.531401\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.108901\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.213433\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.526665\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.214435\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00587725\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.010102\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0261566\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00967151\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.066355\r\n",
      "exploration/env_infos/end_effector_loc Std               0.140951\r\n",
      "exploration/env_infos/end_effector_loc Max               0.526665\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.214435\r\n",
      "evaluation/num steps total                           39000\r\n",
      "evaluation/num paths total                            1950\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0480165\r\n",
      "evaluation/Rewards Std                                   0.0782285\r\n",
      "evaluation/Rewards Max                                   0.126164\r\n",
      "evaluation/Rewards Min                                  -0.41994\r\n",
      "evaluation/Returns Mean                                 -0.96033\r\n",
      "evaluation/Returns Std                                   1.18413\r\n",
      "evaluation/Returns Max                                   1.46737\r\n",
      "evaluation/Returns Min                                  -4.37367\r\n",
      "evaluation/Actions Mean                                 -0.00414721\r\n",
      "evaluation/Actions Std                                   0.0802625\r\n",
      "evaluation/Actions Max                                   0.694843\r\n",
      "evaluation/Actions Min                                  -0.459985\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.96033\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.128172\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.191939\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.687729\r\n",
      "evaluation/env_infos/final/reward_dist Min               7.50658e-55\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00474013\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00666446\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0236359\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.32193e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.21812\r\n",
      "evaluation/env_infos/reward_dist Std                     0.280866\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997752\r\n",
      "evaluation/env_infos/reward_dist Min                     7.50658e-55\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0434787\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0336598\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00432534\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.196545\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.291472\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.192055\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.003761\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.777198\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0723119\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0876899\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.001881\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.777198\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00148153\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.259823\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.916763\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.518816\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00192197\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121905\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0347421\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0229992\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0104959\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.177999\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.916763\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.518816\r\n",
      "time/data storing (s)                                    0.00594254\r\n",
      "time/evaluation sampling (s)                             1.02466\r\n",
      "time/exploration sampling (s)                            0.121786\r\n",
      "time/logging (s)                                         0.0203044\r\n",
      "time/saving (s)                                          0.0430995\r\n",
      "time/training (s)                                       51.3954\r\n",
      "time/epoch (s)                                          52.6112\r\n",
      "time/total (s)                                        1883.73\r\n",
      "Epoch                                                   38\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:20:56.699104 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 39 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00465176\n",
      "trainer/QF2 Loss                                         0.00369048\n",
      "trainer/Policy Loss                                      3.05001\n",
      "trainer/Q1 Predictions Mean                             -1.10591\n",
      "trainer/Q1 Predictions Std                               0.834151\n",
      "trainer/Q1 Predictions Max                               0.562254\n",
      "trainer/Q1 Predictions Min                              -3.53048\n",
      "trainer/Q2 Predictions Mean                             -1.11069\n",
      "trainer/Q2 Predictions Std                               0.838387\n",
      "trainer/Q2 Predictions Max                               0.533187\n",
      "trainer/Q2 Predictions Min                              -3.54544\n",
      "trainer/Q Targets Mean                                  -1.12044\n",
      "trainer/Q Targets Std                                    0.850682\n",
      "trainer/Q Targets Max                                    0.573024\n",
      "trainer/Q Targets Min                                   -3.82018\n",
      "trainer/Log Pis Mean                                     1.96661\n",
      "trainer/Log Pis Std                                      1.29437\n",
      "trainer/Log Pis Max                                      4.51742\n",
      "trainer/Log Pis Min                                     -3.32886\n",
      "trainer/Policy mu Mean                                  -0.0369327\n",
      "trainer/Policy mu Std                                    0.40373\n",
      "trainer/Policy mu Max                                    1.93227\n",
      "trainer/Policy mu Min                                   -1.95297\n",
      "trainer/Policy log std Mean                             -2.26104\n",
      "trainer/Policy log std Std                               0.630909\n",
      "trainer/Policy log std Max                               0.0679319\n",
      "trainer/Policy log std Min                              -3.25211\n",
      "trainer/Alpha                                            0.0211491\n",
      "trainer/Alpha Loss                                      -0.128748\n",
      "exploration/num steps total                           5000\n",
      "exploration/num paths total                            250\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.096611\n",
      "exploration/Rewards Std                                  0.0575284\n",
      "exploration/Rewards Max                                  0.00412414\n",
      "exploration/Rewards Min                                 -0.347596\n",
      "exploration/Returns Mean                                -1.93222\n",
      "exploration/Returns Std                                  0.627367\n",
      "exploration/Returns Max                                 -0.980231\n",
      "exploration/Returns Min                                 -2.92456\n",
      "exploration/Actions Mean                                 0.00284611\n",
      "exploration/Actions Std                                  0.172813\n",
      "exploration/Actions Max                                  0.663004\n",
      "exploration/Actions Min                                 -0.495885\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.93222\n",
      "exploration/env_infos/final/reward_dist Mean             0.0084376\n",
      "exploration/env_infos/final/reward_dist Std              0.00801189\n",
      "exploration/env_infos/final/reward_dist Max              0.0207392\n",
      "exploration/env_infos/final/reward_dist Min              1.21597e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00158155\n",
      "exploration/env_infos/initial/reward_dist Std            0.00215424\n",
      "exploration/env_infos/initial/reward_dist Max            0.00577098\n",
      "exploration/env_infos/initial/reward_dist Min            7.05543e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.144398\n",
      "exploration/env_infos/reward_dist Std                    0.204194\n",
      "exploration/env_infos/reward_dist Max                    0.77883\n",
      "exploration/env_infos/reward_dist Min                    1.21597e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.104574\n",
      "exploration/env_infos/final/reward_energy Std            0.050994\n",
      "exploration/env_infos/final/reward_energy Max           -0.0515202\n",
      "exploration/env_infos/final/reward_energy Min           -0.199176\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.328195\n",
      "exploration/env_infos/initial/reward_energy Std          0.100919\n",
      "exploration/env_infos/initial/reward_energy Max         -0.155307\n",
      "exploration/env_infos/initial/reward_energy Min         -0.455193\n",
      "exploration/env_infos/reward_energy Mean                -0.201688\n",
      "exploration/env_infos/reward_energy Std                  0.138083\n",
      "exploration/env_infos/reward_energy Max                 -0.00987293\n",
      "exploration/env_infos/reward_energy Min                 -0.666892\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0602884\n",
      "exploration/env_infos/final/end_effector_loc Std         0.224888\n",
      "exploration/env_infos/final/end_effector_loc Max         0.461392\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.183168\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00121579\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0120786\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0202118\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0188079\n",
      "exploration/env_infos/end_effector_loc Mean              0.0127256\n",
      "exploration/env_infos/end_effector_loc Std               0.176579\n",
      "exploration/env_infos/end_effector_loc Max               0.461392\n",
      "exploration/env_infos/end_effector_loc Min              -0.326562\n",
      "evaluation/num steps total                           40000\n",
      "evaluation/num paths total                            2000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0448132\n",
      "evaluation/Rewards Std                                   0.0731869\n",
      "evaluation/Rewards Max                                   0.189962\n",
      "evaluation/Rewards Min                                  -0.441261\n",
      "evaluation/Returns Mean                                 -0.896264\n",
      "evaluation/Returns Std                                   1.05082\n",
      "evaluation/Returns Max                                   1.57656\n",
      "evaluation/Returns Min                                  -2.66698\n",
      "evaluation/Actions Mean                                 -0.00319786\n",
      "evaluation/Actions Std                                   0.0712629\n",
      "evaluation/Actions Max                                   0.599774\n",
      "evaluation/Actions Min                                  -0.468887\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.896264\n",
      "evaluation/env_infos/final/reward_dist Mean              0.15323\n",
      "evaluation/env_infos/final/reward_dist Std               0.254933\n",
      "evaluation/env_infos/final/reward_dist Max               0.966732\n",
      "evaluation/env_infos/final/reward_dist Min               2.53813e-38\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0120656\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0164234\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0627214\n",
      "evaluation/env_infos/initial/reward_dist Min             1.44918e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.214486\n",
      "evaluation/env_infos/reward_dist Std                     0.275565\n",
      "evaluation/env_infos/reward_dist Max                     0.999193\n",
      "evaluation/env_infos/reward_dist Min                     2.53813e-38\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.035642\n",
      "evaluation/env_infos/final/reward_energy Std             0.0300165\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00204133\n",
      "evaluation/env_infos/final/reward_energy Min            -0.167954\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.244012\n",
      "evaluation/env_infos/initial/reward_energy Std           0.15927\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0111556\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.613234\n",
      "evaluation/env_infos/reward_energy Mean                 -0.066571\n",
      "evaluation/env_infos/reward_energy Std                   0.0757995\n",
      "evaluation/env_infos/reward_energy Max                  -0.000792848\n",
      "evaluation/env_infos/reward_energy Min                  -0.613234\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0476974\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.266261\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.501465\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.749493\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000235358\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0102995\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0299887\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0234443\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0219597\n",
      "evaluation/env_infos/end_effector_loc Std                0.175221\n",
      "evaluation/env_infos/end_effector_loc Max                0.501465\n",
      "evaluation/env_infos/end_effector_loc Min               -0.749493\n",
      "time/data storing (s)                                    0.00596318\n",
      "time/evaluation sampling (s)                             1.07362\n",
      "time/exploration sampling (s)                            0.145088\n",
      "time/logging (s)                                         0.0195276\n",
      "time/saving (s)                                          0.0277462\n",
      "time/training (s)                                       52.9029\n",
      "time/epoch (s)                                          54.1749\n",
      "time/total (s)                                        1938.41\n",
      "Epoch                                                   39\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:21:47.800613 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 40 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00100869\n",
      "trainer/QF2 Loss                                         0.00102457\n",
      "trainer/Policy Loss                                      3.49966\n",
      "trainer/Q1 Predictions Mean                             -1.2473\n",
      "trainer/Q1 Predictions Std                               0.906431\n",
      "trainer/Q1 Predictions Max                               0.882894\n",
      "trainer/Q1 Predictions Min                              -3.76796\n",
      "trainer/Q2 Predictions Mean                             -1.23101\n",
      "trainer/Q2 Predictions Std                               0.901847\n",
      "trainer/Q2 Predictions Max                               0.860766\n",
      "trainer/Q2 Predictions Min                              -3.76169\n",
      "trainer/Q Targets Mean                                  -1.246\n",
      "trainer/Q Targets Std                                    0.903441\n",
      "trainer/Q Targets Max                                    0.862993\n",
      "trainer/Q Targets Min                                   -3.78016\n",
      "trainer/Log Pis Mean                                     2.30713\n",
      "trainer/Log Pis Std                                      1.35306\n",
      "trainer/Log Pis Max                                      4.91661\n",
      "trainer/Log Pis Min                                     -1.78833\n",
      "trainer/Policy mu Mean                                  -0.0353147\n",
      "trainer/Policy mu Std                                    0.439661\n",
      "trainer/Policy mu Max                                    2.21368\n",
      "trainer/Policy mu Min                                   -2.23778\n",
      "trainer/Policy log std Mean                             -2.33871\n",
      "trainer/Policy log std Std                               0.70751\n",
      "trainer/Policy log std Max                               0.0226979\n",
      "trainer/Policy log std Min                              -3.56064\n",
      "trainer/Alpha                                            0.0202971\n",
      "trainer/Alpha Loss                                       1.19701\n",
      "exploration/num steps total                           5100\n",
      "exploration/num paths total                            255\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.104125\n",
      "exploration/Rewards Std                                  0.0663407\n",
      "exploration/Rewards Max                                  0.0459232\n",
      "exploration/Rewards Min                                 -0.238011\n",
      "exploration/Returns Mean                                -2.0825\n",
      "exploration/Returns Std                                  1.00011\n",
      "exploration/Returns Max                                 -0.312036\n",
      "exploration/Returns Min                                 -3.0732\n",
      "exploration/Actions Mean                                -0.000739965\n",
      "exploration/Actions Std                                  0.109163\n",
      "exploration/Actions Max                                  0.312448\n",
      "exploration/Actions Min                                 -0.436365\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.0825\n",
      "exploration/env_infos/final/reward_dist Mean             0.30907\n",
      "exploration/env_infos/final/reward_dist Std              0.285957\n",
      "exploration/env_infos/final/reward_dist Max              0.652643\n",
      "exploration/env_infos/final/reward_dist Min              2.50373e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000692348\n",
      "exploration/env_infos/initial/reward_dist Std            0.00084336\n",
      "exploration/env_infos/initial/reward_dist Max            0.0022963\n",
      "exploration/env_infos/initial/reward_dist Min            3.01038e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.186254\n",
      "exploration/env_infos/reward_dist Std                    0.300881\n",
      "exploration/env_infos/reward_dist Max                    0.995926\n",
      "exploration/env_infos/reward_dist Min                    2.50373e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.057034\n",
      "exploration/env_infos/final/reward_energy Std            0.0201156\n",
      "exploration/env_infos/final/reward_energy Max           -0.0323047\n",
      "exploration/env_infos/final/reward_energy Min           -0.0921777\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.194034\n",
      "exploration/env_infos/initial/reward_energy Std          0.12361\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0853971\n",
      "exploration/env_infos/initial/reward_energy Min         -0.357288\n",
      "exploration/env_infos/reward_energy Mean                -0.125262\n",
      "exploration/env_infos/reward_energy Std                  0.0902418\n",
      "exploration/env_infos/reward_energy Max                 -0.0116367\n",
      "exploration/env_infos/reward_energy Min                 -0.500425\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0414172\n",
      "exploration/env_infos/final/end_effector_loc Std         0.220476\n",
      "exploration/env_infos/final/end_effector_loc Max         0.30641\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.453635\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00438109\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00685323\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00423981\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0178521\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0405139\n",
      "exploration/env_infos/end_effector_loc Std               0.123005\n",
      "exploration/env_infos/end_effector_loc Max               0.30641\n",
      "exploration/env_infos/end_effector_loc Min              -0.453635\n",
      "evaluation/num steps total                           41000\n",
      "evaluation/num paths total                            2050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0483082\n",
      "evaluation/Rewards Std                                   0.0653311\n",
      "evaluation/Rewards Max                                   0.139353\n",
      "evaluation/Rewards Min                                  -0.270565\n",
      "evaluation/Returns Mean                                 -0.966165\n",
      "evaluation/Returns Std                                   0.966574\n",
      "evaluation/Returns Max                                   1.26566\n",
      "evaluation/Returns Min                                  -3.05738\n",
      "evaluation/Actions Mean                                 -0.00185485\n",
      "evaluation/Actions Std                                   0.0636286\n",
      "evaluation/Actions Max                                   0.726745\n",
      "evaluation/Actions Min                                  -0.43009\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.966165\n",
      "evaluation/env_infos/final/reward_dist Mean              0.216233\n",
      "evaluation/env_infos/final/reward_dist Std               0.270733\n",
      "evaluation/env_infos/final/reward_dist Max               0.789602\n",
      "evaluation/env_infos/final/reward_dist Min               1.14164e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00443151\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00670248\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0310264\n",
      "evaluation/env_infos/initial/reward_dist Min             1.03189e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.183887\n",
      "evaluation/env_infos/reward_dist Std                     0.259339\n",
      "evaluation/env_infos/reward_dist Max                     0.995598\n",
      "evaluation/env_infos/reward_dist Min                     1.14164e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0329307\n",
      "evaluation/env_infos/final/reward_energy Std             0.0367194\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00326253\n",
      "evaluation/env_infos/final/reward_energy Min            -0.181805\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.199967\n",
      "evaluation/env_infos/initial/reward_energy Std           0.181478\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0120818\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.859322\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0528298\n",
      "evaluation/env_infos/reward_energy Std                   0.072891\n",
      "evaluation/env_infos/reward_energy Max                  -0.000620618\n",
      "evaluation/env_infos/reward_energy Min                  -0.859322\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0183471\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.224637\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.5635\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.462296\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00197038\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00934178\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0363372\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0215045\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0196911\n",
      "evaluation/env_infos/end_effector_loc Std                0.147722\n",
      "evaluation/env_infos/end_effector_loc Max                0.5635\n",
      "evaluation/env_infos/end_effector_loc Min               -0.462296\n",
      "time/data storing (s)                                    0.0060424\n",
      "time/evaluation sampling (s)                             1.14703\n",
      "time/exploration sampling (s)                            0.120568\n",
      "time/logging (s)                                         0.023381\n",
      "time/saving (s)                                          0.0345917\n",
      "time/training (s)                                       49.2162\n",
      "time/epoch (s)                                          50.5478\n",
      "time/total (s)                                        1989.52\n",
      "Epoch                                                   40\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:22:38.256958 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 41 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00168396\n",
      "trainer/QF2 Loss                                         0.00220799\n",
      "trainer/Policy Loss                                      3.14408\n",
      "trainer/Q1 Predictions Mean                             -1.28074\n",
      "trainer/Q1 Predictions Std                               0.927691\n",
      "trainer/Q1 Predictions Max                               0.870419\n",
      "trainer/Q1 Predictions Min                              -4.29965\n",
      "trainer/Q2 Predictions Mean                             -1.26605\n",
      "trainer/Q2 Predictions Std                               0.915281\n",
      "trainer/Q2 Predictions Max                               0.847792\n",
      "trainer/Q2 Predictions Min                              -4.25999\n",
      "trainer/Q Targets Mean                                  -1.272\n",
      "trainer/Q Targets Std                                    0.924376\n",
      "trainer/Q Targets Max                                    0.886944\n",
      "trainer/Q Targets Min                                   -4.3959\n",
      "trainer/Log Pis Mean                                     1.92489\n",
      "trainer/Log Pis Std                                      1.38235\n",
      "trainer/Log Pis Max                                      4.72843\n",
      "trainer/Log Pis Min                                     -3.56524\n",
      "trainer/Policy mu Mean                                  -0.0810843\n",
      "trainer/Policy mu Std                                    0.525227\n",
      "trainer/Policy mu Max                                    2.03944\n",
      "trainer/Policy mu Min                                   -2.09329\n",
      "trainer/Policy log std Mean                             -2.15478\n",
      "trainer/Policy log std Std                               0.659679\n",
      "trainer/Policy log std Max                               0.168027\n",
      "trainer/Policy log std Min                              -3.37378\n",
      "trainer/Alpha                                            0.0194595\n",
      "trainer/Alpha Loss                                      -0.295847\n",
      "exploration/num steps total                           5200\n",
      "exploration/num paths total                            260\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0680671\n",
      "exploration/Rewards Std                                  0.0969515\n",
      "exploration/Rewards Max                                  0.155049\n",
      "exploration/Rewards Min                                 -0.385479\n",
      "exploration/Returns Mean                                -1.36134\n",
      "exploration/Returns Std                                  1.52177\n",
      "exploration/Returns Max                                  1.67753\n",
      "exploration/Returns Min                                 -2.23076\n",
      "exploration/Actions Mean                                -0.00343716\n",
      "exploration/Actions Std                                  0.151058\n",
      "exploration/Actions Max                                  0.541168\n",
      "exploration/Actions Min                                 -0.619518\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.36134\n",
      "exploration/env_infos/final/reward_dist Mean             0.107333\n",
      "exploration/env_infos/final/reward_dist Std              0.214655\n",
      "exploration/env_infos/final/reward_dist Max              0.536642\n",
      "exploration/env_infos/final/reward_dist Min              1.91326e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00750519\n",
      "exploration/env_infos/initial/reward_dist Std            0.0095002\n",
      "exploration/env_infos/initial/reward_dist Max            0.0236588\n",
      "exploration/env_infos/initial/reward_dist Min            8.15879e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.184163\n",
      "exploration/env_infos/reward_dist Std                    0.268832\n",
      "exploration/env_infos/reward_dist Max                    0.963553\n",
      "exploration/env_infos/reward_dist Min                    1.91326e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0838806\n",
      "exploration/env_infos/final/reward_energy Std            0.0528358\n",
      "exploration/env_infos/final/reward_energy Max           -0.00993333\n",
      "exploration/env_infos/final/reward_energy Min           -0.171211\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.277121\n",
      "exploration/env_infos/initial/reward_energy Std          0.117164\n",
      "exploration/env_infos/initial/reward_energy Max         -0.136926\n",
      "exploration/env_infos/initial/reward_energy Min         -0.451496\n",
      "exploration/env_infos/reward_energy Mean                -0.164643\n",
      "exploration/env_infos/reward_energy Std                  0.13621\n",
      "exploration/env_infos/reward_energy Max                 -0.00993333\n",
      "exploration/env_infos/reward_energy Min                 -0.779586\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0938977\n",
      "exploration/env_infos/final/end_effector_loc Std         0.334133\n",
      "exploration/env_infos/final/end_effector_loc Max         0.576434\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.528275\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00300656\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0102037\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0174867\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0186464\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0499037\n",
      "exploration/env_infos/end_effector_loc Std               0.214938\n",
      "exploration/env_infos/end_effector_loc Max               0.576434\n",
      "exploration/env_infos/end_effector_loc Min              -0.528275\n",
      "evaluation/num steps total                           42000\n",
      "evaluation/num paths total                            2100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0608711\n",
      "evaluation/Rewards Std                                   0.0660185\n",
      "evaluation/Rewards Max                                   0.150941\n",
      "evaluation/Rewards Min                                  -0.350975\n",
      "evaluation/Returns Mean                                 -1.21742\n",
      "evaluation/Returns Std                                   0.907434\n",
      "evaluation/Returns Max                                   1.55601\n",
      "evaluation/Returns Min                                  -3.78743\n",
      "evaluation/Actions Mean                                 -0.00431184\n",
      "evaluation/Actions Std                                   0.0675774\n",
      "evaluation/Actions Max                                   0.743017\n",
      "evaluation/Actions Min                                  -0.372161\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.21742\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0716519\n",
      "evaluation/env_infos/final/reward_dist Std               0.158639\n",
      "evaluation/env_infos/final/reward_dist Max               0.798045\n",
      "evaluation/env_infos/final/reward_dist Min               1.61712e-34\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00737963\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0131185\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0543333\n",
      "evaluation/env_infos/initial/reward_dist Min             1.39123e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.113314\n",
      "evaluation/env_infos/reward_dist Std                     0.204956\n",
      "evaluation/env_infos/reward_dist Max                     0.989855\n",
      "evaluation/env_infos/reward_dist Min                     1.61712e-34\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0280367\n",
      "evaluation/env_infos/final/reward_energy Std             0.0226103\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00126483\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0994695\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.21972\n",
      "evaluation/env_infos/initial/reward_energy Std           0.187972\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0100306\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.79766\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0576055\n",
      "evaluation/env_infos/reward_energy Std                   0.0764997\n",
      "evaluation/env_infos/reward_energy Max                  -0.00024041\n",
      "evaluation/env_infos/reward_energy Min                  -0.79766\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.052907\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.285949\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.632927\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.834439\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000325518\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010218\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0371508\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0186081\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0206627\n",
      "evaluation/env_infos/end_effector_loc Std                0.189767\n",
      "evaluation/env_infos/end_effector_loc Max                0.632927\n",
      "evaluation/env_infos/end_effector_loc Min               -0.834439\n",
      "time/data storing (s)                                    0.00622733\n",
      "time/evaluation sampling (s)                             1.25528\n",
      "time/exploration sampling (s)                            0.129844\n",
      "time/logging (s)                                         0.0197884\n",
      "time/saving (s)                                          0.028496\n",
      "time/training (s)                                       48.4381\n",
      "time/epoch (s)                                          49.8777\n",
      "time/total (s)                                        2039.97\n",
      "Epoch                                                   41\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:23:28.916986 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 42 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00232765\n",
      "trainer/QF2 Loss                                         0.00196826\n",
      "trainer/Policy Loss                                      3.17058\n",
      "trainer/Q1 Predictions Mean                             -1.23097\n",
      "trainer/Q1 Predictions Std                               0.879344\n",
      "trainer/Q1 Predictions Max                               0.846843\n",
      "trainer/Q1 Predictions Min                              -3.92824\n",
      "trainer/Q2 Predictions Mean                             -1.22047\n",
      "trainer/Q2 Predictions Std                               0.87515\n",
      "trainer/Q2 Predictions Max                               0.877428\n",
      "trainer/Q2 Predictions Min                              -4.02351\n",
      "trainer/Q Targets Mean                                  -1.20691\n",
      "trainer/Q Targets Std                                    0.877011\n",
      "trainer/Q Targets Max                                    0.8791\n",
      "trainer/Q Targets Min                                   -3.98936\n",
      "trainer/Log Pis Mean                                     1.97615\n",
      "trainer/Log Pis Std                                      1.39897\n",
      "trainer/Log Pis Max                                      4.39875\n",
      "trainer/Log Pis Min                                     -6.86042\n",
      "trainer/Policy mu Mean                                  -0.0600019\n",
      "trainer/Policy mu Std                                    0.456447\n",
      "trainer/Policy mu Max                                    1.9632\n",
      "trainer/Policy mu Min                                   -2.34574\n",
      "trainer/Policy log std Mean                             -2.22723\n",
      "trainer/Policy log std Std                               0.651054\n",
      "trainer/Policy log std Max                               0.318542\n",
      "trainer/Policy log std Min                              -3.29417\n",
      "trainer/Alpha                                            0.018932\n",
      "trainer/Alpha Loss                                      -0.0946063\n",
      "exploration/num steps total                           5300\n",
      "exploration/num paths total                            265\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0997929\n",
      "exploration/Rewards Std                                  0.0774042\n",
      "exploration/Rewards Max                                  0.0456645\n",
      "exploration/Rewards Min                                 -0.387664\n",
      "exploration/Returns Mean                                -1.99586\n",
      "exploration/Returns Std                                  0.679285\n",
      "exploration/Returns Max                                 -1.19984\n",
      "exploration/Returns Min                                 -3.1255\n",
      "exploration/Actions Mean                                 0.00322054\n",
      "exploration/Actions Std                                  0.115439\n",
      "exploration/Actions Max                                  0.397203\n",
      "exploration/Actions Min                                 -0.655856\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.99586\n",
      "exploration/env_infos/final/reward_dist Mean             0.173812\n",
      "exploration/env_infos/final/reward_dist Std              0.347593\n",
      "exploration/env_infos/final/reward_dist Max              0.868998\n",
      "exploration/env_infos/final/reward_dist Min              1.30088e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0124925\n",
      "exploration/env_infos/initial/reward_dist Std            0.0182556\n",
      "exploration/env_infos/initial/reward_dist Max            0.048431\n",
      "exploration/env_infos/initial/reward_dist Min            0.00049501\n",
      "exploration/env_infos/reward_dist Mean                   0.149636\n",
      "exploration/env_infos/reward_dist Std                    0.237881\n",
      "exploration/env_infos/reward_dist Max                    0.955355\n",
      "exploration/env_infos/reward_dist Min                    1.30088e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.107685\n",
      "exploration/env_infos/final/reward_energy Std            0.0710361\n",
      "exploration/env_infos/final/reward_energy Max           -0.0456689\n",
      "exploration/env_infos/final/reward_energy Min           -0.238688\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.284382\n",
      "exploration/env_infos/initial/reward_energy Std          0.1371\n",
      "exploration/env_infos/initial/reward_energy Max         -0.101893\n",
      "exploration/env_infos/initial/reward_energy Min         -0.478369\n",
      "exploration/env_infos/reward_energy Mean                -0.11742\n",
      "exploration/env_infos/reward_energy Std                  0.113516\n",
      "exploration/env_infos/reward_energy Max                 -0.00351192\n",
      "exploration/env_infos/reward_energy Min                 -0.67572\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0536026\n",
      "exploration/env_infos/final/end_effector_loc Std         0.30729\n",
      "exploration/env_infos/final/end_effector_loc Max         0.404879\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.582663\n",
      "exploration/env_infos/initial/end_effector_loc Mean      2.41379e-05\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0111618\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0198602\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0154714\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0371578\n",
      "exploration/env_infos/end_effector_loc Std               0.189025\n",
      "exploration/env_infos/end_effector_loc Max               0.404879\n",
      "exploration/env_infos/end_effector_loc Min              -0.582663\n",
      "evaluation/num steps total                           43000\n",
      "evaluation/num paths total                            2150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0407991\n",
      "evaluation/Rewards Std                                   0.0732045\n",
      "evaluation/Rewards Max                                   0.14799\n",
      "evaluation/Rewards Min                                  -0.277262\n",
      "evaluation/Returns Mean                                 -0.815982\n",
      "evaluation/Returns Std                                   1.1519\n",
      "evaluation/Returns Max                                   1.93514\n",
      "evaluation/Returns Min                                  -2.98826\n",
      "evaluation/Actions Mean                                 -0.00215839\n",
      "evaluation/Actions Std                                   0.0556499\n",
      "evaluation/Actions Max                                   0.688897\n",
      "evaluation/Actions Min                                  -0.38372\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.815982\n",
      "evaluation/env_infos/final/reward_dist Mean              0.172249\n",
      "evaluation/env_infos/final/reward_dist Std               0.262115\n",
      "evaluation/env_infos/final/reward_dist Max               0.97352\n",
      "evaluation/env_infos/final/reward_dist Min               3.39549e-08\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00603194\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00955924\n",
      "evaluation/env_infos/initial/reward_dist Max             0.043044\n",
      "evaluation/env_infos/initial/reward_dist Min             3.99494e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.204905\n",
      "evaluation/env_infos/reward_dist Std                     0.269287\n",
      "evaluation/env_infos/reward_dist Max                     0.999032\n",
      "evaluation/env_infos/reward_dist Min                     3.39549e-08\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0324261\n",
      "evaluation/env_infos/final/reward_energy Std             0.0198855\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00538197\n",
      "evaluation/env_infos/final/reward_energy Min            -0.105675\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.19806\n",
      "evaluation/env_infos/initial/reward_energy Std           0.155529\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.013025\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.741966\n",
      "evaluation/env_infos/reward_energy Mean                 -0.05092\n",
      "evaluation/env_infos/reward_energy Std                   0.0600856\n",
      "evaluation/env_infos/reward_energy Max                  -0.000925996\n",
      "evaluation/env_infos/reward_energy Min                  -0.741966\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0327002\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.220242\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.339124\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.468071\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00158127\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00876189\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0344448\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.019186\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0181639\n",
      "evaluation/env_infos/end_effector_loc Std                0.146198\n",
      "evaluation/env_infos/end_effector_loc Max                0.339124\n",
      "evaluation/env_infos/end_effector_loc Min               -0.468071\n",
      "time/data storing (s)                                    0.00585332\n",
      "time/evaluation sampling (s)                             0.928645\n",
      "time/exploration sampling (s)                            0.119738\n",
      "time/logging (s)                                         0.0214827\n",
      "time/saving (s)                                          0.0286493\n",
      "time/training (s)                                       48.9651\n",
      "time/epoch (s)                                          50.0695\n",
      "time/total (s)                                        2090.63\n",
      "Epoch                                                   42\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:24:18.714978 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 43 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00179691\n",
      "trainer/QF2 Loss                                         0.00143547\n",
      "trainer/Policy Loss                                      3.17049\n",
      "trainer/Q1 Predictions Mean                             -1.29286\n",
      "trainer/Q1 Predictions Std                               0.917964\n",
      "trainer/Q1 Predictions Max                               0.309563\n",
      "trainer/Q1 Predictions Min                              -3.29669\n",
      "trainer/Q2 Predictions Mean                             -1.28737\n",
      "trainer/Q2 Predictions Std                               0.914148\n",
      "trainer/Q2 Predictions Max                               0.314662\n",
      "trainer/Q2 Predictions Min                              -3.32294\n",
      "trainer/Q Targets Mean                                  -1.30293\n",
      "trainer/Q Targets Std                                    0.915939\n",
      "trainer/Q Targets Max                                    0.300563\n",
      "trainer/Q Targets Min                                   -3.33379\n",
      "trainer/Log Pis Mean                                     1.91037\n",
      "trainer/Log Pis Std                                      1.38665\n",
      "trainer/Log Pis Max                                      4.43863\n",
      "trainer/Log Pis Min                                     -3.65795\n",
      "trainer/Policy mu Mean                                  -0.0151367\n",
      "trainer/Policy mu Std                                    0.448138\n",
      "trainer/Policy mu Max                                    2.24011\n",
      "trainer/Policy mu Min                                   -1.7517\n",
      "trainer/Policy log std Mean                             -2.19247\n",
      "trainer/Policy log std Std                               0.686893\n",
      "trainer/Policy log std Max                              -0.112907\n",
      "trainer/Policy log std Min                              -3.35074\n",
      "trainer/Alpha                                            0.0197019\n",
      "trainer/Alpha Loss                                      -0.351815\n",
      "exploration/num steps total                           5400\n",
      "exploration/num paths total                            270\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0435731\n",
      "exploration/Rewards Std                                  0.0760558\n",
      "exploration/Rewards Max                                  0.097579\n",
      "exploration/Rewards Min                                 -0.238857\n",
      "exploration/Returns Mean                                -0.871461\n",
      "exploration/Returns Std                                  1.19905\n",
      "exploration/Returns Max                                  0.872201\n",
      "exploration/Returns Min                                 -2.43465\n",
      "exploration/Actions Mean                                 0.000904016\n",
      "exploration/Actions Std                                  0.164234\n",
      "exploration/Actions Max                                  0.665469\n",
      "exploration/Actions Min                                 -0.480784\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.871461\n",
      "exploration/env_infos/final/reward_dist Mean             0.188047\n",
      "exploration/env_infos/final/reward_dist Std              0.165937\n",
      "exploration/env_infos/final/reward_dist Max              0.396649\n",
      "exploration/env_infos/final/reward_dist Min              5.34063e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00587974\n",
      "exploration/env_infos/initial/reward_dist Std            0.00583859\n",
      "exploration/env_infos/initial/reward_dist Max            0.0133625\n",
      "exploration/env_infos/initial/reward_dist Min            0.000241879\n",
      "exploration/env_infos/reward_dist Mean                   0.174153\n",
      "exploration/env_infos/reward_dist Std                    0.222262\n",
      "exploration/env_infos/reward_dist Max                    0.962719\n",
      "exploration/env_infos/reward_dist Min                    5.34063e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.179693\n",
      "exploration/env_infos/final/reward_energy Std            0.16901\n",
      "exploration/env_infos/final/reward_energy Max           -0.055373\n",
      "exploration/env_infos/final/reward_energy Min           -0.503984\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.343107\n",
      "exploration/env_infos/initial/reward_energy Std          0.275672\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0313971\n",
      "exploration/env_infos/initial/reward_energy Min         -0.832677\n",
      "exploration/env_infos/reward_energy Mean                -0.182842\n",
      "exploration/env_infos/reward_energy Std                  0.143234\n",
      "exploration/env_infos/reward_energy Max                 -0.0090116\n",
      "exploration/env_infos/reward_energy Min                 -0.832677\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0251731\n",
      "exploration/env_infos/final/end_effector_loc Std         0.204121\n",
      "exploration/env_infos/final/end_effector_loc Max         0.388341\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.413627\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00813757\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0132638\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0332735\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0137954\n",
      "exploration/env_infos/end_effector_loc Mean              0.0410034\n",
      "exploration/env_infos/end_effector_loc Std               0.154788\n",
      "exploration/env_infos/end_effector_loc Max               0.388341\n",
      "exploration/env_infos/end_effector_loc Min              -0.413627\n",
      "evaluation/num steps total                           44000\n",
      "evaluation/num paths total                            2200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0384854\n",
      "evaluation/Rewards Std                                   0.0824981\n",
      "evaluation/Rewards Max                                   0.128542\n",
      "evaluation/Rewards Min                                  -0.604644\n",
      "evaluation/Returns Mean                                 -0.769708\n",
      "evaluation/Returns Std                                   1.21448\n",
      "evaluation/Returns Max                                   1.70811\n",
      "evaluation/Returns Min                                  -3.72825\n",
      "evaluation/Actions Mean                                  2.85512e-05\n",
      "evaluation/Actions Std                                   0.0737116\n",
      "evaluation/Actions Max                                   0.896664\n",
      "evaluation/Actions Min                                  -0.396134\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.769708\n",
      "evaluation/env_infos/final/reward_dist Mean              0.140401\n",
      "evaluation/env_infos/final/reward_dist Std               0.223613\n",
      "evaluation/env_infos/final/reward_dist Max               0.870201\n",
      "evaluation/env_infos/final/reward_dist Min               3.72543e-48\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00854992\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0133749\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0486601\n",
      "evaluation/env_infos/initial/reward_dist Min             2.04566e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.163145\n",
      "evaluation/env_infos/reward_dist Std                     0.235511\n",
      "evaluation/env_infos/reward_dist Max                     0.991443\n",
      "evaluation/env_infos/reward_dist Min                     3.72543e-48\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0523172\n",
      "evaluation/env_infos/final/reward_energy Std             0.0381168\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0146054\n",
      "evaluation/env_infos/final/reward_energy Min            -0.209813\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.233686\n",
      "evaluation/env_infos/initial/reward_energy Std           0.194663\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0105951\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.934557\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0680871\n",
      "evaluation/env_infos/reward_energy Std                   0.0789364\n",
      "evaluation/env_infos/reward_energy Max                  -0.000381695\n",
      "evaluation/env_infos/reward_energy Min                  -0.934557\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0234846\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.225722\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.35554\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.848057\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00019643\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107513\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0448332\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0198067\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.013138\n",
      "evaluation/env_infos/end_effector_loc Std                0.152205\n",
      "evaluation/env_infos/end_effector_loc Max                0.374533\n",
      "evaluation/env_infos/end_effector_loc Min               -0.848057\n",
      "time/data storing (s)                                    0.00608824\n",
      "time/evaluation sampling (s)                             0.921875\n",
      "time/exploration sampling (s)                            0.122193\n",
      "time/logging (s)                                         0.0199914\n",
      "time/saving (s)                                          0.0312314\n",
      "time/training (s)                                       48.1535\n",
      "time/epoch (s)                                          49.2549\n",
      "time/total (s)                                        2140.43\n",
      "Epoch                                                   43\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:25:08.360525 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 44 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00514804\n",
      "trainer/QF2 Loss                                         0.00209302\n",
      "trainer/Policy Loss                                      3.47248\n",
      "trainer/Q1 Predictions Mean                             -1.26958\n",
      "trainer/Q1 Predictions Std                               0.857462\n",
      "trainer/Q1 Predictions Max                               0.294184\n",
      "trainer/Q1 Predictions Min                              -4.02235\n",
      "trainer/Q2 Predictions Mean                             -1.27729\n",
      "trainer/Q2 Predictions Std                               0.862357\n",
      "trainer/Q2 Predictions Max                               0.307744\n",
      "trainer/Q2 Predictions Min                              -3.8982\n",
      "trainer/Q Targets Mean                                  -1.28189\n",
      "trainer/Q Targets Std                                    0.861757\n",
      "trainer/Q Targets Max                                    0.274353\n",
      "trainer/Q Targets Min                                   -3.87402\n",
      "trainer/Log Pis Mean                                     2.23782\n",
      "trainer/Log Pis Std                                      1.12094\n",
      "trainer/Log Pis Max                                      4.65124\n",
      "trainer/Log Pis Min                                     -1.68367\n",
      "trainer/Policy mu Mean                                  -0.0296607\n",
      "trainer/Policy mu Std                                    0.480574\n",
      "trainer/Policy mu Max                                    1.91554\n",
      "trainer/Policy mu Min                                   -2.22765\n",
      "trainer/Policy log std Mean                             -2.26912\n",
      "trainer/Policy log std Std                               0.647726\n",
      "trainer/Policy log std Max                              -0.330767\n",
      "trainer/Policy log std Min                              -3.42514\n",
      "trainer/Alpha                                            0.0201674\n",
      "trainer/Alpha Loss                                       0.927956\n",
      "exploration/num steps total                           5500\n",
      "exploration/num paths total                            275\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.110316\n",
      "exploration/Rewards Std                                  0.0446111\n",
      "exploration/Rewards Max                                 -0.0405898\n",
      "exploration/Rewards Min                                 -0.305822\n",
      "exploration/Returns Mean                                -2.20633\n",
      "exploration/Returns Std                                  0.463514\n",
      "exploration/Returns Max                                 -1.49697\n",
      "exploration/Returns Min                                 -2.92188\n",
      "exploration/Actions Mean                                -0.00454799\n",
      "exploration/Actions Std                                  0.175202\n",
      "exploration/Actions Max                                  0.892627\n",
      "exploration/Actions Min                                 -0.65201\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.20633\n",
      "exploration/env_infos/final/reward_dist Mean             0.1415\n",
      "exploration/env_infos/final/reward_dist Std              0.220856\n",
      "exploration/env_infos/final/reward_dist Max              0.570545\n",
      "exploration/env_infos/final/reward_dist Min              6.60917e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0265436\n",
      "exploration/env_infos/initial/reward_dist Std            0.0459217\n",
      "exploration/env_infos/initial/reward_dist Max            0.11814\n",
      "exploration/env_infos/initial/reward_dist Min            0.00029892\n",
      "exploration/env_infos/reward_dist Mean                   0.132913\n",
      "exploration/env_infos/reward_dist Std                    0.184864\n",
      "exploration/env_infos/reward_dist Max                    0.962837\n",
      "exploration/env_infos/reward_dist Min                    6.60917e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.127695\n",
      "exploration/env_infos/final/reward_energy Std            0.0778899\n",
      "exploration/env_infos/final/reward_energy Max           -0.01644\n",
      "exploration/env_infos/final/reward_energy Min           -0.241239\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.398364\n",
      "exploration/env_infos/initial/reward_energy Std          0.298205\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0612894\n",
      "exploration/env_infos/initial/reward_energy Min         -0.9541\n",
      "exploration/env_infos/reward_energy Mean                -0.193726\n",
      "exploration/env_infos/reward_energy Std                  0.154606\n",
      "exploration/env_infos/reward_energy Max                 -0.00802377\n",
      "exploration/env_infos/reward_energy Min                 -0.9541\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.075066\n",
      "exploration/env_infos/final/end_effector_loc Std         0.182401\n",
      "exploration/env_infos/final/end_effector_loc Max         0.365415\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.152528\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00705928\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0161149\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0446313\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0110213\n",
      "exploration/env_infos/end_effector_loc Mean              0.0640441\n",
      "exploration/env_infos/end_effector_loc Std               0.143859\n",
      "exploration/env_infos/end_effector_loc Max               0.365415\n",
      "exploration/env_infos/end_effector_loc Min              -0.169934\n",
      "evaluation/num steps total                           45000\n",
      "evaluation/num paths total                            2250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0426256\n",
      "evaluation/Rewards Std                                   0.0701317\n",
      "evaluation/Rewards Max                                   0.133894\n",
      "evaluation/Rewards Min                                  -0.388869\n",
      "evaluation/Returns Mean                                 -0.852513\n",
      "evaluation/Returns Std                                   1.00067\n",
      "evaluation/Returns Max                                   1.44587\n",
      "evaluation/Returns Min                                  -2.6023\n",
      "evaluation/Actions Mean                                 -0.00132102\n",
      "evaluation/Actions Std                                   0.0661709\n",
      "evaluation/Actions Max                                   0.800716\n",
      "evaluation/Actions Min                                  -0.462486\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.852513\n",
      "evaluation/env_infos/final/reward_dist Mean              0.193638\n",
      "evaluation/env_infos/final/reward_dist Std               0.258336\n",
      "evaluation/env_infos/final/reward_dist Max               0.88909\n",
      "evaluation/env_infos/final/reward_dist Min               1.93425e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00652676\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0106117\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0407092\n",
      "evaluation/env_infos/initial/reward_dist Min             2.49471e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.220107\n",
      "evaluation/env_infos/reward_dist Std                     0.275421\n",
      "evaluation/env_infos/reward_dist Max                     0.997014\n",
      "evaluation/env_infos/reward_dist Min                     1.93425e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0314485\n",
      "evaluation/env_infos/final/reward_energy Std             0.0192426\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00661003\n",
      "evaluation/env_infos/final/reward_energy Min            -0.124214\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.234249\n",
      "evaluation/env_infos/initial/reward_energy Std           0.178961\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0132336\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.806485\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0588913\n",
      "evaluation/env_infos/reward_energy Std                   0.0727494\n",
      "evaluation/env_infos/reward_energy Max                  -0.000677946\n",
      "evaluation/env_infos/reward_energy Min                  -0.806485\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0184038\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.237079\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.541836\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.525342\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00182083\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010262\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0400358\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0231243\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0180218\n",
      "evaluation/env_infos/end_effector_loc Std                0.155629\n",
      "evaluation/env_infos/end_effector_loc Max                0.541836\n",
      "evaluation/env_infos/end_effector_loc Min               -0.525342\n",
      "time/data storing (s)                                    0.00620427\n",
      "time/evaluation sampling (s)                             0.990386\n",
      "time/exploration sampling (s)                            0.125543\n",
      "time/logging (s)                                         0.0216346\n",
      "time/saving (s)                                          0.0273052\n",
      "time/training (s)                                       47.849\n",
      "time/epoch (s)                                          49.0201\n",
      "time/total (s)                                        2190.07\n",
      "Epoch                                                   44\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:25:58.227678 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 45 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00133322\r\n",
      "trainer/QF2 Loss                                         0.00180009\r\n",
      "trainer/Policy Loss                                      3.25785\r\n",
      "trainer/Q1 Predictions Mean                             -1.22782\r\n",
      "trainer/Q1 Predictions Std                               0.883555\r\n",
      "trainer/Q1 Predictions Max                               0.866859\r\n",
      "trainer/Q1 Predictions Min                              -3.46851\r\n",
      "trainer/Q2 Predictions Mean                             -1.2348\r\n",
      "trainer/Q2 Predictions Std                               0.88783\r\n",
      "trainer/Q2 Predictions Max                               0.821493\r\n",
      "trainer/Q2 Predictions Min                              -3.57511\r\n",
      "trainer/Q Targets Mean                                  -1.23321\r\n",
      "trainer/Q Targets Std                                    0.88593\r\n",
      "trainer/Q Targets Max                                    0.86131\r\n",
      "trainer/Q Targets Min                                   -3.60887\r\n",
      "trainer/Log Pis Mean                                     2.05675\r\n",
      "trainer/Log Pis Std                                      1.41909\r\n",
      "trainer/Log Pis Max                                      5.49655\r\n",
      "trainer/Log Pis Min                                     -3.23069\r\n",
      "trainer/Policy mu Mean                                  -0.0381263\r\n",
      "trainer/Policy mu Std                                    0.470241\r\n",
      "trainer/Policy mu Max                                    2.36792\r\n",
      "trainer/Policy mu Min                                   -2.658\r\n",
      "trainer/Policy log std Mean                             -2.26212\r\n",
      "trainer/Policy log std Std                               0.687229\r\n",
      "trainer/Policy log std Max                              -0.0742826\r\n",
      "trainer/Policy log std Min                              -3.47998\r\n",
      "trainer/Alpha                                            0.0193016\r\n",
      "trainer/Alpha Loss                                       0.22409\r\n",
      "exploration/num steps total                           5600\r\n",
      "exploration/num paths total                            280\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0914792\r\n",
      "exploration/Rewards Std                                  0.0589347\r\n",
      "exploration/Rewards Max                                  0.0580812\r\n",
      "exploration/Rewards Min                                 -0.203012\r\n",
      "exploration/Returns Mean                                -1.82958\r\n",
      "exploration/Returns Std                                  0.845915\r\n",
      "exploration/Returns Max                                 -0.706328\r\n",
      "exploration/Returns Min                                 -3.14392\r\n",
      "exploration/Actions Mean                                -0.00787733\r\n",
      "exploration/Actions Std                                  0.0753757\r\n",
      "exploration/Actions Max                                  0.281317\r\n",
      "exploration/Actions Min                                 -0.371837\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.82958\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00553192\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00511682\r\n",
      "exploration/env_infos/final/reward_dist Max              0.013592\r\n",
      "exploration/env_infos/final/reward_dist Min              2.02044e-10\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0041646\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00764883\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0194435\r\n",
      "exploration/env_infos/initial/reward_dist Min            8.18441e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0942027\r\n",
      "exploration/env_infos/reward_dist Std                    0.205318\r\n",
      "exploration/env_infos/reward_dist Max                    0.941825\r\n",
      "exploration/env_infos/reward_dist Min                    2.02044e-10\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.104496\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0189439\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0914032\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.141702\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.130774\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.127754\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0377726\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.372451\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0859926\r\n",
      "exploration/env_infos/reward_energy Std                  0.0639717\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0149782\r\n",
      "exploration/env_infos/reward_energy Min                 -0.372451\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0591576\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.220635\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.279071\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.362287\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00294406\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00575424\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00190787\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0185918\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0338638\r\n",
      "exploration/env_infos/end_effector_loc Std               0.130641\r\n",
      "exploration/env_infos/end_effector_loc Max               0.279071\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.362287\r\n",
      "evaluation/num steps total                           46000\r\n",
      "evaluation/num paths total                            2300\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0449257\r\n",
      "evaluation/Rewards Std                                   0.0661493\r\n",
      "evaluation/Rewards Max                                   0.16973\r\n",
      "evaluation/Rewards Min                                  -0.364923\r\n",
      "evaluation/Returns Mean                                 -0.898513\r\n",
      "evaluation/Returns Std                                   0.997173\r\n",
      "evaluation/Returns Max                                   1.18311\r\n",
      "evaluation/Returns Min                                  -3.36304\r\n",
      "evaluation/Actions Mean                                 -0.00485289\r\n",
      "evaluation/Actions Std                                   0.0599349\r\n",
      "evaluation/Actions Max                                   0.759772\r\n",
      "evaluation/Actions Min                                  -0.537021\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.898513\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.1568\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.260258\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.958531\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.36489e-32\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00489317\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00818377\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0374308\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.09653e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.195308\r\n",
      "evaluation/env_infos/reward_dist Std                     0.266423\r\n",
      "evaluation/env_infos/reward_dist Max                     0.99889\r\n",
      "evaluation/env_infos/reward_dist Min                     5.36489e-32\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0332184\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0215359\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00366699\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.110636\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.210003\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.173967\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0063661\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.78301\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0524297\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0669522\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000575897\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.78301\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0365421\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.240943\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.37804\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.78369\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000203515\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00963929\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0379886\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0268511\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0133713\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.150777\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.37804\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.78369\r\n",
      "time/data storing (s)                                    0.00631765\r\n",
      "time/evaluation sampling (s)                             0.93308\r\n",
      "time/exploration sampling (s)                            0.119889\r\n",
      "time/logging (s)                                         0.0194492\r\n",
      "time/saving (s)                                          0.0274583\r\n",
      "time/training (s)                                       48.2002\r\n",
      "time/epoch (s)                                          49.3064\r\n",
      "time/total (s)                                        2239.93\r\n",
      "Epoch                                                   45\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:26:47.692778 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 46 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00180625\r\n",
      "trainer/QF2 Loss                                         0.00121679\r\n",
      "trainer/Policy Loss                                      3.3006\r\n",
      "trainer/Q1 Predictions Mean                             -1.20323\r\n",
      "trainer/Q1 Predictions Std                               0.870797\r\n",
      "trainer/Q1 Predictions Max                               0.777513\r\n",
      "trainer/Q1 Predictions Min                              -3.87698\r\n",
      "trainer/Q2 Predictions Mean                             -1.21083\r\n",
      "trainer/Q2 Predictions Std                               0.869105\r\n",
      "trainer/Q2 Predictions Max                               0.76718\r\n",
      "trainer/Q2 Predictions Min                              -3.88355\r\n",
      "trainer/Q Targets Mean                                  -1.198\r\n",
      "trainer/Q Targets Std                                    0.867148\r\n",
      "trainer/Q Targets Max                                    0.774883\r\n",
      "trainer/Q Targets Min                                   -3.95118\r\n",
      "trainer/Log Pis Mean                                     2.14406\r\n",
      "trainer/Log Pis Std                                      1.41146\r\n",
      "trainer/Log Pis Max                                      4.73335\r\n",
      "trainer/Log Pis Min                                     -4.49968\r\n",
      "trainer/Policy mu Mean                                  -0.0415354\r\n",
      "trainer/Policy mu Std                                    0.414742\r\n",
      "trainer/Policy mu Max                                    2.11765\r\n",
      "trainer/Policy mu Min                                   -2.19126\r\n",
      "trainer/Policy log std Mean                             -2.29704\r\n",
      "trainer/Policy log std Std                               0.659085\r\n",
      "trainer/Policy log std Max                              -0.389866\r\n",
      "trainer/Policy log std Min                              -3.61471\r\n",
      "trainer/Alpha                                            0.0211232\r\n",
      "trainer/Alpha Loss                                       0.555727\r\n",
      "exploration/num steps total                           5700\r\n",
      "exploration/num paths total                            285\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0666996\r\n",
      "exploration/Rewards Std                                  0.0657716\r\n",
      "exploration/Rewards Max                                  0.065223\r\n",
      "exploration/Rewards Min                                 -0.212952\r\n",
      "exploration/Returns Mean                                -1.33399\r\n",
      "exploration/Returns Std                                  0.617276\r\n",
      "exploration/Returns Max                                 -0.55971\r\n",
      "exploration/Returns Min                                 -2.25145\r\n",
      "exploration/Actions Mean                                -0.0073845\r\n",
      "exploration/Actions Std                                  0.07359\r\n",
      "exploration/Actions Max                                  0.235814\r\n",
      "exploration/Actions Min                                 -0.178387\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.33399\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0510983\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0970759\r\n",
      "exploration/env_infos/final/reward_dist Max              0.245111\r\n",
      "exploration/env_infos/final/reward_dist Min              9.92984e-12\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0127167\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0145471\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0309757\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.036e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.216725\r\n",
      "exploration/env_infos/reward_dist Std                    0.260446\r\n",
      "exploration/env_infos/reward_dist Max                    0.945976\r\n",
      "exploration/env_infos/reward_dist Min                    9.92984e-12\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0756615\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0426945\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0260554\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.131171\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.203869\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0787257\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0944211\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.331151\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0895104\r\n",
      "exploration/env_infos/reward_energy Std                  0.0541104\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00175507\r\n",
      "exploration/env_infos/reward_energy Min                 -0.331151\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.089556\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.282327\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.293386\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.530585\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000574965\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00770517\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0117907\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00891935\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0345045\r\n",
      "exploration/env_infos/end_effector_loc Std               0.171707\r\n",
      "exploration/env_infos/end_effector_loc Max               0.293386\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.530585\r\n",
      "evaluation/num steps total                           47000\r\n",
      "evaluation/num paths total                            2350\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0364661\r\n",
      "evaluation/Rewards Std                                   0.0703731\r\n",
      "evaluation/Rewards Max                                   0.137808\r\n",
      "evaluation/Rewards Min                                  -0.473036\r\n",
      "evaluation/Returns Mean                                 -0.729323\r\n",
      "evaluation/Returns Std                                   1.02687\r\n",
      "evaluation/Returns Max                                   1.62319\r\n",
      "evaluation/Returns Min                                  -2.43959\r\n",
      "evaluation/Actions Mean                                 -0.00435476\r\n",
      "evaluation/Actions Std                                   0.0628677\r\n",
      "evaluation/Actions Max                                   0.671049\r\n",
      "evaluation/Actions Min                                  -0.494859\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.729323\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.206855\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.272947\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.881948\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.07557e-21\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00491419\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0113527\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.056656\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.6243e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.228436\r\n",
      "evaluation/env_infos/reward_dist Std                     0.281647\r\n",
      "evaluation/env_infos/reward_dist Max                     0.995058\r\n",
      "evaluation/env_infos/reward_dist Min                     8.07557e-21\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0480047\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0301464\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00888427\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.151865\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.223417\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.182239\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0285926\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.691569\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0576549\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0679598\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000762207\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.691569\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00243726\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.214285\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.537981\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.555696\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00243906\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00989742\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0335524\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0247429\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0160267\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.143022\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.537981\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.555696\r\n",
      "time/data storing (s)                                    0.00585508\r\n",
      "time/evaluation sampling (s)                             0.925254\r\n",
      "time/exploration sampling (s)                            0.116871\r\n",
      "time/logging (s)                                         0.0199369\r\n",
      "time/saving (s)                                          0.0314321\r\n",
      "time/training (s)                                       47.7331\r\n",
      "time/epoch (s)                                          48.8325\r\n",
      "time/total (s)                                        2289.4\r\n",
      "Epoch                                                   46\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:27:37.216364 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 47 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000772312\r\n",
      "trainer/QF2 Loss                                         0.000824431\r\n",
      "trainer/Policy Loss                                      3.16122\r\n",
      "trainer/Q1 Predictions Mean                             -1.25007\r\n",
      "trainer/Q1 Predictions Std                               0.920104\r\n",
      "trainer/Q1 Predictions Max                               0.741039\r\n",
      "trainer/Q1 Predictions Min                              -3.90529\r\n",
      "trainer/Q2 Predictions Mean                             -1.2532\r\n",
      "trainer/Q2 Predictions Std                               0.91853\r\n",
      "trainer/Q2 Predictions Max                               0.721413\r\n",
      "trainer/Q2 Predictions Min                              -3.88922\r\n",
      "trainer/Q Targets Mean                                  -1.25432\r\n",
      "trainer/Q Targets Std                                    0.918556\r\n",
      "trainer/Q Targets Max                                    0.71873\r\n",
      "trainer/Q Targets Min                                   -3.91395\r\n",
      "trainer/Log Pis Mean                                     1.93045\r\n",
      "trainer/Log Pis Std                                      1.3722\r\n",
      "trainer/Log Pis Max                                      4.59248\r\n",
      "trainer/Log Pis Min                                     -3.22309\r\n",
      "trainer/Policy mu Mean                                   0.00498908\r\n",
      "trainer/Policy mu Std                                    0.394771\r\n",
      "trainer/Policy mu Max                                    2.22674\r\n",
      "trainer/Policy mu Min                                   -2.43052\r\n",
      "trainer/Policy log std Mean                             -2.28634\r\n",
      "trainer/Policy log std Std                               0.604496\r\n",
      "trainer/Policy log std Max                              -0.490974\r\n",
      "trainer/Policy log std Min                              -3.37191\r\n",
      "trainer/Alpha                                            0.0225553\r\n",
      "trainer/Alpha Loss                                      -0.263783\r\n",
      "exploration/num steps total                           5800\r\n",
      "exploration/num paths total                            290\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0842748\r\n",
      "exploration/Rewards Std                                  0.0720142\r\n",
      "exploration/Rewards Max                                  0.0878651\r\n",
      "exploration/Rewards Min                                 -0.261893\r\n",
      "exploration/Returns Mean                                -1.6855\r\n",
      "exploration/Returns Std                                  1.09259\r\n",
      "exploration/Returns Max                                  0.118656\r\n",
      "exploration/Returns Min                                 -3.01718\r\n",
      "exploration/Actions Mean                                -0.00881422\r\n",
      "exploration/Actions Std                                  0.0867115\r\n",
      "exploration/Actions Max                                  0.321821\r\n",
      "exploration/Actions Min                                 -0.23753\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.6855\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.129986\r\n",
      "exploration/env_infos/final/reward_dist Std              0.251092\r\n",
      "exploration/env_infos/final/reward_dist Max              0.632008\r\n",
      "exploration/env_infos/final/reward_dist Min              9.39601e-07\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00949262\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0105501\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0263037\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.03933e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0848214\r\n",
      "exploration/env_infos/reward_dist Std                    0.142633\r\n",
      "exploration/env_infos/reward_dist Max                    0.632008\r\n",
      "exploration/env_infos/reward_dist Min                    9.39601e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0858194\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0448204\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0197691\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.155297\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0990303\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0284387\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0636247\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.136522\r\n",
      "exploration/env_infos/reward_energy Mean                -0.103841\r\n",
      "exploration/env_infos/reward_energy Std                  0.0664099\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00906101\r\n",
      "exploration/env_infos/reward_energy Min                 -0.333277\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0395587\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.190541\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.26983\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.271059\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000330027\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00362778\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00662972\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00574206\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00747467\r\n",
      "exploration/env_infos/end_effector_loc Std               0.109412\r\n",
      "exploration/env_infos/end_effector_loc Max               0.26983\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.271059\r\n",
      "evaluation/num steps total                           48000\r\n",
      "evaluation/num paths total                            2400\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0536058\r\n",
      "evaluation/Rewards Std                                   0.0758541\r\n",
      "evaluation/Rewards Max                                   0.141393\r\n",
      "evaluation/Rewards Min                                  -0.29647\r\n",
      "evaluation/Returns Mean                                 -1.07212\r\n",
      "evaluation/Returns Std                                   1.16692\r\n",
      "evaluation/Returns Max                                   1.6342\r\n",
      "evaluation/Returns Min                                  -3.4031\r\n",
      "evaluation/Actions Mean                                 -0.00453724\r\n",
      "evaluation/Actions Std                                   0.0634694\r\n",
      "evaluation/Actions Max                                   0.578566\r\n",
      "evaluation/Actions Min                                  -0.782282\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.07212\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.240359\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.293567\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.911011\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.39676e-11\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00303703\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00524433\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0234547\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.56908e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.217998\r\n",
      "evaluation/env_infos/reward_dist Std                     0.282234\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998804\r\n",
      "evaluation/env_infos/reward_dist Min                     1.39676e-11\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0499546\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0663692\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00765487\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.496414\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.223019\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.181668\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0173532\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.791118\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0552238\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0710509\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00214488\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.791118\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0347935\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.22125\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.454531\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.478069\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000570594\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0101538\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0289283\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0391141\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00812148\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.142849\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.454531\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.478069\r\n",
      "time/data storing (s)                                    0.00623003\r\n",
      "time/evaluation sampling (s)                             1.0821\r\n",
      "time/exploration sampling (s)                            0.121712\r\n",
      "time/logging (s)                                         0.0194352\r\n",
      "time/saving (s)                                          0.029252\r\n",
      "time/training (s)                                       47.665\r\n",
      "time/epoch (s)                                          48.9237\r\n",
      "time/total (s)                                        2338.92\r\n",
      "Epoch                                                   47\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:28:27.445290 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 48 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00140605\r\n",
      "trainer/QF2 Loss                                         0.00189348\r\n",
      "trainer/Policy Loss                                      3.13918\r\n",
      "trainer/Q1 Predictions Mean                             -1.26029\r\n",
      "trainer/Q1 Predictions Std                               0.809146\r\n",
      "trainer/Q1 Predictions Max                               0.676398\r\n",
      "trainer/Q1 Predictions Min                              -3.18065\r\n",
      "trainer/Q2 Predictions Mean                             -1.2412\r\n",
      "trainer/Q2 Predictions Std                               0.818403\r\n",
      "trainer/Q2 Predictions Max                               0.69731\r\n",
      "trainer/Q2 Predictions Min                              -3.19027\r\n",
      "trainer/Q Targets Mean                                  -1.25467\r\n",
      "trainer/Q Targets Std                                    0.812906\r\n",
      "trainer/Q Targets Max                                    0.721645\r\n",
      "trainer/Q Targets Min                                   -3.20633\r\n",
      "trainer/Log Pis Mean                                     1.9091\r\n",
      "trainer/Log Pis Std                                      1.36949\r\n",
      "trainer/Log Pis Max                                      4.26524\r\n",
      "trainer/Log Pis Min                                     -2.93645\r\n",
      "trainer/Policy mu Mean                                   0.0089957\r\n",
      "trainer/Policy mu Std                                    0.38401\r\n",
      "trainer/Policy mu Max                                    2.11745\r\n",
      "trainer/Policy mu Min                                   -2.19406\r\n",
      "trainer/Policy log std Mean                             -2.25356\r\n",
      "trainer/Policy log std Std                               0.615106\r\n",
      "trainer/Policy log std Max                              -0.172213\r\n",
      "trainer/Policy log std Min                              -3.49457\r\n",
      "trainer/Alpha                                            0.0219018\r\n",
      "trainer/Alpha Loss                                      -0.347346\r\n",
      "exploration/num steps total                           5900\r\n",
      "exploration/num paths total                            295\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                 0.0142619\r\n",
      "exploration/Rewards Std                                  0.0700927\r\n",
      "exploration/Rewards Max                                  0.1446\r\n",
      "exploration/Rewards Min                                 -0.198239\r\n",
      "exploration/Returns Mean                                 0.285238\r\n",
      "exploration/Returns Std                                  0.495815\r\n",
      "exploration/Returns Max                                  0.9695\r\n",
      "exploration/Returns Min                                 -0.446173\r\n",
      "exploration/Actions Mean                                -0.00369404\r\n",
      "exploration/Actions Std                                  0.103531\r\n",
      "exploration/Actions Max                                  0.354684\r\n",
      "exploration/Actions Min                                 -0.447581\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                              0.285238\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.212626\r\n",
      "exploration/env_infos/final/reward_dist Std              0.148882\r\n",
      "exploration/env_infos/final/reward_dist Max              0.435638\r\n",
      "exploration/env_infos/final/reward_dist Min              0.031906\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00222309\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00357354\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00930372\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.11583e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.220871\r\n",
      "exploration/env_infos/reward_dist Std                    0.232415\r\n",
      "exploration/env_infos/reward_dist Max                    0.966461\r\n",
      "exploration/env_infos/reward_dist Min                    5.11583e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0884368\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0321576\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0382499\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.130005\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.326467\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0839846\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.238551\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.485947\r\n",
      "exploration/env_infos/reward_energy Mean                -0.118771\r\n",
      "exploration/env_infos/reward_energy Std                  0.0857804\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0118613\r\n",
      "exploration/env_infos/reward_energy Min                 -0.485947\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.063301\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.180458\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.234343\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.329599\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00407139\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0112012\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0116175\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0223791\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0464229\r\n",
      "exploration/env_infos/end_effector_loc Std               0.136903\r\n",
      "exploration/env_infos/end_effector_loc Max               0.234343\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.329599\r\n",
      "evaluation/num steps total                           49000\r\n",
      "evaluation/num paths total                            2450\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0217769\r\n",
      "evaluation/Rewards Std                                   0.0740153\r\n",
      "evaluation/Rewards Max                                   0.163972\r\n",
      "evaluation/Rewards Min                                  -0.31599\r\n",
      "evaluation/Returns Mean                                 -0.435538\r\n",
      "evaluation/Returns Std                                   1.1532\r\n",
      "evaluation/Returns Max                                   1.64226\r\n",
      "evaluation/Returns Min                                  -2.48523\r\n",
      "evaluation/Actions Mean                                 -0.00499238\r\n",
      "evaluation/Actions Std                                   0.0590125\r\n",
      "evaluation/Actions Max                                   0.72745\r\n",
      "evaluation/Actions Min                                  -0.544509\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.435538\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.301201\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.294248\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.98839\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.18478e-18\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00639194\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0129643\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0749154\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.56392e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.273682\r\n",
      "evaluation/env_infos/reward_dist Std                     0.313692\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999557\r\n",
      "evaluation/env_infos/reward_dist Min                     1.18478e-18\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0369287\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0207534\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00740611\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.137138\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.210588\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.17006\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0237958\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.822303\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0544486\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0636407\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00188456\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.822303\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0381344\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.20867\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.42014\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.493696\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000428435\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00956041\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0363725\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0272255\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0134033\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.136622\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.42014\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.493696\r\n",
      "time/data storing (s)                                    0.00637889\r\n",
      "time/evaluation sampling (s)                             0.967533\r\n",
      "time/exploration sampling (s)                            0.124843\r\n",
      "time/logging (s)                                         0.0197451\r\n",
      "time/saving (s)                                          0.029266\r\n",
      "time/training (s)                                       48.48\r\n",
      "time/epoch (s)                                          49.6278\r\n",
      "time/total (s)                                        2389.15\r\n",
      "Epoch                                                   48\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:29:16.519837 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 49 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00148556\n",
      "trainer/QF2 Loss                                         0.00115168\n",
      "trainer/Policy Loss                                      3.09452\n",
      "trainer/Q1 Predictions Mean                             -1.17653\n",
      "trainer/Q1 Predictions Std                               0.813911\n",
      "trainer/Q1 Predictions Max                               0.768927\n",
      "trainer/Q1 Predictions Min                              -2.98511\n",
      "trainer/Q2 Predictions Mean                             -1.17763\n",
      "trainer/Q2 Predictions Std                               0.81243\n",
      "trainer/Q2 Predictions Max                               0.772343\n",
      "trainer/Q2 Predictions Min                              -2.97027\n",
      "trainer/Q Targets Mean                                  -1.18483\n",
      "trainer/Q Targets Std                                    0.810869\n",
      "trainer/Q Targets Max                                    0.767076\n",
      "trainer/Q Targets Min                                   -3.02307\n",
      "trainer/Log Pis Mean                                     1.94984\n",
      "trainer/Log Pis Std                                      1.35955\n",
      "trainer/Log Pis Max                                      4.6352\n",
      "trainer/Log Pis Min                                     -3.95424\n",
      "trainer/Policy mu Mean                                  -0.0339717\n",
      "trainer/Policy mu Std                                    0.434981\n",
      "trainer/Policy mu Max                                    1.96066\n",
      "trainer/Policy mu Min                                   -2.61393\n",
      "trainer/Policy log std Mean                             -2.26316\n",
      "trainer/Policy log std Std                               0.605205\n",
      "trainer/Policy log std Max                              -0.261482\n",
      "trainer/Policy log std Min                              -3.31569\n",
      "trainer/Alpha                                            0.0234111\n",
      "trainer/Alpha Loss                                      -0.188343\n",
      "exploration/num steps total                           6000\n",
      "exploration/num paths total                            300\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0944623\n",
      "exploration/Rewards Std                                  0.0633662\n",
      "exploration/Rewards Max                                  0.0541827\n",
      "exploration/Rewards Min                                 -0.252207\n",
      "exploration/Returns Mean                                -1.88925\n",
      "exploration/Returns Std                                  0.861632\n",
      "exploration/Returns Max                                 -0.342163\n",
      "exploration/Returns Min                                 -3.00147\n",
      "exploration/Actions Mean                                -6.54088e-05\n",
      "exploration/Actions Std                                  0.101417\n",
      "exploration/Actions Max                                  0.502421\n",
      "exploration/Actions Min                                 -0.34216\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.88925\n",
      "exploration/env_infos/final/reward_dist Mean             0.175851\n",
      "exploration/env_infos/final/reward_dist Std              0.341848\n",
      "exploration/env_infos/final/reward_dist Max              0.859398\n",
      "exploration/env_infos/final/reward_dist Min              2.62442e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000422854\n",
      "exploration/env_infos/initial/reward_dist Std            0.000689362\n",
      "exploration/env_infos/initial/reward_dist Max            0.00178991\n",
      "exploration/env_infos/initial/reward_dist Min            6.57361e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.152567\n",
      "exploration/env_infos/reward_dist Std                    0.268384\n",
      "exploration/env_infos/reward_dist Max                    0.985489\n",
      "exploration/env_infos/reward_dist Min                    2.62442e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0930777\n",
      "exploration/env_infos/final/reward_energy Std            0.027806\n",
      "exploration/env_infos/final/reward_energy Max           -0.0555416\n",
      "exploration/env_infos/final/reward_energy Min           -0.133899\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.217908\n",
      "exploration/env_infos/initial/reward_energy Std          0.124444\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0673704\n",
      "exploration/env_infos/initial/reward_energy Min         -0.438353\n",
      "exploration/env_infos/reward_energy Mean                -0.11635\n",
      "exploration/env_infos/reward_energy Std                  0.0838667\n",
      "exploration/env_infos/reward_energy Max                 -0.00797141\n",
      "exploration/env_infos/reward_energy Min                 -0.502748\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00291215\n",
      "exploration/env_infos/final/end_effector_loc Std         0.261752\n",
      "exploration/env_infos/final/end_effector_loc Max         0.50921\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.410664\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000503452\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00885773\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0210718\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0106323\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00103228\n",
      "exploration/env_infos/end_effector_loc Std               0.15476\n",
      "exploration/env_infos/end_effector_loc Max               0.50921\n",
      "exploration/env_infos/end_effector_loc Min              -0.410664\n",
      "evaluation/num steps total                           50000\n",
      "evaluation/num paths total                            2500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0292299\n",
      "evaluation/Rewards Std                                   0.0746181\n",
      "evaluation/Rewards Max                                   0.17562\n",
      "evaluation/Rewards Min                                  -0.389378\n",
      "evaluation/Returns Mean                                 -0.584598\n",
      "evaluation/Returns Std                                   1.15166\n",
      "evaluation/Returns Max                                   2.36187\n",
      "evaluation/Returns Min                                  -3.02301\n",
      "evaluation/Actions Mean                                 -0.00029266\n",
      "evaluation/Actions Std                                   0.0668412\n",
      "evaluation/Actions Max                                   0.72327\n",
      "evaluation/Actions Min                                  -0.639678\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.584598\n",
      "evaluation/env_infos/final/reward_dist Mean              0.254046\n",
      "evaluation/env_infos/final/reward_dist Std               0.285796\n",
      "evaluation/env_infos/final/reward_dist Max               0.962007\n",
      "evaluation/env_infos/final/reward_dist Min               1.55334e-16\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00495188\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0103338\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0628671\n",
      "evaluation/env_infos/initial/reward_dist Min             2.72122e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.2712\n",
      "evaluation/env_infos/reward_dist Std                     0.30397\n",
      "evaluation/env_infos/reward_dist Max                     0.99649\n",
      "evaluation/env_infos/reward_dist Min                     1.55334e-16\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0467803\n",
      "evaluation/env_infos/final/reward_energy Std             0.0483226\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00205352\n",
      "evaluation/env_infos/final/reward_energy Min            -0.21577\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.229702\n",
      "evaluation/env_infos/initial/reward_energy Std           0.199972\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0180527\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.777579\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0598246\n",
      "evaluation/env_infos/reward_energy Std                   0.0731894\n",
      "evaluation/env_infos/reward_energy Max                  -0.00142167\n",
      "evaluation/env_infos/reward_energy Min                  -0.777579\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0054863\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.229811\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.478404\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.526183\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00073003\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107428\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0361635\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0319839\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00597361\n",
      "evaluation/env_infos/end_effector_loc Std                0.150332\n",
      "evaluation/env_infos/end_effector_loc Max                0.478404\n",
      "evaluation/env_infos/end_effector_loc Min               -0.526183\n",
      "time/data storing (s)                                    0.00614742\n",
      "time/evaluation sampling (s)                             0.929181\n",
      "time/exploration sampling (s)                            0.121381\n",
      "time/logging (s)                                         0.0205667\n",
      "time/saving (s)                                          0.0326913\n",
      "time/training (s)                                       47.361\n",
      "time/epoch (s)                                          48.471\n",
      "time/total (s)                                        2438.22\n",
      "Epoch                                                   49\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:30:07.669214 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 50 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000930431\n",
      "trainer/QF2 Loss                                         0.00196105\n",
      "trainer/Policy Loss                                      3.11011\n",
      "trainer/Q1 Predictions Mean                             -1.16912\n",
      "trainer/Q1 Predictions Std                               0.826708\n",
      "trainer/Q1 Predictions Max                               0.738173\n",
      "trainer/Q1 Predictions Min                              -3.66051\n",
      "trainer/Q2 Predictions Mean                             -1.16703\n",
      "trainer/Q2 Predictions Std                               0.82754\n",
      "trainer/Q2 Predictions Max                               0.741095\n",
      "trainer/Q2 Predictions Min                              -3.6351\n",
      "trainer/Q Targets Mean                                  -1.16978\n",
      "trainer/Q Targets Std                                    0.831268\n",
      "trainer/Q Targets Max                                    0.769946\n",
      "trainer/Q Targets Min                                   -3.60348\n",
      "trainer/Log Pis Mean                                     1.94895\n",
      "trainer/Log Pis Std                                      1.35798\n",
      "trainer/Log Pis Max                                      4.52307\n",
      "trainer/Log Pis Min                                     -3.07042\n",
      "trainer/Policy mu Mean                                  -0.00712857\n",
      "trainer/Policy mu Std                                    0.311059\n",
      "trainer/Policy mu Max                                    1.8665\n",
      "trainer/Policy mu Min                                   -2.59518\n",
      "trainer/Policy log std Mean                             -2.3282\n",
      "trainer/Policy log std Std                               0.561481\n",
      "trainer/Policy log std Max                              -0.479059\n",
      "trainer/Policy log std Min                              -3.41356\n",
      "trainer/Alpha                                            0.0232164\n",
      "trainer/Alpha Loss                                      -0.192145\n",
      "exploration/num steps total                           6100\n",
      "exploration/num paths total                            305\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0810045\n",
      "exploration/Rewards Std                                  0.0928655\n",
      "exploration/Rewards Max                                  0.0734728\n",
      "exploration/Rewards Min                                 -0.345576\n",
      "exploration/Returns Mean                                -1.62009\n",
      "exploration/Returns Std                                  1.56618\n",
      "exploration/Returns Max                                 -0.104671\n",
      "exploration/Returns Min                                 -4.5468\n",
      "exploration/Actions Mean                                 0.00283664\n",
      "exploration/Actions Std                                  0.159671\n",
      "exploration/Actions Max                                  0.600139\n",
      "exploration/Actions Min                                 -0.502598\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.62009\n",
      "exploration/env_infos/final/reward_dist Mean             0.19838\n",
      "exploration/env_infos/final/reward_dist Std              0.28641\n",
      "exploration/env_infos/final/reward_dist Max              0.749568\n",
      "exploration/env_infos/final/reward_dist Min              0.0029007\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0144001\n",
      "exploration/env_infos/initial/reward_dist Std            0.018645\n",
      "exploration/env_infos/initial/reward_dist Max            0.0473659\n",
      "exploration/env_infos/initial/reward_dist Min            1.67525e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.211619\n",
      "exploration/env_infos/reward_dist Std                    0.256774\n",
      "exploration/env_infos/reward_dist Max                    0.860062\n",
      "exploration/env_infos/reward_dist Min                    1.67525e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.183333\n",
      "exploration/env_infos/final/reward_energy Std            0.138159\n",
      "exploration/env_infos/final/reward_energy Max           -0.0855159\n",
      "exploration/env_infos/final/reward_energy Min           -0.454748\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.416215\n",
      "exploration/env_infos/initial/reward_energy Std          0.261392\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0135358\n",
      "exploration/env_infos/initial/reward_energy Min         -0.750177\n",
      "exploration/env_infos/reward_energy Mean                -0.178796\n",
      "exploration/env_infos/reward_energy Std                  0.137979\n",
      "exploration/env_infos/reward_energy Max                 -0.0135358\n",
      "exploration/env_infos/reward_energy Min                 -0.750177\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0284484\n",
      "exploration/env_infos/final/end_effector_loc Std         0.186544\n",
      "exploration/env_infos/final/end_effector_loc Max         0.278515\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.272251\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0089079\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0149198\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0300069\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0098107\n",
      "exploration/env_infos/end_effector_loc Mean              0.0221502\n",
      "exploration/env_infos/end_effector_loc Std               0.140647\n",
      "exploration/env_infos/end_effector_loc Max               0.304501\n",
      "exploration/env_infos/end_effector_loc Min              -0.272251\n",
      "evaluation/num steps total                           51000\n",
      "evaluation/num paths total                            2550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0452181\n",
      "evaluation/Rewards Std                                   0.0748472\n",
      "evaluation/Rewards Max                                   0.150032\n",
      "evaluation/Rewards Min                                  -0.358304\n",
      "evaluation/Returns Mean                                 -0.904362\n",
      "evaluation/Returns Std                                   1.11873\n",
      "evaluation/Returns Max                                   2.00225\n",
      "evaluation/Returns Min                                  -4.63484\n",
      "evaluation/Actions Mean                                 -0.000862576\n",
      "evaluation/Actions Std                                   0.0642702\n",
      "evaluation/Actions Max                                   0.848763\n",
      "evaluation/Actions Min                                  -0.481476\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.904362\n",
      "evaluation/env_infos/final/reward_dist Mean              0.24121\n",
      "evaluation/env_infos/final/reward_dist Std               0.300392\n",
      "evaluation/env_infos/final/reward_dist Max               0.871451\n",
      "evaluation/env_infos/final/reward_dist Min               1.10995e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0047505\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0122326\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0677827\n",
      "evaluation/env_infos/initial/reward_dist Min             2.38074e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.224137\n",
      "evaluation/env_infos/reward_dist Std                     0.276229\n",
      "evaluation/env_infos/reward_dist Max                     0.99087\n",
      "evaluation/env_infos/reward_dist Min                     1.10995e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0289703\n",
      "evaluation/env_infos/final/reward_energy Std             0.027025\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00111332\n",
      "evaluation/env_infos/final/reward_energy Min            -0.119721\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.227928\n",
      "evaluation/env_infos/initial/reward_energy Std           0.189499\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0200583\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.918324\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0546202\n",
      "evaluation/env_infos/reward_energy Std                   0.0726597\n",
      "evaluation/env_infos/reward_energy Max                  -0.000357277\n",
      "evaluation/env_infos/reward_energy Min                  -0.918324\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.003238\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.228114\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.499168\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.462797\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00178451\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0103267\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0424382\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0240738\n",
      "evaluation/env_infos/end_effector_loc Mean               0.000761219\n",
      "evaluation/env_infos/end_effector_loc Std                0.146819\n",
      "evaluation/env_infos/end_effector_loc Max                0.499168\n",
      "evaluation/env_infos/end_effector_loc Min               -0.462797\n",
      "time/data storing (s)                                    0.0059458\n",
      "time/evaluation sampling (s)                             1.06906\n",
      "time/exploration sampling (s)                            0.129234\n",
      "time/logging (s)                                         0.020697\n",
      "time/saving (s)                                          0.0507823\n",
      "time/training (s)                                       49.1764\n",
      "time/epoch (s)                                          50.4521\n",
      "time/total (s)                                        2489.37\n",
      "Epoch                                                   50\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:30:58.337709 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 51 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000904333\n",
      "trainer/QF2 Loss                                         0.00117275\n",
      "trainer/Policy Loss                                      3.26863\n",
      "trainer/Q1 Predictions Mean                             -1.21251\n",
      "trainer/Q1 Predictions Std                               0.811331\n",
      "trainer/Q1 Predictions Max                               0.777998\n",
      "trainer/Q1 Predictions Min                              -3.21057\n",
      "trainer/Q2 Predictions Mean                             -1.21649\n",
      "trainer/Q2 Predictions Std                               0.814107\n",
      "trainer/Q2 Predictions Max                               0.821427\n",
      "trainer/Q2 Predictions Min                              -3.20546\n",
      "trainer/Q Targets Mean                                  -1.21778\n",
      "trainer/Q Targets Std                                    0.813465\n",
      "trainer/Q Targets Max                                    0.816371\n",
      "trainer/Q Targets Min                                   -3.22547\n",
      "trainer/Log Pis Mean                                     2.08189\n",
      "trainer/Log Pis Std                                      1.38253\n",
      "trainer/Log Pis Max                                      4.7481\n",
      "trainer/Log Pis Min                                     -4.53578\n",
      "trainer/Policy mu Mean                                  -0.00492311\n",
      "trainer/Policy mu Std                                    0.408224\n",
      "trainer/Policy mu Max                                    2.32458\n",
      "trainer/Policy mu Min                                   -2.40185\n",
      "trainer/Policy log std Mean                             -2.28238\n",
      "trainer/Policy log std Std                               0.644627\n",
      "trainer/Policy log std Max                              -0.33844\n",
      "trainer/Policy log std Min                              -3.33908\n",
      "trainer/Alpha                                            0.0238049\n",
      "trainer/Alpha Loss                                       0.306112\n",
      "exploration/num steps total                           6200\n",
      "exploration/num paths total                            310\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.066813\n",
      "exploration/Rewards Std                                  0.118304\n",
      "exploration/Rewards Max                                  0.108285\n",
      "exploration/Rewards Min                                 -0.316962\n",
      "exploration/Returns Mean                                -1.33626\n",
      "exploration/Returns Std                                  2.14377\n",
      "exploration/Returns Max                                  0.885778\n",
      "exploration/Returns Min                                 -3.94486\n",
      "exploration/Actions Mean                                 0.00984984\n",
      "exploration/Actions Std                                  0.144137\n",
      "exploration/Actions Max                                  0.540499\n",
      "exploration/Actions Min                                 -0.403546\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.33626\n",
      "exploration/env_infos/final/reward_dist Mean             0.210106\n",
      "exploration/env_infos/final/reward_dist Std              0.390984\n",
      "exploration/env_infos/final/reward_dist Max              0.991077\n",
      "exploration/env_infos/final/reward_dist Min              4.75285e-37\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00339216\n",
      "exploration/env_infos/initial/reward_dist Std            0.00326443\n",
      "exploration/env_infos/initial/reward_dist Max            0.00860955\n",
      "exploration/env_infos/initial/reward_dist Min            0.000326628\n",
      "exploration/env_infos/reward_dist Mean                   0.21581\n",
      "exploration/env_infos/reward_dist Std                    0.300116\n",
      "exploration/env_infos/reward_dist Max                    0.991077\n",
      "exploration/env_infos/reward_dist Min                    4.75285e-37\n",
      "exploration/env_infos/final/reward_energy Mean          -0.167861\n",
      "exploration/env_infos/final/reward_energy Std            0.0788281\n",
      "exploration/env_infos/final/reward_energy Max           -0.0787822\n",
      "exploration/env_infos/final/reward_energy Min           -0.301254\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.165379\n",
      "exploration/env_infos/initial/reward_energy Std          0.0586774\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0872216\n",
      "exploration/env_infos/initial/reward_energy Min         -0.224193\n",
      "exploration/env_infos/reward_energy Mean                -0.176492\n",
      "exploration/env_infos/reward_energy Std                  0.102935\n",
      "exploration/env_infos/reward_energy Max                 -0.0223274\n",
      "exploration/env_infos/reward_energy Min                 -0.567271\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0600454\n",
      "exploration/env_infos/final/end_effector_loc Std         0.338078\n",
      "exploration/env_infos/final/end_effector_loc Max         0.934076\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.365077\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0024262\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00571009\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0100606\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0111973\n",
      "exploration/env_infos/end_effector_loc Mean              0.00624268\n",
      "exploration/env_infos/end_effector_loc Std               0.200985\n",
      "exploration/env_infos/end_effector_loc Max               0.934076\n",
      "exploration/env_infos/end_effector_loc Min              -0.436115\n",
      "evaluation/num steps total                           52000\n",
      "evaluation/num paths total                            2600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0407576\n",
      "evaluation/Rewards Std                                   0.0747096\n",
      "evaluation/Rewards Max                                   0.142922\n",
      "evaluation/Rewards Min                                  -0.334146\n",
      "evaluation/Returns Mean                                 -0.815151\n",
      "evaluation/Returns Std                                   1.19759\n",
      "evaluation/Returns Max                                   1.5207\n",
      "evaluation/Returns Min                                  -3.35683\n",
      "evaluation/Actions Mean                                 -0.00261061\n",
      "evaluation/Actions Std                                   0.0629468\n",
      "evaluation/Actions Max                                   0.610671\n",
      "evaluation/Actions Min                                  -0.522691\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.815151\n",
      "evaluation/env_infos/final/reward_dist Mean              0.184067\n",
      "evaluation/env_infos/final/reward_dist Std               0.272561\n",
      "evaluation/env_infos/final/reward_dist Max               0.967197\n",
      "evaluation/env_infos/final/reward_dist Min               2.6597e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00901637\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165273\n",
      "evaluation/env_infos/initial/reward_dist Max             0.099355\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0144e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.202921\n",
      "evaluation/env_infos/reward_dist Std                     0.276672\n",
      "evaluation/env_infos/reward_dist Max                     0.994601\n",
      "evaluation/env_infos/reward_dist Min                     2.6597e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0351837\n",
      "evaluation/env_infos/final/reward_energy Std             0.0218173\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00195016\n",
      "evaluation/env_infos/final/reward_energy Min            -0.114533\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.216192\n",
      "evaluation/env_infos/initial/reward_energy Std           0.170347\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00286727\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.67016\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0577525\n",
      "evaluation/env_infos/reward_energy Std                   0.0678445\n",
      "evaluation/env_infos/reward_energy Max                  -0.000369813\n",
      "evaluation/env_infos/reward_energy Min                  -0.67016\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0293417\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.228897\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.451517\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.710578\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000109087\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0097306\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0305336\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0261346\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0151807\n",
      "evaluation/env_infos/end_effector_loc Std                0.152127\n",
      "evaluation/env_infos/end_effector_loc Max                0.451517\n",
      "evaluation/env_infos/end_effector_loc Min               -0.710578\n",
      "time/data storing (s)                                    0.00593039\n",
      "time/evaluation sampling (s)                             0.944921\n",
      "time/exploration sampling (s)                            0.131909\n",
      "time/logging (s)                                         0.0202564\n",
      "time/saving (s)                                          0.0267373\n",
      "time/training (s)                                       48.9157\n",
      "time/epoch (s)                                          50.0454\n",
      "time/total (s)                                        2540.04\n",
      "Epoch                                                   51\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:31:48.945332 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 52 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000797178\n",
      "trainer/QF2 Loss                                         0.000802408\n",
      "trainer/Policy Loss                                      3.32765\n",
      "trainer/Q1 Predictions Mean                             -1.20978\n",
      "trainer/Q1 Predictions Std                               0.850654\n",
      "trainer/Q1 Predictions Max                               0.615514\n",
      "trainer/Q1 Predictions Min                              -3.37473\n",
      "trainer/Q2 Predictions Mean                             -1.20404\n",
      "trainer/Q2 Predictions Std                               0.849184\n",
      "trainer/Q2 Predictions Max                               0.5924\n",
      "trainer/Q2 Predictions Min                              -3.39909\n",
      "trainer/Q Targets Mean                                  -1.20439\n",
      "trainer/Q Targets Std                                    0.852216\n",
      "trainer/Q Targets Max                                    0.615307\n",
      "trainer/Q Targets Min                                   -3.40374\n",
      "trainer/Log Pis Mean                                     2.13836\n",
      "trainer/Log Pis Std                                      1.40004\n",
      "trainer/Log Pis Max                                      4.878\n",
      "trainer/Log Pis Min                                     -6.29953\n",
      "trainer/Policy mu Mean                                  -0.0219908\n",
      "trainer/Policy mu Std                                    0.289494\n",
      "trainer/Policy mu Max                                    1.76574\n",
      "trainer/Policy mu Min                                   -1.50208\n",
      "trainer/Policy log std Mean                             -2.36337\n",
      "trainer/Policy log std Std                               0.607575\n",
      "trainer/Policy log std Max                              -0.41797\n",
      "trainer/Policy log std Min                              -3.40519\n",
      "trainer/Alpha                                            0.0244192\n",
      "trainer/Alpha Loss                                       0.513728\n",
      "exploration/num steps total                           6300\n",
      "exploration/num paths total                            315\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.11442\n",
      "exploration/Rewards Std                                  0.0682617\n",
      "exploration/Rewards Max                                  0.0495646\n",
      "exploration/Rewards Min                                 -0.24037\n",
      "exploration/Returns Mean                                -2.28839\n",
      "exploration/Returns Std                                  1.03234\n",
      "exploration/Returns Max                                 -0.564303\n",
      "exploration/Returns Min                                 -3.36014\n",
      "exploration/Actions Mean                                -0.00685698\n",
      "exploration/Actions Std                                  0.140205\n",
      "exploration/Actions Max                                  0.475446\n",
      "exploration/Actions Min                                 -0.472318\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.28839\n",
      "exploration/env_infos/final/reward_dist Mean             0.234782\n",
      "exploration/env_infos/final/reward_dist Std              0.265322\n",
      "exploration/env_infos/final/reward_dist Max              0.698243\n",
      "exploration/env_infos/final/reward_dist Min              3.96283e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000980329\n",
      "exploration/env_infos/initial/reward_dist Std            0.00150559\n",
      "exploration/env_infos/initial/reward_dist Max            0.00391369\n",
      "exploration/env_infos/initial/reward_dist Min            1.99762e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.131071\n",
      "exploration/env_infos/reward_dist Std                    0.177697\n",
      "exploration/env_infos/reward_dist Max                    0.698243\n",
      "exploration/env_infos/reward_dist Min                    3.96283e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.204451\n",
      "exploration/env_infos/final/reward_energy Std            0.0928626\n",
      "exploration/env_infos/final/reward_energy Max           -0.0939382\n",
      "exploration/env_infos/final/reward_energy Min           -0.364439\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.188972\n",
      "exploration/env_infos/initial/reward_energy Std          0.111346\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0223325\n",
      "exploration/env_infos/initial/reward_energy Min         -0.296804\n",
      "exploration/env_infos/reward_energy Mean                -0.161384\n",
      "exploration/env_infos/reward_energy Std                  0.115604\n",
      "exploration/env_infos/reward_energy Max                 -0.0223325\n",
      "exploration/env_infos/reward_energy Min                 -0.545517\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0819955\n",
      "exploration/env_infos/final/end_effector_loc Std         0.171436\n",
      "exploration/env_infos/final/end_effector_loc Max         0.158822\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.399789\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00295474\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00716972\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00836537\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0145183\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0430389\n",
      "exploration/env_infos/end_effector_loc Std               0.110306\n",
      "exploration/env_infos/end_effector_loc Max               0.158822\n",
      "exploration/env_infos/end_effector_loc Min              -0.399789\n",
      "evaluation/num steps total                           53000\n",
      "evaluation/num paths total                            2650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0345209\n",
      "evaluation/Rewards Std                                   0.0719625\n",
      "evaluation/Rewards Max                                   0.167016\n",
      "evaluation/Rewards Min                                  -0.262125\n",
      "evaluation/Returns Mean                                 -0.690417\n",
      "evaluation/Returns Std                                   1.07925\n",
      "evaluation/Returns Max                                   1.4289\n",
      "evaluation/Returns Min                                  -2.81398\n",
      "evaluation/Actions Mean                                 -0.00689652\n",
      "evaluation/Actions Std                                   0.0613484\n",
      "evaluation/Actions Max                                   0.713829\n",
      "evaluation/Actions Min                                  -0.407931\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.690417\n",
      "evaluation/env_infos/final/reward_dist Mean              0.22301\n",
      "evaluation/env_infos/final/reward_dist Std               0.316288\n",
      "evaluation/env_infos/final/reward_dist Max               0.987301\n",
      "evaluation/env_infos/final/reward_dist Min               3.21611e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00749662\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126822\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0541257\n",
      "evaluation/env_infos/initial/reward_dist Min             3.63815e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.227714\n",
      "evaluation/env_infos/reward_dist Std                     0.295191\n",
      "evaluation/env_infos/reward_dist Max                     0.99994\n",
      "evaluation/env_infos/reward_dist Min                     3.21611e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0495944\n",
      "evaluation/env_infos/final/reward_energy Std             0.0490761\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00262902\n",
      "evaluation/env_infos/final/reward_energy Min            -0.303293\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.206776\n",
      "evaluation/env_infos/initial/reward_energy Std           0.146424\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0409543\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.77237\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0595007\n",
      "evaluation/env_infos/reward_energy Std                   0.0638909\n",
      "evaluation/env_infos/reward_energy Max                  -0.000880742\n",
      "evaluation/env_infos/reward_energy Min                  -0.77237\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0996025\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.211295\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.384584\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.720414\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00135553\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00885483\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0356915\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0203965\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0485129\n",
      "evaluation/env_infos/end_effector_loc Std                0.143297\n",
      "evaluation/env_infos/end_effector_loc Max                0.384584\n",
      "evaluation/env_infos/end_effector_loc Min               -0.720414\n",
      "time/data storing (s)                                    0.00592283\n",
      "time/evaluation sampling (s)                             0.973389\n",
      "time/exploration sampling (s)                            0.122246\n",
      "time/logging (s)                                         0.0209363\n",
      "time/saving (s)                                          0.0315648\n",
      "time/training (s)                                       48.7493\n",
      "time/epoch (s)                                          49.9033\n",
      "time/total (s)                                        2590.64\n",
      "Epoch                                                   52\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:32:38.947001 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 53 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000889647\n",
      "trainer/QF2 Loss                                         0.000959159\n",
      "trainer/Policy Loss                                      3.11983\n",
      "trainer/Q1 Predictions Mean                             -1.14934\n",
      "trainer/Q1 Predictions Std                               0.899579\n",
      "trainer/Q1 Predictions Max                               0.674168\n",
      "trainer/Q1 Predictions Min                              -3.23604\n",
      "trainer/Q2 Predictions Mean                             -1.15062\n",
      "trainer/Q2 Predictions Std                               0.898039\n",
      "trainer/Q2 Predictions Max                               0.643436\n",
      "trainer/Q2 Predictions Min                              -3.25527\n",
      "trainer/Q Targets Mean                                  -1.1557\n",
      "trainer/Q Targets Std                                    0.901569\n",
      "trainer/Q Targets Max                                    0.674461\n",
      "trainer/Q Targets Min                                   -3.23407\n",
      "trainer/Log Pis Mean                                     1.99344\n",
      "trainer/Log Pis Std                                      1.41008\n",
      "trainer/Log Pis Max                                      4.39949\n",
      "trainer/Log Pis Min                                     -3.50106\n",
      "trainer/Policy mu Mean                                  -0.0350545\n",
      "trainer/Policy mu Std                                    0.38448\n",
      "trainer/Policy mu Max                                    1.96753\n",
      "trainer/Policy mu Min                                   -2.09144\n",
      "trainer/Policy log std Mean                             -2.32457\n",
      "trainer/Policy log std Std                               0.655847\n",
      "trainer/Policy log std Max                              -0.4055\n",
      "trainer/Policy log std Min                              -3.27881\n",
      "trainer/Alpha                                            0.0236831\n",
      "trainer/Alpha Loss                                      -0.0245439\n",
      "exploration/num steps total                           6400\n",
      "exploration/num paths total                            320\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0856305\n",
      "exploration/Rewards Std                                  0.0612731\n",
      "exploration/Rewards Max                                  0.0317994\n",
      "exploration/Rewards Min                                 -0.307927\n",
      "exploration/Returns Mean                                -1.71261\n",
      "exploration/Returns Std                                  0.815306\n",
      "exploration/Returns Max                                 -1.24042\n",
      "exploration/Returns Min                                 -3.33797\n",
      "exploration/Actions Mean                                 0.0057644\n",
      "exploration/Actions Std                                  0.105957\n",
      "exploration/Actions Max                                  0.354285\n",
      "exploration/Actions Min                                 -0.328516\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.71261\n",
      "exploration/env_infos/final/reward_dist Mean             0.0274294\n",
      "exploration/env_infos/final/reward_dist Std              0.0232549\n",
      "exploration/env_infos/final/reward_dist Max              0.0559839\n",
      "exploration/env_infos/final/reward_dist Min              1.73622e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00166328\n",
      "exploration/env_infos/initial/reward_dist Std            0.00305434\n",
      "exploration/env_infos/initial/reward_dist Max            0.00776227\n",
      "exploration/env_infos/initial/reward_dist Min            2.59405e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.101086\n",
      "exploration/env_infos/reward_dist Std                    0.188662\n",
      "exploration/env_infos/reward_dist Max                    0.957581\n",
      "exploration/env_infos/reward_dist Min                    1.73622e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.145106\n",
      "exploration/env_infos/final/reward_energy Std            0.103285\n",
      "exploration/env_infos/final/reward_energy Max           -0.0575602\n",
      "exploration/env_infos/final/reward_energy Min           -0.336434\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.159824\n",
      "exploration/env_infos/initial/reward_energy Std          0.120997\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0432144\n",
      "exploration/env_infos/initial/reward_energy Min         -0.390392\n",
      "exploration/env_infos/reward_energy Mean                -0.123014\n",
      "exploration/env_infos/reward_energy Std                  0.0859521\n",
      "exploration/env_infos/reward_energy Max                 -0.0071946\n",
      "exploration/env_infos/reward_energy Min                 -0.390392\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0877842\n",
      "exploration/env_infos/final/end_effector_loc Std         0.15893\n",
      "exploration/env_infos/final/end_effector_loc Max         0.344685\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.113209\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00336452\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0062378\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0177143\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00513565\n",
      "exploration/env_infos/end_effector_loc Mean              0.0502868\n",
      "exploration/env_infos/end_effector_loc Std               0.0989823\n",
      "exploration/env_infos/end_effector_loc Max               0.344685\n",
      "exploration/env_infos/end_effector_loc Min              -0.117175\n",
      "evaluation/num steps total                           54000\n",
      "evaluation/num paths total                            2700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0352698\n",
      "evaluation/Rewards Std                                   0.0754376\n",
      "evaluation/Rewards Max                                   0.160137\n",
      "evaluation/Rewards Min                                  -0.299178\n",
      "evaluation/Returns Mean                                 -0.705396\n",
      "evaluation/Returns Std                                   1.14416\n",
      "evaluation/Returns Max                                   2.12282\n",
      "evaluation/Returns Min                                  -3.20598\n",
      "evaluation/Actions Mean                                 -0.00130894\n",
      "evaluation/Actions Std                                   0.0631493\n",
      "evaluation/Actions Max                                   0.547755\n",
      "evaluation/Actions Min                                  -0.500634\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.705396\n",
      "evaluation/env_infos/final/reward_dist Mean              0.243749\n",
      "evaluation/env_infos/final/reward_dist Std               0.297426\n",
      "evaluation/env_infos/final/reward_dist Max               0.989333\n",
      "evaluation/env_infos/final/reward_dist Min               5.47785e-08\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00405859\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00961935\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0466112\n",
      "evaluation/env_infos/initial/reward_dist Min             3.09716e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.211643\n",
      "evaluation/env_infos/reward_dist Std                     0.282828\n",
      "evaluation/env_infos/reward_dist Max                     0.989333\n",
      "evaluation/env_infos/reward_dist Min                     5.47785e-08\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.036125\n",
      "evaluation/env_infos/final/reward_energy Std             0.0386398\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00191705\n",
      "evaluation/env_infos/final/reward_energy Min            -0.258554\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.209564\n",
      "evaluation/env_infos/initial/reward_energy Std           0.135322\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0379937\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.549453\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0604533\n",
      "evaluation/env_infos/reward_energy Std                   0.0657609\n",
      "evaluation/env_infos/reward_energy Max                  -0.00100759\n",
      "evaluation/env_infos/reward_energy Min                  -0.549453\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0135416\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.206579\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.388972\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.492167\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       4.04668e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00881957\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0273878\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0250317\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00506232\n",
      "evaluation/env_infos/end_effector_loc Std                0.14432\n",
      "evaluation/env_infos/end_effector_loc Max                0.388972\n",
      "evaluation/env_infos/end_effector_loc Min               -0.492167\n",
      "time/data storing (s)                                    0.00616294\n",
      "time/evaluation sampling (s)                             1.16704\n",
      "time/exploration sampling (s)                            0.12257\n",
      "time/logging (s)                                         0.0201445\n",
      "time/saving (s)                                          0.0264061\n",
      "time/training (s)                                       47.9284\n",
      "time/epoch (s)                                          49.2707\n",
      "time/total (s)                                        2640.64\n",
      "Epoch                                                   53\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:33:29.200411 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 54 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000962548\n",
      "trainer/QF2 Loss                                         0.00127294\n",
      "trainer/Policy Loss                                      3.04722\n",
      "trainer/Q1 Predictions Mean                             -1.09583\n",
      "trainer/Q1 Predictions Std                               0.87141\n",
      "trainer/Q1 Predictions Max                               0.708822\n",
      "trainer/Q1 Predictions Min                              -3.85709\n",
      "trainer/Q2 Predictions Mean                             -1.10058\n",
      "trainer/Q2 Predictions Std                               0.874078\n",
      "trainer/Q2 Predictions Max                               0.677514\n",
      "trainer/Q2 Predictions Min                              -3.82748\n",
      "trainer/Q Targets Mean                                  -1.10428\n",
      "trainer/Q Targets Std                                    0.869968\n",
      "trainer/Q Targets Max                                    0.737687\n",
      "trainer/Q Targets Min                                   -3.83666\n",
      "trainer/Log Pis Mean                                     1.96424\n",
      "trainer/Log Pis Std                                      1.32596\n",
      "trainer/Log Pis Max                                      4.24918\n",
      "trainer/Log Pis Min                                     -3.00118\n",
      "trainer/Policy mu Mean                                   0.0300406\n",
      "trainer/Policy mu Std                                    0.30429\n",
      "trainer/Policy mu Max                                    2.51559\n",
      "trainer/Policy mu Min                                   -1.16532\n",
      "trainer/Policy log std Mean                             -2.29704\n",
      "trainer/Policy log std Std                               0.548778\n",
      "trainer/Policy log std Max                               0.335096\n",
      "trainer/Policy log std Min                              -3.11468\n",
      "trainer/Alpha                                            0.0252713\n",
      "trainer/Alpha Loss                                      -0.131487\n",
      "exploration/num steps total                           6500\n",
      "exploration/num paths total                            325\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0720851\n",
      "exploration/Rewards Std                                  0.0857127\n",
      "exploration/Rewards Max                                  0.125596\n",
      "exploration/Rewards Min                                 -0.287932\n",
      "exploration/Returns Mean                                -1.4417\n",
      "exploration/Returns Std                                  1.30598\n",
      "exploration/Returns Max                                  0.953494\n",
      "exploration/Returns Min                                 -2.7154\n",
      "exploration/Actions Mean                                 0.00526958\n",
      "exploration/Actions Std                                  0.125706\n",
      "exploration/Actions Max                                  0.429613\n",
      "exploration/Actions Min                                 -0.378126\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.4417\n",
      "exploration/env_infos/final/reward_dist Mean             0.21823\n",
      "exploration/env_infos/final/reward_dist Std              0.316596\n",
      "exploration/env_infos/final/reward_dist Max              0.81394\n",
      "exploration/env_infos/final/reward_dist Min              7.6809e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00850705\n",
      "exploration/env_infos/initial/reward_dist Std            0.0116525\n",
      "exploration/env_infos/initial/reward_dist Max            0.0300381\n",
      "exploration/env_infos/initial/reward_dist Min            9.51652e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.137011\n",
      "exploration/env_infos/reward_dist Std                    0.229575\n",
      "exploration/env_infos/reward_dist Max                    0.911513\n",
      "exploration/env_infos/reward_dist Min                    7.6809e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.196526\n",
      "exploration/env_infos/final/reward_energy Std            0.186132\n",
      "exploration/env_infos/final/reward_energy Max           -0.0497952\n",
      "exploration/env_infos/final/reward_energy Min           -0.525635\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.22326\n",
      "exploration/env_infos/initial/reward_energy Std          0.0806646\n",
      "exploration/env_infos/initial/reward_energy Max         -0.105965\n",
      "exploration/env_infos/initial/reward_energy Min         -0.340049\n",
      "exploration/env_infos/reward_energy Mean                -0.151159\n",
      "exploration/env_infos/reward_energy Std                  0.0938652\n",
      "exploration/env_infos/reward_energy Max                 -0.0194159\n",
      "exploration/env_infos/reward_energy Min                 -0.525635\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0404708\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275561\n",
      "exploration/env_infos/final/end_effector_loc Max         0.485724\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.363732\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000195802\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00839056\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0133525\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0120672\n",
      "exploration/env_infos/end_effector_loc Mean              0.0181761\n",
      "exploration/env_infos/end_effector_loc Std               0.180929\n",
      "exploration/env_infos/end_effector_loc Max               0.486604\n",
      "exploration/env_infos/end_effector_loc Min              -0.363732\n",
      "evaluation/num steps total                           55000\n",
      "evaluation/num paths total                            2750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0289836\n",
      "evaluation/Rewards Std                                   0.0701624\n",
      "evaluation/Rewards Max                                   0.149601\n",
      "evaluation/Rewards Min                                  -0.255956\n",
      "evaluation/Returns Mean                                 -0.579673\n",
      "evaluation/Returns Std                                   1.13946\n",
      "evaluation/Returns Max                                   2.02353\n",
      "evaluation/Returns Min                                  -4.24555\n",
      "evaluation/Actions Mean                                 -0.00272927\n",
      "evaluation/Actions Std                                   0.0511113\n",
      "evaluation/Actions Max                                   0.512478\n",
      "evaluation/Actions Min                                  -0.433115\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.579673\n",
      "evaluation/env_infos/final/reward_dist Mean              0.287293\n",
      "evaluation/env_infos/final/reward_dist Std               0.319397\n",
      "evaluation/env_infos/final/reward_dist Max               0.956751\n",
      "evaluation/env_infos/final/reward_dist Min               8.1413e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705835\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00997688\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0535184\n",
      "evaluation/env_infos/initial/reward_dist Min             2.25872e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.233662\n",
      "evaluation/env_infos/reward_dist Std                     0.286773\n",
      "evaluation/env_infos/reward_dist Max                     0.996958\n",
      "evaluation/env_infos/reward_dist Min                     8.1413e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0328541\n",
      "evaluation/env_infos/final/reward_energy Std             0.0237084\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00302008\n",
      "evaluation/env_infos/final/reward_energy Min            -0.135884\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.167543\n",
      "evaluation/env_infos/initial/reward_energy Std           0.147904\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00461402\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.646923\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0465763\n",
      "evaluation/env_infos/reward_energy Std                   0.0554102\n",
      "evaluation/env_infos/reward_energy Max                  -0.000798735\n",
      "evaluation/env_infos/reward_energy Min                  -0.646923\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0190823\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.189965\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.400674\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.511443\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00161684\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00773426\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0256239\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0216558\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0188775\n",
      "evaluation/env_infos/end_effector_loc Std                0.124316\n",
      "evaluation/env_infos/end_effector_loc Max                0.400674\n",
      "evaluation/env_infos/end_effector_loc Min               -0.511443\n",
      "time/data storing (s)                                    0.00618857\n",
      "time/evaluation sampling (s)                             0.942885\n",
      "time/exploration sampling (s)                            0.125751\n",
      "time/logging (s)                                         0.0193036\n",
      "time/saving (s)                                          0.0278495\n",
      "time/training (s)                                       48.4029\n",
      "time/epoch (s)                                          49.5249\n",
      "time/total (s)                                        2690.89\n",
      "Epoch                                                   54\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:34:19.761922 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 55 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00102885\n",
      "trainer/QF2 Loss                                         0.000819663\n",
      "trainer/Policy Loss                                      3.22843\n",
      "trainer/Q1 Predictions Mean                             -1.23474\n",
      "trainer/Q1 Predictions Std                               0.925401\n",
      "trainer/Q1 Predictions Max                               0.725562\n",
      "trainer/Q1 Predictions Min                              -3.12557\n",
      "trainer/Q2 Predictions Mean                             -1.24235\n",
      "trainer/Q2 Predictions Std                               0.928192\n",
      "trainer/Q2 Predictions Max                               0.733514\n",
      "trainer/Q2 Predictions Min                              -3.15993\n",
      "trainer/Q Targets Mean                                  -1.24808\n",
      "trainer/Q Targets Std                                    0.933604\n",
      "trainer/Q Targets Max                                    0.73529\n",
      "trainer/Q Targets Min                                   -3.16942\n",
      "trainer/Log Pis Mean                                     2.0077\n",
      "trainer/Log Pis Std                                      1.30266\n",
      "trainer/Log Pis Max                                      4.25982\n",
      "trainer/Log Pis Min                                     -2.18862\n",
      "trainer/Policy mu Mean                                   0.0101607\n",
      "trainer/Policy mu Std                                    0.242911\n",
      "trainer/Policy mu Max                                    1.4472\n",
      "trainer/Policy mu Min                                   -1.8177\n",
      "trainer/Policy log std Mean                             -2.3369\n",
      "trainer/Policy log std Std                               0.562387\n",
      "trainer/Policy log std Max                              -0.0993328\n",
      "trainer/Policy log std Min                              -3.27176\n",
      "trainer/Alpha                                            0.024771\n",
      "trainer/Alpha Loss                                       0.0284911\n",
      "exploration/num steps total                           6600\n",
      "exploration/num paths total                            330\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.068173\n",
      "exploration/Rewards Std                                  0.0620386\n",
      "exploration/Rewards Max                                  0.0505592\n",
      "exploration/Rewards Min                                 -0.242164\n",
      "exploration/Returns Mean                                -1.36346\n",
      "exploration/Returns Std                                  0.788125\n",
      "exploration/Returns Max                                 -0.653033\n",
      "exploration/Returns Min                                 -2.75536\n",
      "exploration/Actions Mean                                -0.011289\n",
      "exploration/Actions Std                                  0.137039\n",
      "exploration/Actions Max                                  0.439088\n",
      "exploration/Actions Min                                 -0.664701\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.36346\n",
      "exploration/env_infos/final/reward_dist Mean             0.434309\n",
      "exploration/env_infos/final/reward_dist Std              0.406347\n",
      "exploration/env_infos/final/reward_dist Max              0.975398\n",
      "exploration/env_infos/final/reward_dist Min              7.46344e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00309711\n",
      "exploration/env_infos/initial/reward_dist Std            0.00464012\n",
      "exploration/env_infos/initial/reward_dist Max            0.0122256\n",
      "exploration/env_infos/initial/reward_dist Min            5.68639e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.19844\n",
      "exploration/env_infos/reward_dist Std                    0.283594\n",
      "exploration/env_infos/reward_dist Max                    0.975398\n",
      "exploration/env_infos/reward_dist Min                    7.46344e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.162507\n",
      "exploration/env_infos/final/reward_energy Std            0.0444072\n",
      "exploration/env_infos/final/reward_energy Max           -0.0804247\n",
      "exploration/env_infos/final/reward_energy Min           -0.204613\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.141121\n",
      "exploration/env_infos/initial/reward_energy Std          0.0909552\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0425374\n",
      "exploration/env_infos/initial/reward_energy Min         -0.282402\n",
      "exploration/env_infos/reward_energy Mean                -0.155515\n",
      "exploration/env_infos/reward_energy Std                  0.116745\n",
      "exploration/env_infos/reward_energy Max                 -0.00835471\n",
      "exploration/env_infos/reward_energy Min                 -0.678452\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.144016\n",
      "exploration/env_infos/final/end_effector_loc Std         0.244741\n",
      "exploration/env_infos/final/end_effector_loc Max         0.162452\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.734827\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00459959\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00375215\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.000451328\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0121737\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0613593\n",
      "exploration/env_infos/end_effector_loc Std               0.137991\n",
      "exploration/env_infos/end_effector_loc Max               0.167983\n",
      "exploration/env_infos/end_effector_loc Min              -0.734827\n",
      "evaluation/num steps total                           56000\n",
      "evaluation/num paths total                            2800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0400451\n",
      "evaluation/Rewards Std                                   0.0650278\n",
      "evaluation/Rewards Max                                   0.170612\n",
      "evaluation/Rewards Min                                  -0.354082\n",
      "evaluation/Returns Mean                                 -0.800903\n",
      "evaluation/Returns Std                                   0.939539\n",
      "evaluation/Returns Max                                   2.06777\n",
      "evaluation/Returns Min                                  -2.89494\n",
      "evaluation/Actions Mean                                 -0.000836477\n",
      "evaluation/Actions Std                                   0.0549304\n",
      "evaluation/Actions Max                                   0.486804\n",
      "evaluation/Actions Min                                  -0.482494\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.800903\n",
      "evaluation/env_infos/final/reward_dist Mean              0.247987\n",
      "evaluation/env_infos/final/reward_dist Std               0.302858\n",
      "evaluation/env_infos/final/reward_dist Max               0.992416\n",
      "evaluation/env_infos/final/reward_dist Min               3.00595e-13\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00739897\n",
      "evaluation/env_infos/initial/reward_dist Std             0.013757\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0665826\n",
      "evaluation/env_infos/initial/reward_dist Min             1.99076e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.196515\n",
      "evaluation/env_infos/reward_dist Std                     0.275396\n",
      "evaluation/env_infos/reward_dist Max                     0.998573\n",
      "evaluation/env_infos/reward_dist Min                     3.00595e-13\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0383416\n",
      "evaluation/env_infos/final/reward_energy Std             0.0373802\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000383782\n",
      "evaluation/env_infos/final/reward_energy Min            -0.218097\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.178292\n",
      "evaluation/env_infos/initial/reward_energy Std           0.13642\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00652374\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.552565\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0504309\n",
      "evaluation/env_infos/reward_energy Std                   0.0591001\n",
      "evaluation/env_infos/reward_energy Max                  -0.000383782\n",
      "evaluation/env_infos/reward_energy Min                  -0.552565\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0134279\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.20865\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.388189\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.55612\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000408572\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0079266\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0243402\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0241247\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0100764\n",
      "evaluation/env_infos/end_effector_loc Std                0.136591\n",
      "evaluation/env_infos/end_effector_loc Max                0.388189\n",
      "evaluation/env_infos/end_effector_loc Min               -0.55612\n",
      "time/data storing (s)                                    0.00584427\n",
      "time/evaluation sampling (s)                             0.926655\n",
      "time/exploration sampling (s)                            0.118226\n",
      "time/logging (s)                                         0.0208405\n",
      "time/saving (s)                                          0.0294208\n",
      "time/training (s)                                       48.76\n",
      "time/epoch (s)                                          49.861\n",
      "time/total (s)                                        2741.45\n",
      "Epoch                                                   55\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:35:10.135965 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 56 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00157491\n",
      "trainer/QF2 Loss                                         0.00117135\n",
      "trainer/Policy Loss                                      3.15213\n",
      "trainer/Q1 Predictions Mean                             -1.16263\n",
      "trainer/Q1 Predictions Std                               0.950674\n",
      "trainer/Q1 Predictions Max                               0.478922\n",
      "trainer/Q1 Predictions Min                              -3.50658\n",
      "trainer/Q2 Predictions Mean                             -1.15814\n",
      "trainer/Q2 Predictions Std                               0.955007\n",
      "trainer/Q2 Predictions Max                               0.492868\n",
      "trainer/Q2 Predictions Min                              -3.49712\n",
      "trainer/Q Targets Mean                                  -1.16106\n",
      "trainer/Q Targets Std                                    0.953261\n",
      "trainer/Q Targets Max                                    0.491232\n",
      "trainer/Q Targets Min                                   -3.48732\n",
      "trainer/Log Pis Mean                                     2.01035\n",
      "trainer/Log Pis Std                                      1.25948\n",
      "trainer/Log Pis Max                                      4.54169\n",
      "trainer/Log Pis Min                                     -1.51592\n",
      "trainer/Policy mu Mean                                   0.0119769\n",
      "trainer/Policy mu Std                                    0.349966\n",
      "trainer/Policy mu Max                                    2.33042\n",
      "trainer/Policy mu Min                                   -2.07178\n",
      "trainer/Policy log std Mean                             -2.27501\n",
      "trainer/Policy log std Std                               0.604516\n",
      "trainer/Policy log std Max                              -9.02414e-05\n",
      "trainer/Policy log std Min                              -3.19835\n",
      "trainer/Alpha                                            0.023272\n",
      "trainer/Alpha Loss                                       0.0389518\n",
      "exploration/num steps total                           6700\n",
      "exploration/num paths total                            335\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0464959\n",
      "exploration/Rewards Std                                  0.103538\n",
      "exploration/Rewards Max                                  0.12377\n",
      "exploration/Rewards Min                                 -0.462896\n",
      "exploration/Returns Mean                                -0.929918\n",
      "exploration/Returns Std                                  1.01715\n",
      "exploration/Returns Max                                  1.05624\n",
      "exploration/Returns Min                                 -1.81305\n",
      "exploration/Actions Mean                                -0.00393249\n",
      "exploration/Actions Std                                  0.112796\n",
      "exploration/Actions Max                                  0.325751\n",
      "exploration/Actions Min                                 -0.505763\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.929918\n",
      "exploration/env_infos/final/reward_dist Mean             0.112549\n",
      "exploration/env_infos/final/reward_dist Std              0.195326\n",
      "exploration/env_infos/final/reward_dist Max              0.501768\n",
      "exploration/env_infos/final/reward_dist Min              2.51203e-29\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00119381\n",
      "exploration/env_infos/initial/reward_dist Std            0.00142769\n",
      "exploration/env_infos/initial/reward_dist Max            0.00303979\n",
      "exploration/env_infos/initial/reward_dist Min            7.89151e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.115212\n",
      "exploration/env_infos/reward_dist Std                    0.189961\n",
      "exploration/env_infos/reward_dist Max                    0.733262\n",
      "exploration/env_infos/reward_dist Min                    2.51203e-29\n",
      "exploration/env_infos/final/reward_energy Mean          -0.142997\n",
      "exploration/env_infos/final/reward_energy Std            0.0720773\n",
      "exploration/env_infos/final/reward_energy Max           -0.0412584\n",
      "exploration/env_infos/final/reward_energy Min           -0.234376\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.225283\n",
      "exploration/env_infos/initial/reward_energy Std          0.103307\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0749556\n",
      "exploration/env_infos/initial/reward_energy Min         -0.372972\n",
      "exploration/env_infos/reward_energy Mean                -0.125602\n",
      "exploration/env_infos/reward_energy Std                  0.0984943\n",
      "exploration/env_infos/reward_energy Max                 -0.00554143\n",
      "exploration/env_infos/reward_energy Min                 -0.508638\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0617542\n",
      "exploration/env_infos/final/end_effector_loc Std         0.312578\n",
      "exploration/env_infos/final/end_effector_loc Max         0.370501\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.763574\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000208721\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00875998\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00925911\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0155478\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0272883\n",
      "exploration/env_infos/end_effector_loc Std               0.175869\n",
      "exploration/env_infos/end_effector_loc Max               0.370501\n",
      "exploration/env_infos/end_effector_loc Min              -0.763574\n",
      "evaluation/num steps total                           57000\n",
      "evaluation/num paths total                            2850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0252556\n",
      "evaluation/Rewards Std                                   0.0797202\n",
      "evaluation/Rewards Max                                   0.153914\n",
      "evaluation/Rewards Min                                  -0.401524\n",
      "evaluation/Returns Mean                                 -0.505111\n",
      "evaluation/Returns Std                                   1.19445\n",
      "evaluation/Returns Max                                   1.80035\n",
      "evaluation/Returns Min                                  -3.17832\n",
      "evaluation/Actions Mean                                  6.98443e-05\n",
      "evaluation/Actions Std                                   0.0601255\n",
      "evaluation/Actions Max                                   0.539937\n",
      "evaluation/Actions Min                                  -0.470538\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.505111\n",
      "evaluation/env_infos/final/reward_dist Mean              0.18294\n",
      "evaluation/env_infos/final/reward_dist Std               0.248024\n",
      "evaluation/env_infos/final/reward_dist Max               0.877032\n",
      "evaluation/env_infos/final/reward_dist Min               8.65244e-16\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00663239\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0108052\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0497003\n",
      "evaluation/env_infos/initial/reward_dist Min             1.8982e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.222643\n",
      "evaluation/env_infos/reward_dist Std                     0.278685\n",
      "evaluation/env_infos/reward_dist Max                     0.998723\n",
      "evaluation/env_infos/reward_dist Min                     8.65244e-16\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0242486\n",
      "evaluation/env_infos/final/reward_energy Std             0.020891\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00152252\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0993499\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.231996\n",
      "evaluation/env_infos/initial/reward_energy Std           0.144546\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0205511\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.693865\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0539482\n",
      "evaluation/env_infos/reward_energy Std                   0.0657248\n",
      "evaluation/env_infos/reward_energy Max                  -0.000616395\n",
      "evaluation/env_infos/reward_energy Min                  -0.693865\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00781641\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.246606\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.675805\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.508589\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000348321\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0096578\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0269969\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0235269\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00579077\n",
      "evaluation/env_infos/end_effector_loc Std                0.161888\n",
      "evaluation/env_infos/end_effector_loc Max                0.675805\n",
      "evaluation/env_infos/end_effector_loc Min               -0.508589\n",
      "time/data storing (s)                                    0.00612628\n",
      "time/evaluation sampling (s)                             0.964598\n",
      "time/exploration sampling (s)                            0.12095\n",
      "time/logging (s)                                         0.0190855\n",
      "time/saving (s)                                          0.0277266\n",
      "time/training (s)                                       48.5302\n",
      "time/epoch (s)                                          49.6687\n",
      "time/total (s)                                        2791.83\n",
      "Epoch                                                   56\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:36:01.140782 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 57 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00275069\r\n",
      "trainer/QF2 Loss                                         0.00231291\r\n",
      "trainer/Policy Loss                                      2.89529\r\n",
      "trainer/Q1 Predictions Mean                             -1.06349\r\n",
      "trainer/Q1 Predictions Std                               0.945618\r\n",
      "trainer/Q1 Predictions Max                               0.953198\r\n",
      "trainer/Q1 Predictions Min                              -3.5957\r\n",
      "trainer/Q2 Predictions Mean                             -1.06099\r\n",
      "trainer/Q2 Predictions Std                               0.941435\r\n",
      "trainer/Q2 Predictions Max                               0.971961\r\n",
      "trainer/Q2 Predictions Min                              -3.55835\r\n",
      "trainer/Q Targets Mean                                  -1.06335\r\n",
      "trainer/Q Targets Std                                    0.93686\r\n",
      "trainer/Q Targets Max                                    0.928952\r\n",
      "trainer/Q Targets Min                                   -3.56234\r\n",
      "trainer/Log Pis Mean                                     1.85692\r\n",
      "trainer/Log Pis Std                                      1.33347\r\n",
      "trainer/Log Pis Max                                      4.46031\r\n",
      "trainer/Log Pis Min                                     -1.81657\r\n",
      "trainer/Policy mu Mean                                   0.0139534\r\n",
      "trainer/Policy mu Std                                    0.283224\r\n",
      "trainer/Policy mu Max                                    2.00472\r\n",
      "trainer/Policy mu Min                                   -1.44779\r\n",
      "trainer/Policy log std Mean                             -2.28554\r\n",
      "trainer/Policy log std Std                               0.5702\r\n",
      "trainer/Policy log std Max                              -0.334651\r\n",
      "trainer/Policy log std Min                              -3.31153\r\n",
      "trainer/Alpha                                            0.0240695\r\n",
      "trainer/Alpha Loss                                      -0.533004\r\n",
      "exploration/num steps total                           6800\r\n",
      "exploration/num paths total                            340\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.114509\r\n",
      "exploration/Rewards Std                                  0.0843977\r\n",
      "exploration/Rewards Max                                  0.038368\r\n",
      "exploration/Rewards Min                                 -0.348497\r\n",
      "exploration/Returns Mean                                -2.29019\r\n",
      "exploration/Returns Std                                  1.42988\r\n",
      "exploration/Returns Max                                  0.204824\r\n",
      "exploration/Returns Min                                 -4.2106\r\n",
      "exploration/Actions Mean                                 0.0136448\r\n",
      "exploration/Actions Std                                  0.138521\r\n",
      "exploration/Actions Max                                  0.590393\r\n",
      "exploration/Actions Min                                 -0.428866\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.29019\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0207467\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0412265\r\n",
      "exploration/env_infos/final/reward_dist Max              0.103199\r\n",
      "exploration/env_infos/final/reward_dist Min              1.38365e-30\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00431753\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00767973\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0196488\r\n",
      "exploration/env_infos/initial/reward_dist Min            9.62893e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0507233\r\n",
      "exploration/env_infos/reward_dist Std                    0.105087\r\n",
      "exploration/env_infos/reward_dist Max                    0.467848\r\n",
      "exploration/env_infos/reward_dist Min                    1.38365e-30\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0965156\r\n",
      "exploration/env_infos/final/reward_energy Std            0.039809\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.059003\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.172572\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.208815\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.164956\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.068579\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.500196\r\n",
      "exploration/env_infos/reward_energy Mean                -0.1492\r\n",
      "exploration/env_infos/reward_energy Std                  0.128406\r\n",
      "exploration/env_infos/reward_energy Max                 -0.007209\r\n",
      "exploration/env_infos/reward_energy Min                 -0.709183\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0816464\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.309506\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.698407\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.432143\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00295429\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00893255\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0238603\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00749518\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0189547\r\n",
      "exploration/env_infos/end_effector_loc Std               0.189651\r\n",
      "exploration/env_infos/end_effector_loc Max               0.698407\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.432143\r\n",
      "evaluation/num steps total                           58000\r\n",
      "evaluation/num paths total                            2900\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0385933\r\n",
      "evaluation/Rewards Std                                   0.0746077\r\n",
      "evaluation/Rewards Max                                   0.147258\r\n",
      "evaluation/Rewards Min                                  -0.468915\r\n",
      "evaluation/Returns Mean                                 -0.771866\r\n",
      "evaluation/Returns Std                                   1.17149\r\n",
      "evaluation/Returns Max                                   1.66176\r\n",
      "evaluation/Returns Min                                  -3.43558\r\n",
      "evaluation/Actions Mean                                  0.0013187\r\n",
      "evaluation/Actions Std                                   0.072153\r\n",
      "evaluation/Actions Max                                   0.611003\r\n",
      "evaluation/Actions Min                                  -0.671829\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.771866\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.184444\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.24627\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.911879\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.44775e-12\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00619962\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0094042\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0352436\r\n",
      "evaluation/env_infos/initial/reward_dist Min             5.05206e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.19126\r\n",
      "evaluation/env_infos/reward_dist Std                     0.254269\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998387\r\n",
      "evaluation/env_infos/reward_dist Min                     2.44775e-12\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0436377\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0404833\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00572805\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.186249\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.255917\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.194057\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00761029\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.765076\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0626854\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0805365\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000720083\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.765076\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00733826\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.213438\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.622957\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.410639\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00166504\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0112324\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0305501\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0335915\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00836963\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.152515\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.622957\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.516405\r\n",
      "time/data storing (s)                                    0.00619349\r\n",
      "time/evaluation sampling (s)                             0.950932\r\n",
      "time/exploration sampling (s)                            0.123847\r\n",
      "time/logging (s)                                         0.0197732\r\n",
      "time/saving (s)                                          0.0267665\r\n",
      "time/training (s)                                       49.118\r\n",
      "time/epoch (s)                                          50.2456\r\n",
      "time/total (s)                                        2842.83\r\n",
      "Epoch                                                   57\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:36:52.172018 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 58 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00164319\n",
      "trainer/QF2 Loss                                         0.00209785\n",
      "trainer/Policy Loss                                      3.15019\n",
      "trainer/Q1 Predictions Mean                             -1.11974\n",
      "trainer/Q1 Predictions Std                               0.918284\n",
      "trainer/Q1 Predictions Max                               0.734885\n",
      "trainer/Q1 Predictions Min                              -3.40035\n",
      "trainer/Q2 Predictions Mean                             -1.11769\n",
      "trainer/Q2 Predictions Std                               0.921339\n",
      "trainer/Q2 Predictions Max                               0.714749\n",
      "trainer/Q2 Predictions Min                              -3.44487\n",
      "trainer/Q Targets Mean                                  -1.11772\n",
      "trainer/Q Targets Std                                    0.91589\n",
      "trainer/Q Targets Max                                    0.743051\n",
      "trainer/Q Targets Min                                   -3.42247\n",
      "trainer/Log Pis Mean                                     2.05486\n",
      "trainer/Log Pis Std                                      1.3236\n",
      "trainer/Log Pis Max                                      4.31998\n",
      "trainer/Log Pis Min                                     -4.14375\n",
      "trainer/Policy mu Mean                                   0.0210597\n",
      "trainer/Policy mu Std                                    0.288738\n",
      "trainer/Policy mu Max                                    2.71979\n",
      "trainer/Policy mu Min                                   -0.970239\n",
      "trainer/Policy log std Mean                             -2.38724\n",
      "trainer/Policy log std Std                               0.531848\n",
      "trainer/Policy log std Max                              -0.173375\n",
      "trainer/Policy log std Min                              -3.28112\n",
      "trainer/Alpha                                            0.0238687\n",
      "trainer/Alpha Loss                                       0.204923\n",
      "exploration/num steps total                           6900\n",
      "exploration/num paths total                            345\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0859714\n",
      "exploration/Rewards Std                                  0.0536106\n",
      "exploration/Rewards Max                                  0.0532687\n",
      "exploration/Rewards Min                                 -0.27881\n",
      "exploration/Returns Mean                                -1.71943\n",
      "exploration/Returns Std                                  0.639033\n",
      "exploration/Returns Max                                 -0.607264\n",
      "exploration/Returns Min                                 -2.51185\n",
      "exploration/Actions Mean                                -0.0012916\n",
      "exploration/Actions Std                                  0.0990641\n",
      "exploration/Actions Max                                  0.474998\n",
      "exploration/Actions Min                                 -0.448681\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.71943\n",
      "exploration/env_infos/final/reward_dist Mean             0.00539395\n",
      "exploration/env_infos/final/reward_dist Std              0.0107194\n",
      "exploration/env_infos/final/reward_dist Max              0.0268327\n",
      "exploration/env_infos/final/reward_dist Min              1.08748e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0033466\n",
      "exploration/env_infos/initial/reward_dist Std            0.00410521\n",
      "exploration/env_infos/initial/reward_dist Max            0.0105764\n",
      "exploration/env_infos/initial/reward_dist Min            1.55863e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0901495\n",
      "exploration/env_infos/reward_dist Std                    0.19794\n",
      "exploration/env_infos/reward_dist Max                    0.836516\n",
      "exploration/env_infos/reward_dist Min                    1.08748e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.115768\n",
      "exploration/env_infos/final/reward_energy Std            0.075654\n",
      "exploration/env_infos/final/reward_energy Max           -0.0433737\n",
      "exploration/env_infos/final/reward_energy Min           -0.255328\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.197704\n",
      "exploration/env_infos/initial/reward_energy Std          0.159789\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0562673\n",
      "exploration/env_infos/initial/reward_energy Min         -0.507022\n",
      "exploration/env_infos/reward_energy Mean                -0.108724\n",
      "exploration/env_infos/reward_energy Std                  0.088373\n",
      "exploration/env_infos/reward_energy Max                 -0.0136798\n",
      "exploration/env_infos/reward_energy Min                 -0.507022\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.022986\n",
      "exploration/env_infos/final/end_effector_loc Std         0.227693\n",
      "exploration/env_infos/final/end_effector_loc Max         0.423106\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.378734\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000709161\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00895943\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0237499\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00886674\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0106635\n",
      "exploration/env_infos/end_effector_loc Std               0.138647\n",
      "exploration/env_infos/end_effector_loc Max               0.423106\n",
      "exploration/env_infos/end_effector_loc Min              -0.378734\n",
      "evaluation/num steps total                           59000\n",
      "evaluation/num paths total                            2950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0448657\n",
      "evaluation/Rewards Std                                   0.0756484\n",
      "evaluation/Rewards Max                                   0.150122\n",
      "evaluation/Rewards Min                                  -0.471153\n",
      "evaluation/Returns Mean                                 -0.897314\n",
      "evaluation/Returns Std                                   1.12022\n",
      "evaluation/Returns Max                                   2.12075\n",
      "evaluation/Returns Min                                  -3.37421\n",
      "evaluation/Actions Mean                                 -0.0027332\n",
      "evaluation/Actions Std                                   0.066766\n",
      "evaluation/Actions Max                                   0.574976\n",
      "evaluation/Actions Min                                  -0.42351\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.897314\n",
      "evaluation/env_infos/final/reward_dist Mean              0.229852\n",
      "evaluation/env_infos/final/reward_dist Std               0.319736\n",
      "evaluation/env_infos/final/reward_dist Max               0.98053\n",
      "evaluation/env_infos/final/reward_dist Min               7.84886e-09\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00680088\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0107779\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0480318\n",
      "evaluation/env_infos/initial/reward_dist Min             4.7522e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.221785\n",
      "evaluation/env_infos/reward_dist Std                     0.282908\n",
      "evaluation/env_infos/reward_dist Max                     0.999366\n",
      "evaluation/env_infos/reward_dist Min                     7.84886e-09\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0333581\n",
      "evaluation/env_infos/final/reward_energy Std             0.0224749\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0058685\n",
      "evaluation/env_infos/final/reward_energy Min            -0.121765\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.259539\n",
      "evaluation/env_infos/initial/reward_energy Std           0.159973\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0208663\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.595166\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0597231\n",
      "evaluation/env_infos/reward_energy Std                   0.0732358\n",
      "evaluation/env_infos/reward_energy Max                  -0.00114478\n",
      "evaluation/env_infos/reward_energy Min                  -0.595166\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0238759\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.215521\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.540008\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.444285\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00161244\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106578\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0287488\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0211755\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00495946\n",
      "evaluation/env_infos/end_effector_loc Std                0.149934\n",
      "evaluation/env_infos/end_effector_loc Max                0.540008\n",
      "evaluation/env_infos/end_effector_loc Min               -0.444285\n",
      "time/data storing (s)                                    0.00569731\n",
      "time/evaluation sampling (s)                             0.927846\n",
      "time/exploration sampling (s)                            0.123592\n",
      "time/logging (s)                                         0.0208036\n",
      "time/saving (s)                                          0.029133\n",
      "time/training (s)                                       49.2286\n",
      "time/epoch (s)                                          50.3356\n",
      "time/total (s)                                        2893.86\n",
      "Epoch                                                   58\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:37:43.059082 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 59 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00100036\r\n",
      "trainer/QF2 Loss                                         0.00117996\r\n",
      "trainer/Policy Loss                                      3.16391\r\n",
      "trainer/Q1 Predictions Mean                             -1.20832\r\n",
      "trainer/Q1 Predictions Std                               0.884088\r\n",
      "trainer/Q1 Predictions Max                               0.697471\r\n",
      "trainer/Q1 Predictions Min                              -3.30321\r\n",
      "trainer/Q2 Predictions Mean                             -1.20966\r\n",
      "trainer/Q2 Predictions Std                               0.883667\r\n",
      "trainer/Q2 Predictions Max                               0.663405\r\n",
      "trainer/Q2 Predictions Min                              -3.31507\r\n",
      "trainer/Q Targets Mean                                  -1.20493\r\n",
      "trainer/Q Targets Std                                    0.881974\r\n",
      "trainer/Q Targets Max                                    0.703182\r\n",
      "trainer/Q Targets Min                                   -3.30968\r\n",
      "trainer/Log Pis Mean                                     1.98363\r\n",
      "trainer/Log Pis Std                                      1.49457\r\n",
      "trainer/Log Pis Max                                      4.59343\r\n",
      "trainer/Log Pis Min                                     -5.16146\r\n",
      "trainer/Policy mu Mean                                   0.0200297\r\n",
      "trainer/Policy mu Std                                    0.303749\r\n",
      "trainer/Policy mu Max                                    1.90833\r\n",
      "trainer/Policy mu Min                                   -1.58192\r\n",
      "trainer/Policy log std Mean                             -2.29879\r\n",
      "trainer/Policy log std Std                               0.564989\r\n",
      "trainer/Policy log std Max                              -0.647\r\n",
      "trainer/Policy log std Min                              -3.17277\r\n",
      "trainer/Alpha                                            0.0244863\r\n",
      "trainer/Alpha Loss                                      -0.0607216\r\n",
      "exploration/num steps total                           7000\r\n",
      "exploration/num paths total                            350\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0357043\r\n",
      "exploration/Rewards Std                                  0.100909\r\n",
      "exploration/Rewards Max                                  0.153278\r\n",
      "exploration/Rewards Min                                 -0.304826\r\n",
      "exploration/Returns Mean                                -0.714086\r\n",
      "exploration/Returns Std                                  1.50287\r\n",
      "exploration/Returns Max                                  0.966036\r\n",
      "exploration/Returns Min                                 -3.38391\r\n",
      "exploration/Actions Mean                                -0.00125198\r\n",
      "exploration/Actions Std                                  0.121794\r\n",
      "exploration/Actions Max                                  0.403495\r\n",
      "exploration/Actions Min                                 -0.484648\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.714086\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.155929\r\n",
      "exploration/env_infos/final/reward_dist Std              0.27291\r\n",
      "exploration/env_infos/final/reward_dist Max              0.698903\r\n",
      "exploration/env_infos/final/reward_dist Min              2.55365e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00806297\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0100273\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0261061\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000347479\r\n",
      "exploration/env_infos/reward_dist Mean                   0.276461\r\n",
      "exploration/env_infos/reward_dist Std                    0.329264\r\n",
      "exploration/env_infos/reward_dist Max                    0.998494\r\n",
      "exploration/env_infos/reward_dist Min                    1.98604e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.236407\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0858163\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.144897\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.397061\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.338003\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.14878\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.106097\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.532666\r\n",
      "exploration/env_infos/reward_energy Mean                -0.138287\r\n",
      "exploration/env_infos/reward_energy Std                  0.102701\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0114806\r\n",
      "exploration/env_infos/reward_energy Min                 -0.532666\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0355857\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.241019\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.356077\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.343783\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00343107\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0125978\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0190122\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0242324\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0192695\r\n",
      "exploration/env_infos/end_effector_loc Std               0.161608\r\n",
      "exploration/env_infos/end_effector_loc Max               0.358968\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.343783\r\n",
      "evaluation/num steps total                           60000\r\n",
      "evaluation/num paths total                            3000\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0311713\r\n",
      "evaluation/Rewards Std                                   0.0849939\r\n",
      "evaluation/Rewards Max                                   0.175286\r\n",
      "evaluation/Rewards Min                                  -0.605616\r\n",
      "evaluation/Returns Mean                                 -0.623426\r\n",
      "evaluation/Returns Std                                   1.29662\r\n",
      "evaluation/Returns Max                                   2.37875\r\n",
      "evaluation/Returns Min                                  -3.54928\r\n",
      "evaluation/Actions Mean                                 -0.00116192\r\n",
      "evaluation/Actions Std                                   0.0716779\r\n",
      "evaluation/Actions Max                                   0.558293\r\n",
      "evaluation/Actions Min                                  -0.623353\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.623426\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.154981\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.203946\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.945294\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.12328e-10\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00873923\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0164517\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0800767\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.26782e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.217155\r\n",
      "evaluation/env_infos/reward_dist Std                     0.273798\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998314\r\n",
      "evaluation/env_infos/reward_dist Min                     1.12328e-10\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0294734\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0174574\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00167805\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0752896\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.268757\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.189754\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0119763\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.704558\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.061468\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0806215\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.0010991\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.704558\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0182386\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.226561\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.489993\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.556408\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000879087\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0115984\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0279147\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0311676\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00481375\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.157555\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.489993\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.556408\r\n",
      "time/data storing (s)                                    0.00615585\r\n",
      "time/evaluation sampling (s)                             0.94834\r\n",
      "time/exploration sampling (s)                            0.122706\r\n",
      "time/logging (s)                                         0.0195386\r\n",
      "time/saving (s)                                          0.02731\r\n",
      "time/training (s)                                       49.0194\r\n",
      "time/epoch (s)                                          50.1434\r\n",
      "time/total (s)                                        2944.74\r\n",
      "Epoch                                                   59\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:38:33.671008 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 60 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000910253\r\n",
      "trainer/QF2 Loss                                         0.000771204\r\n",
      "trainer/Policy Loss                                      3.29141\r\n",
      "trainer/Q1 Predictions Mean                             -1.25754\r\n",
      "trainer/Q1 Predictions Std                               0.914081\r\n",
      "trainer/Q1 Predictions Max                               0.721091\r\n",
      "trainer/Q1 Predictions Min                              -3.41966\r\n",
      "trainer/Q2 Predictions Mean                             -1.25053\r\n",
      "trainer/Q2 Predictions Std                               0.913657\r\n",
      "trainer/Q2 Predictions Max                               0.692595\r\n",
      "trainer/Q2 Predictions Min                              -3.43692\r\n",
      "trainer/Q Targets Mean                                  -1.25657\r\n",
      "trainer/Q Targets Std                                    0.916918\r\n",
      "trainer/Q Targets Max                                    0.731403\r\n",
      "trainer/Q Targets Min                                   -3.46939\r\n",
      "trainer/Log Pis Mean                                     2.07805\r\n",
      "trainer/Log Pis Std                                      1.33863\r\n",
      "trainer/Log Pis Max                                      4.37033\r\n",
      "trainer/Log Pis Min                                     -4.04538\r\n",
      "trainer/Policy mu Mean                                   0.0267946\r\n",
      "trainer/Policy mu Std                                    0.36244\r\n",
      "trainer/Policy mu Max                                    2.07656\r\n",
      "trainer/Policy mu Min                                   -1.53585\r\n",
      "trainer/Policy log std Mean                             -2.30081\r\n",
      "trainer/Policy log std Std                               0.590523\r\n",
      "trainer/Policy log std Max                              -0.592122\r\n",
      "trainer/Policy log std Min                              -3.19992\r\n",
      "trainer/Alpha                                            0.0247097\r\n",
      "trainer/Alpha Loss                                       0.288865\r\n",
      "exploration/num steps total                           7100\r\n",
      "exploration/num paths total                            355\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0154056\r\n",
      "exploration/Rewards Std                                  0.0825034\r\n",
      "exploration/Rewards Max                                  0.129186\r\n",
      "exploration/Rewards Min                                 -0.210037\r\n",
      "exploration/Returns Mean                                -0.308113\r\n",
      "exploration/Returns Std                                  1.36459\r\n",
      "exploration/Returns Max                                  0.940682\r\n",
      "exploration/Returns Min                                 -2.75529\r\n",
      "exploration/Actions Mean                                -0.00720062\r\n",
      "exploration/Actions Std                                  0.156222\r\n",
      "exploration/Actions Max                                  0.641911\r\n",
      "exploration/Actions Min                                 -0.544515\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.308113\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.396636\r\n",
      "exploration/env_infos/final/reward_dist Std              0.213906\r\n",
      "exploration/env_infos/final/reward_dist Max              0.565241\r\n",
      "exploration/env_infos/final/reward_dist Min              0.00184592\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00601237\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00909332\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0240571\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.56232e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.302789\r\n",
      "exploration/env_infos/reward_dist Std                    0.320021\r\n",
      "exploration/env_infos/reward_dist Max                    0.985848\r\n",
      "exploration/env_infos/reward_dist Min                    1.90783e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.179253\r\n",
      "exploration/env_infos/final/reward_energy Std            0.125143\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0819483\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.411198\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.322828\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.166596\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.176054\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.642803\r\n",
      "exploration/env_infos/reward_energy Mean                -0.174577\r\n",
      "exploration/env_infos/reward_energy Std                  0.135785\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0307951\r\n",
      "exploration/env_infos/reward_energy Min                 -0.674333\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0638332\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.178969\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.252048\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.325716\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00365948\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0123115\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0114623\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0270801\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0279429\r\n",
      "exploration/env_infos/end_effector_loc Std               0.126971\r\n",
      "exploration/env_infos/end_effector_loc Max               0.252048\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.325716\r\n",
      "evaluation/num steps total                           61000\r\n",
      "evaluation/num paths total                            3050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0319247\r\n",
      "evaluation/Rewards Std                                   0.0761666\r\n",
      "evaluation/Rewards Max                                   0.172467\r\n",
      "evaluation/Rewards Min                                  -0.392793\r\n",
      "evaluation/Returns Mean                                 -0.638494\r\n",
      "evaluation/Returns Std                                   1.13474\r\n",
      "evaluation/Returns Max                                   1.85503\r\n",
      "evaluation/Returns Min                                  -3.34657\r\n",
      "evaluation/Actions Mean                                 -0.00172709\r\n",
      "evaluation/Actions Std                                   0.0619821\r\n",
      "evaluation/Actions Max                                   0.547192\r\n",
      "evaluation/Actions Min                                  -0.512221\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.638494\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.192561\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.262467\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.969371\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.55796e-09\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00905353\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0173925\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0820649\r\n",
      "evaluation/env_infos/initial/reward_dist Min             3.68556e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.185854\r\n",
      "evaluation/env_infos/reward_dist Std                     0.262344\r\n",
      "evaluation/env_infos/reward_dist Max                     0.989481\r\n",
      "evaluation/env_infos/reward_dist Min                     1.55796e-09\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0310919\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.023238\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0023828\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.10408\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.242258\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.186106\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00244418\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.649977\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0527523\r\n",
      "evaluation/env_infos/reward_energy Std                   0.070048\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00100867\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.649977\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0133482\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.211717\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.507663\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.438076\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00267867\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0104633\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0273596\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0256111\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0180649\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.146444\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.507663\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.438076\r\n",
      "time/data storing (s)                                    0.00611505\r\n",
      "time/evaluation sampling (s)                             0.948442\r\n",
      "time/exploration sampling (s)                            0.121222\r\n",
      "time/logging (s)                                         0.0202148\r\n",
      "time/saving (s)                                          0.031397\r\n",
      "time/training (s)                                       48.6406\r\n",
      "time/epoch (s)                                          49.768\r\n",
      "time/total (s)                                        2995.35\r\n",
      "Epoch                                                   60\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:39:24.288384 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 61 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00105568\n",
      "trainer/QF2 Loss                                         0.00175368\n",
      "trainer/Policy Loss                                      3.00487\n",
      "trainer/Q1 Predictions Mean                             -1.04616\n",
      "trainer/Q1 Predictions Std                               0.918332\n",
      "trainer/Q1 Predictions Max                               0.54069\n",
      "trainer/Q1 Predictions Min                              -3.35942\n",
      "trainer/Q2 Predictions Mean                             -1.05127\n",
      "trainer/Q2 Predictions Std                               0.920917\n",
      "trainer/Q2 Predictions Max                               0.535654\n",
      "trainer/Q2 Predictions Min                              -3.36752\n",
      "trainer/Q Targets Mean                                  -1.05644\n",
      "trainer/Q Targets Std                                    0.924386\n",
      "trainer/Q Targets Max                                    0.582707\n",
      "trainer/Q Targets Min                                   -3.40541\n",
      "trainer/Log Pis Mean                                     1.97591\n",
      "trainer/Log Pis Std                                      1.28096\n",
      "trainer/Log Pis Max                                      4.2838\n",
      "trainer/Log Pis Min                                     -2.21045\n",
      "trainer/Policy mu Mean                                   0.0130646\n",
      "trainer/Policy mu Std                                    0.277058\n",
      "trainer/Policy mu Max                                    2.25761\n",
      "trainer/Policy mu Min                                   -1.78036\n",
      "trainer/Policy log std Mean                             -2.32234\n",
      "trainer/Policy log std Std                               0.553236\n",
      "trainer/Policy log std Max                              -0.547265\n",
      "trainer/Policy log std Min                              -3.20806\n",
      "trainer/Alpha                                            0.0237785\n",
      "trainer/Alpha Loss                                      -0.0900592\n",
      "exploration/num steps total                           7200\n",
      "exploration/num paths total                            360\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.073905\n",
      "exploration/Rewards Std                                  0.0656004\n",
      "exploration/Rewards Max                                  0.0470261\n",
      "exploration/Rewards Min                                 -0.296308\n",
      "exploration/Returns Mean                                -1.4781\n",
      "exploration/Returns Std                                  0.457388\n",
      "exploration/Returns Max                                 -0.833474\n",
      "exploration/Returns Min                                 -2.03221\n",
      "exploration/Actions Mean                                 0.00757683\n",
      "exploration/Actions Std                                  0.123715\n",
      "exploration/Actions Max                                  0.494467\n",
      "exploration/Actions Min                                 -0.460771\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.4781\n",
      "exploration/env_infos/final/reward_dist Mean             0.147235\n",
      "exploration/env_infos/final/reward_dist Std              0.260573\n",
      "exploration/env_infos/final/reward_dist Max              0.666048\n",
      "exploration/env_infos/final/reward_dist Min              1.53061e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000501368\n",
      "exploration/env_infos/initial/reward_dist Std            0.000903561\n",
      "exploration/env_infos/initial/reward_dist Max            0.00230765\n",
      "exploration/env_infos/initial/reward_dist Min            8.12376e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.253506\n",
      "exploration/env_infos/reward_dist Std                    0.285402\n",
      "exploration/env_infos/reward_dist Max                    0.993824\n",
      "exploration/env_infos/reward_dist Min                    8.12376e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.155918\n",
      "exploration/env_infos/final/reward_energy Std            0.0726233\n",
      "exploration/env_infos/final/reward_energy Max           -0.0419491\n",
      "exploration/env_infos/final/reward_energy Min           -0.221806\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.304519\n",
      "exploration/env_infos/initial/reward_energy Std          0.126373\n",
      "exploration/env_infos/initial/reward_energy Max         -0.122677\n",
      "exploration/env_infos/initial/reward_energy Min         -0.507092\n",
      "exploration/env_infos/reward_energy Mean                -0.143659\n",
      "exploration/env_infos/reward_energy Std                  0.100437\n",
      "exploration/env_infos/reward_energy Max                 -0.0140494\n",
      "exploration/env_infos/reward_energy Min                 -0.507092\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.105632\n",
      "exploration/env_infos/final/end_effector_loc Std         0.248241\n",
      "exploration/env_infos/final/end_effector_loc Max         0.423806\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.382823\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00368603\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0110585\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0247234\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0133906\n",
      "exploration/env_infos/end_effector_loc Mean              0.0542518\n",
      "exploration/env_infos/end_effector_loc Std               0.171138\n",
      "exploration/env_infos/end_effector_loc Max               0.423806\n",
      "exploration/env_infos/end_effector_loc Min              -0.382823\n",
      "evaluation/num steps total                           62000\n",
      "evaluation/num paths total                            3100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0349701\n",
      "evaluation/Rewards Std                                   0.0686943\n",
      "evaluation/Rewards Max                                   0.15767\n",
      "evaluation/Rewards Min                                  -0.334256\n",
      "evaluation/Returns Mean                                 -0.699402\n",
      "evaluation/Returns Std                                   1.02373\n",
      "evaluation/Returns Max                                   1.99234\n",
      "evaluation/Returns Min                                  -2.49243\n",
      "evaluation/Actions Mean                                 -0.00139887\n",
      "evaluation/Actions Std                                   0.0663647\n",
      "evaluation/Actions Max                                   0.484584\n",
      "evaluation/Actions Min                                  -0.54457\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.699402\n",
      "evaluation/env_infos/final/reward_dist Mean              0.308516\n",
      "evaluation/env_infos/final/reward_dist Std               0.336966\n",
      "evaluation/env_infos/final/reward_dist Max               0.977247\n",
      "evaluation/env_infos/final/reward_dist Min               1.26777e-15\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00542546\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111289\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0612433\n",
      "evaluation/env_infos/initial/reward_dist Min             1.69738e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.251645\n",
      "evaluation/env_infos/reward_dist Std                     0.303472\n",
      "evaluation/env_infos/reward_dist Max                     0.997227\n",
      "evaluation/env_infos/reward_dist Min                     1.26777e-15\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0273159\n",
      "evaluation/env_infos/final/reward_energy Std             0.0162696\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00358954\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0737657\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.25675\n",
      "evaluation/env_infos/initial/reward_energy Std           0.176974\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0215772\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.708702\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0569783\n",
      "evaluation/env_infos/reward_energy Std                   0.0746052\n",
      "evaluation/env_infos/reward_energy Max                  -0.00200272\n",
      "evaluation/env_infos/reward_energy Min                  -0.708702\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0229631\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.205326\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.586202\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.384756\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000627761\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0110071\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0242292\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0272285\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00958021\n",
      "evaluation/env_infos/end_effector_loc Std                0.142735\n",
      "evaluation/env_infos/end_effector_loc Max                0.586202\n",
      "evaluation/env_infos/end_effector_loc Min               -0.384756\n",
      "time/data storing (s)                                    0.00590659\n",
      "time/evaluation sampling (s)                             1.07259\n",
      "time/exploration sampling (s)                            0.126069\n",
      "time/logging (s)                                         0.0220156\n",
      "time/saving (s)                                          0.0280948\n",
      "time/training (s)                                       48.5809\n",
      "time/epoch (s)                                          49.8355\n",
      "time/total (s)                                        3045.97\n",
      "Epoch                                                   61\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:40:16.086240 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 62 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00129434\r\n",
      "trainer/QF2 Loss                                         0.000680253\r\n",
      "trainer/Policy Loss                                      2.95074\r\n",
      "trainer/Q1 Predictions Mean                             -1.17644\r\n",
      "trainer/Q1 Predictions Std                               0.899226\r\n",
      "trainer/Q1 Predictions Max                               0.525085\r\n",
      "trainer/Q1 Predictions Min                              -3.44843\r\n",
      "trainer/Q2 Predictions Mean                             -1.18117\r\n",
      "trainer/Q2 Predictions Std                               0.90337\r\n",
      "trainer/Q2 Predictions Max                               0.541824\r\n",
      "trainer/Q2 Predictions Min                              -3.49203\r\n",
      "trainer/Q Targets Mean                                  -1.18365\r\n",
      "trainer/Q Targets Std                                    0.905306\r\n",
      "trainer/Q Targets Max                                    0.573667\r\n",
      "trainer/Q Targets Min                                   -3.47889\r\n",
      "trainer/Log Pis Mean                                     1.79545\r\n",
      "trainer/Log Pis Std                                      1.40121\r\n",
      "trainer/Log Pis Max                                      4.2801\r\n",
      "trainer/Log Pis Min                                     -2.42875\r\n",
      "trainer/Policy mu Mean                                   0.0708358\r\n",
      "trainer/Policy mu Std                                    0.388879\r\n",
      "trainer/Policy mu Max                                    2.06679\r\n",
      "trainer/Policy mu Min                                   -1.72228\r\n",
      "trainer/Policy log std Mean                             -2.23238\r\n",
      "trainer/Policy log std Std                               0.658302\r\n",
      "trainer/Policy log std Max                              -0.300929\r\n",
      "trainer/Policy log std Min                              -3.22385\r\n",
      "trainer/Alpha                                            0.0235042\r\n",
      "trainer/Alpha Loss                                      -0.767003\r\n",
      "exploration/num steps total                           7300\r\n",
      "exploration/num paths total                            365\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0857805\r\n",
      "exploration/Rewards Std                                  0.0709105\r\n",
      "exploration/Rewards Max                                  0.0575597\r\n",
      "exploration/Rewards Min                                 -0.26809\r\n",
      "exploration/Returns Mean                                -1.71561\r\n",
      "exploration/Returns Std                                  0.942379\r\n",
      "exploration/Returns Max                                 -0.344608\r\n",
      "exploration/Returns Min                                 -2.97836\r\n",
      "exploration/Actions Mean                                -0.00522529\r\n",
      "exploration/Actions Std                                  0.0903065\r\n",
      "exploration/Actions Max                                  0.330916\r\n",
      "exploration/Actions Min                                 -0.342629\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.71561\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.176297\r\n",
      "exploration/env_infos/final/reward_dist Std              0.237658\r\n",
      "exploration/env_infos/final/reward_dist Max              0.613579\r\n",
      "exploration/env_infos/final/reward_dist Min              4.47895e-05\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00100493\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00189723\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00479584\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.05865e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.151409\r\n",
      "exploration/env_infos/reward_dist Std                    0.235711\r\n",
      "exploration/env_infos/reward_dist Max                    0.978025\r\n",
      "exploration/env_infos/reward_dist Min                    1.26298e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.126475\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0582611\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0762714\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.240438\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.128808\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.111885\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0296071\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.33972\r\n",
      "exploration/env_infos/reward_energy Mean                -0.107462\r\n",
      "exploration/env_infos/reward_energy Std                  0.0694052\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00721805\r\n",
      "exploration/env_infos/reward_energy Min                 -0.355854\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0158439\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.26492\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.433347\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.399834\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000707502\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00599055\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0165458\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00454253\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00687284\r\n",
      "exploration/env_infos/end_effector_loc Std               0.161924\r\n",
      "exploration/env_infos/end_effector_loc Max               0.433347\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.399834\r\n",
      "evaluation/num steps total                           63000\r\n",
      "evaluation/num paths total                            3150\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0381553\r\n",
      "evaluation/Rewards Std                                   0.0694265\r\n",
      "evaluation/Rewards Max                                   0.145777\r\n",
      "evaluation/Rewards Min                                  -0.266808\r\n",
      "evaluation/Returns Mean                                 -0.763105\r\n",
      "evaluation/Returns Std                                   1.06443\r\n",
      "evaluation/Returns Max                                   1.57341\r\n",
      "evaluation/Returns Min                                  -3.13045\r\n",
      "evaluation/Actions Mean                                  0.00152981\r\n",
      "evaluation/Actions Std                                   0.067083\r\n",
      "evaluation/Actions Max                                   0.669021\r\n",
      "evaluation/Actions Min                                  -0.446875\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.763105\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.173156\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.262628\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.979344\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.53663e-09\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0109861\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0171473\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.083269\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.72922e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.208608\r\n",
      "evaluation/env_infos/reward_dist Std                     0.273029\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996586\r\n",
      "evaluation/env_infos/reward_dist Min                     3.53663e-09\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0461968\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0313036\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00852946\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.137425\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.250803\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.189531\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0056807\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.819806\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0600238\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0734989\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00130816\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.819806\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0620049\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.210418\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.55778\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.401134\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00301884\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106966\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.033451\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0223438\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0375518\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.14463\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.55778\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.401134\r\n",
      "time/data storing (s)                                    0.00590974\r\n",
      "time/evaluation sampling (s)                             1.04159\r\n",
      "time/exploration sampling (s)                            0.124875\r\n",
      "time/logging (s)                                         0.0207194\r\n",
      "time/saving (s)                                          0.0260007\r\n",
      "time/training (s)                                       49.7898\r\n",
      "time/epoch (s)                                          51.0089\r\n",
      "time/total (s)                                        3097.77\r\n",
      "Epoch                                                   62\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:41:06.861259 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 63 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000942235\n",
      "trainer/QF2 Loss                                         0.00117384\n",
      "trainer/Policy Loss                                      3.01789\n",
      "trainer/Q1 Predictions Mean                             -1.07771\n",
      "trainer/Q1 Predictions Std                               0.936465\n",
      "trainer/Q1 Predictions Max                               0.625976\n",
      "trainer/Q1 Predictions Min                              -3.3921\n",
      "trainer/Q2 Predictions Mean                             -1.07853\n",
      "trainer/Q2 Predictions Std                               0.935075\n",
      "trainer/Q2 Predictions Max                               0.610489\n",
      "trainer/Q2 Predictions Min                              -3.32735\n",
      "trainer/Q Targets Mean                                  -1.07353\n",
      "trainer/Q Targets Std                                    0.934164\n",
      "trainer/Q Targets Max                                    0.649058\n",
      "trainer/Q Targets Min                                   -3.4122\n",
      "trainer/Log Pis Mean                                     1.96175\n",
      "trainer/Log Pis Std                                      1.31362\n",
      "trainer/Log Pis Max                                      4.8097\n",
      "trainer/Log Pis Min                                     -2.79176\n",
      "trainer/Policy mu Mean                                  -0.0132112\n",
      "trainer/Policy mu Std                                    0.383031\n",
      "trainer/Policy mu Max                                    1.79938\n",
      "trainer/Policy mu Min                                   -2.89915\n",
      "trainer/Policy log std Mean                             -2.2894\n",
      "trainer/Policy log std Std                               0.589832\n",
      "trainer/Policy log std Max                              -0.452754\n",
      "trainer/Policy log std Min                              -3.24981\n",
      "trainer/Alpha                                            0.0240853\n",
      "trainer/Alpha Loss                                      -0.142589\n",
      "exploration/num steps total                           7400\n",
      "exploration/num paths total                            370\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0371843\n",
      "exploration/Rewards Std                                  0.0711484\n",
      "exploration/Rewards Max                                  0.131016\n",
      "exploration/Rewards Min                                 -0.192008\n",
      "exploration/Returns Mean                                -0.743686\n",
      "exploration/Returns Std                                  1.24351\n",
      "exploration/Returns Max                                  1.5444\n",
      "exploration/Returns Min                                 -2.19717\n",
      "exploration/Actions Mean                                -0.0063401\n",
      "exploration/Actions Std                                  0.209809\n",
      "exploration/Actions Max                                  0.98555\n",
      "exploration/Actions Min                                 -0.705801\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.743686\n",
      "exploration/env_infos/final/reward_dist Mean             0.0838808\n",
      "exploration/env_infos/final/reward_dist Std              0.0743689\n",
      "exploration/env_infos/final/reward_dist Max              0.208822\n",
      "exploration/env_infos/final/reward_dist Min              0.00101741\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0281441\n",
      "exploration/env_infos/initial/reward_dist Std            0.0339494\n",
      "exploration/env_infos/initial/reward_dist Max            0.0891144\n",
      "exploration/env_infos/initial/reward_dist Min            2.60649e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.227107\n",
      "exploration/env_infos/reward_dist Std                    0.300365\n",
      "exploration/env_infos/reward_dist Max                    0.989678\n",
      "exploration/env_infos/reward_dist Min                    2.60649e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.200139\n",
      "exploration/env_infos/final/reward_energy Std            0.191664\n",
      "exploration/env_infos/final/reward_energy Max           -0.0524516\n",
      "exploration/env_infos/final/reward_energy Min           -0.573338\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.339986\n",
      "exploration/env_infos/initial/reward_energy Std          0.243998\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0584991\n",
      "exploration/env_infos/initial/reward_energy Min         -0.721844\n",
      "exploration/env_infos/reward_energy Mean                -0.218542\n",
      "exploration/env_infos/reward_energy Std                  0.200896\n",
      "exploration/env_infos/reward_energy Max                 -0.00744577\n",
      "exploration/env_infos/reward_energy Min                 -1.2303\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0257641\n",
      "exploration/env_infos/final/end_effector_loc Std         0.155103\n",
      "exploration/env_infos/final/end_effector_loc Max         0.186757\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.315281\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00408129\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0142215\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0277512\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0187135\n",
      "exploration/env_infos/end_effector_loc Mean              0.0136356\n",
      "exploration/env_infos/end_effector_loc Std               0.100509\n",
      "exploration/env_infos/end_effector_loc Max               0.222272\n",
      "exploration/env_infos/end_effector_loc Min              -0.315281\n",
      "evaluation/num steps total                           64000\n",
      "evaluation/num paths total                            3200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0337932\n",
      "evaluation/Rewards Std                                   0.0734834\n",
      "evaluation/Rewards Max                                   0.16716\n",
      "evaluation/Rewards Min                                  -0.479548\n",
      "evaluation/Returns Mean                                 -0.675865\n",
      "evaluation/Returns Std                                   1.05375\n",
      "evaluation/Returns Max                                   2.0205\n",
      "evaluation/Returns Min                                  -3.33007\n",
      "evaluation/Actions Mean                                 -0.00344928\n",
      "evaluation/Actions Std                                   0.0677512\n",
      "evaluation/Actions Max                                   0.517866\n",
      "evaluation/Actions Min                                  -0.620179\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.675865\n",
      "evaluation/env_infos/final/reward_dist Mean              0.289341\n",
      "evaluation/env_infos/final/reward_dist Std               0.297997\n",
      "evaluation/env_infos/final/reward_dist Max               0.972476\n",
      "evaluation/env_infos/final/reward_dist Min               3.59881e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00822387\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0144679\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0608943\n",
      "evaluation/env_infos/initial/reward_dist Min             1.70515e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.245142\n",
      "evaluation/env_infos/reward_dist Std                     0.296029\n",
      "evaluation/env_infos/reward_dist Max                     0.999616\n",
      "evaluation/env_infos/reward_dist Min                     3.59881e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0295788\n",
      "evaluation/env_infos/final/reward_energy Std             0.0187964\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00403183\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0933137\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.275649\n",
      "evaluation/env_infos/initial/reward_energy Std           0.190934\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.021071\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.67586\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0568866\n",
      "evaluation/env_infos/reward_energy Std                   0.0772538\n",
      "evaluation/env_infos/reward_energy Max                  -0.00133965\n",
      "evaluation/env_infos/reward_energy Min                  -0.67586\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0303545\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.219674\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.456076\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.657724\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00154087\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0117547\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0258933\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0310089\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0133605\n",
      "evaluation/env_infos/end_effector_loc Std                0.147809\n",
      "evaluation/env_infos/end_effector_loc Max                0.456076\n",
      "evaluation/env_infos/end_effector_loc Min               -0.657724\n",
      "time/data storing (s)                                    0.00628287\n",
      "time/evaluation sampling (s)                             0.997198\n",
      "time/exploration sampling (s)                            0.137963\n",
      "time/logging (s)                                         0.0193612\n",
      "time/saving (s)                                          0.0269921\n",
      "time/training (s)                                       48.7519\n",
      "time/epoch (s)                                          49.9397\n",
      "time/total (s)                                        3148.54\n",
      "Epoch                                                   63\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:41:58.820954 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 64 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00112701\n",
      "trainer/QF2 Loss                                         0.000757776\n",
      "trainer/Policy Loss                                      3.24952\n",
      "trainer/Q1 Predictions Mean                             -1.21636\n",
      "trainer/Q1 Predictions Std                               0.916525\n",
      "trainer/Q1 Predictions Max                               0.592496\n",
      "trainer/Q1 Predictions Min                              -3.5166\n",
      "trainer/Q2 Predictions Mean                             -1.21197\n",
      "trainer/Q2 Predictions Std                               0.909658\n",
      "trainer/Q2 Predictions Max                               0.639571\n",
      "trainer/Q2 Predictions Min                              -3.49487\n",
      "trainer/Q Targets Mean                                  -1.21771\n",
      "trainer/Q Targets Std                                    0.907131\n",
      "trainer/Q Targets Max                                    0.625402\n",
      "trainer/Q Targets Min                                   -3.49704\n",
      "trainer/Log Pis Mean                                     2.06436\n",
      "trainer/Log Pis Std                                      1.4302\n",
      "trainer/Log Pis Max                                      4.58906\n",
      "trainer/Log Pis Min                                     -5.30592\n",
      "trainer/Policy mu Mean                                  -0.014308\n",
      "trainer/Policy mu Std                                    0.374812\n",
      "trainer/Policy mu Max                                    2.01824\n",
      "trainer/Policy mu Min                                   -2.01189\n",
      "trainer/Policy log std Mean                             -2.32367\n",
      "trainer/Policy log std Std                               0.614063\n",
      "trainer/Policy log std Max                              -0.404828\n",
      "trainer/Policy log std Min                              -3.34685\n",
      "trainer/Alpha                                            0.0235163\n",
      "trainer/Alpha Loss                                       0.241341\n",
      "exploration/num steps total                           7500\n",
      "exploration/num paths total                            375\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0594562\n",
      "exploration/Rewards Std                                  0.0498837\n",
      "exploration/Rewards Max                                  0.0535381\n",
      "exploration/Rewards Min                                 -0.213198\n",
      "exploration/Returns Mean                                -1.18912\n",
      "exploration/Returns Std                                  0.49504\n",
      "exploration/Returns Max                                 -0.388363\n",
      "exploration/Returns Min                                 -1.92628\n",
      "exploration/Actions Mean                                -0.00371189\n",
      "exploration/Actions Std                                  0.0897506\n",
      "exploration/Actions Max                                  0.297097\n",
      "exploration/Actions Min                                 -0.364814\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.18912\n",
      "exploration/env_infos/final/reward_dist Mean             0.0259994\n",
      "exploration/env_infos/final/reward_dist Std              0.0477813\n",
      "exploration/env_infos/final/reward_dist Max              0.121481\n",
      "exploration/env_infos/final/reward_dist Min              3.20869e-13\n",
      "exploration/env_infos/initial/reward_dist Mean           0.011129\n",
      "exploration/env_infos/initial/reward_dist Std            0.0073949\n",
      "exploration/env_infos/initial/reward_dist Max            0.022474\n",
      "exploration/env_infos/initial/reward_dist Min            0.000193838\n",
      "exploration/env_infos/reward_dist Mean                   0.0607261\n",
      "exploration/env_infos/reward_dist Std                    0.0972742\n",
      "exploration/env_infos/reward_dist Max                    0.552679\n",
      "exploration/env_infos/reward_dist Min                    3.20869e-13\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0727122\n",
      "exploration/env_infos/final/reward_energy Std            0.0203647\n",
      "exploration/env_infos/final/reward_energy Max           -0.0486831\n",
      "exploration/env_infos/final/reward_energy Min           -0.101659\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.119941\n",
      "exploration/env_infos/initial/reward_energy Std          0.0663326\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0156859\n",
      "exploration/env_infos/initial/reward_energy Min         -0.21802\n",
      "exploration/env_infos/reward_energy Mean                -0.10143\n",
      "exploration/env_infos/reward_energy Std                  0.0764847\n",
      "exploration/env_infos/reward_energy Max                 -0.00231725\n",
      "exploration/env_infos/reward_energy Min                 -0.365442\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0362968\n",
      "exploration/env_infos/final/end_effector_loc Std         0.262768\n",
      "exploration/env_infos/final/end_effector_loc Max         0.2521\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.557117\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000126361\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00484419\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00861546\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00628627\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0143187\n",
      "exploration/env_infos/end_effector_loc Std               0.152962\n",
      "exploration/env_infos/end_effector_loc Max               0.2521\n",
      "exploration/env_infos/end_effector_loc Min              -0.557117\n",
      "evaluation/num steps total                           65000\n",
      "evaluation/num paths total                            3250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0495191\n",
      "evaluation/Rewards Std                                   0.0651038\n",
      "evaluation/Rewards Max                                   0.123942\n",
      "evaluation/Rewards Min                                  -0.452616\n",
      "evaluation/Returns Mean                                 -0.990382\n",
      "evaluation/Returns Std                                   0.88828\n",
      "evaluation/Returns Max                                   1.13745\n",
      "evaluation/Returns Min                                  -3.04689\n",
      "evaluation/Actions Mean                                 -0.00258786\n",
      "evaluation/Actions Std                                   0.0590669\n",
      "evaluation/Actions Max                                   0.537628\n",
      "evaluation/Actions Min                                  -0.533727\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.990382\n",
      "evaluation/env_infos/final/reward_dist Mean              0.243938\n",
      "evaluation/env_infos/final/reward_dist Std               0.268659\n",
      "evaluation/env_infos/final/reward_dist Max               0.94236\n",
      "evaluation/env_infos/final/reward_dist Min               8.20793e-10\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00623206\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0121003\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0706875\n",
      "evaluation/env_infos/initial/reward_dist Min             2.07393e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.219161\n",
      "evaluation/env_infos/reward_dist Std                     0.273043\n",
      "evaluation/env_infos/reward_dist Max                     0.996959\n",
      "evaluation/env_infos/reward_dist Min                     8.20793e-10\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0261896\n",
      "evaluation/env_infos/final/reward_energy Std             0.0253068\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00245949\n",
      "evaluation/env_infos/final/reward_energy Min            -0.140367\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.219068\n",
      "evaluation/env_infos/initial/reward_energy Std           0.168972\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0100567\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.569234\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0511746\n",
      "evaluation/env_infos/reward_energy Std                   0.0661238\n",
      "evaluation/env_infos/reward_energy Max                  -0.00151638\n",
      "evaluation/env_infos/reward_energy Min                  -0.569234\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0151702\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.205901\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.397796\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.3982\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       8.0154e-06\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0097815\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0268814\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0266863\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00126738\n",
      "evaluation/env_infos/end_effector_loc Std                0.141582\n",
      "evaluation/env_infos/end_effector_loc Max                0.397796\n",
      "evaluation/env_infos/end_effector_loc Min               -0.3982\n",
      "time/data storing (s)                                    0.00625765\n",
      "time/evaluation sampling (s)                             0.953028\n",
      "time/exploration sampling (s)                            0.122617\n",
      "time/logging (s)                                         0.0192978\n",
      "time/saving (s)                                          0.0404132\n",
      "time/training (s)                                       50.014\n",
      "time/epoch (s)                                          51.1556\n",
      "time/total (s)                                        3200.5\n",
      "Epoch                                                   64\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:42:50.006422 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 65 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000648101\r\n",
      "trainer/QF2 Loss                                         0.00169041\r\n",
      "trainer/Policy Loss                                      3.02899\r\n",
      "trainer/Q1 Predictions Mean                             -1.18135\r\n",
      "trainer/Q1 Predictions Std                               0.892795\r\n",
      "trainer/Q1 Predictions Max                               0.662833\r\n",
      "trainer/Q1 Predictions Min                              -3.4206\r\n",
      "trainer/Q2 Predictions Mean                             -1.18155\r\n",
      "trainer/Q2 Predictions Std                               0.898471\r\n",
      "trainer/Q2 Predictions Max                               0.672452\r\n",
      "trainer/Q2 Predictions Min                              -3.45382\r\n",
      "trainer/Q Targets Mean                                  -1.18666\r\n",
      "trainer/Q Targets Std                                    0.895162\r\n",
      "trainer/Q Targets Max                                    0.650226\r\n",
      "trainer/Q Targets Min                                   -3.46276\r\n",
      "trainer/Log Pis Mean                                     1.88205\r\n",
      "trainer/Log Pis Std                                      1.45828\r\n",
      "trainer/Log Pis Max                                      4.30047\r\n",
      "trainer/Log Pis Min                                     -5.88849\r\n",
      "trainer/Policy mu Mean                                  -0.0233983\r\n",
      "trainer/Policy mu Std                                    0.413483\r\n",
      "trainer/Policy mu Max                                    1.98413\r\n",
      "trainer/Policy mu Min                                   -2.14074\r\n",
      "trainer/Policy log std Mean                             -2.22379\r\n",
      "trainer/Policy log std Std                               0.642145\r\n",
      "trainer/Policy log std Max                              -0.324001\r\n",
      "trainer/Policy log std Min                              -3.22432\r\n",
      "trainer/Alpha                                            0.0244798\r\n",
      "trainer/Alpha Loss                                      -0.437554\r\n",
      "exploration/num steps total                           7600\r\n",
      "exploration/num paths total                            380\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0475422\r\n",
      "exploration/Rewards Std                                  0.0796578\r\n",
      "exploration/Rewards Max                                  0.100015\r\n",
      "exploration/Rewards Min                                 -0.231651\r\n",
      "exploration/Returns Mean                                -0.950843\r\n",
      "exploration/Returns Std                                  1.18458\r\n",
      "exploration/Returns Max                                  0.994647\r\n",
      "exploration/Returns Min                                 -2.52016\r\n",
      "exploration/Actions Mean                                 0.000729205\r\n",
      "exploration/Actions Std                                  0.145918\r\n",
      "exploration/Actions Max                                  0.457938\r\n",
      "exploration/Actions Min                                 -0.597549\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.950843\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.320502\r\n",
      "exploration/env_infos/final/reward_dist Std              0.301855\r\n",
      "exploration/env_infos/final/reward_dist Max              0.837569\r\n",
      "exploration/env_infos/final/reward_dist Min              0.0001067\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00693304\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00958976\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.024833\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.59125e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.412851\r\n",
      "exploration/env_infos/reward_dist Std                    0.331301\r\n",
      "exploration/env_infos/reward_dist Max                    0.984953\r\n",
      "exploration/env_infos/reward_dist Min                    5.59125e-05\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.150018\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0753236\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0564143\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.268537\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.372499\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.16677\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.179539\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.603427\r\n",
      "exploration/env_infos/reward_energy Mean                -0.168888\r\n",
      "exploration/env_infos/reward_energy Std                  0.118584\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0170631\r\n",
      "exploration/env_infos/reward_energy Min                 -0.603427\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0154362\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.180421\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.313196\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.232659\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0005715\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0144182\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0228969\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0298774\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.000524272\r\n",
      "exploration/env_infos/end_effector_loc Std               0.139888\r\n",
      "exploration/env_infos/end_effector_loc Max               0.313196\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.243132\r\n",
      "evaluation/num steps total                           66000\r\n",
      "evaluation/num paths total                            3300\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0299933\r\n",
      "evaluation/Rewards Std                                   0.0754878\r\n",
      "evaluation/Rewards Max                                   0.171565\r\n",
      "evaluation/Rewards Min                                  -0.482801\r\n",
      "evaluation/Returns Mean                                 -0.599866\r\n",
      "evaluation/Returns Std                                   1.10414\r\n",
      "evaluation/Returns Max                                   2.42025\r\n",
      "evaluation/Returns Min                                  -3.02132\r\n",
      "evaluation/Actions Mean                                 -0.00389032\r\n",
      "evaluation/Actions Std                                   0.0772714\r\n",
      "evaluation/Actions Max                                   0.699701\r\n",
      "evaluation/Actions Min                                  -0.782878\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.599866\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.208672\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.263601\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.927081\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.09769e-08\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00405309\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00780151\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0346105\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.80407e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.195918\r\n",
      "evaluation/env_infos/reward_dist Std                     0.259791\r\n",
      "evaluation/env_infos/reward_dist Max                     0.991079\r\n",
      "evaluation/env_infos/reward_dist Min                     8.09769e-08\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0308077\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0261926\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00388651\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.160158\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.304569\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.243232\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0238988\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.870067\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0602614\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0913266\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000976836\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.870067\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0523563\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.209482\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.361782\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.5487\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00139061\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0137103\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0349851\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0391439\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0219369\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.151546\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.413374\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.5487\r\n",
      "time/data storing (s)                                    0.00621225\r\n",
      "time/evaluation sampling (s)                             1.00932\r\n",
      "time/exploration sampling (s)                            0.131759\r\n",
      "time/logging (s)                                         0.0204011\r\n",
      "time/saving (s)                                          0.0718837\r\n",
      "time/training (s)                                       49.0604\r\n",
      "time/epoch (s)                                          50.3\r\n",
      "time/total (s)                                        3251.68\r\n",
      "Epoch                                                   65\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:43:40.347736 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 66 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000855934\n",
      "trainer/QF2 Loss                                         0.00124961\n",
      "trainer/Policy Loss                                      3.14899\n",
      "trainer/Q1 Predictions Mean                             -1.19832\n",
      "trainer/Q1 Predictions Std                               0.894354\n",
      "trainer/Q1 Predictions Max                               0.384688\n",
      "trainer/Q1 Predictions Min                              -3.1853\n",
      "trainer/Q2 Predictions Mean                             -1.19282\n",
      "trainer/Q2 Predictions Std                               0.891185\n",
      "trainer/Q2 Predictions Max                               0.401995\n",
      "trainer/Q2 Predictions Min                              -3.16843\n",
      "trainer/Q Targets Mean                                  -1.19722\n",
      "trainer/Q Targets Std                                    0.895412\n",
      "trainer/Q Targets Max                                    0.399836\n",
      "trainer/Q Targets Min                                   -3.17558\n",
      "trainer/Log Pis Mean                                     1.97612\n",
      "trainer/Log Pis Std                                      1.41783\n",
      "trainer/Log Pis Max                                      4.40991\n",
      "trainer/Log Pis Min                                     -6.51051\n",
      "trainer/Policy mu Mean                                  -0.0211367\n",
      "trainer/Policy mu Std                                    0.347615\n",
      "trainer/Policy mu Max                                    1.87751\n",
      "trainer/Policy mu Min                                   -2.25394\n",
      "trainer/Policy log std Mean                             -2.29746\n",
      "trainer/Policy log std Std                               0.576919\n",
      "trainer/Policy log std Max                              -0.31567\n",
      "trainer/Policy log std Min                              -3.22645\n",
      "trainer/Alpha                                            0.0251636\n",
      "trainer/Alpha Loss                                      -0.0879829\n",
      "exploration/num steps total                           7700\n",
      "exploration/num paths total                            385\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.132713\n",
      "exploration/Rewards Std                                  0.0999738\n",
      "exploration/Rewards Max                                  0.0734355\n",
      "exploration/Rewards Min                                 -0.571278\n",
      "exploration/Returns Mean                                -2.65427\n",
      "exploration/Returns Std                                  1.29148\n",
      "exploration/Returns Max                                 -0.846789\n",
      "exploration/Returns Min                                 -4.78243\n",
      "exploration/Actions Mean                                -0.00486689\n",
      "exploration/Actions Std                                  0.106677\n",
      "exploration/Actions Max                                  0.753166\n",
      "exploration/Actions Min                                 -0.283076\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.65427\n",
      "exploration/env_infos/final/reward_dist Mean             0.203374\n",
      "exploration/env_infos/final/reward_dist Std              0.204803\n",
      "exploration/env_infos/final/reward_dist Max              0.569228\n",
      "exploration/env_infos/final/reward_dist Min              5.68186e-16\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000983536\n",
      "exploration/env_infos/initial/reward_dist Std            0.00176785\n",
      "exploration/env_infos/initial/reward_dist Max            0.00451645\n",
      "exploration/env_infos/initial/reward_dist Min            4.60139e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.106176\n",
      "exploration/env_infos/reward_dist Std                    0.173328\n",
      "exploration/env_infos/reward_dist Max                    0.643627\n",
      "exploration/env_infos/reward_dist Min                    2.7158e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.100492\n",
      "exploration/env_infos/final/reward_energy Std            0.0652611\n",
      "exploration/env_infos/final/reward_energy Max           -0.0375425\n",
      "exploration/env_infos/final/reward_energy Min           -0.223123\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.287176\n",
      "exploration/env_infos/initial/reward_energy Std          0.262698\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0459533\n",
      "exploration/env_infos/initial/reward_energy Min         -0.76013\n",
      "exploration/env_infos/reward_energy Mean                -0.115906\n",
      "exploration/env_infos/reward_energy Std                  0.0968152\n",
      "exploration/env_infos/reward_energy Max                 -0.00877634\n",
      "exploration/env_infos/reward_energy Min                 -0.76013\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0792425\n",
      "exploration/env_infos/final/end_effector_loc Std         0.190777\n",
      "exploration/env_infos/final/end_effector_loc Max         0.483355\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.1684\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00621469\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0122771\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0376583\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00513287\n",
      "exploration/env_infos/end_effector_loc Mean              0.0667561\n",
      "exploration/env_infos/end_effector_loc Std               0.135095\n",
      "exploration/env_infos/end_effector_loc Max               0.516079\n",
      "exploration/env_infos/end_effector_loc Min              -0.1684\n",
      "evaluation/num steps total                           67000\n",
      "evaluation/num paths total                            3350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0340597\n",
      "evaluation/Rewards Std                                   0.0806457\n",
      "evaluation/Rewards Max                                   0.172406\n",
      "evaluation/Rewards Min                                  -0.499849\n",
      "evaluation/Returns Mean                                 -0.681194\n",
      "evaluation/Returns Std                                   1.25488\n",
      "evaluation/Returns Max                                   1.97083\n",
      "evaluation/Returns Min                                  -3.34324\n",
      "evaluation/Actions Mean                                 -0.000174391\n",
      "evaluation/Actions Std                                   0.0793134\n",
      "evaluation/Actions Max                                   0.605503\n",
      "evaluation/Actions Min                                  -0.79918\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.681194\n",
      "evaluation/env_infos/final/reward_dist Mean              0.189768\n",
      "evaluation/env_infos/final/reward_dist Std               0.286768\n",
      "evaluation/env_infos/final/reward_dist Max               0.92536\n",
      "evaluation/env_infos/final/reward_dist Min               9.55792e-14\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00929873\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0203855\n",
      "evaluation/env_infos/initial/reward_dist Max             0.108773\n",
      "evaluation/env_infos/initial/reward_dist Min             1.69884e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.177469\n",
      "evaluation/env_infos/reward_dist Std                     0.261715\n",
      "evaluation/env_infos/reward_dist Max                     0.989603\n",
      "evaluation/env_infos/reward_dist Min                     9.55792e-14\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0366713\n",
      "evaluation/env_infos/final/reward_energy Std             0.0275638\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00486182\n",
      "evaluation/env_infos/final/reward_energy Min            -0.125001\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.31426\n",
      "evaluation/env_infos/initial/reward_energy Std           0.216202\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0125632\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.802159\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0654413\n",
      "evaluation/env_infos/reward_energy Std                   0.0910974\n",
      "evaluation/env_infos/reward_energy Max                  -0.000341131\n",
      "evaluation/env_infos/reward_energy Min                  -0.802159\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0099878\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.237221\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.617628\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.443127\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00119045\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134336\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0302751\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.039959\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00732146\n",
      "evaluation/env_infos/end_effector_loc Std                0.164609\n",
      "evaluation/env_infos/end_effector_loc Max                0.617628\n",
      "evaluation/env_infos/end_effector_loc Min               -0.443127\n",
      "time/data storing (s)                                    0.00711906\n",
      "time/evaluation sampling (s)                             0.989428\n",
      "time/exploration sampling (s)                            0.125825\n",
      "time/logging (s)                                         0.0195654\n",
      "time/saving (s)                                          0.0284077\n",
      "time/training (s)                                       48.3144\n",
      "time/epoch (s)                                          49.4847\n",
      "time/total (s)                                        3302.02\n",
      "Epoch                                                   66\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:44:31.973275 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 67 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00075922\n",
      "trainer/QF2 Loss                                         0.000949998\n",
      "trainer/Policy Loss                                      3.3201\n",
      "trainer/Q1 Predictions Mean                             -1.21289\n",
      "trainer/Q1 Predictions Std                               0.924484\n",
      "trainer/Q1 Predictions Max                               0.485234\n",
      "trainer/Q1 Predictions Min                              -3.23115\n",
      "trainer/Q2 Predictions Mean                             -1.21808\n",
      "trainer/Q2 Predictions Std                               0.923907\n",
      "trainer/Q2 Predictions Max                               0.487811\n",
      "trainer/Q2 Predictions Min                              -3.20986\n",
      "trainer/Q Targets Mean                                  -1.22089\n",
      "trainer/Q Targets Std                                    0.926782\n",
      "trainer/Q Targets Max                                    0.465817\n",
      "trainer/Q Targets Min                                   -3.23826\n",
      "trainer/Log Pis Mean                                     2.13844\n",
      "trainer/Log Pis Std                                      1.18806\n",
      "trainer/Log Pis Max                                      4.29328\n",
      "trainer/Log Pis Min                                     -1.22385\n",
      "trainer/Policy mu Mean                                  -0.0259944\n",
      "trainer/Policy mu Std                                    0.349733\n",
      "trainer/Policy mu Max                                    1.518\n",
      "trainer/Policy mu Min                                   -2.12073\n",
      "trainer/Policy log std Mean                             -2.30102\n",
      "trainer/Policy log std Std                               0.597576\n",
      "trainer/Policy log std Max                              -0.605386\n",
      "trainer/Policy log std Min                              -3.2379\n",
      "trainer/Alpha                                            0.0249499\n",
      "trainer/Alpha Loss                                       0.511169\n",
      "exploration/num steps total                           7800\n",
      "exploration/num paths total                            390\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0657425\n",
      "exploration/Rewards Std                                  0.0757542\n",
      "exploration/Rewards Max                                  0.104427\n",
      "exploration/Rewards Min                                 -0.290518\n",
      "exploration/Returns Mean                                -1.31485\n",
      "exploration/Returns Std                                  1.25449\n",
      "exploration/Returns Max                                  0.794102\n",
      "exploration/Returns Min                                 -3.05106\n",
      "exploration/Actions Mean                                 0.0116453\n",
      "exploration/Actions Std                                  0.176091\n",
      "exploration/Actions Max                                  0.673549\n",
      "exploration/Actions Min                                 -0.569984\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.31485\n",
      "exploration/env_infos/final/reward_dist Mean             0.132009\n",
      "exploration/env_infos/final/reward_dist Std              0.230968\n",
      "exploration/env_infos/final/reward_dist Max              0.591202\n",
      "exploration/env_infos/final/reward_dist Min              2.89476e-13\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000289791\n",
      "exploration/env_infos/initial/reward_dist Std            0.000285776\n",
      "exploration/env_infos/initial/reward_dist Max            0.000809926\n",
      "exploration/env_infos/initial/reward_dist Min            1.42164e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0838754\n",
      "exploration/env_infos/reward_dist Std                    0.126254\n",
      "exploration/env_infos/reward_dist Max                    0.595019\n",
      "exploration/env_infos/reward_dist Min                    2.89476e-13\n",
      "exploration/env_infos/final/reward_energy Mean          -0.165765\n",
      "exploration/env_infos/final/reward_energy Std            0.0719715\n",
      "exploration/env_infos/final/reward_energy Max           -0.10009\n",
      "exploration/env_infos/final/reward_energy Min           -0.304557\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.405639\n",
      "exploration/env_infos/initial/reward_energy Std          0.245798\n",
      "exploration/env_infos/initial/reward_energy Max         -0.069331\n",
      "exploration/env_infos/initial/reward_energy Min         -0.691237\n",
      "exploration/env_infos/reward_energy Mean                -0.191103\n",
      "exploration/env_infos/reward_energy Std                  0.160521\n",
      "exploration/env_infos/reward_energy Max                 -0.0149823\n",
      "exploration/env_infos/reward_energy Min                 -0.691237\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0756313\n",
      "exploration/env_infos/final/end_effector_loc Std         0.281515\n",
      "exploration/env_infos/final/end_effector_loc Max         0.489768\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.465968\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00380092\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0163326\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0332064\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0225607\n",
      "exploration/env_infos/end_effector_loc Mean              0.0344955\n",
      "exploration/env_infos/end_effector_loc Std               0.181424\n",
      "exploration/env_infos/end_effector_loc Max               0.489768\n",
      "exploration/env_infos/end_effector_loc Min              -0.465968\n",
      "evaluation/num steps total                           68000\n",
      "evaluation/num paths total                            3400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0321907\n",
      "evaluation/Rewards Std                                   0.0770213\n",
      "evaluation/Rewards Max                                   0.142828\n",
      "evaluation/Rewards Min                                  -0.36089\n",
      "evaluation/Returns Mean                                 -0.643815\n",
      "evaluation/Returns Std                                   1.17002\n",
      "evaluation/Returns Max                                   1.97297\n",
      "evaluation/Returns Min                                  -2.99402\n",
      "evaluation/Actions Mean                                 -0.000633986\n",
      "evaluation/Actions Std                                   0.071544\n",
      "evaluation/Actions Max                                   0.47078\n",
      "evaluation/Actions Min                                  -0.894235\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.643815\n",
      "evaluation/env_infos/final/reward_dist Mean              0.251694\n",
      "evaluation/env_infos/final/reward_dist Std               0.288206\n",
      "evaluation/env_infos/final/reward_dist Max               0.915688\n",
      "evaluation/env_infos/final/reward_dist Min               4.635e-24\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00696117\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0105953\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0392162\n",
      "evaluation/env_infos/initial/reward_dist Min             8.54638e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.240685\n",
      "evaluation/env_infos/reward_dist Std                     0.2977\n",
      "evaluation/env_infos/reward_dist Max                     0.999534\n",
      "evaluation/env_infos/reward_dist Min                     4.635e-24\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0343013\n",
      "evaluation/env_infos/final/reward_energy Std             0.0286176\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00169119\n",
      "evaluation/env_infos/final/reward_energy Min            -0.120874\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.263778\n",
      "evaluation/env_infos/initial/reward_energy Std           0.216962\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00179167\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.971309\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0587379\n",
      "evaluation/env_infos/reward_energy Std                   0.0823879\n",
      "evaluation/env_infos/reward_energy Max                  -0.00065436\n",
      "evaluation/env_infos/reward_energy Min                  -0.971309\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0397012\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.202691\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.350242\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.742064\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00239876\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0118347\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.023539\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0447118\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0261946\n",
      "evaluation/env_infos/end_effector_loc Std                0.139885\n",
      "evaluation/env_infos/end_effector_loc Max                0.350242\n",
      "evaluation/env_infos/end_effector_loc Min               -0.742064\n",
      "time/data storing (s)                                    0.00635146\n",
      "time/evaluation sampling (s)                             0.982102\n",
      "time/exploration sampling (s)                            0.12431\n",
      "time/logging (s)                                         0.0202161\n",
      "time/saving (s)                                          0.0284238\n",
      "time/training (s)                                       49.6215\n",
      "time/epoch (s)                                          50.7829\n",
      "time/total (s)                                        3353.64\n",
      "Epoch                                                   67\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:45:23.798288 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 68 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000888035\n",
      "trainer/QF2 Loss                                         0.00151346\n",
      "trainer/Policy Loss                                      3.04064\n",
      "trainer/Q1 Predictions Mean                             -1.20245\n",
      "trainer/Q1 Predictions Std                               0.835866\n",
      "trainer/Q1 Predictions Max                               0.370069\n",
      "trainer/Q1 Predictions Min                              -3.42912\n",
      "trainer/Q2 Predictions Mean                             -1.20539\n",
      "trainer/Q2 Predictions Std                               0.841336\n",
      "trainer/Q2 Predictions Max                               0.371297\n",
      "trainer/Q2 Predictions Min                              -3.47163\n",
      "trainer/Q Targets Mean                                  -1.20046\n",
      "trainer/Q Targets Std                                    0.832977\n",
      "trainer/Q Targets Max                                    0.353883\n",
      "trainer/Q Targets Min                                   -3.41238\n",
      "trainer/Log Pis Mean                                     1.86922\n",
      "trainer/Log Pis Std                                      1.43422\n",
      "trainer/Log Pis Max                                      4.60232\n",
      "trainer/Log Pis Min                                     -4.13714\n",
      "trainer/Policy mu Mean                                  -0.063984\n",
      "trainer/Policy mu Std                                    0.365088\n",
      "trainer/Policy mu Max                                    1.47946\n",
      "trainer/Policy mu Min                                   -2.17666\n",
      "trainer/Policy log std Mean                             -2.19922\n",
      "trainer/Policy log std Std                               0.618955\n",
      "trainer/Policy log std Max                              -0.410267\n",
      "trainer/Policy log std Min                              -3.24423\n",
      "trainer/Alpha                                            0.0240515\n",
      "trainer/Alpha Loss                                      -0.487281\n",
      "exploration/num steps total                           7900\n",
      "exploration/num paths total                            395\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0706893\n",
      "exploration/Rewards Std                                  0.0545477\n",
      "exploration/Rewards Max                                  0.0353145\n",
      "exploration/Rewards Min                                 -0.247806\n",
      "exploration/Returns Mean                                -1.41379\n",
      "exploration/Returns Std                                  0.522147\n",
      "exploration/Returns Max                                 -0.67925\n",
      "exploration/Returns Min                                 -1.93599\n",
      "exploration/Actions Mean                                -0.00250477\n",
      "exploration/Actions Std                                  0.156093\n",
      "exploration/Actions Max                                  0.652319\n",
      "exploration/Actions Min                                 -0.61264\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.41379\n",
      "exploration/env_infos/final/reward_dist Mean             0.125434\n",
      "exploration/env_infos/final/reward_dist Std              0.0973046\n",
      "exploration/env_infos/final/reward_dist Max              0.25041\n",
      "exploration/env_infos/final/reward_dist Min              0.000141741\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0122318\n",
      "exploration/env_infos/initial/reward_dist Std            0.015943\n",
      "exploration/env_infos/initial/reward_dist Max            0.0407408\n",
      "exploration/env_infos/initial/reward_dist Min            0.000129805\n",
      "exploration/env_infos/reward_dist Mean                   0.159576\n",
      "exploration/env_infos/reward_dist Std                    0.173991\n",
      "exploration/env_infos/reward_dist Max                    0.630665\n",
      "exploration/env_infos/reward_dist Min                    0.000129805\n",
      "exploration/env_infos/final/reward_energy Mean          -0.12543\n",
      "exploration/env_infos/final/reward_energy Std            0.0146607\n",
      "exploration/env_infos/final/reward_energy Max           -0.102752\n",
      "exploration/env_infos/final/reward_energy Min           -0.143966\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.404132\n",
      "exploration/env_infos/initial/reward_energy Std          0.244043\n",
      "exploration/env_infos/initial/reward_energy Max         -0.162386\n",
      "exploration/env_infos/initial/reward_energy Min         -0.765848\n",
      "exploration/env_infos/reward_energy Mean                -0.163626\n",
      "exploration/env_infos/reward_energy Std                  0.148221\n",
      "exploration/env_infos/reward_energy Max                 -0.0149933\n",
      "exploration/env_infos/reward_energy Min                 -0.765848\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0960747\n",
      "exploration/env_infos/final/end_effector_loc Std         0.160555\n",
      "exploration/env_infos/final/end_effector_loc Max         0.373366\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.157553\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00448275\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0160781\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0326159\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0200627\n",
      "exploration/env_infos/end_effector_loc Mean              0.0697581\n",
      "exploration/env_infos/end_effector_loc Std               0.124124\n",
      "exploration/env_infos/end_effector_loc Max               0.373366\n",
      "exploration/env_infos/end_effector_loc Min              -0.157553\n",
      "evaluation/num steps total                           69000\n",
      "evaluation/num paths total                            3450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.043497\n",
      "evaluation/Rewards Std                                   0.0704057\n",
      "evaluation/Rewards Max                                   0.137576\n",
      "evaluation/Rewards Min                                  -0.385436\n",
      "evaluation/Returns Mean                                 -0.86994\n",
      "evaluation/Returns Std                                   1.14933\n",
      "evaluation/Returns Max                                   2.21989\n",
      "evaluation/Returns Min                                  -2.48968\n",
      "evaluation/Actions Mean                                 -0.00305311\n",
      "evaluation/Actions Std                                   0.0627783\n",
      "evaluation/Actions Max                                   0.46985\n",
      "evaluation/Actions Min                                  -0.859427\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.86994\n",
      "evaluation/env_infos/final/reward_dist Mean              0.253489\n",
      "evaluation/env_infos/final/reward_dist Std               0.295864\n",
      "evaluation/env_infos/final/reward_dist Max               0.95718\n",
      "evaluation/env_infos/final/reward_dist Min               2.40782e-10\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00620161\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116465\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0670635\n",
      "evaluation/env_infos/initial/reward_dist Min             5.96689e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.196669\n",
      "evaluation/env_infos/reward_dist Std                     0.281902\n",
      "evaluation/env_infos/reward_dist Max                     0.999892\n",
      "evaluation/env_infos/reward_dist Min                     2.40782e-10\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0301773\n",
      "evaluation/env_infos/final/reward_energy Std             0.029502\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00508925\n",
      "evaluation/env_infos/final/reward_energy Min            -0.171254\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.227451\n",
      "evaluation/env_infos/initial/reward_energy Std           0.228838\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00589152\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.965555\n",
      "evaluation/env_infos/reward_energy Mean                 -0.047678\n",
      "evaluation/env_infos/reward_energy Std                   0.0750179\n",
      "evaluation/env_infos/reward_energy Max                  -0.000642842\n",
      "evaluation/env_infos/reward_energy Min                  -0.965555\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0517957\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.202866\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.349172\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.591506\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00189136\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0112494\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0234925\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0429714\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0249174\n",
      "evaluation/env_infos/end_effector_loc Std                0.132511\n",
      "evaluation/env_infos/end_effector_loc Max                0.349172\n",
      "evaluation/env_infos/end_effector_loc Min               -0.591506\n",
      "time/data storing (s)                                    0.0090329\n",
      "time/evaluation sampling (s)                             0.968899\n",
      "time/exploration sampling (s)                            0.127854\n",
      "time/logging (s)                                         0.0227924\n",
      "time/saving (s)                                          0.0270063\n",
      "time/training (s)                                       49.7492\n",
      "time/epoch (s)                                          50.9047\n",
      "time/total (s)                                        3405.47\n",
      "Epoch                                                   68\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:46:15.444026 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 69 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00124092\n",
      "trainer/QF2 Loss                                         0.000827176\n",
      "trainer/Policy Loss                                      3.1592\n",
      "trainer/Q1 Predictions Mean                             -1.2101\n",
      "trainer/Q1 Predictions Std                               0.904185\n",
      "trainer/Q1 Predictions Max                               0.51518\n",
      "trainer/Q1 Predictions Min                              -3.40901\n",
      "trainer/Q2 Predictions Mean                             -1.21317\n",
      "trainer/Q2 Predictions Std                               0.910472\n",
      "trainer/Q2 Predictions Max                               0.519179\n",
      "trainer/Q2 Predictions Min                              -3.43796\n",
      "trainer/Q Targets Mean                                  -1.20706\n",
      "trainer/Q Targets Std                                    0.912801\n",
      "trainer/Q Targets Max                                    0.526384\n",
      "trainer/Q Targets Min                                   -3.41644\n",
      "trainer/Log Pis Mean                                     1.97866\n",
      "trainer/Log Pis Std                                      1.41837\n",
      "trainer/Log Pis Max                                      4.55097\n",
      "trainer/Log Pis Min                                     -6.9796\n",
      "trainer/Policy mu Mean                                  -0.0469033\n",
      "trainer/Policy mu Std                                    0.308508\n",
      "trainer/Policy mu Max                                    1.25643\n",
      "trainer/Policy mu Min                                   -2.29944\n",
      "trainer/Policy log std Mean                             -2.31462\n",
      "trainer/Policy log std Std                               0.589691\n",
      "trainer/Policy log std Max                              -0.467705\n",
      "trainer/Policy log std Min                              -3.396\n",
      "trainer/Alpha                                            0.0233802\n",
      "trainer/Alpha Loss                                      -0.0801387\n",
      "exploration/num steps total                           8000\n",
      "exploration/num paths total                            400\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.075139\n",
      "exploration/Rewards Std                                  0.0921055\n",
      "exploration/Rewards Max                                  0.0884348\n",
      "exploration/Rewards Min                                 -0.322604\n",
      "exploration/Returns Mean                                -1.50278\n",
      "exploration/Returns Std                                  1.10371\n",
      "exploration/Returns Max                                  0.00732886\n",
      "exploration/Returns Min                                 -3.23859\n",
      "exploration/Actions Mean                                 0.0105203\n",
      "exploration/Actions Std                                  0.103957\n",
      "exploration/Actions Max                                  0.247927\n",
      "exploration/Actions Min                                 -0.785389\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.50278\n",
      "exploration/env_infos/final/reward_dist Mean             0.0526106\n",
      "exploration/env_infos/final/reward_dist Std              0.0989551\n",
      "exploration/env_infos/final/reward_dist Max              0.250405\n",
      "exploration/env_infos/final/reward_dist Min              3.56775e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.013693\n",
      "exploration/env_infos/initial/reward_dist Std            0.0212503\n",
      "exploration/env_infos/initial/reward_dist Max            0.0558517\n",
      "exploration/env_infos/initial/reward_dist Min            6.37671e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.165863\n",
      "exploration/env_infos/reward_dist Std                    0.262373\n",
      "exploration/env_infos/reward_dist Max                    0.97247\n",
      "exploration/env_infos/reward_dist Min                    3.56775e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.13881\n",
      "exploration/env_infos/final/reward_energy Std            0.114942\n",
      "exploration/env_infos/final/reward_energy Max           -0.0200862\n",
      "exploration/env_infos/final/reward_energy Min           -0.345058\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.28653\n",
      "exploration/env_infos/initial/reward_energy Std          0.325721\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0199006\n",
      "exploration/env_infos/initial/reward_energy Min         -0.922706\n",
      "exploration/env_infos/reward_energy Mean                -0.103017\n",
      "exploration/env_infos/reward_energy Std                  0.105938\n",
      "exploration/env_infos/reward_energy Max                 -0.00735794\n",
      "exploration/env_infos/reward_energy Min                 -0.922706\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0231722\n",
      "exploration/env_infos/final/end_effector_loc Std         0.296037\n",
      "exploration/env_infos/final/end_effector_loc Max         0.542097\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.440915\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00669399\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0137997\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0109158\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0392695\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0472769\n",
      "exploration/env_infos/end_effector_loc Std               0.187665\n",
      "exploration/env_infos/end_effector_loc Max               0.542097\n",
      "exploration/env_infos/end_effector_loc Min              -0.440915\n",
      "evaluation/num steps total                           70000\n",
      "evaluation/num paths total                            3500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0337881\n",
      "evaluation/Rewards Std                                   0.0656875\n",
      "evaluation/Rewards Max                                   0.158172\n",
      "evaluation/Rewards Min                                  -0.300021\n",
      "evaluation/Returns Mean                                 -0.675762\n",
      "evaluation/Returns Std                                   0.913787\n",
      "evaluation/Returns Max                                   1.81159\n",
      "evaluation/Returns Min                                  -2.45363\n",
      "evaluation/Actions Mean                                  0.0021955\n",
      "evaluation/Actions Std                                   0.0593349\n",
      "evaluation/Actions Max                                   0.694803\n",
      "evaluation/Actions Min                                  -0.585303\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.675762\n",
      "evaluation/env_infos/final/reward_dist Mean              0.215267\n",
      "evaluation/env_infos/final/reward_dist Std               0.273699\n",
      "evaluation/env_infos/final/reward_dist Max               0.945441\n",
      "evaluation/env_infos/final/reward_dist Min               1.61114e-10\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00579047\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0108018\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0524274\n",
      "evaluation/env_infos/initial/reward_dist Min             9.28371e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.195164\n",
      "evaluation/env_infos/reward_dist Std                     0.268841\n",
      "evaluation/env_infos/reward_dist Max                     0.99729\n",
      "evaluation/env_infos/reward_dist Min                     1.61114e-10\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0348836\n",
      "evaluation/env_infos/final/reward_energy Std             0.0230238\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00913908\n",
      "evaluation/env_infos/final/reward_energy Min            -0.127271\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.227483\n",
      "evaluation/env_infos/initial/reward_energy Std           0.190157\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00603766\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.757191\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0494121\n",
      "evaluation/env_infos/reward_energy Std                   0.0678923\n",
      "evaluation/env_infos/reward_energy Max                  -0.000767588\n",
      "evaluation/env_infos/reward_energy Min                  -0.757191\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0251863\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.210133\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.460985\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.442473\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00085296\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0104479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0347401\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0292652\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0199829\n",
      "evaluation/env_infos/end_effector_loc Std                0.138323\n",
      "evaluation/env_infos/end_effector_loc Max                0.460985\n",
      "evaluation/env_infos/end_effector_loc Min               -0.482229\n",
      "time/data storing (s)                                    0.00628913\n",
      "time/evaluation sampling (s)                             0.962012\n",
      "time/exploration sampling (s)                            0.120787\n",
      "time/logging (s)                                         0.0197162\n",
      "time/saving (s)                                          0.0270479\n",
      "time/training (s)                                       49.6442\n",
      "time/epoch (s)                                          50.7801\n",
      "time/total (s)                                        3457.11\n",
      "Epoch                                                   69\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:47:06.692120 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 70 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000740445\n",
      "trainer/QF2 Loss                                         0.00124124\n",
      "trainer/Policy Loss                                      3.2248\n",
      "trainer/Q1 Predictions Mean                             -1.1905\n",
      "trainer/Q1 Predictions Std                               0.937817\n",
      "trainer/Q1 Predictions Max                               0.648472\n",
      "trainer/Q1 Predictions Min                              -3.62208\n",
      "trainer/Q2 Predictions Mean                             -1.1949\n",
      "trainer/Q2 Predictions Std                               0.933977\n",
      "trainer/Q2 Predictions Max                               0.644928\n",
      "trainer/Q2 Predictions Min                              -3.62417\n",
      "trainer/Q Targets Mean                                  -1.19769\n",
      "trainer/Q Targets Std                                    0.937346\n",
      "trainer/Q Targets Max                                    0.632598\n",
      "trainer/Q Targets Min                                   -3.62652\n",
      "trainer/Log Pis Mean                                     2.06681\n",
      "trainer/Log Pis Std                                      1.39605\n",
      "trainer/Log Pis Max                                      4.61627\n",
      "trainer/Log Pis Min                                     -4.63062\n",
      "trainer/Policy mu Mean                                  -0.0232026\n",
      "trainer/Policy mu Std                                    0.288238\n",
      "trainer/Policy mu Max                                    1.69177\n",
      "trainer/Policy mu Min                                   -1.48109\n",
      "trainer/Policy log std Mean                             -2.37854\n",
      "trainer/Policy log std Std                               0.567448\n",
      "trainer/Policy log std Max                              -0.606399\n",
      "trainer/Policy log std Min                              -3.36688\n",
      "trainer/Alpha                                            0.0227153\n",
      "trainer/Alpha Loss                                       0.252937\n",
      "exploration/num steps total                           8100\n",
      "exploration/num paths total                            405\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0814054\n",
      "exploration/Rewards Std                                  0.0853408\n",
      "exploration/Rewards Max                                  0.0646248\n",
      "exploration/Rewards Min                                 -0.386321\n",
      "exploration/Returns Mean                                -1.62811\n",
      "exploration/Returns Std                                  0.979542\n",
      "exploration/Returns Max                                 -0.620122\n",
      "exploration/Returns Min                                 -3.2492\n",
      "exploration/Actions Mean                                -0.00653808\n",
      "exploration/Actions Std                                  0.129825\n",
      "exploration/Actions Max                                  0.322096\n",
      "exploration/Actions Min                                 -0.585202\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.62811\n",
      "exploration/env_infos/final/reward_dist Mean             0.0215539\n",
      "exploration/env_infos/final/reward_dist Std              0.0389861\n",
      "exploration/env_infos/final/reward_dist Max              0.0994407\n",
      "exploration/env_infos/final/reward_dist Min              1.09433e-21\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00265131\n",
      "exploration/env_infos/initial/reward_dist Std            0.00217704\n",
      "exploration/env_infos/initial/reward_dist Max            0.0065618\n",
      "exploration/env_infos/initial/reward_dist Min            0.00012041\n",
      "exploration/env_infos/reward_dist Mean                   0.0930214\n",
      "exploration/env_infos/reward_dist Std                    0.189052\n",
      "exploration/env_infos/reward_dist Max                    0.894516\n",
      "exploration/env_infos/reward_dist Min                    1.09433e-21\n",
      "exploration/env_infos/final/reward_energy Mean          -0.15889\n",
      "exploration/env_infos/final/reward_energy Std            0.0875147\n",
      "exploration/env_infos/final/reward_energy Max           -0.0544388\n",
      "exploration/env_infos/final/reward_energy Min           -0.278897\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.293352\n",
      "exploration/env_infos/initial/reward_energy Std          0.22578\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0454059\n",
      "exploration/env_infos/initial/reward_energy Min         -0.627839\n",
      "exploration/env_infos/reward_energy Mean                -0.132403\n",
      "exploration/env_infos/reward_energy Std                  0.127529\n",
      "exploration/env_infos/reward_energy Max                 -0.00528715\n",
      "exploration/env_infos/reward_energy Min                 -0.627839\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.169658\n",
      "exploration/env_infos/final/end_effector_loc Std         0.269109\n",
      "exploration/env_infos/final/end_effector_loc Max         0.138746\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.750845\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00577273\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0117459\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.013316\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0284278\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0917222\n",
      "exploration/env_infos/end_effector_loc Std               0.194528\n",
      "exploration/env_infos/end_effector_loc Max               0.24826\n",
      "exploration/env_infos/end_effector_loc Min              -0.750845\n",
      "evaluation/num steps total                           71000\n",
      "evaluation/num paths total                            3550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.037265\n",
      "evaluation/Rewards Std                                   0.0711602\n",
      "evaluation/Rewards Max                                   0.131332\n",
      "evaluation/Rewards Min                                  -0.406731\n",
      "evaluation/Returns Mean                                 -0.745301\n",
      "evaluation/Returns Std                                   1.09448\n",
      "evaluation/Returns Max                                   1.8221\n",
      "evaluation/Returns Min                                  -3.4152\n",
      "evaluation/Actions Mean                                  8.08568e-05\n",
      "evaluation/Actions Std                                   0.0670808\n",
      "evaluation/Actions Max                                   0.749085\n",
      "evaluation/Actions Min                                  -0.686547\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.745301\n",
      "evaluation/env_infos/final/reward_dist Mean              0.239921\n",
      "evaluation/env_infos/final/reward_dist Std               0.305884\n",
      "evaluation/env_infos/final/reward_dist Max               0.999096\n",
      "evaluation/env_infos/final/reward_dist Min               3.9552e-14\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00501537\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00780752\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0328681\n",
      "evaluation/env_infos/initial/reward_dist Min             3.45055e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.175894\n",
      "evaluation/env_infos/reward_dist Std                     0.253281\n",
      "evaluation/env_infos/reward_dist Max                     0.999233\n",
      "evaluation/env_infos/reward_dist Min                     3.9552e-14\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0276986\n",
      "evaluation/env_infos/final/reward_energy Std             0.021391\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00415611\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0892554\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.244925\n",
      "evaluation/env_infos/initial/reward_energy Std           0.228119\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0109452\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.806575\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0530278\n",
      "evaluation/env_infos/reward_energy Std                   0.0786623\n",
      "evaluation/env_infos/reward_energy Max                  -0.000603251\n",
      "evaluation/env_infos/reward_energy Min                  -0.806575\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0189222\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.204948\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.421139\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.488112\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00158072\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0117275\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0374542\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0343273\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0153829\n",
      "evaluation/env_infos/end_effector_loc Std                0.138587\n",
      "evaluation/env_infos/end_effector_loc Max                0.434081\n",
      "evaluation/env_infos/end_effector_loc Min               -0.488112\n",
      "time/data storing (s)                                    0.00591415\n",
      "time/evaluation sampling (s)                             0.942027\n",
      "time/exploration sampling (s)                            0.12481\n",
      "time/logging (s)                                         0.0189514\n",
      "time/saving (s)                                          0.0276381\n",
      "time/training (s)                                       49.2554\n",
      "time/epoch (s)                                          50.3748\n",
      "time/total (s)                                        3508.35\n",
      "Epoch                                                   70\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:47:59.672968 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 71 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00105194\n",
      "trainer/QF2 Loss                                         0.00118813\n",
      "trainer/Policy Loss                                      3.29066\n",
      "trainer/Q1 Predictions Mean                             -1.22228\n",
      "trainer/Q1 Predictions Std                               0.918679\n",
      "trainer/Q1 Predictions Max                               0.673504\n",
      "trainer/Q1 Predictions Min                              -3.56036\n",
      "trainer/Q2 Predictions Mean                             -1.23882\n",
      "trainer/Q2 Predictions Std                               0.924491\n",
      "trainer/Q2 Predictions Max                               0.673888\n",
      "trainer/Q2 Predictions Min                              -3.58614\n",
      "trainer/Q Targets Mean                                  -1.22133\n",
      "trainer/Q Targets Std                                    0.916095\n",
      "trainer/Q Targets Max                                    0.656484\n",
      "trainer/Q Targets Min                                   -3.56641\n",
      "trainer/Log Pis Mean                                     2.08877\n",
      "trainer/Log Pis Std                                      1.25761\n",
      "trainer/Log Pis Max                                      4.36889\n",
      "trainer/Log Pis Min                                     -2.41424\n",
      "trainer/Policy mu Mean                                  -0.0126979\n",
      "trainer/Policy mu Std                                    0.350848\n",
      "trainer/Policy mu Max                                    1.92226\n",
      "trainer/Policy mu Min                                   -2.31034\n",
      "trainer/Policy log std Mean                             -2.33852\n",
      "trainer/Policy log std Std                               0.62682\n",
      "trainer/Policy log std Max                              -0.209682\n",
      "trainer/Policy log std Min                              -3.29411\n",
      "trainer/Alpha                                            0.0244248\n",
      "trainer/Alpha Loss                                       0.329553\n",
      "exploration/num steps total                           8200\n",
      "exploration/num paths total                            410\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0477145\n",
      "exploration/Rewards Std                                  0.0661977\n",
      "exploration/Rewards Max                                  0.12666\n",
      "exploration/Rewards Min                                 -0.210022\n",
      "exploration/Returns Mean                                -0.95429\n",
      "exploration/Returns Std                                  1.07273\n",
      "exploration/Returns Max                                  0.640646\n",
      "exploration/Returns Min                                 -2.37701\n",
      "exploration/Actions Mean                                 0.0158785\n",
      "exploration/Actions Std                                  0.144265\n",
      "exploration/Actions Max                                  0.457846\n",
      "exploration/Actions Min                                 -0.870302\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.95429\n",
      "exploration/env_infos/final/reward_dist Mean             0.154384\n",
      "exploration/env_infos/final/reward_dist Std              0.144835\n",
      "exploration/env_infos/final/reward_dist Max              0.372057\n",
      "exploration/env_infos/final/reward_dist Min              5.45249e-23\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0264992\n",
      "exploration/env_infos/initial/reward_dist Std            0.0315581\n",
      "exploration/env_infos/initial/reward_dist Max            0.089135\n",
      "exploration/env_infos/initial/reward_dist Min            0.00477981\n",
      "exploration/env_infos/reward_dist Mean                   0.145667\n",
      "exploration/env_infos/reward_dist Std                    0.213958\n",
      "exploration/env_infos/reward_dist Max                    0.904871\n",
      "exploration/env_infos/reward_dist Min                    5.45249e-23\n",
      "exploration/env_infos/final/reward_energy Mean          -0.140016\n",
      "exploration/env_infos/final/reward_energy Std            0.0365297\n",
      "exploration/env_infos/final/reward_energy Max           -0.108621\n",
      "exploration/env_infos/final/reward_energy Min           -0.189611\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.342095\n",
      "exploration/env_infos/initial/reward_energy Std          0.289502\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0875669\n",
      "exploration/env_infos/initial/reward_energy Min         -0.899218\n",
      "exploration/env_infos/reward_energy Mean                -0.161832\n",
      "exploration/env_infos/reward_energy Std                  0.126251\n",
      "exploration/env_infos/reward_energy Max                 -0.0108688\n",
      "exploration/env_infos/reward_energy Min                 -0.899218\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.146976\n",
      "exploration/env_infos/final/end_effector_loc Std         0.212811\n",
      "exploration/env_infos/final/end_effector_loc Max         0.596162\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.150922\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00107815\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0158079\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0162154\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0435151\n",
      "exploration/env_infos/end_effector_loc Mean              0.0484854\n",
      "exploration/env_infos/end_effector_loc Std               0.151175\n",
      "exploration/env_infos/end_effector_loc Max               0.596162\n",
      "exploration/env_infos/end_effector_loc Min              -0.318036\n",
      "evaluation/num steps total                           72000\n",
      "evaluation/num paths total                            3600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0429823\n",
      "evaluation/Rewards Std                                   0.0758717\n",
      "evaluation/Rewards Max                                   0.134593\n",
      "evaluation/Rewards Min                                  -0.362035\n",
      "evaluation/Returns Mean                                 -0.859645\n",
      "evaluation/Returns Std                                   1.16623\n",
      "evaluation/Returns Max                                   1.92213\n",
      "evaluation/Returns Min                                  -3.14564\n",
      "evaluation/Actions Mean                                  0.0052421\n",
      "evaluation/Actions Std                                   0.0640518\n",
      "evaluation/Actions Max                                   0.504432\n",
      "evaluation/Actions Min                                  -0.745015\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.859645\n",
      "evaluation/env_infos/final/reward_dist Mean              0.163925\n",
      "evaluation/env_infos/final/reward_dist Std               0.241661\n",
      "evaluation/env_infos/final/reward_dist Max               0.898138\n",
      "evaluation/env_infos/final/reward_dist Min               7.71711e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00565724\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0095964\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0356544\n",
      "evaluation/env_infos/initial/reward_dist Min             9.52489e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.196571\n",
      "evaluation/env_infos/reward_dist Std                     0.273584\n",
      "evaluation/env_infos/reward_dist Max                     0.995964\n",
      "evaluation/env_infos/reward_dist Min                     7.71711e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0326406\n",
      "evaluation/env_infos/final/reward_energy Std             0.0317023\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00127948\n",
      "evaluation/env_infos/final/reward_energy Min            -0.207209\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.241362\n",
      "evaluation/env_infos/initial/reward_energy Std           0.203121\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0148206\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.761402\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0524787\n",
      "evaluation/env_infos/reward_energy Std                   0.0742039\n",
      "evaluation/env_infos/reward_energy Max                  -0.000579488\n",
      "evaluation/env_infos/reward_energy Min                  -0.761402\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0377808\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.230286\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.524489\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.469461\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00128018\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0110794\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0252216\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0372507\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00773606\n",
      "evaluation/env_infos/end_effector_loc Std                0.148162\n",
      "evaluation/env_infos/end_effector_loc Max                0.524489\n",
      "evaluation/env_infos/end_effector_loc Min               -0.469461\n",
      "time/data storing (s)                                    0.00615142\n",
      "time/evaluation sampling (s)                             0.968624\n",
      "time/exploration sampling (s)                            0.132655\n",
      "time/logging (s)                                         0.0202637\n",
      "time/saving (s)                                          0.0285706\n",
      "time/training (s)                                       50.8811\n",
      "time/epoch (s)                                          52.0374\n",
      "time/total (s)                                        3561.33\n",
      "Epoch                                                   71\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:48:52.952009 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 72 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00203907\n",
      "trainer/QF2 Loss                                         0.00110651\n",
      "trainer/Policy Loss                                      3.58704\n",
      "trainer/Q1 Predictions Mean                             -1.35396\n",
      "trainer/Q1 Predictions Std                               0.901364\n",
      "trainer/Q1 Predictions Max                               0.613782\n",
      "trainer/Q1 Predictions Min                              -3.6494\n",
      "trainer/Q2 Predictions Mean                             -1.35574\n",
      "trainer/Q2 Predictions Std                               0.8957\n",
      "trainer/Q2 Predictions Max                               0.604466\n",
      "trainer/Q2 Predictions Min                              -3.61608\n",
      "trainer/Q Targets Mean                                  -1.35517\n",
      "trainer/Q Targets Std                                    0.903811\n",
      "trainer/Q Targets Max                                    0.593633\n",
      "trainer/Q Targets Min                                   -3.63126\n",
      "trainer/Log Pis Mean                                     2.2694\n",
      "trainer/Log Pis Std                                      1.46753\n",
      "trainer/Log Pis Max                                      4.68375\n",
      "trainer/Log Pis Min                                     -3.89124\n",
      "trainer/Policy mu Mean                                  -0.0040709\n",
      "trainer/Policy mu Std                                    0.357316\n",
      "trainer/Policy mu Max                                    1.54054\n",
      "trainer/Policy mu Min                                   -1.50731\n",
      "trainer/Policy log std Mean                             -2.38166\n",
      "trainer/Policy log std Std                               0.684374\n",
      "trainer/Policy log std Max                              -0.373534\n",
      "trainer/Policy log std Min                              -3.3968\n",
      "trainer/Alpha                                            0.023129\n",
      "trainer/Alpha Loss                                       1.01499\n",
      "exploration/num steps total                           8300\n",
      "exploration/num paths total                            415\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0797587\n",
      "exploration/Rewards Std                                  0.0663624\n",
      "exploration/Rewards Max                                  0.0972\n",
      "exploration/Rewards Min                                 -0.308914\n",
      "exploration/Returns Mean                                -1.59517\n",
      "exploration/Returns Std                                  0.669224\n",
      "exploration/Returns Max                                 -0.41733\n",
      "exploration/Returns Min                                 -2.35501\n",
      "exploration/Actions Mean                                 0.00278791\n",
      "exploration/Actions Std                                  0.164992\n",
      "exploration/Actions Max                                  0.651813\n",
      "exploration/Actions Min                                 -0.626012\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.59517\n",
      "exploration/env_infos/final/reward_dist Mean             0.0157672\n",
      "exploration/env_infos/final/reward_dist Std              0.0219765\n",
      "exploration/env_infos/final/reward_dist Max              0.0593413\n",
      "exploration/env_infos/final/reward_dist Min              0.000543812\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000569659\n",
      "exploration/env_infos/initial/reward_dist Std            0.000938269\n",
      "exploration/env_infos/initial/reward_dist Max            0.00244454\n",
      "exploration/env_infos/initial/reward_dist Min            6.03688e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.159405\n",
      "exploration/env_infos/reward_dist Std                    0.230158\n",
      "exploration/env_infos/reward_dist Max                    0.968974\n",
      "exploration/env_infos/reward_dist Min                    6.03688e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.1575\n",
      "exploration/env_infos/final/reward_energy Std            0.0726524\n",
      "exploration/env_infos/final/reward_energy Max           -0.0678079\n",
      "exploration/env_infos/final/reward_energy Min           -0.288895\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.435846\n",
      "exploration/env_infos/initial/reward_energy Std          0.16088\n",
      "exploration/env_infos/initial/reward_energy Max         -0.188419\n",
      "exploration/env_infos/initial/reward_energy Min         -0.630787\n",
      "exploration/env_infos/reward_energy Mean                -0.187976\n",
      "exploration/env_infos/reward_energy Std                  0.138295\n",
      "exploration/env_infos/reward_energy Max                 -0.0143908\n",
      "exploration/env_infos/reward_energy Min                 -0.662879\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0427086\n",
      "exploration/env_infos/final/end_effector_loc Std         0.240704\n",
      "exploration/env_infos/final/end_effector_loc Max         0.432875\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.391139\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00777065\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0144714\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0161474\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0313006\n",
      "exploration/env_infos/end_effector_loc Mean             -0.042238\n",
      "exploration/env_infos/end_effector_loc Std               0.16094\n",
      "exploration/env_infos/end_effector_loc Max               0.432875\n",
      "exploration/env_infos/end_effector_loc Min              -0.391139\n",
      "evaluation/num steps total                           73000\n",
      "evaluation/num paths total                            3650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0434725\n",
      "evaluation/Rewards Std                                   0.0714693\n",
      "evaluation/Rewards Max                                   0.148178\n",
      "evaluation/Rewards Min                                  -0.376771\n",
      "evaluation/Returns Mean                                 -0.869449\n",
      "evaluation/Returns Std                                   1.04659\n",
      "evaluation/Returns Max                                   1.66349\n",
      "evaluation/Returns Min                                  -4.0235\n",
      "evaluation/Actions Mean                                  0.00258983\n",
      "evaluation/Actions Std                                   0.0627585\n",
      "evaluation/Actions Max                                   0.71219\n",
      "evaluation/Actions Min                                  -0.671002\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.869449\n",
      "evaluation/env_infos/final/reward_dist Mean              0.263271\n",
      "evaluation/env_infos/final/reward_dist Std               0.289422\n",
      "evaluation/env_infos/final/reward_dist Max               0.945987\n",
      "evaluation/env_infos/final/reward_dist Min               6.12488e-08\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00433166\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00882008\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0335203\n",
      "evaluation/env_infos/initial/reward_dist Min             3.06859e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.228942\n",
      "evaluation/env_infos/reward_dist Std                     0.281406\n",
      "evaluation/env_infos/reward_dist Max                     0.999293\n",
      "evaluation/env_infos/reward_dist Min                     6.12488e-08\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0356146\n",
      "evaluation/env_infos/final/reward_energy Std             0.0174326\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00150945\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0713217\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.231501\n",
      "evaluation/env_infos/initial/reward_energy Std           0.193734\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0237598\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.783714\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0524723\n",
      "evaluation/env_infos/reward_energy Std                   0.0716752\n",
      "evaluation/env_infos/reward_energy Max                  -0.000616586\n",
      "evaluation/env_infos/reward_energy Min                  -0.783714\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.018818\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23007\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.463597\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.427347\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000252688\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106697\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0356095\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0335501\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00553084\n",
      "evaluation/env_infos/end_effector_loc Std                0.150948\n",
      "evaluation/env_infos/end_effector_loc Max                0.463597\n",
      "evaluation/env_infos/end_effector_loc Min               -0.427347\n",
      "time/data storing (s)                                    0.0060765\n",
      "time/evaluation sampling (s)                             1.04938\n",
      "time/exploration sampling (s)                            0.119755\n",
      "time/logging (s)                                         0.0189535\n",
      "time/saving (s)                                          0.0273363\n",
      "time/training (s)                                       51.0993\n",
      "time/epoch (s)                                          52.3208\n",
      "time/total (s)                                        3614.61\n",
      "Epoch                                                   72\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:49:43.695251 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 73 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000806133\n",
      "trainer/QF2 Loss                                         0.000783424\n",
      "trainer/Policy Loss                                      3.22335\n",
      "trainer/Q1 Predictions Mean                             -1.23189\n",
      "trainer/Q1 Predictions Std                               0.828286\n",
      "trainer/Q1 Predictions Max                               0.405773\n",
      "trainer/Q1 Predictions Min                              -3.72635\n",
      "trainer/Q2 Predictions Mean                             -1.23478\n",
      "trainer/Q2 Predictions Std                               0.83192\n",
      "trainer/Q2 Predictions Max                               0.389598\n",
      "trainer/Q2 Predictions Min                              -3.71164\n",
      "trainer/Q Targets Mean                                  -1.24163\n",
      "trainer/Q Targets Std                                    0.833362\n",
      "trainer/Q Targets Max                                    0.393086\n",
      "trainer/Q Targets Min                                   -3.6925\n",
      "trainer/Log Pis Mean                                     2.0306\n",
      "trainer/Log Pis Std                                      1.50186\n",
      "trainer/Log Pis Max                                      4.40393\n",
      "trainer/Log Pis Min                                     -5.86245\n",
      "trainer/Policy mu Mean                                   0.0211938\n",
      "trainer/Policy mu Std                                    0.402302\n",
      "trainer/Policy mu Max                                    2.5229\n",
      "trainer/Policy mu Min                                   -1.8859\n",
      "trainer/Policy log std Mean                             -2.29462\n",
      "trainer/Policy log std Std                               0.673298\n",
      "trainer/Policy log std Max                              -0.17862\n",
      "trainer/Policy log std Min                              -3.31398\n",
      "trainer/Alpha                                            0.022204\n",
      "trainer/Alpha Loss                                       0.116514\n",
      "exploration/num steps total                           8400\n",
      "exploration/num paths total                            420\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.12934\n",
      "exploration/Rewards Std                                  0.0622869\n",
      "exploration/Rewards Max                                  0.0109897\n",
      "exploration/Rewards Min                                 -0.280624\n",
      "exploration/Returns Mean                                -2.5868\n",
      "exploration/Returns Std                                  0.972337\n",
      "exploration/Returns Max                                 -1.22575\n",
      "exploration/Returns Min                                 -4.03137\n",
      "exploration/Actions Mean                                 0.00952321\n",
      "exploration/Actions Std                                  0.123871\n",
      "exploration/Actions Max                                  0.369705\n",
      "exploration/Actions Min                                 -0.453797\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.5868\n",
      "exploration/env_infos/final/reward_dist Mean             0.0613293\n",
      "exploration/env_infos/final/reward_dist Std              0.119207\n",
      "exploration/env_infos/final/reward_dist Max              0.299712\n",
      "exploration/env_infos/final/reward_dist Min              1.51892e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0123453\n",
      "exploration/env_infos/initial/reward_dist Std            0.0243654\n",
      "exploration/env_infos/initial/reward_dist Max            0.0610753\n",
      "exploration/env_infos/initial/reward_dist Min            4.43703e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0883533\n",
      "exploration/env_infos/reward_dist Std                    0.153165\n",
      "exploration/env_infos/reward_dist Max                    0.624476\n",
      "exploration/env_infos/reward_dist Min                    1.51892e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135736\n",
      "exploration/env_infos/final/reward_energy Std            0.0862212\n",
      "exploration/env_infos/final/reward_energy Max           -0.075431\n",
      "exploration/env_infos/final/reward_energy Min           -0.305169\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.237579\n",
      "exploration/env_infos/initial/reward_energy Std          0.157601\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0288708\n",
      "exploration/env_infos/initial/reward_energy Min         -0.466267\n",
      "exploration/env_infos/reward_energy Mean                -0.143165\n",
      "exploration/env_infos/reward_energy Std                  0.101848\n",
      "exploration/env_infos/reward_energy Max                 -0.0177816\n",
      "exploration/env_infos/reward_energy Min                 -0.466267\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.106811\n",
      "exploration/env_infos/final/end_effector_loc Std         0.225585\n",
      "exploration/env_infos/final/end_effector_loc Max         0.446613\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.317589\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00243841\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0097804\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0105241\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0226899\n",
      "exploration/env_infos/end_effector_loc Mean              0.0331094\n",
      "exploration/env_infos/end_effector_loc Std               0.160288\n",
      "exploration/env_infos/end_effector_loc Max               0.446613\n",
      "exploration/env_infos/end_effector_loc Min              -0.379721\n",
      "evaluation/num steps total                           74000\n",
      "evaluation/num paths total                            3700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0292891\n",
      "evaluation/Rewards Std                                   0.0767467\n",
      "evaluation/Rewards Max                                   0.150599\n",
      "evaluation/Rewards Min                                  -0.403618\n",
      "evaluation/Returns Mean                                 -0.585782\n",
      "evaluation/Returns Std                                   1.12425\n",
      "evaluation/Returns Max                                   2.0678\n",
      "evaluation/Returns Min                                  -3.33476\n",
      "evaluation/Actions Mean                                  0.00571409\n",
      "evaluation/Actions Std                                   0.0770095\n",
      "evaluation/Actions Max                                   0.70886\n",
      "evaluation/Actions Min                                  -0.620107\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.585782\n",
      "evaluation/env_infos/final/reward_dist Mean              0.332346\n",
      "evaluation/env_infos/final/reward_dist Std               0.302437\n",
      "evaluation/env_infos/final/reward_dist Max               0.912312\n",
      "evaluation/env_infos/final/reward_dist Min               2.4528e-10\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00570299\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0115424\n",
      "evaluation/env_infos/initial/reward_dist Max             0.06256\n",
      "evaluation/env_infos/initial/reward_dist Min             1.71665e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.271971\n",
      "evaluation/env_infos/reward_dist Std                     0.305125\n",
      "evaluation/env_infos/reward_dist Max                     0.995979\n",
      "evaluation/env_infos/reward_dist Min                     2.4528e-10\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0656914\n",
      "evaluation/env_infos/final/reward_energy Std             0.0751436\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00703423\n",
      "evaluation/env_infos/final/reward_energy Min            -0.412681\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.29501\n",
      "evaluation/env_infos/initial/reward_energy Std           0.215771\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0242172\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.715126\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0653831\n",
      "evaluation/env_infos/reward_energy Std                   0.0874716\n",
      "evaluation/env_infos/reward_energy Max                  -0.00065501\n",
      "evaluation/env_infos/reward_energy Min                  -0.715126\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0533879\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.191203\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.519116\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.355925\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00119322\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0128671\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.035443\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0310054\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0241777\n",
      "evaluation/env_infos/end_effector_loc Std                0.140459\n",
      "evaluation/env_infos/end_effector_loc Max                0.519116\n",
      "evaluation/env_infos/end_effector_loc Min               -0.355925\n",
      "time/data storing (s)                                    0.00614967\n",
      "time/evaluation sampling (s)                             0.965363\n",
      "time/exploration sampling (s)                            0.123398\n",
      "time/logging (s)                                         0.0196946\n",
      "time/saving (s)                                          0.0274436\n",
      "time/training (s)                                       48.6433\n",
      "time/epoch (s)                                          49.7854\n",
      "time/total (s)                                        3665.35\n",
      "Epoch                                                   73\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:50:35.550715 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 74 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00249525\n",
      "trainer/QF2 Loss                                         0.00115505\n",
      "trainer/Policy Loss                                      3.23272\n",
      "trainer/Q1 Predictions Mean                             -1.21638\n",
      "trainer/Q1 Predictions Std                               0.778044\n",
      "trainer/Q1 Predictions Max                               0.297575\n",
      "trainer/Q1 Predictions Min                              -3.47568\n",
      "trainer/Q2 Predictions Mean                             -1.22741\n",
      "trainer/Q2 Predictions Std                               0.781095\n",
      "trainer/Q2 Predictions Max                               0.277683\n",
      "trainer/Q2 Predictions Min                              -3.49661\n",
      "trainer/Q Targets Mean                                  -1.22874\n",
      "trainer/Q Targets Std                                    0.78023\n",
      "trainer/Q Targets Max                                    0.274389\n",
      "trainer/Q Targets Min                                   -3.48379\n",
      "trainer/Log Pis Mean                                     2.03653\n",
      "trainer/Log Pis Std                                      1.3374\n",
      "trainer/Log Pis Max                                      4.81666\n",
      "trainer/Log Pis Min                                     -1.99818\n",
      "trainer/Policy mu Mean                                   0.0440313\n",
      "trainer/Policy mu Std                                    0.346454\n",
      "trainer/Policy mu Max                                    2.36416\n",
      "trainer/Policy mu Min                                   -1.44244\n",
      "trainer/Policy log std Mean                             -2.30928\n",
      "trainer/Policy log std Std                               0.568384\n",
      "trainer/Policy log std Max                              -0.56864\n",
      "trainer/Policy log std Min                              -3.35732\n",
      "trainer/Alpha                                            0.0230341\n",
      "trainer/Alpha Loss                                       0.137766\n",
      "exploration/num steps total                           8500\n",
      "exploration/num paths total                            425\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0569911\n",
      "exploration/Rewards Std                                  0.0806438\n",
      "exploration/Rewards Max                                  0.107956\n",
      "exploration/Rewards Min                                 -0.238819\n",
      "exploration/Returns Mean                                -1.13982\n",
      "exploration/Returns Std                                  1.21551\n",
      "exploration/Returns Max                                  0.460324\n",
      "exploration/Returns Min                                 -2.519\n",
      "exploration/Actions Mean                                -0.00525505\n",
      "exploration/Actions Std                                  0.199953\n",
      "exploration/Actions Max                                  0.596637\n",
      "exploration/Actions Min                                 -0.768763\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.13982\n",
      "exploration/env_infos/final/reward_dist Mean             0.329095\n",
      "exploration/env_infos/final/reward_dist Std              0.322878\n",
      "exploration/env_infos/final/reward_dist Max              0.861204\n",
      "exploration/env_infos/final/reward_dist Min              0.000495677\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000522519\n",
      "exploration/env_infos/initial/reward_dist Std            0.000770475\n",
      "exploration/env_infos/initial/reward_dist Max            0.00204766\n",
      "exploration/env_infos/initial/reward_dist Min            1.54387e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.237039\n",
      "exploration/env_infos/reward_dist Std                    0.32146\n",
      "exploration/env_infos/reward_dist Max                    0.957666\n",
      "exploration/env_infos/reward_dist Min                    1.21342e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.156382\n",
      "exploration/env_infos/final/reward_energy Std            0.0744943\n",
      "exploration/env_infos/final/reward_energy Max           -0.0557408\n",
      "exploration/env_infos/final/reward_energy Min           -0.263611\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.395558\n",
      "exploration/env_infos/initial/reward_energy Std          0.127964\n",
      "exploration/env_infos/initial/reward_energy Max         -0.192435\n",
      "exploration/env_infos/initial/reward_energy Min         -0.570217\n",
      "exploration/env_infos/reward_energy Mean                -0.235806\n",
      "exploration/env_infos/reward_energy Std                  0.156246\n",
      "exploration/env_infos/reward_energy Max                 -0.0207172\n",
      "exploration/env_infos/reward_energy Min                 -0.768836\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.082481\n",
      "exploration/env_infos/final/end_effector_loc Std         0.167306\n",
      "exploration/env_infos/final/end_effector_loc Max         0.322817\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.160154\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0099899\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0107821\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0279198\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00961659\n",
      "exploration/env_infos/end_effector_loc Mean              0.0761052\n",
      "exploration/env_infos/end_effector_loc Std               0.155549\n",
      "exploration/env_infos/end_effector_loc Max               0.444431\n",
      "exploration/env_infos/end_effector_loc Min              -0.167071\n",
      "evaluation/num steps total                           75000\n",
      "evaluation/num paths total                            3750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0423616\n",
      "evaluation/Rewards Std                                   0.0761716\n",
      "evaluation/Rewards Max                                   0.146965\n",
      "evaluation/Rewards Min                                  -0.513107\n",
      "evaluation/Returns Mean                                 -0.847232\n",
      "evaluation/Returns Std                                   1.15661\n",
      "evaluation/Returns Max                                   1.5479\n",
      "evaluation/Returns Min                                  -3.16723\n",
      "evaluation/Actions Mean                                  0.000129341\n",
      "evaluation/Actions Std                                   0.0708608\n",
      "evaluation/Actions Max                                   0.580463\n",
      "evaluation/Actions Min                                  -0.648864\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.847232\n",
      "evaluation/env_infos/final/reward_dist Mean              0.260324\n",
      "evaluation/env_infos/final/reward_dist Std               0.291887\n",
      "evaluation/env_infos/final/reward_dist Max               0.925933\n",
      "evaluation/env_infos/final/reward_dist Min               2.4377e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0080491\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0149861\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0625899\n",
      "evaluation/env_infos/initial/reward_dist Min             2.13698e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.223204\n",
      "evaluation/env_infos/reward_dist Std                     0.268199\n",
      "evaluation/env_infos/reward_dist Max                     0.990803\n",
      "evaluation/env_infos/reward_dist Min                     2.4377e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0311617\n",
      "evaluation/env_infos/final/reward_energy Std             0.0249822\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00198978\n",
      "evaluation/env_infos/final/reward_energy Min            -0.130694\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.277526\n",
      "evaluation/env_infos/initial/reward_energy Std           0.233553\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0119592\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.726338\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0531135\n",
      "evaluation/env_infos/reward_energy Std                   0.0849794\n",
      "evaluation/env_infos/reward_energy Max                  -0.000609295\n",
      "evaluation/env_infos/reward_energy Min                  -0.726338\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.030844\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.214154\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.567595\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.559294\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00132592\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127554\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0290232\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0324432\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.018532\n",
      "evaluation/env_infos/end_effector_loc Std                0.148326\n",
      "evaluation/env_infos/end_effector_loc Max                0.567595\n",
      "evaluation/env_infos/end_effector_loc Min               -0.559294\n",
      "time/data storing (s)                                    0.00612156\n",
      "time/evaluation sampling (s)                             0.975156\n",
      "time/exploration sampling (s)                            0.120781\n",
      "time/logging (s)                                         0.0201419\n",
      "time/saving (s)                                          0.0273629\n",
      "time/training (s)                                       49.7781\n",
      "time/epoch (s)                                          50.9277\n",
      "time/total (s)                                        3717.2\n",
      "Epoch                                                   74\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:51:26.834709 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 75 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000801156\n",
      "trainer/QF2 Loss                                         0.000679667\n",
      "trainer/Policy Loss                                      3.07374\n",
      "trainer/Q1 Predictions Mean                             -1.16483\n",
      "trainer/Q1 Predictions Std                               0.846426\n",
      "trainer/Q1 Predictions Max                               0.312031\n",
      "trainer/Q1 Predictions Min                              -3.57877\n",
      "trainer/Q2 Predictions Mean                             -1.15518\n",
      "trainer/Q2 Predictions Std                               0.843713\n",
      "trainer/Q2 Predictions Max                               0.319115\n",
      "trainer/Q2 Predictions Min                              -3.54158\n",
      "trainer/Q Targets Mean                                  -1.15649\n",
      "trainer/Q Targets Std                                    0.8429\n",
      "trainer/Q Targets Max                                    0.285059\n",
      "trainer/Q Targets Min                                   -3.5804\n",
      "trainer/Log Pis Mean                                     1.93247\n",
      "trainer/Log Pis Std                                      1.34371\n",
      "trainer/Log Pis Max                                      4.50373\n",
      "trainer/Log Pis Min                                     -2.90817\n",
      "trainer/Policy mu Mean                                   0.0390822\n",
      "trainer/Policy mu Std                                    0.311858\n",
      "trainer/Policy mu Max                                    2.27378\n",
      "trainer/Policy mu Min                                   -1.5767\n",
      "trainer/Policy log std Mean                             -2.32712\n",
      "trainer/Policy log std Std                               0.594704\n",
      "trainer/Policy log std Max                              -0.355377\n",
      "trainer/Policy log std Min                              -3.27906\n",
      "trainer/Alpha                                            0.0218766\n",
      "trainer/Alpha Loss                                      -0.258117\n",
      "exploration/num steps total                           8600\n",
      "exploration/num paths total                            430\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0359862\n",
      "exploration/Rewards Std                                  0.0751418\n",
      "exploration/Rewards Max                                  0.1234\n",
      "exploration/Rewards Min                                 -0.19682\n",
      "exploration/Returns Mean                                -0.719725\n",
      "exploration/Returns Std                                  1.24743\n",
      "exploration/Returns Max                                  1.3253\n",
      "exploration/Returns Min                                 -1.99298\n",
      "exploration/Actions Mean                                -0.00281195\n",
      "exploration/Actions Std                                  0.0981107\n",
      "exploration/Actions Max                                  0.360846\n",
      "exploration/Actions Min                                 -0.485509\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.719725\n",
      "exploration/env_infos/final/reward_dist Mean             0.0285022\n",
      "exploration/env_infos/final/reward_dist Std              0.0463461\n",
      "exploration/env_infos/final/reward_dist Max              0.120618\n",
      "exploration/env_infos/final/reward_dist Min              0.000348466\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00764244\n",
      "exploration/env_infos/initial/reward_dist Std            0.0062837\n",
      "exploration/env_infos/initial/reward_dist Max            0.0150581\n",
      "exploration/env_infos/initial/reward_dist Min            6.71052e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.210208\n",
      "exploration/env_infos/reward_dist Std                    0.253692\n",
      "exploration/env_infos/reward_dist Max                    0.904017\n",
      "exploration/env_infos/reward_dist Min                    6.71052e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.134822\n",
      "exploration/env_infos/final/reward_energy Std            0.079286\n",
      "exploration/env_infos/final/reward_energy Max           -0.0399667\n",
      "exploration/env_infos/final/reward_energy Min           -0.274274\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.242363\n",
      "exploration/env_infos/initial/reward_energy Std          0.160608\n",
      "exploration/env_infos/initial/reward_energy Max         -0.115736\n",
      "exploration/env_infos/initial/reward_energy Min         -0.54433\n",
      "exploration/env_infos/reward_energy Mean                -0.110381\n",
      "exploration/env_infos/reward_energy Std                  0.0841616\n",
      "exploration/env_infos/reward_energy Max                 -0.0027177\n",
      "exploration/env_infos/reward_energy Min                 -0.54433\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0883973\n",
      "exploration/env_infos/final/end_effector_loc Std         0.250657\n",
      "exploration/env_infos/final/end_effector_loc Max         0.293097\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.414758\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00444171\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00927035\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0132402\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0242755\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0417394\n",
      "exploration/env_infos/end_effector_loc Std               0.153266\n",
      "exploration/env_infos/end_effector_loc Max               0.303538\n",
      "exploration/env_infos/end_effector_loc Min              -0.414758\n",
      "evaluation/num steps total                           76000\n",
      "evaluation/num paths total                            3800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0283473\n",
      "evaluation/Rewards Std                                   0.0744621\n",
      "evaluation/Rewards Max                                   0.173437\n",
      "evaluation/Rewards Min                                  -0.449283\n",
      "evaluation/Returns Mean                                 -0.566946\n",
      "evaluation/Returns Std                                   1.06053\n",
      "evaluation/Returns Max                                   2.1372\n",
      "evaluation/Returns Min                                  -2.62536\n",
      "evaluation/Actions Mean                                  0.00387675\n",
      "evaluation/Actions Std                                   0.0744823\n",
      "evaluation/Actions Max                                   0.645614\n",
      "evaluation/Actions Min                                  -0.721518\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.566946\n",
      "evaluation/env_infos/final/reward_dist Mean              0.177107\n",
      "evaluation/env_infos/final/reward_dist Std               0.247138\n",
      "evaluation/env_infos/final/reward_dist Max               0.970728\n",
      "evaluation/env_infos/final/reward_dist Min               1.8316e-14\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00802419\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0151859\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0752677\n",
      "evaluation/env_infos/initial/reward_dist Min             1.90323e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.256436\n",
      "evaluation/env_infos/reward_dist Std                     0.301822\n",
      "evaluation/env_infos/reward_dist Max                     0.997697\n",
      "evaluation/env_infos/reward_dist Min                     1.8316e-14\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0436717\n",
      "evaluation/env_infos/final/reward_energy Std             0.0374292\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00319084\n",
      "evaluation/env_infos/final/reward_energy Min            -0.176448\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.285161\n",
      "evaluation/env_infos/initial/reward_energy Std           0.207109\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0195039\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.743322\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0623513\n",
      "evaluation/env_infos/reward_energy Std                   0.0850741\n",
      "evaluation/env_infos/reward_energy Max                  -0.00159326\n",
      "evaluation/env_infos/reward_energy Min                  -0.743322\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.010475\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.249449\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.638322\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.62457\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00188941\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123164\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0322807\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0360759\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00225352\n",
      "evaluation/env_infos/end_effector_loc Std                0.168793\n",
      "evaluation/env_infos/end_effector_loc Max                0.638322\n",
      "evaluation/env_infos/end_effector_loc Min               -0.62457\n",
      "time/data storing (s)                                    0.00648339\n",
      "time/evaluation sampling (s)                             0.962633\n",
      "time/exploration sampling (s)                            0.125045\n",
      "time/logging (s)                                         0.01887\n",
      "time/saving (s)                                          0.0315847\n",
      "time/training (s)                                       49.216\n",
      "time/epoch (s)                                          50.3606\n",
      "time/total (s)                                        3768.48\n",
      "Epoch                                                   75\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:52:18.307401 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 76 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00158635\n",
      "trainer/QF2 Loss                                         0.00254859\n",
      "trainer/Policy Loss                                      3.39167\n",
      "trainer/Q1 Predictions Mean                             -1.2074\n",
      "trainer/Q1 Predictions Std                               0.881948\n",
      "trainer/Q1 Predictions Max                               0.392986\n",
      "trainer/Q1 Predictions Min                              -3.47955\n",
      "trainer/Q2 Predictions Mean                             -1.21272\n",
      "trainer/Q2 Predictions Std                               0.885254\n",
      "trainer/Q2 Predictions Max                               0.364869\n",
      "trainer/Q2 Predictions Min                              -3.49519\n",
      "trainer/Q Targets Mean                                  -1.21095\n",
      "trainer/Q Targets Std                                    0.88516\n",
      "trainer/Q Targets Max                                    0.409387\n",
      "trainer/Q Targets Min                                   -3.49782\n",
      "trainer/Log Pis Mean                                     2.20797\n",
      "trainer/Log Pis Std                                      1.42314\n",
      "trainer/Log Pis Max                                      4.67562\n",
      "trainer/Log Pis Min                                     -3.15711\n",
      "trainer/Policy mu Mean                                   0.0537672\n",
      "trainer/Policy mu Std                                    0.389417\n",
      "trainer/Policy mu Max                                    2.4902\n",
      "trainer/Policy mu Min                                   -2.30952\n",
      "trainer/Policy log std Mean                             -2.36123\n",
      "trainer/Policy log std Std                               0.690802\n",
      "trainer/Policy log std Max                              -0.272028\n",
      "trainer/Policy log std Min                              -3.42283\n",
      "trainer/Alpha                                            0.0210984\n",
      "trainer/Alpha Loss                                       0.802562\n",
      "exploration/num steps total                           8700\n",
      "exploration/num paths total                            435\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.135896\n",
      "exploration/Rewards Std                                  0.0704115\n",
      "exploration/Rewards Max                                  0.0362389\n",
      "exploration/Rewards Min                                 -0.336301\n",
      "exploration/Returns Mean                                -2.71791\n",
      "exploration/Returns Std                                  0.942178\n",
      "exploration/Returns Max                                 -1.51146\n",
      "exploration/Returns Min                                 -3.95158\n",
      "exploration/Actions Mean                                 0.00829086\n",
      "exploration/Actions Std                                  0.167428\n",
      "exploration/Actions Max                                  0.911515\n",
      "exploration/Actions Min                                 -0.653444\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.71791\n",
      "exploration/env_infos/final/reward_dist Mean             0.0210574\n",
      "exploration/env_infos/final/reward_dist Std              0.0415459\n",
      "exploration/env_infos/final/reward_dist Max              0.104148\n",
      "exploration/env_infos/final/reward_dist Min              2.07124e-24\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0111181\n",
      "exploration/env_infos/initial/reward_dist Std            0.0150822\n",
      "exploration/env_infos/initial/reward_dist Max            0.0385141\n",
      "exploration/env_infos/initial/reward_dist Min            1.79304e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.141074\n",
      "exploration/env_infos/reward_dist Std                    0.25088\n",
      "exploration/env_infos/reward_dist Max                    0.991473\n",
      "exploration/env_infos/reward_dist Min                    7.50755e-25\n",
      "exploration/env_infos/final/reward_energy Mean          -0.236443\n",
      "exploration/env_infos/final/reward_energy Std            0.0752964\n",
      "exploration/env_infos/final/reward_energy Max           -0.119441\n",
      "exploration/env_infos/final/reward_energy Min           -0.351643\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.507234\n",
      "exploration/env_infos/initial/reward_energy Std          0.383537\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0731667\n",
      "exploration/env_infos/initial/reward_energy Min         -1.0532\n",
      "exploration/env_infos/reward_energy Mean                -0.175533\n",
      "exploration/env_infos/reward_energy Std                  0.159342\n",
      "exploration/env_infos/reward_energy Max                 -0.00983688\n",
      "exploration/env_infos/reward_energy Min                 -1.0532\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.064628\n",
      "exploration/env_infos/final/end_effector_loc Std         0.362832\n",
      "exploration/env_infos/final/end_effector_loc Max         0.822217\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.341633\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00219232\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0223758\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0455757\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0326722\n",
      "exploration/env_infos/end_effector_loc Mean              0.0140184\n",
      "exploration/env_infos/end_effector_loc Std               0.261301\n",
      "exploration/env_infos/end_effector_loc Max               0.825958\n",
      "exploration/env_infos/end_effector_loc Min              -0.44637\n",
      "evaluation/num steps total                           77000\n",
      "evaluation/num paths total                            3850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0388937\n",
      "evaluation/Rewards Std                                   0.0746453\n",
      "evaluation/Rewards Max                                   0.163021\n",
      "evaluation/Rewards Min                                  -0.523614\n",
      "evaluation/Returns Mean                                 -0.777875\n",
      "evaluation/Returns Std                                   1.08866\n",
      "evaluation/Returns Max                                   1.66865\n",
      "evaluation/Returns Min                                  -3.39181\n",
      "evaluation/Actions Mean                                  0.00263502\n",
      "evaluation/Actions Std                                   0.0658765\n",
      "evaluation/Actions Max                                   0.567238\n",
      "evaluation/Actions Min                                  -0.457359\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.777875\n",
      "evaluation/env_infos/final/reward_dist Mean              0.195894\n",
      "evaluation/env_infos/final/reward_dist Std               0.25966\n",
      "evaluation/env_infos/final/reward_dist Max               0.857648\n",
      "evaluation/env_infos/final/reward_dist Min               2.21638e-23\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00758687\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0125806\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0541804\n",
      "evaluation/env_infos/initial/reward_dist Min             2.21215e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.218387\n",
      "evaluation/env_infos/reward_dist Std                     0.275327\n",
      "evaluation/env_infos/reward_dist Max                     0.995904\n",
      "evaluation/env_infos/reward_dist Min                     2.21638e-23\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0465709\n",
      "evaluation/env_infos/final/reward_energy Std             0.0402913\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00349963\n",
      "evaluation/env_infos/final/reward_energy Min            -0.212943\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.252588\n",
      "evaluation/env_infos/initial/reward_energy Std           0.176537\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0212913\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.611768\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0586978\n",
      "evaluation/env_infos/reward_energy Std                   0.0724424\n",
      "evaluation/env_infos/reward_energy Max                  -0.000353679\n",
      "evaluation/env_infos/reward_energy Min                  -0.611768\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0235606\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.244576\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.790215\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.466934\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000774639\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0108677\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0283619\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.022868\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0088492\n",
      "evaluation/env_infos/end_effector_loc Std                0.164537\n",
      "evaluation/env_infos/end_effector_loc Max                0.790215\n",
      "evaluation/env_infos/end_effector_loc Min               -0.466934\n",
      "time/data storing (s)                                    0.00642746\n",
      "time/evaluation sampling (s)                             0.960538\n",
      "time/exploration sampling (s)                            0.129806\n",
      "time/logging (s)                                         0.0195395\n",
      "time/saving (s)                                          0.0304285\n",
      "time/training (s)                                       49.3392\n",
      "time/epoch (s)                                          50.4859\n",
      "time/total (s)                                        3819.96\n",
      "Epoch                                                   76\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:53:09.760689 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 77 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00115289\n",
      "trainer/QF2 Loss                                         0.00200745\n",
      "trainer/Policy Loss                                      3.20738\n",
      "trainer/Q1 Predictions Mean                             -1.28385\n",
      "trainer/Q1 Predictions Std                               0.819276\n",
      "trainer/Q1 Predictions Max                               0.326233\n",
      "trainer/Q1 Predictions Min                              -3.64869\n",
      "trainer/Q2 Predictions Mean                             -1.28508\n",
      "trainer/Q2 Predictions Std                               0.817298\n",
      "trainer/Q2 Predictions Max                               0.30126\n",
      "trainer/Q2 Predictions Min                              -3.64083\n",
      "trainer/Q Targets Mean                                  -1.28221\n",
      "trainer/Q Targets Std                                    0.823347\n",
      "trainer/Q Targets Max                                    0.333774\n",
      "trainer/Q Targets Min                                   -3.63985\n",
      "trainer/Log Pis Mean                                     1.95149\n",
      "trainer/Log Pis Std                                      1.40749\n",
      "trainer/Log Pis Max                                      4.80908\n",
      "trainer/Log Pis Min                                     -3.38247\n",
      "trainer/Policy mu Mean                                   0.0284362\n",
      "trainer/Policy mu Std                                    0.388496\n",
      "trainer/Policy mu Max                                    1.96102\n",
      "trainer/Policy mu Min                                   -1.4916\n",
      "trainer/Policy log std Mean                             -2.25909\n",
      "trainer/Policy log std Std                               0.630938\n",
      "trainer/Policy log std Max                              -0.36999\n",
      "trainer/Policy log std Min                              -3.38758\n",
      "trainer/Alpha                                            0.0200385\n",
      "trainer/Alpha Loss                                      -0.189686\n",
      "exploration/num steps total                           8800\n",
      "exploration/num paths total                            440\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0188355\n",
      "exploration/Rewards Std                                  0.0763507\n",
      "exploration/Rewards Max                                  0.10285\n",
      "exploration/Rewards Min                                 -0.38708\n",
      "exploration/Returns Mean                                -0.37671\n",
      "exploration/Returns Std                                  0.613506\n",
      "exploration/Returns Max                                  0.715167\n",
      "exploration/Returns Min                                 -1.1182\n",
      "exploration/Actions Mean                                 0.00566794\n",
      "exploration/Actions Std                                  0.0972567\n",
      "exploration/Actions Max                                  0.327327\n",
      "exploration/Actions Min                                 -0.447285\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.37671\n",
      "exploration/env_infos/final/reward_dist Mean             0.121282\n",
      "exploration/env_infos/final/reward_dist Std              0.239286\n",
      "exploration/env_infos/final/reward_dist Max              0.599847\n",
      "exploration/env_infos/final/reward_dist Min              1.69928e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.007287\n",
      "exploration/env_infos/initial/reward_dist Std            0.00734989\n",
      "exploration/env_infos/initial/reward_dist Max            0.0197745\n",
      "exploration/env_infos/initial/reward_dist Min            9.95028e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.258218\n",
      "exploration/env_infos/reward_dist Std                    0.284331\n",
      "exploration/env_infos/reward_dist Max                    0.919488\n",
      "exploration/env_infos/reward_dist Min                    9.95028e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.130184\n",
      "exploration/env_infos/final/reward_energy Std            0.0287176\n",
      "exploration/env_infos/final/reward_energy Max           -0.0841441\n",
      "exploration/env_infos/final/reward_energy Min           -0.162095\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.270444\n",
      "exploration/env_infos/initial/reward_energy Std          0.14\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0515584\n",
      "exploration/env_infos/initial/reward_energy Min         -0.480843\n",
      "exploration/env_infos/reward_energy Mean                -0.105892\n",
      "exploration/env_infos/reward_energy Std                  0.0881408\n",
      "exploration/env_infos/reward_energy Max                 -0.00580219\n",
      "exploration/env_infos/reward_energy Min                 -0.480843\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0402834\n",
      "exploration/env_infos/final/end_effector_loc Std         0.290418\n",
      "exploration/env_infos/final/end_effector_loc Max         0.441504\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.412144\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000227859\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0107644\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0123837\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0223642\n",
      "exploration/env_infos/end_effector_loc Mean              0.0216795\n",
      "exploration/env_infos/end_effector_loc Std               0.187341\n",
      "exploration/env_infos/end_effector_loc Max               0.441504\n",
      "exploration/env_infos/end_effector_loc Min              -0.412144\n",
      "evaluation/num steps total                           78000\n",
      "evaluation/num paths total                            3900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0553315\n",
      "evaluation/Rewards Std                                   0.0714524\n",
      "evaluation/Rewards Max                                   0.116271\n",
      "evaluation/Rewards Min                                  -0.396638\n",
      "evaluation/Returns Mean                                 -1.10663\n",
      "evaluation/Returns Std                                   0.928168\n",
      "evaluation/Returns Max                                   0.961958\n",
      "evaluation/Returns Min                                  -3.27174\n",
      "evaluation/Actions Mean                                  0.00286731\n",
      "evaluation/Actions Std                                   0.078204\n",
      "evaluation/Actions Max                                   0.508293\n",
      "evaluation/Actions Min                                  -0.774223\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.10663\n",
      "evaluation/env_infos/final/reward_dist Mean              0.122445\n",
      "evaluation/env_infos/final/reward_dist Std               0.211365\n",
      "evaluation/env_infos/final/reward_dist Max               0.926485\n",
      "evaluation/env_infos/final/reward_dist Min               1.33392e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0103561\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0120886\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0496234\n",
      "evaluation/env_infos/initial/reward_dist Min             1.40896e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.201157\n",
      "evaluation/env_infos/reward_dist Std                     0.25238\n",
      "evaluation/env_infos/reward_dist Max                     0.996598\n",
      "evaluation/env_infos/reward_dist Min                     1.33392e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0383613\n",
      "evaluation/env_infos/final/reward_energy Std             0.0281116\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00727415\n",
      "evaluation/env_infos/final/reward_energy Min            -0.126658\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.301877\n",
      "evaluation/env_infos/initial/reward_energy Std           0.246128\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0173254\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.896623\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0611916\n",
      "evaluation/env_infos/reward_energy Std                   0.0922158\n",
      "evaluation/env_infos/reward_energy Max                  -0.0015499\n",
      "evaluation/env_infos/reward_energy Min                  -0.896623\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0103918\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.269145\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.58969\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.653243\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00422437\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0131069\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0254146\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0387112\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0185867\n",
      "evaluation/env_infos/end_effector_loc Std                0.17482\n",
      "evaluation/env_infos/end_effector_loc Max                0.58969\n",
      "evaluation/env_infos/end_effector_loc Min               -0.653243\n",
      "time/data storing (s)                                    0.00635227\n",
      "time/evaluation sampling (s)                             1.05053\n",
      "time/exploration sampling (s)                            0.122257\n",
      "time/logging (s)                                         0.0197882\n",
      "time/saving (s)                                          0.0298267\n",
      "time/training (s)                                       49.2371\n",
      "time/epoch (s)                                          50.4658\n",
      "time/total (s)                                        3871.41\n",
      "Epoch                                                   77\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:54:01.305961 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 78 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00112282\n",
      "trainer/QF2 Loss                                         0.00102611\n",
      "trainer/Policy Loss                                      3.10641\n",
      "trainer/Q1 Predictions Mean                             -1.23407\n",
      "trainer/Q1 Predictions Std                               0.934416\n",
      "trainer/Q1 Predictions Max                               0.584456\n",
      "trainer/Q1 Predictions Min                              -4.15263\n",
      "trainer/Q2 Predictions Mean                             -1.22129\n",
      "trainer/Q2 Predictions Std                               0.927774\n",
      "trainer/Q2 Predictions Max                               0.701715\n",
      "trainer/Q2 Predictions Min                              -4.12529\n",
      "trainer/Q Targets Mean                                  -1.22801\n",
      "trainer/Q Targets Std                                    0.920335\n",
      "trainer/Q Targets Max                                    0.517513\n",
      "trainer/Q Targets Min                                   -4.10349\n",
      "trainer/Log Pis Mean                                     1.92057\n",
      "trainer/Log Pis Std                                      1.48581\n",
      "trainer/Log Pis Max                                      4.82259\n",
      "trainer/Log Pis Min                                     -3.93736\n",
      "trainer/Policy mu Mean                                   0.0280391\n",
      "trainer/Policy mu Std                                    0.4038\n",
      "trainer/Policy mu Max                                    1.99695\n",
      "trainer/Policy mu Min                                   -2.12258\n",
      "trainer/Policy log std Mean                             -2.22829\n",
      "trainer/Policy log std Std                               0.670325\n",
      "trainer/Policy log std Max                              -0.571095\n",
      "trainer/Policy log std Min                              -3.4102\n",
      "trainer/Alpha                                            0.0210596\n",
      "trainer/Alpha Loss                                      -0.306638\n",
      "exploration/num steps total                           8900\n",
      "exploration/num paths total                            445\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.007041\n",
      "exploration/Rewards Std                                  0.0925463\n",
      "exploration/Rewards Max                                  0.158804\n",
      "exploration/Rewards Min                                 -0.227081\n",
      "exploration/Returns Mean                                -0.14082\n",
      "exploration/Returns Std                                  1.49489\n",
      "exploration/Returns Max                                  1.64165\n",
      "exploration/Returns Min                                 -2.08335\n",
      "exploration/Actions Mean                                 0.00321455\n",
      "exploration/Actions Std                                  0.158992\n",
      "exploration/Actions Max                                  0.876447\n",
      "exploration/Actions Min                                 -0.582274\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.14082\n",
      "exploration/env_infos/final/reward_dist Mean             0.062752\n",
      "exploration/env_infos/final/reward_dist Std              0.096094\n",
      "exploration/env_infos/final/reward_dist Max              0.252441\n",
      "exploration/env_infos/final/reward_dist Min              7.84959e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00734006\n",
      "exploration/env_infos/initial/reward_dist Std            0.00758424\n",
      "exploration/env_infos/initial/reward_dist Max            0.0167297\n",
      "exploration/env_infos/initial/reward_dist Min            7.83941e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.297249\n",
      "exploration/env_infos/reward_dist Std                    0.314557\n",
      "exploration/env_infos/reward_dist Max                    0.998196\n",
      "exploration/env_infos/reward_dist Min                    7.83941e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.219606\n",
      "exploration/env_infos/final/reward_energy Std            0.185214\n",
      "exploration/env_infos/final/reward_energy Max           -0.0211485\n",
      "exploration/env_infos/final/reward_energy Min           -0.538784\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.293736\n",
      "exploration/env_infos/initial/reward_energy Std          0.182159\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0912179\n",
      "exploration/env_infos/initial/reward_energy Min         -0.628851\n",
      "exploration/env_infos/reward_energy Mean                -0.166147\n",
      "exploration/env_infos/reward_energy Std                  0.151568\n",
      "exploration/env_infos/reward_energy Max                 -0.00294685\n",
      "exploration/env_infos/reward_energy Min                 -0.881474\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0556377\n",
      "exploration/env_infos/final/end_effector_loc Std         0.248346\n",
      "exploration/env_infos/final/end_effector_loc Max         0.255628\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.419011\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00349619\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0117092\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0123665\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0291137\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0332648\n",
      "exploration/env_infos/end_effector_loc Std               0.165968\n",
      "exploration/env_infos/end_effector_loc Max               0.255628\n",
      "exploration/env_infos/end_effector_loc Min              -0.419011\n",
      "evaluation/num steps total                           79000\n",
      "evaluation/num paths total                            3950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0448703\n",
      "evaluation/Rewards Std                                   0.0715749\n",
      "evaluation/Rewards Max                                   0.124368\n",
      "evaluation/Rewards Min                                  -0.308353\n",
      "evaluation/Returns Mean                                 -0.897405\n",
      "evaluation/Returns Std                                   0.998748\n",
      "evaluation/Returns Max                                   1.27352\n",
      "evaluation/Returns Min                                  -2.76986\n",
      "evaluation/Actions Mean                                  0.0014345\n",
      "evaluation/Actions Std                                   0.0641545\n",
      "evaluation/Actions Max                                   0.764892\n",
      "evaluation/Actions Min                                  -0.555003\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.897405\n",
      "evaluation/env_infos/final/reward_dist Mean              0.179576\n",
      "evaluation/env_infos/final/reward_dist Std               0.238684\n",
      "evaluation/env_infos/final/reward_dist Max               0.87216\n",
      "evaluation/env_infos/final/reward_dist Min               1.3409e-13\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00650019\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0123906\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0563801\n",
      "evaluation/env_infos/initial/reward_dist Min             1.32567e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.198536\n",
      "evaluation/env_infos/reward_dist Std                     0.265776\n",
      "evaluation/env_infos/reward_dist Max                     0.997769\n",
      "evaluation/env_infos/reward_dist Min                     1.3409e-13\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0329318\n",
      "evaluation/env_infos/final/reward_energy Std             0.0201254\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00359043\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0740779\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.239874\n",
      "evaluation/env_infos/initial/reward_energy Std           0.201862\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0187697\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.838223\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0532128\n",
      "evaluation/env_infos/reward_energy Std                   0.0735126\n",
      "evaluation/env_infos/reward_energy Max                  -0.000403096\n",
      "evaluation/env_infos/reward_energy Min                  -0.838223\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0121983\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.239932\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.466572\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.54997\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000641858\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0110656\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0382446\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0277502\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00544337\n",
      "evaluation/env_infos/end_effector_loc Std                0.157971\n",
      "evaluation/env_infos/end_effector_loc Max                0.466572\n",
      "evaluation/env_infos/end_effector_loc Min               -0.54997\n",
      "time/data storing (s)                                    0.00614756\n",
      "time/evaluation sampling (s)                             0.958045\n",
      "time/exploration sampling (s)                            0.122614\n",
      "time/logging (s)                                         0.0188266\n",
      "time/saving (s)                                          0.0273797\n",
      "time/training (s)                                       49.3651\n",
      "time/epoch (s)                                          50.4981\n",
      "time/total (s)                                        3922.95\n",
      "Epoch                                                   78\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:54:52.788493 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 79 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00108862\n",
      "trainer/QF2 Loss                                         0.00132941\n",
      "trainer/Policy Loss                                      3.12971\n",
      "trainer/Q1 Predictions Mean                             -1.15366\n",
      "trainer/Q1 Predictions Std                               0.836161\n",
      "trainer/Q1 Predictions Max                               0.707018\n",
      "trainer/Q1 Predictions Min                              -3.53441\n",
      "trainer/Q2 Predictions Mean                             -1.15915\n",
      "trainer/Q2 Predictions Std                               0.837803\n",
      "trainer/Q2 Predictions Max                               0.700341\n",
      "trainer/Q2 Predictions Min                              -3.57062\n",
      "trainer/Q Targets Mean                                  -1.16347\n",
      "trainer/Q Targets Std                                    0.839902\n",
      "trainer/Q Targets Max                                    0.683447\n",
      "trainer/Q Targets Min                                   -3.61218\n",
      "trainer/Log Pis Mean                                     2.01827\n",
      "trainer/Log Pis Std                                      1.28416\n",
      "trainer/Log Pis Max                                      4.43253\n",
      "trainer/Log Pis Min                                     -2.30807\n",
      "trainer/Policy mu Mean                                   0.0929125\n",
      "trainer/Policy mu Std                                    0.438768\n",
      "trainer/Policy mu Max                                    2.00255\n",
      "trainer/Policy mu Min                                   -2.17196\n",
      "trainer/Policy log std Mean                             -2.2406\n",
      "trainer/Policy log std Std                               0.657478\n",
      "trainer/Policy log std Max                              -0.400571\n",
      "trainer/Policy log std Min                              -3.32607\n",
      "trainer/Alpha                                            0.021302\n",
      "trainer/Alpha Loss                                       0.0703174\n",
      "exploration/num steps total                           9000\n",
      "exploration/num paths total                            450\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0894132\n",
      "exploration/Rewards Std                                  0.0464199\n",
      "exploration/Rewards Max                                  0.0138742\n",
      "exploration/Rewards Min                                 -0.226194\n",
      "exploration/Returns Mean                                -1.78826\n",
      "exploration/Returns Std                                  0.450297\n",
      "exploration/Returns Max                                 -1.08859\n",
      "exploration/Returns Min                                 -2.3649\n",
      "exploration/Actions Mean                                -0.000422304\n",
      "exploration/Actions Std                                  0.0786785\n",
      "exploration/Actions Max                                  0.346699\n",
      "exploration/Actions Min                                 -0.321792\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.78826\n",
      "exploration/env_infos/final/reward_dist Mean             0.248766\n",
      "exploration/env_infos/final/reward_dist Std              0.325618\n",
      "exploration/env_infos/final/reward_dist Max              0.809971\n",
      "exploration/env_infos/final/reward_dist Min              1.37719e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00114747\n",
      "exploration/env_infos/initial/reward_dist Std            0.000860833\n",
      "exploration/env_infos/initial/reward_dist Max            0.00228366\n",
      "exploration/env_infos/initial/reward_dist Min            2.94566e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.229676\n",
      "exploration/env_infos/reward_dist Std                    0.314539\n",
      "exploration/env_infos/reward_dist Max                    0.964968\n",
      "exploration/env_infos/reward_dist Min                    1.37719e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0996253\n",
      "exploration/env_infos/final/reward_energy Std            0.0671409\n",
      "exploration/env_infos/final/reward_energy Max           -0.0100884\n",
      "exploration/env_infos/final/reward_energy Min           -0.193603\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.168563\n",
      "exploration/env_infos/initial/reward_energy Std          0.123227\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0296055\n",
      "exploration/env_infos/initial/reward_energy Min         -0.361976\n",
      "exploration/env_infos/reward_energy Mean                -0.0913038\n",
      "exploration/env_infos/reward_energy Std                  0.0635972\n",
      "exploration/env_infos/reward_energy Max                 -0.00596792\n",
      "exploration/env_infos/reward_energy Min                 -0.361976\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0501825\n",
      "exploration/env_infos/final/end_effector_loc Std         0.264837\n",
      "exploration/env_infos/final/end_effector_loc Max         0.513534\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.378301\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00250194\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00694538\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0173349\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0110001\n",
      "exploration/env_infos/end_effector_loc Mean              0.0364171\n",
      "exploration/env_infos/end_effector_loc Std               0.153491\n",
      "exploration/env_infos/end_effector_loc Max               0.513534\n",
      "exploration/env_infos/end_effector_loc Min              -0.378301\n",
      "evaluation/num steps total                           80000\n",
      "evaluation/num paths total                            4000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0457434\n",
      "evaluation/Rewards Std                                   0.0698893\n",
      "evaluation/Rewards Max                                   0.172115\n",
      "evaluation/Rewards Min                                  -0.436871\n",
      "evaluation/Returns Mean                                 -0.914867\n",
      "evaluation/Returns Std                                   0.995065\n",
      "evaluation/Returns Max                                   1.79228\n",
      "evaluation/Returns Min                                  -3.40989\n",
      "evaluation/Actions Mean                                 -0.00441688\n",
      "evaluation/Actions Std                                   0.0751083\n",
      "evaluation/Actions Max                                   0.853372\n",
      "evaluation/Actions Min                                  -0.857846\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.914867\n",
      "evaluation/env_infos/final/reward_dist Mean              0.13984\n",
      "evaluation/env_infos/final/reward_dist Std               0.218677\n",
      "evaluation/env_infos/final/reward_dist Max               0.872702\n",
      "evaluation/env_infos/final/reward_dist Min               1.02766e-13\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00929417\n",
      "evaluation/env_infos/initial/reward_dist Std             0.015164\n",
      "evaluation/env_infos/initial/reward_dist Max             0.059225\n",
      "evaluation/env_infos/initial/reward_dist Min             1.06797e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.208254\n",
      "evaluation/env_infos/reward_dist Std                     0.275031\n",
      "evaluation/env_infos/reward_dist Max                     0.99519\n",
      "evaluation/env_infos/reward_dist Min                     1.02766e-13\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0311672\n",
      "evaluation/env_infos/final/reward_energy Std             0.022733\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0055508\n",
      "evaluation/env_infos/final/reward_energy Min            -0.102023\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.289939\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255276\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0127539\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.886644\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0570468\n",
      "evaluation/env_infos/reward_energy Std                   0.0898176\n",
      "evaluation/env_infos/reward_energy Max                  -0.00200308\n",
      "evaluation/env_infos/reward_energy Min                  -0.886644\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0658771\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.22932\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.408648\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.583764\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000638074\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.013643\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0426686\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0428923\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.028706\n",
      "evaluation/env_infos/end_effector_loc Std                0.153348\n",
      "evaluation/env_infos/end_effector_loc Max                0.408648\n",
      "evaluation/env_infos/end_effector_loc Min               -0.583764\n",
      "time/data storing (s)                                    0.00647429\n",
      "time/evaluation sampling (s)                             0.958433\n",
      "time/exploration sampling (s)                            0.120616\n",
      "time/logging (s)                                         0.0197746\n",
      "time/saving (s)                                          0.0276202\n",
      "time/training (s)                                       49.38\n",
      "time/epoch (s)                                          50.5129\n",
      "time/total (s)                                        3974.43\n",
      "Epoch                                                   79\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:55:44.640218 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 80 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00118497\n",
      "trainer/QF2 Loss                                         0.000769386\n",
      "trainer/Policy Loss                                      3.16145\n",
      "trainer/Q1 Predictions Mean                             -1.25917\n",
      "trainer/Q1 Predictions Std                               0.88522\n",
      "trainer/Q1 Predictions Max                               0.482569\n",
      "trainer/Q1 Predictions Min                              -3.70019\n",
      "trainer/Q2 Predictions Mean                             -1.25554\n",
      "trainer/Q2 Predictions Std                               0.88542\n",
      "trainer/Q2 Predictions Max                               0.482087\n",
      "trainer/Q2 Predictions Min                              -3.67869\n",
      "trainer/Q Targets Mean                                  -1.24843\n",
      "trainer/Q Targets Std                                    0.881595\n",
      "trainer/Q Targets Max                                    0.450391\n",
      "trainer/Q Targets Min                                   -3.70701\n",
      "trainer/Log Pis Mean                                     1.95027\n",
      "trainer/Log Pis Std                                      1.34122\n",
      "trainer/Log Pis Max                                      4.51569\n",
      "trainer/Log Pis Min                                     -2.40281\n",
      "trainer/Policy mu Mean                                   0.0384644\n",
      "trainer/Policy mu Std                                    0.379984\n",
      "trainer/Policy mu Max                                    1.96366\n",
      "trainer/Policy mu Min                                   -2.03092\n",
      "trainer/Policy log std Mean                             -2.27422\n",
      "trainer/Policy log std Std                               0.62086\n",
      "trainer/Policy log std Max                              -0.279904\n",
      "trainer/Policy log std Min                              -3.25952\n",
      "trainer/Alpha                                            0.0221582\n",
      "trainer/Alpha Loss                                      -0.18943\n",
      "exploration/num steps total                           9100\n",
      "exploration/num paths total                            455\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0987029\n",
      "exploration/Rewards Std                                  0.0712281\n",
      "exploration/Rewards Max                                  0.0572412\n",
      "exploration/Rewards Min                                 -0.329928\n",
      "exploration/Returns Mean                                -1.97406\n",
      "exploration/Returns Std                                  0.883105\n",
      "exploration/Returns Max                                 -1.07695\n",
      "exploration/Returns Min                                 -3.17607\n",
      "exploration/Actions Mean                                 0.00105555\n",
      "exploration/Actions Std                                  0.0811975\n",
      "exploration/Actions Max                                  0.312887\n",
      "exploration/Actions Min                                 -0.303607\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.97406\n",
      "exploration/env_infos/final/reward_dist Mean             0.0806738\n",
      "exploration/env_infos/final/reward_dist Std              0.102967\n",
      "exploration/env_infos/final/reward_dist Max              0.27226\n",
      "exploration/env_infos/final/reward_dist Min              1.45682e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000883761\n",
      "exploration/env_infos/initial/reward_dist Std            0.000982118\n",
      "exploration/env_infos/initial/reward_dist Max            0.00231436\n",
      "exploration/env_infos/initial/reward_dist Min            1.07781e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.107962\n",
      "exploration/env_infos/reward_dist Std                    0.188743\n",
      "exploration/env_infos/reward_dist Max                    0.848193\n",
      "exploration/env_infos/reward_dist Min                    1.07781e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0909446\n",
      "exploration/env_infos/final/reward_energy Std            0.0365556\n",
      "exploration/env_infos/final/reward_energy Max           -0.0362712\n",
      "exploration/env_infos/final/reward_energy Min           -0.123689\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.178512\n",
      "exploration/env_infos/initial/reward_energy Std          0.122834\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0515218\n",
      "exploration/env_infos/initial/reward_energy Min         -0.336479\n",
      "exploration/env_infos/reward_energy Mean                -0.0953092\n",
      "exploration/env_infos/reward_energy Std                  0.0640659\n",
      "exploration/env_infos/reward_energy Max                 -0.00773428\n",
      "exploration/env_infos/reward_energy Min                 -0.336479\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0133222\n",
      "exploration/env_infos/final/end_effector_loc Std         0.22525\n",
      "exploration/env_infos/final/end_effector_loc Max         0.330169\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.359379\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00104777\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00758919\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0156444\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0151803\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00122511\n",
      "exploration/env_infos/end_effector_loc Std               0.142129\n",
      "exploration/env_infos/end_effector_loc Max               0.330169\n",
      "exploration/env_infos/end_effector_loc Min              -0.359379\n",
      "evaluation/num steps total                           81000\n",
      "evaluation/num paths total                            4050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0399715\n",
      "evaluation/Rewards Std                                   0.0843433\n",
      "evaluation/Rewards Max                                   0.162101\n",
      "evaluation/Rewards Min                                  -0.416127\n",
      "evaluation/Returns Mean                                 -0.79943\n",
      "evaluation/Returns Std                                   1.3376\n",
      "evaluation/Returns Max                                   1.72343\n",
      "evaluation/Returns Min                                  -3.43488\n",
      "evaluation/Actions Mean                                  0.0011736\n",
      "evaluation/Actions Std                                   0.078501\n",
      "evaluation/Actions Max                                   0.788552\n",
      "evaluation/Actions Min                                  -0.734922\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.79943\n",
      "evaluation/env_infos/final/reward_dist Mean              0.278374\n",
      "evaluation/env_infos/final/reward_dist Std               0.291154\n",
      "evaluation/env_infos/final/reward_dist Max               0.895931\n",
      "evaluation/env_infos/final/reward_dist Min               1.84815e-31\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00794191\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0138255\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0740953\n",
      "evaluation/env_infos/initial/reward_dist Min             4.55178e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.306087\n",
      "evaluation/env_infos/reward_dist Std                     0.319191\n",
      "evaluation/env_infos/reward_dist Max                     0.999754\n",
      "evaluation/env_infos/reward_dist Min                     1.84815e-31\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.04376\n",
      "evaluation/env_infos/final/reward_energy Std             0.0416648\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00431031\n",
      "evaluation/env_infos/final/reward_energy Min            -0.228632\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.300562\n",
      "evaluation/env_infos/initial/reward_energy Std           0.252266\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00605428\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.997225\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0623022\n",
      "evaluation/env_infos/reward_energy Std                   0.0919022\n",
      "evaluation/env_infos/reward_energy Max                  -0.000925155\n",
      "evaluation/env_infos/reward_energy Min                  -0.997225\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00109292\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.223106\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.781436\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.507554\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000961411\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.01384\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0394276\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0367461\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00458159\n",
      "evaluation/env_infos/end_effector_loc Std                0.154348\n",
      "evaluation/env_infos/end_effector_loc Max                0.781436\n",
      "evaluation/env_infos/end_effector_loc Min               -0.507554\n",
      "time/data storing (s)                                    0.00637663\n",
      "time/evaluation sampling (s)                             0.944802\n",
      "time/exploration sampling (s)                            0.121846\n",
      "time/logging (s)                                         0.0193146\n",
      "time/saving (s)                                          0.0263919\n",
      "time/training (s)                                       49.6716\n",
      "time/epoch (s)                                          50.7903\n",
      "time/total (s)                                        4026.28\n",
      "Epoch                                                   80\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:56:36.451613 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 81 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00123253\n",
      "trainer/QF2 Loss                                         0.000957539\n",
      "trainer/Policy Loss                                      3.0359\n",
      "trainer/Q1 Predictions Mean                             -1.12634\n",
      "trainer/Q1 Predictions Std                               0.844112\n",
      "trainer/Q1 Predictions Max                               0.885433\n",
      "trainer/Q1 Predictions Min                              -3.59039\n",
      "trainer/Q2 Predictions Mean                             -1.12369\n",
      "trainer/Q2 Predictions Std                               0.84137\n",
      "trainer/Q2 Predictions Max                               0.85952\n",
      "trainer/Q2 Predictions Min                              -3.61817\n",
      "trainer/Q Targets Mean                                  -1.11833\n",
      "trainer/Q Targets Std                                    0.844478\n",
      "trainer/Q Targets Max                                    0.905784\n",
      "trainer/Q Targets Min                                   -3.63388\n",
      "trainer/Log Pis Mean                                     1.94734\n",
      "trainer/Log Pis Std                                      1.44072\n",
      "trainer/Log Pis Max                                      4.61727\n",
      "trainer/Log Pis Min                                     -4.56838\n",
      "trainer/Policy mu Mean                                   0.0421179\n",
      "trainer/Policy mu Std                                    0.415029\n",
      "trainer/Policy mu Max                                    1.81251\n",
      "trainer/Policy mu Min                                   -1.88923\n",
      "trainer/Policy log std Mean                             -2.23508\n",
      "trainer/Policy log std Std                               0.665985\n",
      "trainer/Policy log std Max                              -0.399036\n",
      "trainer/Policy log std Min                              -3.28378\n",
      "trainer/Alpha                                            0.0223862\n",
      "trainer/Alpha Loss                                      -0.199996\n",
      "exploration/num steps total                           9200\n",
      "exploration/num paths total                            460\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0789217\n",
      "exploration/Rewards Std                                  0.077375\n",
      "exploration/Rewards Max                                  0.0986343\n",
      "exploration/Rewards Min                                 -0.249873\n",
      "exploration/Returns Mean                                -1.57843\n",
      "exploration/Returns Std                                  1.00553\n",
      "exploration/Returns Max                                  0.101055\n",
      "exploration/Returns Min                                 -2.80316\n",
      "exploration/Actions Mean                                -0.00478884\n",
      "exploration/Actions Std                                  0.163611\n",
      "exploration/Actions Max                                  0.642568\n",
      "exploration/Actions Min                                 -0.833816\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.57843\n",
      "exploration/env_infos/final/reward_dist Mean             0.224277\n",
      "exploration/env_infos/final/reward_dist Std              0.370558\n",
      "exploration/env_infos/final/reward_dist Max              0.961577\n",
      "exploration/env_infos/final/reward_dist Min              9.60199e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00360291\n",
      "exploration/env_infos/initial/reward_dist Std            0.00526173\n",
      "exploration/env_infos/initial/reward_dist Max            0.013552\n",
      "exploration/env_infos/initial/reward_dist Min            3.59217e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.233519\n",
      "exploration/env_infos/reward_dist Std                    0.291032\n",
      "exploration/env_infos/reward_dist Max                    0.961577\n",
      "exploration/env_infos/reward_dist Min                    9.60199e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.143987\n",
      "exploration/env_infos/final/reward_energy Std            0.0706867\n",
      "exploration/env_infos/final/reward_energy Max           -0.0273083\n",
      "exploration/env_infos/final/reward_energy Min           -0.24092\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.268023\n",
      "exploration/env_infos/initial/reward_energy Std          0.180559\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0890067\n",
      "exploration/env_infos/initial/reward_energy Min         -0.521311\n",
      "exploration/env_infos/reward_energy Mean                -0.174561\n",
      "exploration/env_infos/reward_energy Std                  0.152026\n",
      "exploration/env_infos/reward_energy Max                 -0.00705695\n",
      "exploration/env_infos/reward_energy Min                 -0.890501\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.101455\n",
      "exploration/env_infos/final/end_effector_loc Std         0.263439\n",
      "exploration/env_infos/final/end_effector_loc Max         0.377548\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.484904\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00241432\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0111677\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0252183\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00669702\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0427475\n",
      "exploration/env_infos/end_effector_loc Std               0.167546\n",
      "exploration/env_infos/end_effector_loc Max               0.377548\n",
      "exploration/env_infos/end_effector_loc Min              -0.484904\n",
      "evaluation/num steps total                           82000\n",
      "evaluation/num paths total                            4100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0401291\n",
      "evaluation/Rewards Std                                   0.0812656\n",
      "evaluation/Rewards Max                                   0.149735\n",
      "evaluation/Rewards Min                                  -0.347338\n",
      "evaluation/Returns Mean                                 -0.802582\n",
      "evaluation/Returns Std                                   1.27938\n",
      "evaluation/Returns Max                                   1.65276\n",
      "evaluation/Returns Min                                  -3.10534\n",
      "evaluation/Actions Mean                                  0.00477686\n",
      "evaluation/Actions Std                                   0.0724492\n",
      "evaluation/Actions Max                                   0.516293\n",
      "evaluation/Actions Min                                  -0.59724\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.802582\n",
      "evaluation/env_infos/final/reward_dist Mean              0.20278\n",
      "evaluation/env_infos/final/reward_dist Std               0.235835\n",
      "evaluation/env_infos/final/reward_dist Max               0.832151\n",
      "evaluation/env_infos/final/reward_dist Min               4.20862e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0085333\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0173206\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0734066\n",
      "evaluation/env_infos/initial/reward_dist Min             1.95351e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.244884\n",
      "evaluation/env_infos/reward_dist Std                     0.29365\n",
      "evaluation/env_infos/reward_dist Max                     0.995843\n",
      "evaluation/env_infos/reward_dist Min                     4.20862e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.031667\n",
      "evaluation/env_infos/final/reward_energy Std             0.027656\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00283989\n",
      "evaluation/env_infos/final/reward_energy Min            -0.130073\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.275938\n",
      "evaluation/env_infos/initial/reward_energy Std           0.181151\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0152479\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.72073\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0630416\n",
      "evaluation/env_infos/reward_energy Std                   0.0810503\n",
      "evaluation/env_infos/reward_energy Max                  -0.000760239\n",
      "evaluation/env_infos/reward_energy Min                  -0.72073\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0434003\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.231944\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.652607\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.556796\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000207206\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116685\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0258147\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.029862\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0153983\n",
      "evaluation/env_infos/end_effector_loc Std                0.159602\n",
      "evaluation/env_infos/end_effector_loc Max                0.652607\n",
      "evaluation/env_infos/end_effector_loc Min               -0.556796\n",
      "time/data storing (s)                                    0.00613975\n",
      "time/evaluation sampling (s)                             1.19732\n",
      "time/exploration sampling (s)                            0.142037\n",
      "time/logging (s)                                         0.0190983\n",
      "time/saving (s)                                          0.0294669\n",
      "time/training (s)                                       49.5602\n",
      "time/epoch (s)                                          50.9543\n",
      "time/total (s)                                        4078.09\n",
      "Epoch                                                   81\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:57:28.226832 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 82 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00217281\n",
      "trainer/QF2 Loss                                         0.0011669\n",
      "trainer/Policy Loss                                      3.12834\n",
      "trainer/Q1 Predictions Mean                             -1.09499\n",
      "trainer/Q1 Predictions Std                               0.80314\n",
      "trainer/Q1 Predictions Max                               0.880711\n",
      "trainer/Q1 Predictions Min                              -3.27177\n",
      "trainer/Q2 Predictions Mean                             -1.10839\n",
      "trainer/Q2 Predictions Std                               0.808321\n",
      "trainer/Q2 Predictions Max                               0.855497\n",
      "trainer/Q2 Predictions Min                              -3.25295\n",
      "trainer/Q Targets Mean                                  -1.11752\n",
      "trainer/Q Targets Std                                    0.814465\n",
      "trainer/Q Targets Max                                    0.838658\n",
      "trainer/Q Targets Min                                   -3.27814\n",
      "trainer/Log Pis Mean                                     2.06036\n",
      "trainer/Log Pis Std                                      1.4178\n",
      "trainer/Log Pis Max                                      4.92373\n",
      "trainer/Log Pis Min                                     -4.40481\n",
      "trainer/Policy mu Mean                                   0.0471574\n",
      "trainer/Policy mu Std                                    0.3763\n",
      "trainer/Policy mu Max                                    1.79985\n",
      "trainer/Policy mu Min                                   -1.88676\n",
      "trainer/Policy log std Mean                             -2.31071\n",
      "trainer/Policy log std Std                               0.629466\n",
      "trainer/Policy log std Max                              -0.400236\n",
      "trainer/Policy log std Min                              -3.64712\n",
      "trainer/Alpha                                            0.023415\n",
      "trainer/Alpha Loss                                       0.226656\n",
      "exploration/num steps total                           9300\n",
      "exploration/num paths total                            465\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0655059\n",
      "exploration/Rewards Std                                  0.081509\n",
      "exploration/Rewards Max                                  0.104851\n",
      "exploration/Rewards Min                                 -0.315271\n",
      "exploration/Returns Mean                                -1.31012\n",
      "exploration/Returns Std                                  0.70526\n",
      "exploration/Returns Max                                 -0.202074\n",
      "exploration/Returns Min                                 -2.38131\n",
      "exploration/Actions Mean                                -0.00341782\n",
      "exploration/Actions Std                                  0.106784\n",
      "exploration/Actions Max                                  0.405353\n",
      "exploration/Actions Min                                 -0.357317\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.31012\n",
      "exploration/env_infos/final/reward_dist Mean             0.32335\n",
      "exploration/env_infos/final/reward_dist Std              0.39695\n",
      "exploration/env_infos/final/reward_dist Max              0.979553\n",
      "exploration/env_infos/final/reward_dist Min              0.000115061\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00609586\n",
      "exploration/env_infos/initial/reward_dist Std            0.00809048\n",
      "exploration/env_infos/initial/reward_dist Max            0.0204337\n",
      "exploration/env_infos/initial/reward_dist Min            4.87792e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.248117\n",
      "exploration/env_infos/reward_dist Std                    0.289736\n",
      "exploration/env_infos/reward_dist Max                    0.979553\n",
      "exploration/env_infos/reward_dist Min                    4.87792e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.153359\n",
      "exploration/env_infos/final/reward_energy Std            0.0867256\n",
      "exploration/env_infos/final/reward_energy Max           -0.0359569\n",
      "exploration/env_infos/final/reward_energy Min           -0.27051\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.252088\n",
      "exploration/env_infos/initial/reward_energy Std          0.152824\n",
      "exploration/env_infos/initial/reward_energy Max         -0.034966\n",
      "exploration/env_infos/initial/reward_energy Min         -0.423958\n",
      "exploration/env_infos/reward_energy Mean                -0.128158\n",
      "exploration/env_infos/reward_energy Std                  0.0800282\n",
      "exploration/env_infos/reward_energy Max                 -0.0160949\n",
      "exploration/env_infos/reward_energy Min                 -0.423958\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.03295\n",
      "exploration/env_infos/final/end_effector_loc Std         0.219714\n",
      "exploration/env_infos/final/end_effector_loc Max         0.325088\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.332476\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000798558\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0103919\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0202676\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0140608\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0114794\n",
      "exploration/env_infos/end_effector_loc Std               0.14373\n",
      "exploration/env_infos/end_effector_loc Max               0.325088\n",
      "exploration/env_infos/end_effector_loc Min              -0.332476\n",
      "evaluation/num steps total                           83000\n",
      "evaluation/num paths total                            4150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0452157\n",
      "evaluation/Rewards Std                                   0.0698681\n",
      "evaluation/Rewards Max                                   0.154154\n",
      "evaluation/Rewards Min                                  -0.30587\n",
      "evaluation/Returns Mean                                 -0.904314\n",
      "evaluation/Returns Std                                   1.11094\n",
      "evaluation/Returns Max                                   1.28962\n",
      "evaluation/Returns Min                                  -3.3344\n",
      "evaluation/Actions Mean                                 -0.00221289\n",
      "evaluation/Actions Std                                   0.0742632\n",
      "evaluation/Actions Max                                   0.874106\n",
      "evaluation/Actions Min                                  -0.575372\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.904314\n",
      "evaluation/env_infos/final/reward_dist Mean              0.154511\n",
      "evaluation/env_infos/final/reward_dist Std               0.211195\n",
      "evaluation/env_infos/final/reward_dist Max               0.755779\n",
      "evaluation/env_infos/final/reward_dist Min               2.49186e-09\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00709559\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116293\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0552604\n",
      "evaluation/env_infos/initial/reward_dist Min             3.30054e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.196677\n",
      "evaluation/env_infos/reward_dist Std                     0.252106\n",
      "evaluation/env_infos/reward_dist Max                     0.999584\n",
      "evaluation/env_infos/reward_dist Min                     2.49186e-09\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0394181\n",
      "evaluation/env_infos/final/reward_energy Std             0.0505049\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00343804\n",
      "evaluation/env_infos/final/reward_energy Min            -0.256471\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.275609\n",
      "evaluation/env_infos/initial/reward_energy Std           0.214373\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0100217\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.874141\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0624997\n",
      "evaluation/env_infos/reward_energy Std                   0.0844608\n",
      "evaluation/env_infos/reward_energy Max                  -0.000178518\n",
      "evaluation/env_infos/reward_energy Min                  -0.874141\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0193739\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.226817\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.372034\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.431169\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0018283\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122087\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0437053\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0287686\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0039534\n",
      "evaluation/env_infos/end_effector_loc Std                0.158036\n",
      "evaluation/env_infos/end_effector_loc Max                0.372212\n",
      "evaluation/env_infos/end_effector_loc Min               -0.431169\n",
      "time/data storing (s)                                    0.00644248\n",
      "time/evaluation sampling (s)                             0.976377\n",
      "time/exploration sampling (s)                            0.121944\n",
      "time/logging (s)                                         0.0200988\n",
      "time/saving (s)                                          0.0313416\n",
      "time/training (s)                                       49.5401\n",
      "time/epoch (s)                                          50.6963\n",
      "time/total (s)                                        4129.86\n",
      "Epoch                                                   82\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:58:20.405550 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 83 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000911529\r\n",
      "trainer/QF2 Loss                                         0.0011679\r\n",
      "trainer/Policy Loss                                      3.12911\r\n",
      "trainer/Q1 Predictions Mean                             -1.24019\r\n",
      "trainer/Q1 Predictions Std                               0.932339\r\n",
      "trainer/Q1 Predictions Max                               1.01708\r\n",
      "trainer/Q1 Predictions Min                              -4.32937\r\n",
      "trainer/Q2 Predictions Mean                             -1.24759\r\n",
      "trainer/Q2 Predictions Std                               0.928601\r\n",
      "trainer/Q2 Predictions Max                               1.03655\r\n",
      "trainer/Q2 Predictions Min                              -4.26155\r\n",
      "trainer/Q Targets Mean                                  -1.23755\r\n",
      "trainer/Q Targets Std                                    0.933028\r\n",
      "trainer/Q Targets Max                                    1.00757\r\n",
      "trainer/Q Targets Min                                   -4.32167\r\n",
      "trainer/Log Pis Mean                                     1.9192\r\n",
      "trainer/Log Pis Std                                      1.4301\r\n",
      "trainer/Log Pis Max                                      4.36626\r\n",
      "trainer/Log Pis Min                                     -4.21741\r\n",
      "trainer/Policy mu Mean                                   0.0572984\r\n",
      "trainer/Policy mu Std                                    0.370872\r\n",
      "trainer/Policy mu Max                                    2.1014\r\n",
      "trainer/Policy mu Min                                   -1.75823\r\n",
      "trainer/Policy log std Mean                             -2.23451\r\n",
      "trainer/Policy log std Std                               0.599021\r\n",
      "trainer/Policy log std Max                              -0.112739\r\n",
      "trainer/Policy log std Min                              -3.2008\r\n",
      "trainer/Alpha                                            0.0249358\r\n",
      "trainer/Alpha Loss                                      -0.298245\r\n",
      "exploration/num steps total                           9400\r\n",
      "exploration/num paths total                            470\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0694048\r\n",
      "exploration/Rewards Std                                  0.0744803\r\n",
      "exploration/Rewards Max                                  0.118009\r\n",
      "exploration/Rewards Min                                 -0.220312\r\n",
      "exploration/Returns Mean                                -1.3881\r\n",
      "exploration/Returns Std                                  1.03031\r\n",
      "exploration/Returns Max                                  0.0800738\r\n",
      "exploration/Returns Min                                 -2.74623\r\n",
      "exploration/Actions Mean                                -0.00347294\r\n",
      "exploration/Actions Std                                  0.0875973\r\n",
      "exploration/Actions Max                                  0.230634\r\n",
      "exploration/Actions Min                                 -0.351315\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.3881\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0422057\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0802872\r\n",
      "exploration/env_infos/final/reward_dist Max              0.202656\r\n",
      "exploration/env_infos/final/reward_dist Min              5.89505e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00430395\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00481702\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0112033\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.30108e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.171535\r\n",
      "exploration/env_infos/reward_dist Std                    0.281551\r\n",
      "exploration/env_infos/reward_dist Max                    0.964229\r\n",
      "exploration/env_infos/reward_dist Min                    3.30108e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0753729\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0345326\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0305626\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.118375\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.133817\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.097009\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0130986\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.270711\r\n",
      "exploration/env_infos/reward_energy Mean                -0.10566\r\n",
      "exploration/env_infos/reward_energy Std                  0.0648593\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0102124\r\n",
      "exploration/env_infos/reward_energy Min                 -0.351704\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00637979\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.263977\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.357213\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.399817\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00253812\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00526356\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0115122\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00655201\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0224531\r\n",
      "exploration/env_infos/end_effector_loc Std               0.16462\r\n",
      "exploration/env_infos/end_effector_loc Max               0.357213\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.399817\r\n",
      "evaluation/num steps total                           84000\r\n",
      "evaluation/num paths total                            4200\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.027867\r\n",
      "evaluation/Rewards Std                                   0.0820352\r\n",
      "evaluation/Rewards Max                                   0.161523\r\n",
      "evaluation/Rewards Min                                  -0.322608\r\n",
      "evaluation/Returns Mean                                 -0.557339\r\n",
      "evaluation/Returns Std                                   1.22083\r\n",
      "evaluation/Returns Max                                   2.12155\r\n",
      "evaluation/Returns Min                                  -3.18693\r\n",
      "evaluation/Actions Mean                                  0.000829077\r\n",
      "evaluation/Actions Std                                   0.0769622\r\n",
      "evaluation/Actions Max                                   0.777106\r\n",
      "evaluation/Actions Min                                  -0.83075\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.557339\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.215732\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.266981\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.921039\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.43763e-16\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0059576\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.010744\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0511102\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.62016e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.241769\r\n",
      "evaluation/env_infos/reward_dist Std                     0.277137\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997119\r\n",
      "evaluation/env_infos/reward_dist Min                     8.43763e-16\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0364781\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0387664\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00626635\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.152302\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.305227\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.227328\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0216277\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.910939\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0650487\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0872721\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00110328\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.910939\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00477948\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.224692\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.477948\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.524181\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00019439\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134542\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0388553\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0415375\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00388124\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.154471\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.477948\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.524181\r\n",
      "time/data storing (s)                                    0.0064251\r\n",
      "time/evaluation sampling (s)                             0.96226\r\n",
      "time/exploration sampling (s)                            0.12405\r\n",
      "time/logging (s)                                         0.0197449\r\n",
      "time/saving (s)                                          0.028533\r\n",
      "time/training (s)                                       50.0137\r\n",
      "time/epoch (s)                                          51.1547\r\n",
      "time/total (s)                                        4182.04\r\n",
      "Epoch                                                   83\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:59:12.520727 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 84 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00129902\n",
      "trainer/QF2 Loss                                         0.00125048\n",
      "trainer/Policy Loss                                      3.12399\n",
      "trainer/Q1 Predictions Mean                             -1.2584\n",
      "trainer/Q1 Predictions Std                               0.885303\n",
      "trainer/Q1 Predictions Max                               1.01572\n",
      "trainer/Q1 Predictions Min                              -3.56972\n",
      "trainer/Q2 Predictions Mean                             -1.26884\n",
      "trainer/Q2 Predictions Std                               0.890924\n",
      "trainer/Q2 Predictions Max                               0.993801\n",
      "trainer/Q2 Predictions Min                              -3.60556\n",
      "trainer/Q Targets Mean                                  -1.2737\n",
      "trainer/Q Targets Std                                    0.889635\n",
      "trainer/Q Targets Max                                    0.965952\n",
      "trainer/Q Targets Min                                   -3.59767\n",
      "trainer/Log Pis Mean                                     1.89549\n",
      "trainer/Log Pis Std                                      1.28161\n",
      "trainer/Log Pis Max                                      4.14695\n",
      "trainer/Log Pis Min                                     -3.19218\n",
      "trainer/Policy mu Mean                                   0.068161\n",
      "trainer/Policy mu Std                                    0.432444\n",
      "trainer/Policy mu Max                                    2.27172\n",
      "trainer/Policy mu Min                                   -2.05889\n",
      "trainer/Policy log std Mean                             -2.18306\n",
      "trainer/Policy log std Std                               0.598999\n",
      "trainer/Policy log std Max                              -0.182956\n",
      "trainer/Policy log std Min                              -3.17785\n",
      "trainer/Alpha                                            0.0249745\n",
      "trainer/Alpha Loss                                      -0.385455\n",
      "exploration/num steps total                           9500\n",
      "exploration/num paths total                            475\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0640535\n",
      "exploration/Rewards Std                                  0.0870408\n",
      "exploration/Rewards Max                                  0.118514\n",
      "exploration/Rewards Min                                 -0.318441\n",
      "exploration/Returns Mean                                -1.28107\n",
      "exploration/Returns Std                                  1.3414\n",
      "exploration/Returns Max                                  0.397207\n",
      "exploration/Returns Min                                 -3.42722\n",
      "exploration/Actions Mean                                 0.00786422\n",
      "exploration/Actions Std                                  0.132458\n",
      "exploration/Actions Max                                  0.331396\n",
      "exploration/Actions Min                                 -0.533591\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.28107\n",
      "exploration/env_infos/final/reward_dist Mean             0.187001\n",
      "exploration/env_infos/final/reward_dist Std              0.186125\n",
      "exploration/env_infos/final/reward_dist Max              0.437891\n",
      "exploration/env_infos/final/reward_dist Min              1.21996e-20\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00656486\n",
      "exploration/env_infos/initial/reward_dist Std            0.00888525\n",
      "exploration/env_infos/initial/reward_dist Max            0.0224756\n",
      "exploration/env_infos/initial/reward_dist Min            2.75233e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.249377\n",
      "exploration/env_infos/reward_dist Std                    0.321277\n",
      "exploration/env_infos/reward_dist Max                    0.999782\n",
      "exploration/env_infos/reward_dist Min                    1.21996e-20\n",
      "exploration/env_infos/final/reward_energy Mean          -0.130072\n",
      "exploration/env_infos/final/reward_energy Std            0.0854345\n",
      "exploration/env_infos/final/reward_energy Max           -0.0326181\n",
      "exploration/env_infos/final/reward_energy Min           -0.251404\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.252079\n",
      "exploration/env_infos/initial/reward_energy Std          0.18185\n",
      "exploration/env_infos/initial/reward_energy Max         -0.049225\n",
      "exploration/env_infos/initial/reward_energy Min         -0.534219\n",
      "exploration/env_infos/reward_energy Mean                -0.148691\n",
      "exploration/env_infos/reward_energy Std                  0.114476\n",
      "exploration/env_infos/reward_energy Max                 -0.00616094\n",
      "exploration/env_infos/reward_energy Min                 -0.534219\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.116804\n",
      "exploration/env_infos/final/end_effector_loc Std         0.253858\n",
      "exploration/env_infos/final/end_effector_loc Max         0.556489\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.157156\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00260378\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0106764\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0124076\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0266796\n",
      "exploration/env_infos/end_effector_loc Mean              0.0408645\n",
      "exploration/env_infos/end_effector_loc Std               0.176379\n",
      "exploration/env_infos/end_effector_loc Max               0.556489\n",
      "exploration/env_infos/end_effector_loc Min              -0.269997\n",
      "evaluation/num steps total                           85000\n",
      "evaluation/num paths total                            4250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0538219\n",
      "evaluation/Rewards Std                                   0.0750689\n",
      "evaluation/Rewards Max                                   0.142585\n",
      "evaluation/Rewards Min                                  -0.405209\n",
      "evaluation/Returns Mean                                 -1.07644\n",
      "evaluation/Returns Std                                   1.16781\n",
      "evaluation/Returns Max                                   2.00544\n",
      "evaluation/Returns Min                                  -4.0354\n",
      "evaluation/Actions Mean                                  0.00566819\n",
      "evaluation/Actions Std                                   0.0893464\n",
      "evaluation/Actions Max                                   0.849898\n",
      "evaluation/Actions Min                                  -0.79042\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.07644\n",
      "evaluation/env_infos/final/reward_dist Mean              0.170578\n",
      "evaluation/env_infos/final/reward_dist Std               0.283726\n",
      "evaluation/env_infos/final/reward_dist Max               0.96184\n",
      "evaluation/env_infos/final/reward_dist Min               5.23923e-20\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00724402\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0131716\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0751983\n",
      "evaluation/env_infos/initial/reward_dist Min             2.45823e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.205084\n",
      "evaluation/env_infos/reward_dist Std                     0.271259\n",
      "evaluation/env_infos/reward_dist Max                     0.999616\n",
      "evaluation/env_infos/reward_dist Min                     5.23923e-20\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0479697\n",
      "evaluation/env_infos/final/reward_energy Std             0.0376954\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00800594\n",
      "evaluation/env_infos/final/reward_energy Min            -0.174316\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.344791\n",
      "evaluation/env_infos/initial/reward_energy Std           0.238269\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0149446\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.912386\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0763209\n",
      "evaluation/env_infos/reward_energy Std                   0.101019\n",
      "evaluation/env_infos/reward_energy Max                  -0.00173586\n",
      "evaluation/env_infos/reward_energy Min                  -0.912386\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0984018\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.234496\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.571399\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.462117\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00377589\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0143286\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0424949\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.039521\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0539995\n",
      "evaluation/env_infos/end_effector_loc Std                0.165336\n",
      "evaluation/env_infos/end_effector_loc Max                0.571399\n",
      "evaluation/env_infos/end_effector_loc Min               -0.462117\n",
      "time/data storing (s)                                    0.00756658\n",
      "time/evaluation sampling (s)                             0.981662\n",
      "time/exploration sampling (s)                            0.123709\n",
      "time/logging (s)                                         0.0195073\n",
      "time/saving (s)                                          0.0286405\n",
      "time/training (s)                                       49.8644\n",
      "time/epoch (s)                                          51.0255\n",
      "time/total (s)                                        4234.15\n",
      "Epoch                                                   84\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:00:04.446012 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 85 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00165457\n",
      "trainer/QF2 Loss                                         0.00144343\n",
      "trainer/Policy Loss                                      3.22674\n",
      "trainer/Q1 Predictions Mean                             -1.1773\n",
      "trainer/Q1 Predictions Std                               0.888574\n",
      "trainer/Q1 Predictions Max                               0.592964\n",
      "trainer/Q1 Predictions Min                              -3.51586\n",
      "trainer/Q2 Predictions Mean                             -1.16572\n",
      "trainer/Q2 Predictions Std                               0.877363\n",
      "trainer/Q2 Predictions Max                               0.596741\n",
      "trainer/Q2 Predictions Min                              -3.52227\n",
      "trainer/Q Targets Mean                                  -1.17816\n",
      "trainer/Q Targets Std                                    0.87805\n",
      "trainer/Q Targets Max                                    0.569674\n",
      "trainer/Q Targets Min                                   -3.49734\n",
      "trainer/Log Pis Mean                                     2.12091\n",
      "trainer/Log Pis Std                                      1.21158\n",
      "trainer/Log Pis Max                                      4.60932\n",
      "trainer/Log Pis Min                                     -3.45735\n",
      "trainer/Policy mu Mean                                  -0.00433461\n",
      "trainer/Policy mu Std                                    0.508591\n",
      "trainer/Policy mu Max                                    2.10657\n",
      "trainer/Policy mu Min                                   -2.02141\n",
      "trainer/Policy log std Mean                             -2.19434\n",
      "trainer/Policy log std Std                               0.645687\n",
      "trainer/Policy log std Max                              -0.292423\n",
      "trainer/Policy log std Min                              -3.28252\n",
      "trainer/Alpha                                            0.0248108\n",
      "trainer/Alpha Loss                                       0.446991\n",
      "exploration/num steps total                           9600\n",
      "exploration/num paths total                            480\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0882633\n",
      "exploration/Rewards Std                                  0.0694509\n",
      "exploration/Rewards Max                                  0.027882\n",
      "exploration/Rewards Min                                 -0.323987\n",
      "exploration/Returns Mean                                -1.76527\n",
      "exploration/Returns Std                                  0.709696\n",
      "exploration/Returns Max                                 -0.628432\n",
      "exploration/Returns Min                                 -2.56523\n",
      "exploration/Actions Mean                                 0.00817267\n",
      "exploration/Actions Std                                  0.144853\n",
      "exploration/Actions Max                                  0.80931\n",
      "exploration/Actions Min                                 -0.508049\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.76527\n",
      "exploration/env_infos/final/reward_dist Mean             0.341214\n",
      "exploration/env_infos/final/reward_dist Std              0.381646\n",
      "exploration/env_infos/final/reward_dist Max              0.853187\n",
      "exploration/env_infos/final/reward_dist Min              3.65797e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000385915\n",
      "exploration/env_infos/initial/reward_dist Std            0.000534258\n",
      "exploration/env_infos/initial/reward_dist Max            0.00140722\n",
      "exploration/env_infos/initial/reward_dist Min            4.84452e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.202589\n",
      "exploration/env_infos/reward_dist Std                    0.249388\n",
      "exploration/env_infos/reward_dist Max                    0.853187\n",
      "exploration/env_infos/reward_dist Min                    4.84452e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.192559\n",
      "exploration/env_infos/final/reward_energy Std            0.0743445\n",
      "exploration/env_infos/final/reward_energy Max           -0.0753322\n",
      "exploration/env_infos/final/reward_energy Min           -0.275846\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.409411\n",
      "exploration/env_infos/initial/reward_energy Std          0.232496\n",
      "exploration/env_infos/initial/reward_energy Max         -0.123857\n",
      "exploration/env_infos/initial/reward_energy Min         -0.810147\n",
      "exploration/env_infos/reward_energy Mean                -0.152061\n",
      "exploration/env_infos/reward_energy Std                  0.137752\n",
      "exploration/env_infos/reward_energy Max                 -0.00849166\n",
      "exploration/env_infos/reward_energy Min                 -0.810147\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.164468\n",
      "exploration/env_infos/final/end_effector_loc Std         0.208719\n",
      "exploration/env_infos/final/end_effector_loc Max         0.421959\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.2747\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00920182\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0138714\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0404655\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0107686\n",
      "exploration/env_infos/end_effector_loc Mean              0.102684\n",
      "exploration/env_infos/end_effector_loc Std               0.161117\n",
      "exploration/env_infos/end_effector_loc Max               0.421959\n",
      "exploration/env_infos/end_effector_loc Min              -0.2747\n",
      "evaluation/num steps total                           86000\n",
      "evaluation/num paths total                            4300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0405534\n",
      "evaluation/Rewards Std                                   0.0693112\n",
      "evaluation/Rewards Max                                   0.172365\n",
      "evaluation/Rewards Min                                  -0.371512\n",
      "evaluation/Returns Mean                                 -0.811069\n",
      "evaluation/Returns Std                                   1.07402\n",
      "evaluation/Returns Max                                   2.14383\n",
      "evaluation/Returns Min                                  -2.79068\n",
      "evaluation/Actions Mean                                 -0.00204364\n",
      "evaluation/Actions Std                                   0.0623805\n",
      "evaluation/Actions Max                                   0.749641\n",
      "evaluation/Actions Min                                  -0.537112\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.811069\n",
      "evaluation/env_infos/final/reward_dist Mean              0.177852\n",
      "evaluation/env_infos/final/reward_dist Std               0.274192\n",
      "evaluation/env_infos/final/reward_dist Max               0.967453\n",
      "evaluation/env_infos/final/reward_dist Min               3.62339e-12\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00669412\n",
      "evaluation/env_infos/initial/reward_dist Std             0.010592\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0465162\n",
      "evaluation/env_infos/initial/reward_dist Min             1.17394e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.226026\n",
      "evaluation/env_infos/reward_dist Std                     0.295598\n",
      "evaluation/env_infos/reward_dist Max                     0.997835\n",
      "evaluation/env_infos/reward_dist Min                     3.62339e-12\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0260265\n",
      "evaluation/env_infos/final/reward_energy Std             0.0177282\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00129057\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0764897\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.224911\n",
      "evaluation/env_infos/initial/reward_energy Std           0.209315\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00533578\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.840692\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0512236\n",
      "evaluation/env_infos/reward_energy Std                   0.0718828\n",
      "evaluation/env_infos/reward_energy Max                  -0.00129057\n",
      "evaluation/env_infos/reward_energy Min                  -0.840692\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00354496\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.239449\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.530748\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.579958\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00256439\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0105556\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0374821\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0268556\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00818991\n",
      "evaluation/env_infos/end_effector_loc Std                0.153127\n",
      "evaluation/env_infos/end_effector_loc Max                0.530748\n",
      "evaluation/env_infos/end_effector_loc Min               -0.579958\n",
      "time/data storing (s)                                    0.00636503\n",
      "time/evaluation sampling (s)                             0.922978\n",
      "time/exploration sampling (s)                            0.121103\n",
      "time/logging (s)                                         0.0196777\n",
      "time/saving (s)                                          0.0272638\n",
      "time/training (s)                                       49.9606\n",
      "time/epoch (s)                                          51.058\n",
      "time/total (s)                                        4286.07\n",
      "Epoch                                                   85\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:00:57.744364 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 86 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00129748\n",
      "trainer/QF2 Loss                                         0.00159807\n",
      "trainer/Policy Loss                                      3.20363\n",
      "trainer/Q1 Predictions Mean                             -1.24969\n",
      "trainer/Q1 Predictions Std                               0.864312\n",
      "trainer/Q1 Predictions Max                               0.967666\n",
      "trainer/Q1 Predictions Min                              -3.50322\n",
      "trainer/Q2 Predictions Mean                             -1.25656\n",
      "trainer/Q2 Predictions Std                               0.871129\n",
      "trainer/Q2 Predictions Max                               1.0587\n",
      "trainer/Q2 Predictions Min                              -3.52259\n",
      "trainer/Q Targets Mean                                  -1.2476\n",
      "trainer/Q Targets Std                                    0.869855\n",
      "trainer/Q Targets Max                                    0.990498\n",
      "trainer/Q Targets Min                                   -3.55776\n",
      "trainer/Log Pis Mean                                     1.99011\n",
      "trainer/Log Pis Std                                      1.45835\n",
      "trainer/Log Pis Max                                      5.22183\n",
      "trainer/Log Pis Min                                     -4.64913\n",
      "trainer/Policy mu Mean                                   0.0225547\n",
      "trainer/Policy mu Std                                    0.487364\n",
      "trainer/Policy mu Max                                    1.75962\n",
      "trainer/Policy mu Min                                   -2.003\n",
      "trainer/Policy log std Mean                             -2.22356\n",
      "trainer/Policy log std Std                               0.655712\n",
      "trainer/Policy log std Max                              -0.358768\n",
      "trainer/Policy log std Min                              -3.31403\n",
      "trainer/Alpha                                            0.025159\n",
      "trainer/Alpha Loss                                      -0.0364305\n",
      "exploration/num steps total                           9700\n",
      "exploration/num paths total                            485\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0405601\n",
      "exploration/Rewards Std                                  0.0845126\n",
      "exploration/Rewards Max                                  0.146788\n",
      "exploration/Rewards Min                                 -0.255262\n",
      "exploration/Returns Mean                                -0.811201\n",
      "exploration/Returns Std                                  1.32701\n",
      "exploration/Returns Max                                  1.20583\n",
      "exploration/Returns Min                                 -2.23302\n",
      "exploration/Actions Mean                                 0.00724008\n",
      "exploration/Actions Std                                  0.166817\n",
      "exploration/Actions Max                                  0.67266\n",
      "exploration/Actions Min                                 -0.502044\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.811201\n",
      "exploration/env_infos/final/reward_dist Mean             0.141286\n",
      "exploration/env_infos/final/reward_dist Std              0.174101\n",
      "exploration/env_infos/final/reward_dist Max              0.394104\n",
      "exploration/env_infos/final/reward_dist Min              6.17836e-09\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00517923\n",
      "exploration/env_infos/initial/reward_dist Std            0.00950721\n",
      "exploration/env_infos/initial/reward_dist Max            0.0241849\n",
      "exploration/env_infos/initial/reward_dist Min            0.000132978\n",
      "exploration/env_infos/reward_dist Mean                   0.249773\n",
      "exploration/env_infos/reward_dist Std                    0.32766\n",
      "exploration/env_infos/reward_dist Max                    0.992131\n",
      "exploration/env_infos/reward_dist Min                    6.17836e-09\n",
      "exploration/env_infos/final/reward_energy Mean          -0.181015\n",
      "exploration/env_infos/final/reward_energy Std            0.0879557\n",
      "exploration/env_infos/final/reward_energy Max           -0.0502244\n",
      "exploration/env_infos/final/reward_energy Min           -0.326092\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.394081\n",
      "exploration/env_infos/initial/reward_energy Std          0.287435\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0240513\n",
      "exploration/env_infos/initial/reward_energy Min         -0.774144\n",
      "exploration/env_infos/reward_energy Mean                -0.181276\n",
      "exploration/env_infos/reward_energy Std                  0.151325\n",
      "exploration/env_infos/reward_energy Max                 -0.00523701\n",
      "exploration/env_infos/reward_energy Min                 -0.774852\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.159881\n",
      "exploration/env_infos/final/end_effector_loc Std         0.222916\n",
      "exploration/env_infos/final/end_effector_loc Max         0.465826\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.255565\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00628158\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0160605\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.033633\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0177935\n",
      "exploration/env_infos/end_effector_loc Mean              0.095798\n",
      "exploration/env_infos/end_effector_loc Std               0.162419\n",
      "exploration/env_infos/end_effector_loc Max               0.465826\n",
      "exploration/env_infos/end_effector_loc Min              -0.255565\n",
      "evaluation/num steps total                           87000\n",
      "evaluation/num paths total                            4350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0376271\n",
      "evaluation/Rewards Std                                   0.077562\n",
      "evaluation/Rewards Max                                   0.153759\n",
      "evaluation/Rewards Min                                  -0.397413\n",
      "evaluation/Returns Mean                                 -0.752541\n",
      "evaluation/Returns Std                                   1.15447\n",
      "evaluation/Returns Max                                   2.09004\n",
      "evaluation/Returns Min                                  -3.12594\n",
      "evaluation/Actions Mean                                  0.00233796\n",
      "evaluation/Actions Std                                   0.0694317\n",
      "evaluation/Actions Max                                   0.772927\n",
      "evaluation/Actions Min                                  -0.696229\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.752541\n",
      "evaluation/env_infos/final/reward_dist Mean              0.181363\n",
      "evaluation/env_infos/final/reward_dist Std               0.296049\n",
      "evaluation/env_infos/final/reward_dist Max               0.845088\n",
      "evaluation/env_infos/final/reward_dist Min               1.25359e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00830589\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0164791\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0799672\n",
      "evaluation/env_infos/initial/reward_dist Min             2.58153e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.191612\n",
      "evaluation/env_infos/reward_dist Std                     0.282457\n",
      "evaluation/env_infos/reward_dist Max                     0.995149\n",
      "evaluation/env_infos/reward_dist Min                     1.25359e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0299075\n",
      "evaluation/env_infos/final/reward_energy Std             0.0246576\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00319455\n",
      "evaluation/env_infos/final/reward_energy Min            -0.129137\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.273203\n",
      "evaluation/env_infos/initial/reward_energy Std           0.219737\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0119349\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.780251\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0553102\n",
      "evaluation/env_infos/reward_energy Std                   0.0811987\n",
      "evaluation/env_infos/reward_energy Max                  -0.000955415\n",
      "evaluation/env_infos/reward_energy Min                  -0.780251\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0448512\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.254939\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.615278\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.508895\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00132691\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123246\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0386464\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0348114\n",
      "evaluation/env_infos/end_effector_loc Mean               0.021614\n",
      "evaluation/env_infos/end_effector_loc Std                0.168504\n",
      "evaluation/env_infos/end_effector_loc Max                0.615278\n",
      "evaluation/env_infos/end_effector_loc Min               -0.508895\n",
      "time/data storing (s)                                    0.00622189\n",
      "time/evaluation sampling (s)                             0.96321\n",
      "time/exploration sampling (s)                            0.121596\n",
      "time/logging (s)                                         0.0231536\n",
      "time/saving (s)                                          0.0318708\n",
      "time/training (s)                                       51.0946\n",
      "time/epoch (s)                                          52.2407\n",
      "time/total (s)                                        4339.37\n",
      "Epoch                                                   86\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:01:51.369377 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 87 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000818184\n",
      "trainer/QF2 Loss                                         0.00104762\n",
      "trainer/Policy Loss                                      3.13268\n",
      "trainer/Q1 Predictions Mean                             -1.19569\n",
      "trainer/Q1 Predictions Std                               0.890925\n",
      "trainer/Q1 Predictions Max                               0.706173\n",
      "trainer/Q1 Predictions Min                              -3.71725\n",
      "trainer/Q2 Predictions Mean                             -1.19835\n",
      "trainer/Q2 Predictions Std                               0.892363\n",
      "trainer/Q2 Predictions Max                               0.729951\n",
      "trainer/Q2 Predictions Min                              -3.70518\n",
      "trainer/Q Targets Mean                                  -1.19247\n",
      "trainer/Q Targets Std                                    0.888742\n",
      "trainer/Q Targets Max                                    0.716619\n",
      "trainer/Q Targets Min                                   -3.6725\n",
      "trainer/Log Pis Mean                                     1.98188\n",
      "trainer/Log Pis Std                                      1.23737\n",
      "trainer/Log Pis Max                                      4.50345\n",
      "trainer/Log Pis Min                                     -2.47065\n",
      "trainer/Policy mu Mean                                  -0.00104988\n",
      "trainer/Policy mu Std                                    0.462169\n",
      "trainer/Policy mu Max                                    1.88489\n",
      "trainer/Policy mu Min                                   -1.94046\n",
      "trainer/Policy log std Mean                             -2.15749\n",
      "trainer/Policy log std Std                               0.623412\n",
      "trainer/Policy log std Max                              -0.264871\n",
      "trainer/Policy log std Min                              -3.31156\n",
      "trainer/Alpha                                            0.0239479\n",
      "trainer/Alpha Loss                                      -0.0676332\n",
      "exploration/num steps total                           9800\n",
      "exploration/num paths total                            490\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0552733\n",
      "exploration/Rewards Std                                  0.0691529\n",
      "exploration/Rewards Max                                  0.116912\n",
      "exploration/Rewards Min                                 -0.303359\n",
      "exploration/Returns Mean                                -1.10547\n",
      "exploration/Returns Std                                  0.944261\n",
      "exploration/Returns Max                                  0.652969\n",
      "exploration/Returns Min                                 -2.15984\n",
      "exploration/Actions Mean                                -0.00406479\n",
      "exploration/Actions Std                                  0.135518\n",
      "exploration/Actions Max                                  0.490985\n",
      "exploration/Actions Min                                 -0.394579\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.10547\n",
      "exploration/env_infos/final/reward_dist Mean             0.126666\n",
      "exploration/env_infos/final/reward_dist Std              0.232508\n",
      "exploration/env_infos/final/reward_dist Max              0.591314\n",
      "exploration/env_infos/final/reward_dist Min              1.94211e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00636688\n",
      "exploration/env_infos/initial/reward_dist Std            0.0077805\n",
      "exploration/env_infos/initial/reward_dist Max            0.0192861\n",
      "exploration/env_infos/initial/reward_dist Min            2.392e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.173535\n",
      "exploration/env_infos/reward_dist Std                    0.254259\n",
      "exploration/env_infos/reward_dist Max                    0.940545\n",
      "exploration/env_infos/reward_dist Min                    1.94211e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.111802\n",
      "exploration/env_infos/final/reward_energy Std            0.0499984\n",
      "exploration/env_infos/final/reward_energy Max           -0.0451931\n",
      "exploration/env_infos/final/reward_energy Min           -0.197351\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.216029\n",
      "exploration/env_infos/initial/reward_energy Std          0.11834\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0383808\n",
      "exploration/env_infos/initial/reward_energy Min         -0.382375\n",
      "exploration/env_infos/reward_energy Mean                -0.147128\n",
      "exploration/env_infos/reward_energy Std                  0.122949\n",
      "exploration/env_infos/reward_energy Max                 -0.00441384\n",
      "exploration/env_infos/reward_energy Min                 -0.609851\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0605702\n",
      "exploration/env_infos/final/end_effector_loc Std         0.224757\n",
      "exploration/env_infos/final/end_effector_loc Max         0.348284\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.412666\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000603048\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00868779\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0156214\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0146443\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0260649\n",
      "exploration/env_infos/end_effector_loc Std               0.148381\n",
      "exploration/env_infos/end_effector_loc Max               0.348284\n",
      "exploration/env_infos/end_effector_loc Min              -0.412666\n",
      "evaluation/num steps total                           88000\n",
      "evaluation/num paths total                            4400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0415772\n",
      "evaluation/Rewards Std                                   0.0669372\n",
      "evaluation/Rewards Max                                   0.161025\n",
      "evaluation/Rewards Min                                  -0.301447\n",
      "evaluation/Returns Mean                                 -0.831544\n",
      "evaluation/Returns Std                                   1.01158\n",
      "evaluation/Returns Max                                   1.97968\n",
      "evaluation/Returns Min                                  -2.54669\n",
      "evaluation/Actions Mean                                 -0.000162246\n",
      "evaluation/Actions Std                                   0.0634118\n",
      "evaluation/Actions Max                                   0.524722\n",
      "evaluation/Actions Min                                  -0.797972\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.831544\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0895304\n",
      "evaluation/env_infos/final/reward_dist Std               0.202131\n",
      "evaluation/env_infos/final/reward_dist Max               0.863403\n",
      "evaluation/env_infos/final/reward_dist Min               5.36645e-80\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00681301\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0104155\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0376638\n",
      "evaluation/env_infos/initial/reward_dist Min             1.82287e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.151494\n",
      "evaluation/env_infos/reward_dist Std                     0.247932\n",
      "evaluation/env_infos/reward_dist Max                     0.99859\n",
      "evaluation/env_infos/reward_dist Min                     5.36645e-80\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0291649\n",
      "evaluation/env_infos/final/reward_energy Std             0.0160018\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00541957\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0774534\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.216959\n",
      "evaluation/env_infos/initial/reward_energy Std           0.207305\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0143931\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.851791\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0536683\n",
      "evaluation/env_infos/reward_energy Std                   0.0718462\n",
      "evaluation/env_infos/reward_energy Max                  -0.000183159\n",
      "evaluation/env_infos/reward_energy Min                  -0.851791\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0212593\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.266932\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.598552\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00152293\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0104995\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0262361\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0398986\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0140362\n",
      "evaluation/env_infos/end_effector_loc Std                0.166177\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.598552\n",
      "time/data storing (s)                                    0.00614191\n",
      "time/evaluation sampling (s)                             1.06943\n",
      "time/exploration sampling (s)                            0.124807\n",
      "time/logging (s)                                         0.0226338\n",
      "time/saving (s)                                          0.0278119\n",
      "time/training (s)                                       50.9642\n",
      "time/epoch (s)                                          52.215\n",
      "time/total (s)                                        4392.99\n",
      "Epoch                                                   87\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:02:43.554605 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 88 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000981648\n",
      "trainer/QF2 Loss                                         0.00130634\n",
      "trainer/Policy Loss                                      2.98707\n",
      "trainer/Q1 Predictions Mean                             -1.21616\n",
      "trainer/Q1 Predictions Std                               0.929423\n",
      "trainer/Q1 Predictions Max                               0.731953\n",
      "trainer/Q1 Predictions Min                              -4.15235\n",
      "trainer/Q2 Predictions Mean                             -1.22002\n",
      "trainer/Q2 Predictions Std                               0.934365\n",
      "trainer/Q2 Predictions Max                               0.739354\n",
      "trainer/Q2 Predictions Min                              -4.18078\n",
      "trainer/Q Targets Mean                                  -1.21584\n",
      "trainer/Q Targets Std                                    0.932287\n",
      "trainer/Q Targets Max                                    0.716339\n",
      "trainer/Q Targets Min                                   -4.18247\n",
      "trainer/Log Pis Mean                                     1.82446\n",
      "trainer/Log Pis Std                                      1.31431\n",
      "trainer/Log Pis Max                                      4.24314\n",
      "trainer/Log Pis Min                                     -4.2052\n",
      "trainer/Policy mu Mean                                   0.0358812\n",
      "trainer/Policy mu Std                                    0.458929\n",
      "trainer/Policy mu Max                                    1.98868\n",
      "trainer/Policy mu Min                                   -2.28705\n",
      "trainer/Policy log std Mean                             -2.1645\n",
      "trainer/Policy log std Std                               0.597663\n",
      "trainer/Policy log std Max                              -0.245055\n",
      "trainer/Policy log std Min                              -3.22957\n",
      "trainer/Alpha                                            0.0242162\n",
      "trainer/Alpha Loss                                      -0.653221\n",
      "exploration/num steps total                           9900\n",
      "exploration/num paths total                            495\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0802414\n",
      "exploration/Rewards Std                                  0.0746953\n",
      "exploration/Rewards Max                                  0.091021\n",
      "exploration/Rewards Min                                 -0.438809\n",
      "exploration/Returns Mean                                -1.60483\n",
      "exploration/Returns Std                                  0.691684\n",
      "exploration/Returns Max                                 -0.889954\n",
      "exploration/Returns Min                                 -2.47496\n",
      "exploration/Actions Mean                                -0.0115487\n",
      "exploration/Actions Std                                  0.132635\n",
      "exploration/Actions Max                                  0.571721\n",
      "exploration/Actions Min                                 -0.718002\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.60483\n",
      "exploration/env_infos/final/reward_dist Mean             0.0880904\n",
      "exploration/env_infos/final/reward_dist Std              0.172051\n",
      "exploration/env_infos/final/reward_dist Max              0.432133\n",
      "exploration/env_infos/final/reward_dist Min              1.71257e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00205509\n",
      "exploration/env_infos/initial/reward_dist Std            0.00236799\n",
      "exploration/env_infos/initial/reward_dist Max            0.006636\n",
      "exploration/env_infos/initial/reward_dist Min            1.48642e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.140591\n",
      "exploration/env_infos/reward_dist Std                    0.205212\n",
      "exploration/env_infos/reward_dist Max                    0.829545\n",
      "exploration/env_infos/reward_dist Min                    1.71257e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.143857\n",
      "exploration/env_infos/final/reward_energy Std            0.0352667\n",
      "exploration/env_infos/final/reward_energy Max           -0.0765655\n",
      "exploration/env_infos/final/reward_energy Min           -0.179163\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.391644\n",
      "exploration/env_infos/initial/reward_energy Std          0.340021\n",
      "exploration/env_infos/initial/reward_energy Max         -0.102578\n",
      "exploration/env_infos/initial/reward_energy Min         -0.917819\n",
      "exploration/env_infos/reward_energy Mean                -0.13998\n",
      "exploration/env_infos/reward_energy Std                  0.125922\n",
      "exploration/env_infos/reward_energy Max                 -0.00792356\n",
      "exploration/env_infos/reward_energy Min                 -0.917819\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.140932\n",
      "exploration/env_infos/final/end_effector_loc Std         0.285457\n",
      "exploration/env_infos/final/end_effector_loc Max         0.404155\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.467046\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0042077\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0178478\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0285861\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0359001\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0689383\n",
      "exploration/env_infos/end_effector_loc Std               0.179785\n",
      "exploration/env_infos/end_effector_loc Max               0.404155\n",
      "exploration/env_infos/end_effector_loc Min              -0.467046\n",
      "evaluation/num steps total                           89000\n",
      "evaluation/num paths total                            4450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0317249\n",
      "evaluation/Rewards Std                                   0.0748911\n",
      "evaluation/Rewards Max                                   0.151566\n",
      "evaluation/Rewards Min                                  -0.375015\n",
      "evaluation/Returns Mean                                 -0.634498\n",
      "evaluation/Returns Std                                   1.1088\n",
      "evaluation/Returns Max                                   2.06022\n",
      "evaluation/Returns Min                                  -3.09119\n",
      "evaluation/Actions Mean                                  0.00190528\n",
      "evaluation/Actions Std                                   0.0854064\n",
      "evaluation/Actions Max                                   0.881849\n",
      "evaluation/Actions Min                                  -0.691339\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.634498\n",
      "evaluation/env_infos/final/reward_dist Mean              0.218766\n",
      "evaluation/env_infos/final/reward_dist Std               0.286927\n",
      "evaluation/env_infos/final/reward_dist Max               0.870831\n",
      "evaluation/env_infos/final/reward_dist Min               2.58234e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0127695\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0244131\n",
      "evaluation/env_infos/initial/reward_dist Max             0.148787\n",
      "evaluation/env_infos/initial/reward_dist Min             3.57733e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.232995\n",
      "evaluation/env_infos/reward_dist Std                     0.282536\n",
      "evaluation/env_infos/reward_dist Max                     0.993777\n",
      "evaluation/env_infos/reward_dist Min                     2.58234e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0389151\n",
      "evaluation/env_infos/final/reward_energy Std             0.0413194\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00488238\n",
      "evaluation/env_infos/final/reward_energy Min            -0.255086\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.345574\n",
      "evaluation/env_infos/initial/reward_energy Std           0.250843\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0138063\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.956398\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0686445\n",
      "evaluation/env_infos/reward_energy Std                   0.0994167\n",
      "evaluation/env_infos/reward_energy Max                  -0.00154439\n",
      "evaluation/env_infos/reward_energy Min                  -0.956398\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.055626\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.242859\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.545713\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.481961\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00366971\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0146445\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0440925\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.034567\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0339314\n",
      "evaluation/env_infos/end_effector_loc Std                0.167813\n",
      "evaluation/env_infos/end_effector_loc Max                0.545713\n",
      "evaluation/env_infos/end_effector_loc Min               -0.481961\n",
      "time/data storing (s)                                    0.00643455\n",
      "time/evaluation sampling (s)                             0.97336\n",
      "time/exploration sampling (s)                            0.122809\n",
      "time/logging (s)                                         0.0201219\n",
      "time/saving (s)                                          0.0284136\n",
      "time/training (s)                                       49.7682\n",
      "time/epoch (s)                                          50.9193\n",
      "time/total (s)                                        4445.17\n",
      "Epoch                                                   88\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:03:36.286924 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 89 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000918292\n",
      "trainer/QF2 Loss                                         0.000930786\n",
      "trainer/Policy Loss                                      3.27761\n",
      "trainer/Q1 Predictions Mean                             -1.28066\n",
      "trainer/Q1 Predictions Std                               0.934483\n",
      "trainer/Q1 Predictions Max                               0.255915\n",
      "trainer/Q1 Predictions Min                              -4.15804\n",
      "trainer/Q2 Predictions Mean                             -1.28648\n",
      "trainer/Q2 Predictions Std                               0.936971\n",
      "trainer/Q2 Predictions Max                               0.273252\n",
      "trainer/Q2 Predictions Min                              -4.17828\n",
      "trainer/Q Targets Mean                                  -1.29196\n",
      "trainer/Q Targets Std                                    0.936759\n",
      "trainer/Q Targets Max                                    0.244243\n",
      "trainer/Q Targets Min                                   -4.20185\n",
      "trainer/Log Pis Mean                                     2.06003\n",
      "trainer/Log Pis Std                                      1.37973\n",
      "trainer/Log Pis Max                                      4.45503\n",
      "trainer/Log Pis Min                                     -2.3337\n",
      "trainer/Policy mu Mean                                  -0.00739481\n",
      "trainer/Policy mu Std                                    0.493236\n",
      "trainer/Policy mu Max                                    1.81448\n",
      "trainer/Policy mu Min                                   -2.23327\n",
      "trainer/Policy log std Mean                             -2.25411\n",
      "trainer/Policy log std Std                               0.686981\n",
      "trainer/Policy log std Max                              -0.390846\n",
      "trainer/Policy log std Min                              -3.32819\n",
      "trainer/Alpha                                            0.0241882\n",
      "trainer/Alpha Loss                                       0.223396\n",
      "exploration/num steps total                          10000\n",
      "exploration/num paths total                            500\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0612039\n",
      "exploration/Rewards Std                                  0.0807869\n",
      "exploration/Rewards Max                                  0.104559\n",
      "exploration/Rewards Min                                 -0.19295\n",
      "exploration/Returns Mean                                -1.22408\n",
      "exploration/Returns Std                                  1.40793\n",
      "exploration/Returns Max                                  0.900986\n",
      "exploration/Returns Min                                 -2.90271\n",
      "exploration/Actions Mean                                -0.00122823\n",
      "exploration/Actions Std                                  0.1856\n",
      "exploration/Actions Max                                  0.767092\n",
      "exploration/Actions Min                                 -0.541122\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.22408\n",
      "exploration/env_infos/final/reward_dist Mean             0.240831\n",
      "exploration/env_infos/final/reward_dist Std              0.243973\n",
      "exploration/env_infos/final/reward_dist Max              0.680926\n",
      "exploration/env_infos/final/reward_dist Min              7.95252e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0126996\n",
      "exploration/env_infos/initial/reward_dist Std            0.0212316\n",
      "exploration/env_infos/initial/reward_dist Max            0.0549511\n",
      "exploration/env_infos/initial/reward_dist Min            4.75962e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.282793\n",
      "exploration/env_infos/reward_dist Std                    0.332394\n",
      "exploration/env_infos/reward_dist Max                    0.989661\n",
      "exploration/env_infos/reward_dist Min                    7.95252e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.125719\n",
      "exploration/env_infos/final/reward_energy Std            0.0525482\n",
      "exploration/env_infos/final/reward_energy Max           -0.0723622\n",
      "exploration/env_infos/final/reward_energy Min           -0.224129\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.483852\n",
      "exploration/env_infos/initial/reward_energy Std          0.295779\n",
      "exploration/env_infos/initial/reward_energy Max         -0.115577\n",
      "exploration/env_infos/initial/reward_energy Min         -0.767423\n",
      "exploration/env_infos/reward_energy Mean                -0.197203\n",
      "exploration/env_infos/reward_energy Std                  0.173231\n",
      "exploration/env_infos/reward_energy Max                 -0.0182971\n",
      "exploration/env_infos/reward_energy Min                 -0.767423\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0196198\n",
      "exploration/env_infos/final/end_effector_loc Std         0.168958\n",
      "exploration/env_infos/final/end_effector_loc Max         0.228291\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.282634\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00305943\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0198151\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0383546\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0270561\n",
      "exploration/env_infos/end_effector_loc Mean              0.0282793\n",
      "exploration/env_infos/end_effector_loc Std               0.135157\n",
      "exploration/env_infos/end_effector_loc Max               0.268655\n",
      "exploration/env_infos/end_effector_loc Min              -0.284885\n",
      "evaluation/num steps total                           90000\n",
      "evaluation/num paths total                            4500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0631126\n",
      "evaluation/Rewards Std                                   0.0670304\n",
      "evaluation/Rewards Max                                   0.174123\n",
      "evaluation/Rewards Min                                  -0.409353\n",
      "evaluation/Returns Mean                                 -1.26225\n",
      "evaluation/Returns Std                                   1.02516\n",
      "evaluation/Returns Max                                   1.97936\n",
      "evaluation/Returns Min                                  -3.4802\n",
      "evaluation/Actions Mean                                  0.00145968\n",
      "evaluation/Actions Std                                   0.0842775\n",
      "evaluation/Actions Max                                   0.912068\n",
      "evaluation/Actions Min                                  -0.605884\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.26225\n",
      "evaluation/env_infos/final/reward_dist Mean              0.14718\n",
      "evaluation/env_infos/final/reward_dist Std               0.246658\n",
      "evaluation/env_infos/final/reward_dist Max               0.867079\n",
      "evaluation/env_infos/final/reward_dist Min               2.47159e-28\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00724697\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0161419\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0783672\n",
      "evaluation/env_infos/initial/reward_dist Min             5.52658e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.16354\n",
      "evaluation/env_infos/reward_dist Std                     0.254053\n",
      "evaluation/env_infos/reward_dist Max                     0.997438\n",
      "evaluation/env_infos/reward_dist Min                     2.47159e-28\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0316948\n",
      "evaluation/env_infos/final/reward_energy Std             0.0206433\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00442239\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0995702\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.309555\n",
      "evaluation/env_infos/initial/reward_energy Std           0.261282\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0227814\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.932711\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0650379\n",
      "evaluation/env_infos/reward_energy Std                   0.0998985\n",
      "evaluation/env_infos/reward_energy Max                  -0.00131532\n",
      "evaluation/env_infos/reward_energy Min                  -0.932711\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0236309\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.235447\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.496211\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.476883\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00242129\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0141157\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0456034\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0302942\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0162439\n",
      "evaluation/env_infos/end_effector_loc Std                0.160844\n",
      "evaluation/env_infos/end_effector_loc Max                0.496211\n",
      "evaluation/env_infos/end_effector_loc Min               -0.476883\n",
      "time/data storing (s)                                    0.00623988\n",
      "time/evaluation sampling (s)                             1.11098\n",
      "time/exploration sampling (s)                            0.122857\n",
      "time/logging (s)                                         0.0197739\n",
      "time/saving (s)                                          0.0278975\n",
      "time/training (s)                                       50.2628\n",
      "time/epoch (s)                                          51.5505\n",
      "time/total (s)                                        4497.9\n",
      "Epoch                                                   89\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:04:29.242062 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 90 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000895234\n",
      "trainer/QF2 Loss                                         0.00122647\n",
      "trainer/Policy Loss                                      3.14839\n",
      "trainer/Q1 Predictions Mean                             -1.28321\n",
      "trainer/Q1 Predictions Std                               0.933062\n",
      "trainer/Q1 Predictions Max                               0.49515\n",
      "trainer/Q1 Predictions Min                              -3.99135\n",
      "trainer/Q2 Predictions Mean                             -1.2917\n",
      "trainer/Q2 Predictions Std                               0.931587\n",
      "trainer/Q2 Predictions Max                               0.482504\n",
      "trainer/Q2 Predictions Min                              -3.96505\n",
      "trainer/Q Targets Mean                                  -1.28641\n",
      "trainer/Q Targets Std                                    0.932607\n",
      "trainer/Q Targets Max                                    0.495866\n",
      "trainer/Q Targets Min                                   -4.03918\n",
      "trainer/Log Pis Mean                                     1.91503\n",
      "trainer/Log Pis Std                                      1.44992\n",
      "trainer/Log Pis Max                                      4.61978\n",
      "trainer/Log Pis Min                                     -3.88537\n",
      "trainer/Policy mu Mean                                  -0.0115212\n",
      "trainer/Policy mu Std                                    0.538949\n",
      "trainer/Policy mu Max                                    1.7615\n",
      "trainer/Policy mu Min                                   -2.30573\n",
      "trainer/Policy log std Mean                             -2.15391\n",
      "trainer/Policy log std Std                               0.724783\n",
      "trainer/Policy log std Max                              -0.35327\n",
      "trainer/Policy log std Min                              -3.25747\n",
      "trainer/Alpha                                            0.0222315\n",
      "trainer/Alpha Loss                                      -0.323414\n",
      "exploration/num steps total                          10100\n",
      "exploration/num paths total                            505\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0640445\n",
      "exploration/Rewards Std                                  0.0539819\n",
      "exploration/Rewards Max                                  0.0604245\n",
      "exploration/Rewards Min                                 -0.247535\n",
      "exploration/Returns Mean                                -1.28089\n",
      "exploration/Returns Std                                  0.576319\n",
      "exploration/Returns Max                                 -0.482723\n",
      "exploration/Returns Min                                 -2.28409\n",
      "exploration/Actions Mean                                -0.00248075\n",
      "exploration/Actions Std                                  0.151822\n",
      "exploration/Actions Max                                  0.827062\n",
      "exploration/Actions Min                                 -0.524844\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.28089\n",
      "exploration/env_infos/final/reward_dist Mean             0.155504\n",
      "exploration/env_infos/final/reward_dist Std              0.268438\n",
      "exploration/env_infos/final/reward_dist Max              0.688465\n",
      "exploration/env_infos/final/reward_dist Min              0.000484602\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0231321\n",
      "exploration/env_infos/initial/reward_dist Std            0.0307698\n",
      "exploration/env_infos/initial/reward_dist Max            0.0808322\n",
      "exploration/env_infos/initial/reward_dist Min            2.56369e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.209764\n",
      "exploration/env_infos/reward_dist Std                    0.262251\n",
      "exploration/env_infos/reward_dist Max                    0.952438\n",
      "exploration/env_infos/reward_dist Min                    2.56369e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0736341\n",
      "exploration/env_infos/final/reward_energy Std            0.0260017\n",
      "exploration/env_infos/final/reward_energy Max           -0.0319291\n",
      "exploration/env_infos/final/reward_energy Min           -0.107295\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.347094\n",
      "exploration/env_infos/initial/reward_energy Std          0.298752\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0418956\n",
      "exploration/env_infos/initial/reward_energy Min         -0.827783\n",
      "exploration/env_infos/reward_energy Mean                -0.15456\n",
      "exploration/env_infos/reward_energy Std                  0.149076\n",
      "exploration/env_infos/reward_energy Max                 -0.00599167\n",
      "exploration/env_infos/reward_energy Min                 -0.827783\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.006534\n",
      "exploration/env_infos/final/end_effector_loc Std         0.228605\n",
      "exploration/env_infos/final/end_effector_loc Max         0.359282\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.383096\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00373933\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0157536\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0413531\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0262422\n",
      "exploration/env_infos/end_effector_loc Mean              0.0143589\n",
      "exploration/env_infos/end_effector_loc Std               0.159816\n",
      "exploration/env_infos/end_effector_loc Max               0.359282\n",
      "exploration/env_infos/end_effector_loc Min              -0.383096\n",
      "evaluation/num steps total                           91000\n",
      "evaluation/num paths total                            4550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0296138\n",
      "evaluation/Rewards Std                                   0.0782509\n",
      "evaluation/Rewards Max                                   0.145895\n",
      "evaluation/Rewards Min                                  -0.544654\n",
      "evaluation/Returns Mean                                 -0.592276\n",
      "evaluation/Returns Std                                   1.20783\n",
      "evaluation/Returns Max                                   1.80973\n",
      "evaluation/Returns Min                                  -3.3443\n",
      "evaluation/Actions Mean                                 -0.00101399\n",
      "evaluation/Actions Std                                   0.0816152\n",
      "evaluation/Actions Max                                   0.748629\n",
      "evaluation/Actions Min                                  -0.836749\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.592276\n",
      "evaluation/env_infos/final/reward_dist Mean              0.187155\n",
      "evaluation/env_infos/final/reward_dist Std               0.249522\n",
      "evaluation/env_infos/final/reward_dist Max               0.986245\n",
      "evaluation/env_infos/final/reward_dist Min               2.82653e-15\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0118386\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0185637\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0666917\n",
      "evaluation/env_infos/initial/reward_dist Min             1.84977e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.250391\n",
      "evaluation/env_infos/reward_dist Std                     0.287731\n",
      "evaluation/env_infos/reward_dist Max                     0.99436\n",
      "evaluation/env_infos/reward_dist Min                     2.82653e-15\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0311094\n",
      "evaluation/env_infos/final/reward_energy Std             0.0402332\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00170903\n",
      "evaluation/env_infos/final/reward_energy Min            -0.205377\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.340163\n",
      "evaluation/env_infos/initial/reward_energy Std           0.236537\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0236464\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.916235\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0639605\n",
      "evaluation/env_infos/reward_energy Std                   0.0960895\n",
      "evaluation/env_infos/reward_energy Max                  -0.000380631\n",
      "evaluation/env_infos/reward_energy Min                  -0.916235\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0296881\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.234018\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.564402\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.480756\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00106831\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0146094\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0374315\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0418375\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0128439\n",
      "evaluation/env_infos/end_effector_loc Std                0.160212\n",
      "evaluation/env_infos/end_effector_loc Max                0.564402\n",
      "evaluation/env_infos/end_effector_loc Min               -0.480756\n",
      "time/data storing (s)                                    0.00613816\n",
      "time/evaluation sampling (s)                             0.969765\n",
      "time/exploration sampling (s)                            0.125667\n",
      "time/logging (s)                                         0.0217116\n",
      "time/saving (s)                                          0.0349665\n",
      "time/training (s)                                       50.6326\n",
      "time/epoch (s)                                          51.7908\n",
      "time/total (s)                                        4550.85\n",
      "Epoch                                                   90\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:05:25.217702 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 91 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0013359\n",
      "trainer/QF2 Loss                                         0.00124171\n",
      "trainer/Policy Loss                                      3.18944\n",
      "trainer/Q1 Predictions Mean                             -1.19046\n",
      "trainer/Q1 Predictions Std                               0.826393\n",
      "trainer/Q1 Predictions Max                               0.324618\n",
      "trainer/Q1 Predictions Min                              -3.6053\n",
      "trainer/Q2 Predictions Mean                             -1.19093\n",
      "trainer/Q2 Predictions Std                               0.822938\n",
      "trainer/Q2 Predictions Max                               0.299039\n",
      "trainer/Q2 Predictions Min                              -3.5694\n",
      "trainer/Q Targets Mean                                  -1.17859\n",
      "trainer/Q Targets Std                                    0.820569\n",
      "trainer/Q Targets Max                                    0.308083\n",
      "trainer/Q Targets Min                                   -3.50365\n",
      "trainer/Log Pis Mean                                     2.03805\n",
      "trainer/Log Pis Std                                      1.6628\n",
      "trainer/Log Pis Max                                      5.27781\n",
      "trainer/Log Pis Min                                     -5.89721\n",
      "trainer/Policy mu Mean                                   0.0354141\n",
      "trainer/Policy mu Std                                    0.462657\n",
      "trainer/Policy mu Max                                    1.9775\n",
      "trainer/Policy mu Min                                   -1.87699\n",
      "trainer/Policy log std Mean                             -2.26635\n",
      "trainer/Policy log std Std                               0.695037\n",
      "trainer/Policy log std Max                              -0.387627\n",
      "trainer/Policy log std Min                              -3.35686\n",
      "trainer/Alpha                                            0.0210638\n",
      "trainer/Alpha Loss                                       0.146857\n",
      "exploration/num steps total                          10200\n",
      "exploration/num paths total                            510\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.111756\n",
      "exploration/Rewards Std                                  0.0928162\n",
      "exploration/Rewards Max                                  0.0802499\n",
      "exploration/Rewards Min                                 -0.507052\n",
      "exploration/Returns Mean                                -2.23512\n",
      "exploration/Returns Std                                  1.12716\n",
      "exploration/Returns Max                                 -0.823016\n",
      "exploration/Returns Min                                 -3.69764\n",
      "exploration/Actions Mean                                 0.00135658\n",
      "exploration/Actions Std                                  0.177566\n",
      "exploration/Actions Max                                  0.766834\n",
      "exploration/Actions Min                                 -0.850952\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.23512\n",
      "exploration/env_infos/final/reward_dist Mean             0.106889\n",
      "exploration/env_infos/final/reward_dist Std              0.213748\n",
      "exploration/env_infos/final/reward_dist Max              0.534385\n",
      "exploration/env_infos/final/reward_dist Min              4.3892e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0087379\n",
      "exploration/env_infos/initial/reward_dist Std            0.0114393\n",
      "exploration/env_infos/initial/reward_dist Max            0.0285592\n",
      "exploration/env_infos/initial/reward_dist Min            9.88944e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.104828\n",
      "exploration/env_infos/reward_dist Std                    0.219769\n",
      "exploration/env_infos/reward_dist Max                    0.82933\n",
      "exploration/env_infos/reward_dist Min                    3.4765e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0985401\n",
      "exploration/env_infos/final/reward_energy Std            0.0433066\n",
      "exploration/env_infos/final/reward_energy Max           -0.0641348\n",
      "exploration/env_infos/final/reward_energy Min           -0.179693\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.461212\n",
      "exploration/env_infos/initial/reward_energy Std          0.355651\n",
      "exploration/env_infos/initial/reward_energy Max         -0.030878\n",
      "exploration/env_infos/initial/reward_energy Min         -0.922342\n",
      "exploration/env_infos/reward_energy Mean                -0.170439\n",
      "exploration/env_infos/reward_energy Std                  0.184428\n",
      "exploration/env_infos/reward_energy Max                 -0.011102\n",
      "exploration/env_infos/reward_energy Min                 -0.922342\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0848391\n",
      "exploration/env_infos/final/end_effector_loc Std         0.259044\n",
      "exploration/env_infos/final/end_effector_loc Max         0.397434\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.386804\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00742208\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0192072\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0330802\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0425476\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0717348\n",
      "exploration/env_infos/end_effector_loc Std               0.181818\n",
      "exploration/env_infos/end_effector_loc Max               0.397434\n",
      "exploration/env_infos/end_effector_loc Min              -0.420192\n",
      "evaluation/num steps total                           92000\n",
      "evaluation/num paths total                            4600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0344288\n",
      "evaluation/Rewards Std                                   0.075688\n",
      "evaluation/Rewards Max                                   0.140705\n",
      "evaluation/Rewards Min                                  -0.406303\n",
      "evaluation/Returns Mean                                 -0.688575\n",
      "evaluation/Returns Std                                   1.20596\n",
      "evaluation/Returns Max                                   2.16156\n",
      "evaluation/Returns Min                                  -3.54785\n",
      "evaluation/Actions Mean                                  0.00464167\n",
      "evaluation/Actions Std                                   0.0805526\n",
      "evaluation/Actions Max                                   0.827204\n",
      "evaluation/Actions Min                                  -0.594616\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.688575\n",
      "evaluation/env_infos/final/reward_dist Mean              0.222165\n",
      "evaluation/env_infos/final/reward_dist Std               0.249258\n",
      "evaluation/env_infos/final/reward_dist Max               0.796109\n",
      "evaluation/env_infos/final/reward_dist Min               5.14051e-40\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0109302\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0197742\n",
      "evaluation/env_infos/initial/reward_dist Max             0.11732\n",
      "evaluation/env_infos/initial/reward_dist Min             9.22454e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.252931\n",
      "evaluation/env_infos/reward_dist Std                     0.284102\n",
      "evaluation/env_infos/reward_dist Max                     0.998528\n",
      "evaluation/env_infos/reward_dist Min                     5.14051e-40\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0392162\n",
      "evaluation/env_infos/final/reward_energy Std             0.0449696\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0103304\n",
      "evaluation/env_infos/final/reward_energy Min            -0.287313\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.304706\n",
      "evaluation/env_infos/initial/reward_energy Std           0.25966\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.032112\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.874032\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0623752\n",
      "evaluation/env_infos/reward_energy Std                   0.0955503\n",
      "evaluation/env_infos/reward_energy Max                  -0.00112869\n",
      "evaluation/env_infos/reward_energy Min                  -0.874032\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0328608\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.226617\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.605849\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.344753\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00229877\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0139661\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0413602\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0297308\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0172818\n",
      "evaluation/env_infos/end_effector_loc Std                0.149407\n",
      "evaluation/env_infos/end_effector_loc Max                0.605849\n",
      "evaluation/env_infos/end_effector_loc Min               -0.344753\n",
      "time/data storing (s)                                    0.00621119\n",
      "time/evaluation sampling (s)                             1.15265\n",
      "time/exploration sampling (s)                            0.14741\n",
      "time/logging (s)                                         0.0204036\n",
      "time/saving (s)                                          0.0288289\n",
      "time/training (s)                                       53.3987\n",
      "time/epoch (s)                                          54.7542\n",
      "time/total (s)                                        4606.83\n",
      "Epoch                                                   91\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:06:19.382040 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 92 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00201842\n",
      "trainer/QF2 Loss                                         0.0019482\n",
      "trainer/Policy Loss                                      3.15806\n",
      "trainer/Q1 Predictions Mean                             -1.28496\n",
      "trainer/Q1 Predictions Std                               0.973809\n",
      "trainer/Q1 Predictions Max                               0.646414\n",
      "trainer/Q1 Predictions Min                              -4.17533\n",
      "trainer/Q2 Predictions Mean                             -1.28514\n",
      "trainer/Q2 Predictions Std                               0.971535\n",
      "trainer/Q2 Predictions Max                               0.639607\n",
      "trainer/Q2 Predictions Min                              -4.19981\n",
      "trainer/Q Targets Mean                                  -1.30298\n",
      "trainer/Q Targets Std                                    0.978052\n",
      "trainer/Q Targets Max                                    0.585661\n",
      "trainer/Q Targets Min                                   -4.27923\n",
      "trainer/Log Pis Mean                                     1.93776\n",
      "trainer/Log Pis Std                                      1.54795\n",
      "trainer/Log Pis Max                                      7.19185\n",
      "trainer/Log Pis Min                                     -2.61064\n",
      "trainer/Policy mu Mean                                  -0.0770262\n",
      "trainer/Policy mu Std                                    0.624622\n",
      "trainer/Policy mu Max                                    2.38489\n",
      "trainer/Policy mu Min                                   -2.90399\n",
      "trainer/Policy log std Mean                             -2.05641\n",
      "trainer/Policy log std Std                               0.723417\n",
      "trainer/Policy log std Max                              -0.0430002\n",
      "trainer/Policy log std Min                              -3.45056\n",
      "trainer/Alpha                                            0.0213343\n",
      "trainer/Alpha Loss                                      -0.239476\n",
      "exploration/num steps total                          10300\n",
      "exploration/num paths total                            515\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0351474\n",
      "exploration/Rewards Std                                  0.0857104\n",
      "exploration/Rewards Max                                  0.124257\n",
      "exploration/Rewards Min                                 -0.325015\n",
      "exploration/Returns Mean                                -0.702948\n",
      "exploration/Returns Std                                  1.07739\n",
      "exploration/Returns Max                                  0.889091\n",
      "exploration/Returns Min                                 -1.78588\n",
      "exploration/Actions Mean                                 0.0100113\n",
      "exploration/Actions Std                                  0.140976\n",
      "exploration/Actions Max                                  0.48463\n",
      "exploration/Actions Min                                 -0.759252\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.702948\n",
      "exploration/env_infos/final/reward_dist Mean             0.312823\n",
      "exploration/env_infos/final/reward_dist Std              0.393657\n",
      "exploration/env_infos/final/reward_dist Max              0.939643\n",
      "exploration/env_infos/final/reward_dist Min              6.46452e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0045168\n",
      "exploration/env_infos/initial/reward_dist Std            0.00683804\n",
      "exploration/env_infos/initial/reward_dist Max            0.0179175\n",
      "exploration/env_infos/initial/reward_dist Min            3.79492e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.26871\n",
      "exploration/env_infos/reward_dist Std                    0.353844\n",
      "exploration/env_infos/reward_dist Max                    0.998031\n",
      "exploration/env_infos/reward_dist Min                    6.46452e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.183503\n",
      "exploration/env_infos/final/reward_energy Std            0.12635\n",
      "exploration/env_infos/final/reward_energy Max           -0.0276726\n",
      "exploration/env_infos/final/reward_energy Min           -0.377441\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.45649\n",
      "exploration/env_infos/initial/reward_energy Std          0.253298\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0402386\n",
      "exploration/env_infos/initial/reward_energy Min         -0.833935\n",
      "exploration/env_infos/reward_energy Mean                -0.157365\n",
      "exploration/env_infos/reward_energy Std                  0.123227\n",
      "exploration/env_infos/reward_energy Max                 -0.0168796\n",
      "exploration/env_infos/reward_energy Min                 -0.833935\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00415793\n",
      "exploration/env_infos/final/end_effector_loc Std         0.214327\n",
      "exploration/env_infos/final/end_effector_loc Max         0.438653\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.228936\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00525244\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0176944\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0242315\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0379626\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0233486\n",
      "exploration/env_infos/end_effector_loc Std               0.166144\n",
      "exploration/env_infos/end_effector_loc Max               0.438653\n",
      "exploration/env_infos/end_effector_loc Min              -0.320126\n",
      "evaluation/num steps total                           93000\n",
      "evaluation/num paths total                            4650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.045148\n",
      "evaluation/Rewards Std                                   0.0760671\n",
      "evaluation/Rewards Max                                   0.121493\n",
      "evaluation/Rewards Min                                  -0.523611\n",
      "evaluation/Returns Mean                                 -0.90296\n",
      "evaluation/Returns Std                                   1.09819\n",
      "evaluation/Returns Max                                   1.32445\n",
      "evaluation/Returns Min                                  -3.52642\n",
      "evaluation/Actions Mean                                  0.0049153\n",
      "evaluation/Actions Std                                   0.073647\n",
      "evaluation/Actions Max                                   0.695832\n",
      "evaluation/Actions Min                                  -0.751403\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.90296\n",
      "evaluation/env_infos/final/reward_dist Mean              0.286862\n",
      "evaluation/env_infos/final/reward_dist Std               0.305922\n",
      "evaluation/env_infos/final/reward_dist Max               0.965275\n",
      "evaluation/env_infos/final/reward_dist Min               1.024e-40\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00871863\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0159236\n",
      "evaluation/env_infos/initial/reward_dist Max             0.064876\n",
      "evaluation/env_infos/initial/reward_dist Min             1.09472e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.228392\n",
      "evaluation/env_infos/reward_dist Std                     0.287444\n",
      "evaluation/env_infos/reward_dist Max                     0.999514\n",
      "evaluation/env_infos/reward_dist Min                     1.024e-40\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0304411\n",
      "evaluation/env_infos/final/reward_energy Std             0.0262543\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00216157\n",
      "evaluation/env_infos/final/reward_energy Min            -0.108308\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.276411\n",
      "evaluation/env_infos/initial/reward_energy Std           0.236804\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00465378\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.835442\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0563545\n",
      "evaluation/env_infos/reward_energy Std                   0.087865\n",
      "evaluation/env_infos/reward_energy Max                  -0.000495699\n",
      "evaluation/env_infos/reward_energy Min                  -0.835442\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0312519\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.213039\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.648235\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.415151\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000871627\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.012839\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0347916\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0375701\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00683468\n",
      "evaluation/env_infos/end_effector_loc Std                0.149621\n",
      "evaluation/env_infos/end_effector_loc Max                0.648235\n",
      "evaluation/env_infos/end_effector_loc Min               -0.415886\n",
      "time/data storing (s)                                    0.00640034\n",
      "time/evaluation sampling (s)                             0.970173\n",
      "time/exploration sampling (s)                            0.166264\n",
      "time/logging (s)                                         0.0202447\n",
      "time/saving (s)                                          0.0281876\n",
      "time/training (s)                                       51.7825\n",
      "time/epoch (s)                                          52.9737\n",
      "time/total (s)                                        4660.99\n",
      "Epoch                                                   92\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:07:12.171132 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 93 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0014338\n",
      "trainer/QF2 Loss                                         0.00112451\n",
      "trainer/Policy Loss                                      3.01086\n",
      "trainer/Q1 Predictions Mean                             -1.13184\n",
      "trainer/Q1 Predictions Std                               0.876876\n",
      "trainer/Q1 Predictions Max                               0.561032\n",
      "trainer/Q1 Predictions Min                              -3.39829\n",
      "trainer/Q2 Predictions Mean                             -1.12656\n",
      "trainer/Q2 Predictions Std                               0.88286\n",
      "trainer/Q2 Predictions Max                               0.57183\n",
      "trainer/Q2 Predictions Min                              -3.41693\n",
      "trainer/Q Targets Mean                                  -1.13012\n",
      "trainer/Q Targets Std                                    0.88325\n",
      "trainer/Q Targets Max                                    0.587334\n",
      "trainer/Q Targets Min                                   -3.39888\n",
      "trainer/Log Pis Mean                                     1.92171\n",
      "trainer/Log Pis Std                                      1.49466\n",
      "trainer/Log Pis Max                                      6.45196\n",
      "trainer/Log Pis Min                                     -3.35057\n",
      "trainer/Policy mu Mean                                   0.0318728\n",
      "trainer/Policy mu Std                                    0.499652\n",
      "trainer/Policy mu Max                                    2.09104\n",
      "trainer/Policy mu Min                                   -2.47841\n",
      "trainer/Policy log std Mean                             -2.15308\n",
      "trainer/Policy log std Std                               0.726284\n",
      "trainer/Policy log std Max                              -0.255523\n",
      "trainer/Policy log std Min                              -3.42699\n",
      "trainer/Alpha                                            0.0199785\n",
      "trainer/Alpha Loss                                      -0.306377\n",
      "exploration/num steps total                          10400\n",
      "exploration/num paths total                            520\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0818316\n",
      "exploration/Rewards Std                                  0.0690585\n",
      "exploration/Rewards Max                                  0.0427293\n",
      "exploration/Rewards Min                                 -0.286246\n",
      "exploration/Returns Mean                                -1.63663\n",
      "exploration/Returns Std                                  0.867621\n",
      "exploration/Returns Max                                 -0.425124\n",
      "exploration/Returns Min                                 -2.74545\n",
      "exploration/Actions Mean                                 0.00204908\n",
      "exploration/Actions Std                                  0.195256\n",
      "exploration/Actions Max                                  0.659228\n",
      "exploration/Actions Min                                 -0.789074\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.63663\n",
      "exploration/env_infos/final/reward_dist Mean             0.152231\n",
      "exploration/env_infos/final/reward_dist Std              0.159121\n",
      "exploration/env_infos/final/reward_dist Max              0.415779\n",
      "exploration/env_infos/final/reward_dist Min              1.40171e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0085615\n",
      "exploration/env_infos/initial/reward_dist Std            0.0153773\n",
      "exploration/env_infos/initial/reward_dist Max            0.0392272\n",
      "exploration/env_infos/initial/reward_dist Min            3.56095e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.236366\n",
      "exploration/env_infos/reward_dist Std                    0.276688\n",
      "exploration/env_infos/reward_dist Max                    0.957929\n",
      "exploration/env_infos/reward_dist Min                    1.40171e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.181601\n",
      "exploration/env_infos/final/reward_energy Std            0.119576\n",
      "exploration/env_infos/final/reward_energy Max           -0.0562047\n",
      "exploration/env_infos/final/reward_energy Min           -0.409284\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.471291\n",
      "exploration/env_infos/initial/reward_energy Std          0.33233\n",
      "exploration/env_infos/initial/reward_energy Max         -0.137056\n",
      "exploration/env_infos/initial/reward_energy Min         -0.969636\n",
      "exploration/env_infos/reward_energy Mean                -0.205306\n",
      "exploration/env_infos/reward_energy Std                  0.184681\n",
      "exploration/env_infos/reward_energy Max                 -0.0122159\n",
      "exploration/env_infos/reward_energy Min                 -0.969636\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00852849\n",
      "exploration/env_infos/final/end_effector_loc Std         0.228825\n",
      "exploration/env_infos/final/end_effector_loc Max         0.436804\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.267153\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0046731\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0198459\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0281761\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0394537\n",
      "exploration/env_infos/end_effector_loc Mean              0.0151867\n",
      "exploration/env_infos/end_effector_loc Std               0.171908\n",
      "exploration/env_infos/end_effector_loc Max               0.436804\n",
      "exploration/env_infos/end_effector_loc Min              -0.267153\n",
      "evaluation/num steps total                           94000\n",
      "evaluation/num paths total                            4700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.037927\n",
      "evaluation/Rewards Std                                   0.0698449\n",
      "evaluation/Rewards Max                                   0.138208\n",
      "evaluation/Rewards Min                                  -0.41867\n",
      "evaluation/Returns Mean                                 -0.75854\n",
      "evaluation/Returns Std                                   1.0095\n",
      "evaluation/Returns Max                                   1.63549\n",
      "evaluation/Returns Min                                  -3.27317\n",
      "evaluation/Actions Mean                                  0.00359706\n",
      "evaluation/Actions Std                                   0.0828737\n",
      "evaluation/Actions Max                                   0.933374\n",
      "evaluation/Actions Min                                  -0.621186\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.75854\n",
      "evaluation/env_infos/final/reward_dist Mean              0.155648\n",
      "evaluation/env_infos/final/reward_dist Std               0.228578\n",
      "evaluation/env_infos/final/reward_dist Max               0.857586\n",
      "evaluation/env_infos/final/reward_dist Min               3.37008e-24\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00775459\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0113553\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0485993\n",
      "evaluation/env_infos/initial/reward_dist Min             1.78501e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.201788\n",
      "evaluation/env_infos/reward_dist Std                     0.263917\n",
      "evaluation/env_infos/reward_dist Max                     0.991164\n",
      "evaluation/env_infos/reward_dist Min                     3.37008e-24\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0467003\n",
      "evaluation/env_infos/final/reward_energy Std             0.0511996\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00287057\n",
      "evaluation/env_infos/final/reward_energy Min            -0.269855\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.331864\n",
      "evaluation/env_infos/initial/reward_energy Std           0.249472\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0123212\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.01084\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0653904\n",
      "evaluation/env_infos/reward_energy Std                   0.0973965\n",
      "evaluation/env_infos/reward_energy Max                  -0.000810198\n",
      "evaluation/env_infos/reward_energy Min                  -1.01084\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0669496\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.2236\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.697174\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.371247\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00293552\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0143821\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0466687\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0310593\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0374862\n",
      "evaluation/env_infos/end_effector_loc Std                0.15652\n",
      "evaluation/env_infos/end_effector_loc Max                0.697174\n",
      "evaluation/env_infos/end_effector_loc Min               -0.371247\n",
      "time/data storing (s)                                    0.00662253\n",
      "time/evaluation sampling (s)                             0.978208\n",
      "time/exploration sampling (s)                            0.123657\n",
      "time/logging (s)                                         0.022938\n",
      "time/saving (s)                                          0.028302\n",
      "time/training (s)                                       50.4988\n",
      "time/epoch (s)                                          51.6585\n",
      "time/total (s)                                        4713.78\n",
      "Epoch                                                   93\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:08:04.380005 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 94 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000575166\r\n",
      "trainer/QF2 Loss                                         0.00141247\r\n",
      "trainer/Policy Loss                                      2.90604\r\n",
      "trainer/Q1 Predictions Mean                             -1.06254\r\n",
      "trainer/Q1 Predictions Std                               0.869546\r\n",
      "trainer/Q1 Predictions Max                               0.88482\r\n",
      "trainer/Q1 Predictions Min                              -4.19677\r\n",
      "trainer/Q2 Predictions Mean                             -1.05963\r\n",
      "trainer/Q2 Predictions Std                               0.868019\r\n",
      "trainer/Q2 Predictions Max                               0.843896\r\n",
      "trainer/Q2 Predictions Min                              -4.16778\r\n",
      "trainer/Q Targets Mean                                  -1.06553\r\n",
      "trainer/Q Targets Std                                    0.866299\r\n",
      "trainer/Q Targets Max                                    0.872233\r\n",
      "trainer/Q Targets Min                                   -4.21349\r\n",
      "trainer/Log Pis Mean                                     1.88531\r\n",
      "trainer/Log Pis Std                                      1.38779\r\n",
      "trainer/Log Pis Max                                      4.79531\r\n",
      "trainer/Log Pis Min                                     -2.008\r\n",
      "trainer/Policy mu Mean                                   0.0300278\r\n",
      "trainer/Policy mu Std                                    0.53769\r\n",
      "trainer/Policy mu Max                                    2.46504\r\n",
      "trainer/Policy mu Min                                   -3.38\r\n",
      "trainer/Policy log std Mean                             -2.1591\r\n",
      "trainer/Policy log std Std                               0.761353\r\n",
      "trainer/Policy log std Max                               0.350627\r\n",
      "trainer/Policy log std Min                              -3.52934\r\n",
      "trainer/Alpha                                            0.0184449\r\n",
      "trainer/Alpha Loss                                      -0.457943\r\n",
      "exploration/num steps total                          10500\r\n",
      "exploration/num paths total                            525\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0172575\r\n",
      "exploration/Rewards Std                                  0.0946582\r\n",
      "exploration/Rewards Max                                  0.135774\r\n",
      "exploration/Rewards Min                                 -0.351568\r\n",
      "exploration/Returns Mean                                -0.345149\r\n",
      "exploration/Returns Std                                  1.4798\r\n",
      "exploration/Returns Max                                  1.61737\r\n",
      "exploration/Returns Min                                 -2.95785\r\n",
      "exploration/Actions Mean                                 0.00239823\r\n",
      "exploration/Actions Std                                  0.165584\r\n",
      "exploration/Actions Max                                  0.913645\r\n",
      "exploration/Actions Min                                 -0.555215\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.345149\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.362632\r\n",
      "exploration/env_infos/final/reward_dist Std              0.309477\r\n",
      "exploration/env_infos/final/reward_dist Max              0.862452\r\n",
      "exploration/env_infos/final/reward_dist Min              1.2122e-11\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00562342\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00402658\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0102902\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.9579e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.413632\r\n",
      "exploration/env_infos/reward_dist Std                    0.357234\r\n",
      "exploration/env_infos/reward_dist Max                    0.999367\r\n",
      "exploration/env_infos/reward_dist Min                    1.2122e-11\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.136633\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0902252\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0414426\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.288889\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.377781\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.41088\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0268315\r\n",
      "exploration/env_infos/initial/reward_energy Min         -1.14064\r\n",
      "exploration/env_infos/reward_energy Mean                -0.171077\r\n",
      "exploration/env_infos/reward_energy Std                  0.159938\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0146434\r\n",
      "exploration/env_infos/reward_energy Min                 -1.14064\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.121866\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.18381\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.478936\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.205619\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0109828\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0163953\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0456822\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00591066\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0761319\r\n",
      "exploration/env_infos/end_effector_loc Std               0.133831\r\n",
      "exploration/env_infos/end_effector_loc Max               0.478936\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.205619\r\n",
      "evaluation/num steps total                           95000\r\n",
      "evaluation/num paths total                            4750\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0383952\r\n",
      "evaluation/Rewards Std                                   0.0730915\r\n",
      "evaluation/Rewards Max                                   0.163157\r\n",
      "evaluation/Rewards Min                                  -0.305742\r\n",
      "evaluation/Returns Mean                                 -0.767904\r\n",
      "evaluation/Returns Std                                   1.11961\r\n",
      "evaluation/Returns Max                                   1.63791\r\n",
      "evaluation/Returns Min                                  -3.29782\r\n",
      "evaluation/Actions Mean                                  0.00531959\r\n",
      "evaluation/Actions Std                                   0.0718794\r\n",
      "evaluation/Actions Max                                   0.940012\r\n",
      "evaluation/Actions Min                                  -0.614148\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.767904\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.244458\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.330343\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.999125\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.5093e-30\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0112611\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0201979\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.089424\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.82273e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.220571\r\n",
      "evaluation/env_infos/reward_dist Std                     0.290503\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999125\r\n",
      "evaluation/env_infos/reward_dist Min                     2.5093e-30\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.032574\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0324394\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00354249\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.193749\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.264577\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.236459\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00811426\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.955299\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0581937\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0836862\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000311058\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.955299\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0693796\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.216363\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.685141\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.338513\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00126381\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0124818\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0470006\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0307074\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0346644\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.144093\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.685141\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.338513\r\n",
      "time/data storing (s)                                    0.00655697\r\n",
      "time/evaluation sampling (s)                             0.898263\r\n",
      "time/exploration sampling (s)                            0.122426\r\n",
      "time/logging (s)                                         0.0209988\r\n",
      "time/saving (s)                                          0.0279227\r\n",
      "time/training (s)                                       50.0855\r\n",
      "time/epoch (s)                                          51.1617\r\n",
      "time/total (s)                                        4765.98\r\n",
      "Epoch                                                   94\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:09:02.808405 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 95 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00150455\r\n",
      "trainer/QF2 Loss                                         0.00171364\r\n",
      "trainer/Policy Loss                                      3.1628\r\n",
      "trainer/Q1 Predictions Mean                             -1.12038\r\n",
      "trainer/Q1 Predictions Std                               0.905319\r\n",
      "trainer/Q1 Predictions Max                               0.617779\r\n",
      "trainer/Q1 Predictions Min                              -3.98002\r\n",
      "trainer/Q2 Predictions Mean                             -1.12686\r\n",
      "trainer/Q2 Predictions Std                               0.911765\r\n",
      "trainer/Q2 Predictions Max                               0.611186\r\n",
      "trainer/Q2 Predictions Min                              -3.89522\r\n",
      "trainer/Q Targets Mean                                  -1.11254\r\n",
      "trainer/Q Targets Std                                    0.903717\r\n",
      "trainer/Q Targets Max                                    0.661685\r\n",
      "trainer/Q Targets Min                                   -3.8289\r\n",
      "trainer/Log Pis Mean                                     2.07918\r\n",
      "trainer/Log Pis Std                                      1.48106\r\n",
      "trainer/Log Pis Max                                      5.83534\r\n",
      "trainer/Log Pis Min                                     -5.82418\r\n",
      "trainer/Policy mu Mean                                   0.0225377\r\n",
      "trainer/Policy mu Std                                    0.509012\r\n",
      "trainer/Policy mu Max                                    1.72659\r\n",
      "trainer/Policy mu Min                                   -2.65173\r\n",
      "trainer/Policy log std Mean                             -2.23232\r\n",
      "trainer/Policy log std Std                               0.730752\r\n",
      "trainer/Policy log std Max                              -0.344891\r\n",
      "trainer/Policy log std Min                              -3.51284\r\n",
      "trainer/Alpha                                            0.0183444\r\n",
      "trainer/Alpha Loss                                       0.316729\r\n",
      "exploration/num steps total                          10600\r\n",
      "exploration/num paths total                            530\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0567882\r\n",
      "exploration/Rewards Std                                  0.0724923\r\n",
      "exploration/Rewards Max                                  0.0563545\r\n",
      "exploration/Rewards Min                                 -0.397578\r\n",
      "exploration/Returns Mean                                -1.13576\r\n",
      "exploration/Returns Std                                  0.799289\r\n",
      "exploration/Returns Max                                  0.0465602\r\n",
      "exploration/Returns Min                                 -1.89012\r\n",
      "exploration/Actions Mean                                 0.00396645\r\n",
      "exploration/Actions Std                                  0.166481\r\n",
      "exploration/Actions Max                                  0.793514\r\n",
      "exploration/Actions Min                                 -0.525175\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.13576\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.25648\r\n",
      "exploration/env_infos/final/reward_dist Std              0.275651\r\n",
      "exploration/env_infos/final/reward_dist Max              0.698508\r\n",
      "exploration/env_infos/final/reward_dist Min              0.000211249\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00572162\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00856935\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0221821\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.73739e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.234048\r\n",
      "exploration/env_infos/reward_dist Std                    0.229953\r\n",
      "exploration/env_infos/reward_dist Max                    0.909768\r\n",
      "exploration/env_infos/reward_dist Min                    3.73739e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.114499\r\n",
      "exploration/env_infos/final/reward_energy Std            0.067706\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.029226\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.228797\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.534307\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.235819\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.320975\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.927839\r\n",
      "exploration/env_infos/reward_energy Mean                -0.181105\r\n",
      "exploration/env_infos/reward_energy Std                  0.150548\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0190372\r\n",
      "exploration/env_infos/reward_energy Min                 -0.927839\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0892881\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.198012\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.209681\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.384362\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00040517\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0206447\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0396757\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0220526\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0476102\r\n",
      "exploration/env_infos/end_effector_loc Std               0.157341\r\n",
      "exploration/env_infos/end_effector_loc Max               0.248043\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.384362\r\n",
      "evaluation/num steps total                           96000\r\n",
      "evaluation/num paths total                            4800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0373031\r\n",
      "evaluation/Rewards Std                                   0.0763979\r\n",
      "evaluation/Rewards Max                                   0.142775\r\n",
      "evaluation/Rewards Min                                  -0.390081\r\n",
      "evaluation/Returns Mean                                 -0.746062\r\n",
      "evaluation/Returns Std                                   1.10995\r\n",
      "evaluation/Returns Max                                   2.01122\r\n",
      "evaluation/Returns Min                                  -3.67219\r\n",
      "evaluation/Actions Mean                                  0.00326278\r\n",
      "evaluation/Actions Std                                   0.0818462\r\n",
      "evaluation/Actions Max                                   0.843851\r\n",
      "evaluation/Actions Min                                  -0.745022\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.746062\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.197377\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.272315\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.878838\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.41335e-20\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00691069\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117136\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0479428\r\n",
      "evaluation/env_infos/initial/reward_dist Min             4.30124e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.217883\r\n",
      "evaluation/env_infos/reward_dist Std                     0.271134\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999407\r\n",
      "evaluation/env_infos/reward_dist Min                     6.41335e-20\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0296439\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0247428\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00256603\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.115229\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.351727\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.253632\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0168715\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.972023\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0621754\r\n",
      "evaluation/env_infos/reward_energy Std                   0.09774\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000525087\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.972023\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0124072\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.232332\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.517792\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.58924\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000714163\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0153147\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0421926\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0372511\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0130597\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.160262\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.517792\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.58924\r\n",
      "time/data storing (s)                                    0.00629272\r\n",
      "time/evaluation sampling (s)                             0.974577\r\n",
      "time/exploration sampling (s)                            0.145661\r\n",
      "time/logging (s)                                         0.0195828\r\n",
      "time/saving (s)                                          0.026886\r\n",
      "time/training (s)                                       56.0145\r\n",
      "time/epoch (s)                                          57.1875\r\n",
      "time/total (s)                                        4824.41\r\n",
      "Epoch                                                   95\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:09:54.155729 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 96 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000883497\n",
      "trainer/QF2 Loss                                         0.000841947\n",
      "trainer/Policy Loss                                      3.37279\n",
      "trainer/Q1 Predictions Mean                             -1.19661\n",
      "trainer/Q1 Predictions Std                               0.899509\n",
      "trainer/Q1 Predictions Max                               0.407772\n",
      "trainer/Q1 Predictions Min                              -4.45912\n",
      "trainer/Q2 Predictions Mean                             -1.19789\n",
      "trainer/Q2 Predictions Std                               0.899174\n",
      "trainer/Q2 Predictions Max                               0.440796\n",
      "trainer/Q2 Predictions Min                              -4.47851\n",
      "trainer/Q Targets Mean                                  -1.1978\n",
      "trainer/Q Targets Std                                    0.900722\n",
      "trainer/Q Targets Max                                    0.438111\n",
      "trainer/Q Targets Min                                   -4.40732\n",
      "trainer/Log Pis Mean                                     2.23425\n",
      "trainer/Log Pis Std                                      1.32238\n",
      "trainer/Log Pis Max                                      4.7751\n",
      "trainer/Log Pis Min                                     -3.32216\n",
      "trainer/Policy mu Mean                                  -0.00471572\n",
      "trainer/Policy mu Std                                    0.479822\n",
      "trainer/Policy mu Max                                    1.89065\n",
      "trainer/Policy mu Min                                   -2.31487\n",
      "trainer/Policy log std Mean                             -2.30655\n",
      "trainer/Policy log std Std                               0.731435\n",
      "trainer/Policy log std Max                              -0.315712\n",
      "trainer/Policy log std Min                              -3.66109\n",
      "trainer/Alpha                                            0.0185629\n",
      "trainer/Alpha Loss                                       0.934254\n",
      "exploration/num steps total                          10700\n",
      "exploration/num paths total                            535\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.115735\n",
      "exploration/Rewards Std                                  0.0810013\n",
      "exploration/Rewards Max                                  0.0281302\n",
      "exploration/Rewards Min                                 -0.290902\n",
      "exploration/Returns Mean                                -2.31471\n",
      "exploration/Returns Std                                  1.3228\n",
      "exploration/Returns Max                                 -0.692794\n",
      "exploration/Returns Min                                 -3.88908\n",
      "exploration/Actions Mean                                 0.0046468\n",
      "exploration/Actions Std                                  0.109443\n",
      "exploration/Actions Max                                  0.396446\n",
      "exploration/Actions Min                                 -0.390406\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.31471\n",
      "exploration/env_infos/final/reward_dist Mean             0.0984841\n",
      "exploration/env_infos/final/reward_dist Std              0.157248\n",
      "exploration/env_infos/final/reward_dist Max              0.410846\n",
      "exploration/env_infos/final/reward_dist Min              3.45943e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000909593\n",
      "exploration/env_infos/initial/reward_dist Std            0.00106688\n",
      "exploration/env_infos/initial/reward_dist Max            0.00286898\n",
      "exploration/env_infos/initial/reward_dist Min            9.06045e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.125656\n",
      "exploration/env_infos/reward_dist Std                    0.200791\n",
      "exploration/env_infos/reward_dist Max                    0.930638\n",
      "exploration/env_infos/reward_dist Min                    3.45943e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.102377\n",
      "exploration/env_infos/final/reward_energy Std            0.0966614\n",
      "exploration/env_infos/final/reward_energy Max           -0.0111978\n",
      "exploration/env_infos/final/reward_energy Min           -0.290041\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.250458\n",
      "exploration/env_infos/initial/reward_energy Std          0.197926\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0256638\n",
      "exploration/env_infos/initial/reward_energy Min         -0.517304\n",
      "exploration/env_infos/reward_energy Mean                -0.117453\n",
      "exploration/env_infos/reward_energy Std                  0.101013\n",
      "exploration/env_infos/reward_energy Max                 -0.00456853\n",
      "exploration/env_infos/reward_energy Min                 -0.517304\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0657797\n",
      "exploration/env_infos/final/end_effector_loc Std         0.26195\n",
      "exploration/env_infos/final/end_effector_loc Max         0.429042\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.615995\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00428533\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0104411\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0198223\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0114497\n",
      "exploration/env_infos/end_effector_loc Mean              0.0390457\n",
      "exploration/env_infos/end_effector_loc Std               0.170559\n",
      "exploration/env_infos/end_effector_loc Max               0.429042\n",
      "exploration/env_infos/end_effector_loc Min              -0.615995\n",
      "evaluation/num steps total                           97000\n",
      "evaluation/num paths total                            4850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0443671\n",
      "evaluation/Rewards Std                                   0.0725628\n",
      "evaluation/Rewards Max                                   0.16074\n",
      "evaluation/Rewards Min                                  -0.528982\n",
      "evaluation/Returns Mean                                 -0.887343\n",
      "evaluation/Returns Std                                   1.12241\n",
      "evaluation/Returns Max                                   1.92782\n",
      "evaluation/Returns Min                                  -3.04686\n",
      "evaluation/Actions Mean                                  0.000858705\n",
      "evaluation/Actions Std                                   0.074789\n",
      "evaluation/Actions Max                                   0.920749\n",
      "evaluation/Actions Min                                  -0.750138\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.887343\n",
      "evaluation/env_infos/final/reward_dist Mean              0.228429\n",
      "evaluation/env_infos/final/reward_dist Std               0.29801\n",
      "evaluation/env_infos/final/reward_dist Max               0.993791\n",
      "evaluation/env_infos/final/reward_dist Min               1.76441e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00518622\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00780548\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0355283\n",
      "evaluation/env_infos/initial/reward_dist Min             1.82504e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.210347\n",
      "evaluation/env_infos/reward_dist Std                     0.272978\n",
      "evaluation/env_infos/reward_dist Max                     0.993791\n",
      "evaluation/env_infos/reward_dist Min                     1.76441e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0303496\n",
      "evaluation/env_infos/final/reward_energy Std             0.0273852\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00283179\n",
      "evaluation/env_infos/final/reward_energy Min            -0.131541\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.287943\n",
      "evaluation/env_infos/initial/reward_energy Std           0.248825\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0303032\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.03495\n",
      "evaluation/env_infos/reward_energy Mean                 -0.057992\n",
      "evaluation/env_infos/reward_energy Std                   0.0884601\n",
      "evaluation/env_infos/reward_energy Max                  -0.000460406\n",
      "evaluation/env_infos/reward_energy Min                  -1.03495\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0129032\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23056\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.512697\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.508846\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000316935\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134511\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0460375\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0375069\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00767605\n",
      "evaluation/env_infos/end_effector_loc Std                0.157321\n",
      "evaluation/env_infos/end_effector_loc Max                0.512697\n",
      "evaluation/env_infos/end_effector_loc Min               -0.508846\n",
      "time/data storing (s)                                    0.0060773\n",
      "time/evaluation sampling (s)                             1.04454\n",
      "time/exploration sampling (s)                            0.124235\n",
      "time/logging (s)                                         0.019913\n",
      "time/saving (s)                                          0.0266915\n",
      "time/training (s)                                       48.8824\n",
      "time/epoch (s)                                          50.1039\n",
      "time/total (s)                                        4875.75\n",
      "Epoch                                                   96\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:10:44.981476 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 97 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00243217\r\n",
      "trainer/QF2 Loss                                         0.00107363\r\n",
      "trainer/Policy Loss                                      3.1433\r\n",
      "trainer/Q1 Predictions Mean                             -1.1494\r\n",
      "trainer/Q1 Predictions Std                               0.899417\r\n",
      "trainer/Q1 Predictions Max                               0.498959\r\n",
      "trainer/Q1 Predictions Min                              -3.47488\r\n",
      "trainer/Q2 Predictions Mean                             -1.15529\r\n",
      "trainer/Q2 Predictions Std                               0.894786\r\n",
      "trainer/Q2 Predictions Max                               0.51023\r\n",
      "trainer/Q2 Predictions Min                              -3.49772\r\n",
      "trainer/Q Targets Mean                                  -1.14702\r\n",
      "trainer/Q Targets Std                                    0.894284\r\n",
      "trainer/Q Targets Max                                    0.499111\r\n",
      "trainer/Q Targets Min                                   -3.47578\r\n",
      "trainer/Log Pis Mean                                     2.02343\r\n",
      "trainer/Log Pis Std                                      1.41223\r\n",
      "trainer/Log Pis Max                                      5.02056\r\n",
      "trainer/Log Pis Min                                     -2.8398\r\n",
      "trainer/Policy mu Mean                                   0.0378364\r\n",
      "trainer/Policy mu Std                                    0.463256\r\n",
      "trainer/Policy mu Max                                    1.99209\r\n",
      "trainer/Policy mu Min                                   -1.8005\r\n",
      "trainer/Policy log std Mean                             -2.23187\r\n",
      "trainer/Policy log std Std                               0.726524\r\n",
      "trainer/Policy log std Max                              -0.210703\r\n",
      "trainer/Policy log std Min                              -3.55341\r\n",
      "trainer/Alpha                                            0.0200249\r\n",
      "trainer/Alpha Loss                                       0.0915984\r\n",
      "exploration/num steps total                          10800\r\n",
      "exploration/num paths total                            540\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0818915\r\n",
      "exploration/Rewards Std                                  0.057921\r\n",
      "exploration/Rewards Max                                  0.00999299\r\n",
      "exploration/Rewards Min                                 -0.489532\r\n",
      "exploration/Returns Mean                                -1.63783\r\n",
      "exploration/Returns Std                                  0.57\r\n",
      "exploration/Returns Max                                 -0.918101\r\n",
      "exploration/Returns Min                                 -2.19039\r\n",
      "exploration/Actions Mean                                -0.00106122\r\n",
      "exploration/Actions Std                                  0.0925317\r\n",
      "exploration/Actions Max                                  0.636148\r\n",
      "exploration/Actions Min                                 -0.377055\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.63783\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0238632\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0247944\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0677638\r\n",
      "exploration/env_infos/final/reward_dist Min              3.89459e-13\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00709544\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0066611\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0169295\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000703665\r\n",
      "exploration/env_infos/reward_dist Mean                   0.17143\r\n",
      "exploration/env_infos/reward_dist Std                    0.27067\r\n",
      "exploration/env_infos/reward_dist Max                    0.953414\r\n",
      "exploration/env_infos/reward_dist Min                    3.89459e-13\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0635346\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0066821\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0544741\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.0694498\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.241331\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.230022\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0630128\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.666893\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0947653\r\n",
      "exploration/env_infos/reward_energy Std                  0.0902552\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00208586\r\n",
      "exploration/env_infos/reward_energy Min                 -0.666893\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.033052\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.254081\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.392388\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.530112\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00317792\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0113507\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0318074\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0100079\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0155677\r\n",
      "exploration/env_infos/end_effector_loc Std               0.166183\r\n",
      "exploration/env_infos/end_effector_loc Max               0.392388\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.530112\r\n",
      "evaluation/num steps total                           98000\r\n",
      "evaluation/num paths total                            4900\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0454009\r\n",
      "evaluation/Rewards Std                                   0.0754837\r\n",
      "evaluation/Rewards Max                                   0.167728\r\n",
      "evaluation/Rewards Min                                  -0.352284\r\n",
      "evaluation/Returns Mean                                 -0.908019\r\n",
      "evaluation/Returns Std                                   1.21583\r\n",
      "evaluation/Returns Max                                   2.26575\r\n",
      "evaluation/Returns Min                                  -3.81297\r\n",
      "evaluation/Actions Mean                                  0.00136363\r\n",
      "evaluation/Actions Std                                   0.0766206\r\n",
      "evaluation/Actions Max                                   0.792495\r\n",
      "evaluation/Actions Min                                  -0.779216\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.908019\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.286075\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.313702\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.993061\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.60753e-14\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00707156\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0122914\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0536082\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.4641e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.221913\r\n",
      "evaluation/env_infos/reward_dist Std                     0.286642\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993061\r\n",
      "evaluation/env_infos/reward_dist Min                     1.22722e-15\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0324604\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0399525\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000605197\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.222632\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.26703\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.256598\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00738408\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.960393\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0597438\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0904202\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000418231\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.960393\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0135372\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.202409\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.472529\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.432194\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000944507\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0130592\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0396248\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0389608\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0101462\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.142761\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.472529\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.432194\r\n",
      "time/data storing (s)                                    0.0061194\r\n",
      "time/evaluation sampling (s)                             0.990104\r\n",
      "time/exploration sampling (s)                            0.117125\r\n",
      "time/logging (s)                                         0.0196509\r\n",
      "time/saving (s)                                          0.0269158\r\n",
      "time/training (s)                                       48.4003\r\n",
      "time/epoch (s)                                          49.5602\r\n",
      "time/total (s)                                        4926.57\r\n",
      "Epoch                                                   97\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:11:35.887616 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 98 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00144975\n",
      "trainer/QF2 Loss                                         0.00172845\n",
      "trainer/Policy Loss                                      3.07361\n",
      "trainer/Q1 Predictions Mean                             -1.22053\n",
      "trainer/Q1 Predictions Std                               0.900333\n",
      "trainer/Q1 Predictions Max                               0.617642\n",
      "trainer/Q1 Predictions Min                              -4.47952\n",
      "trainer/Q2 Predictions Mean                             -1.20952\n",
      "trainer/Q2 Predictions Std                               0.897617\n",
      "trainer/Q2 Predictions Max                               0.636625\n",
      "trainer/Q2 Predictions Min                              -4.46961\n",
      "trainer/Q Targets Mean                                  -1.21595\n",
      "trainer/Q Targets Std                                    0.900843\n",
      "trainer/Q Targets Max                                    0.640196\n",
      "trainer/Q Targets Min                                   -4.45638\n",
      "trainer/Log Pis Mean                                     1.913\n",
      "trainer/Log Pis Std                                      1.42197\n",
      "trainer/Log Pis Max                                      4.4241\n",
      "trainer/Log Pis Min                                     -5.29478\n",
      "trainer/Policy mu Mean                                   0.0733059\n",
      "trainer/Policy mu Std                                    0.485325\n",
      "trainer/Policy mu Max                                    1.98306\n",
      "trainer/Policy mu Min                                   -1.9008\n",
      "trainer/Policy log std Mean                             -2.21569\n",
      "trainer/Policy log std Std                               0.699978\n",
      "trainer/Policy log std Max                              -0.423967\n",
      "trainer/Policy log std Min                              -3.32254\n",
      "trainer/Alpha                                            0.0217272\n",
      "trainer/Alpha Loss                                      -0.333201\n",
      "exploration/num steps total                          10900\n",
      "exploration/num paths total                            545\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0969706\n",
      "exploration/Rewards Std                                  0.067214\n",
      "exploration/Rewards Max                                  0.018034\n",
      "exploration/Rewards Min                                 -0.371506\n",
      "exploration/Returns Mean                                -1.93941\n",
      "exploration/Returns Std                                  0.421089\n",
      "exploration/Returns Max                                 -1.24701\n",
      "exploration/Returns Min                                 -2.29355\n",
      "exploration/Actions Mean                                 0.0032202\n",
      "exploration/Actions Std                                  0.120697\n",
      "exploration/Actions Max                                  0.726185\n",
      "exploration/Actions Min                                 -0.460558\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.93941\n",
      "exploration/env_infos/final/reward_dist Mean             0.0837498\n",
      "exploration/env_infos/final/reward_dist Std              0.115326\n",
      "exploration/env_infos/final/reward_dist Max              0.293262\n",
      "exploration/env_infos/final/reward_dist Min              1.15959e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0110637\n",
      "exploration/env_infos/initial/reward_dist Std            0.0170339\n",
      "exploration/env_infos/initial/reward_dist Max            0.0447221\n",
      "exploration/env_infos/initial/reward_dist Min            4.72197e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.179506\n",
      "exploration/env_infos/reward_dist Std                    0.287915\n",
      "exploration/env_infos/reward_dist Max                    0.997792\n",
      "exploration/env_infos/reward_dist Min                    1.15959e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.187537\n",
      "exploration/env_infos/final/reward_energy Std            0.0668322\n",
      "exploration/env_infos/final/reward_energy Max           -0.0671042\n",
      "exploration/env_infos/final/reward_energy Min           -0.250447\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.402983\n",
      "exploration/env_infos/initial/reward_energy Std          0.227197\n",
      "exploration/env_infos/initial/reward_energy Max         -0.101441\n",
      "exploration/env_infos/initial/reward_energy Min         -0.732578\n",
      "exploration/env_infos/reward_energy Mean                -0.120822\n",
      "exploration/env_infos/reward_energy Std                  0.120657\n",
      "exploration/env_infos/reward_energy Max                 -0.00788197\n",
      "exploration/env_infos/reward_energy Min                 -0.732578\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.19879\n",
      "exploration/env_infos/final/end_effector_loc Std         0.244863\n",
      "exploration/env_infos/final/end_effector_loc Max         0.517868\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.269374\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0095715\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0132629\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0363093\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00682692\n",
      "exploration/env_infos/end_effector_loc Mean              0.11517\n",
      "exploration/env_infos/end_effector_loc Std               0.169987\n",
      "exploration/env_infos/end_effector_loc Max               0.517868\n",
      "exploration/env_infos/end_effector_loc Min              -0.269374\n",
      "evaluation/num steps total                           99000\n",
      "evaluation/num paths total                            4950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0426496\n",
      "evaluation/Rewards Std                                   0.0714846\n",
      "evaluation/Rewards Max                                   0.122247\n",
      "evaluation/Rewards Min                                  -0.34042\n",
      "evaluation/Returns Mean                                 -0.852991\n",
      "evaluation/Returns Std                                   1.05748\n",
      "evaluation/Returns Max                                   1.04957\n",
      "evaluation/Returns Min                                  -2.99592\n",
      "evaluation/Actions Mean                                  0.00467506\n",
      "evaluation/Actions Std                                   0.0609315\n",
      "evaluation/Actions Max                                   0.826764\n",
      "evaluation/Actions Min                                  -0.556945\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.852991\n",
      "evaluation/env_infos/final/reward_dist Mean              0.242709\n",
      "evaluation/env_infos/final/reward_dist Std               0.323032\n",
      "evaluation/env_infos/final/reward_dist Max               0.892938\n",
      "evaluation/env_infos/final/reward_dist Min               7.33519e-45\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00477655\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00866049\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0349819\n",
      "evaluation/env_infos/initial/reward_dist Min             1.05462e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.219155\n",
      "evaluation/env_infos/reward_dist Std                     0.301018\n",
      "evaluation/env_infos/reward_dist Max                     0.997626\n",
      "evaluation/env_infos/reward_dist Min                     7.33519e-45\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0310761\n",
      "evaluation/env_infos/final/reward_energy Std             0.0256573\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00387819\n",
      "evaluation/env_infos/final/reward_energy Min            -0.112876\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.231871\n",
      "evaluation/env_infos/initial/reward_energy Std           0.173976\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0265547\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.883941\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0506253\n",
      "evaluation/env_infos/reward_energy Std                   0.0700434\n",
      "evaluation/env_infos/reward_energy Max                  -0.00163092\n",
      "evaluation/env_infos/reward_energy Min                  -0.883941\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0422702\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.227213\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.588666\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.387873\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000637238\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0102291\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0413382\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0278473\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0145428\n",
      "evaluation/env_infos/end_effector_loc Std                0.148714\n",
      "evaluation/env_infos/end_effector_loc Max                0.588666\n",
      "evaluation/env_infos/end_effector_loc Min               -0.387873\n",
      "time/data storing (s)                                    0.00616116\n",
      "time/evaluation sampling (s)                             1.04534\n",
      "time/exploration sampling (s)                            0.119852\n",
      "time/logging (s)                                         0.0200074\n",
      "time/saving (s)                                          0.0270994\n",
      "time/training (s)                                       48.433\n",
      "time/epoch (s)                                          49.6515\n",
      "time/total (s)                                        4977.48\n",
      "Epoch                                                   98\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:12:26.112880 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 99 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.00177027\n",
      "trainer/QF2 Loss                                          0.0015849\n",
      "trainer/Policy Loss                                       3.21083\n",
      "trainer/Q1 Predictions Mean                              -1.14515\n",
      "trainer/Q1 Predictions Std                                0.921126\n",
      "trainer/Q1 Predictions Max                                0.798337\n",
      "trainer/Q1 Predictions Min                               -3.2957\n",
      "trainer/Q2 Predictions Mean                              -1.14952\n",
      "trainer/Q2 Predictions Std                                0.921889\n",
      "trainer/Q2 Predictions Max                                0.78482\n",
      "trainer/Q2 Predictions Min                               -3.31543\n",
      "trainer/Q Targets Mean                                   -1.16268\n",
      "trainer/Q Targets Std                                     0.918443\n",
      "trainer/Q Targets Max                                     0.758549\n",
      "trainer/Q Targets Min                                    -3.33608\n",
      "trainer/Log Pis Mean                                      2.11084\n",
      "trainer/Log Pis Std                                       1.45191\n",
      "trainer/Log Pis Max                                       4.66329\n",
      "trainer/Log Pis Min                                      -2.42879\n",
      "trainer/Policy mu Mean                                   -0.00858343\n",
      "trainer/Policy mu Std                                     0.469079\n",
      "trainer/Policy mu Max                                     1.8055\n",
      "trainer/Policy mu Min                                    -1.96514\n",
      "trainer/Policy log std Mean                              -2.26178\n",
      "trainer/Policy log std Std                                0.735147\n",
      "trainer/Policy log std Max                               -0.390337\n",
      "trainer/Policy log std Min                               -3.48512\n",
      "trainer/Alpha                                             0.0209694\n",
      "trainer/Alpha Loss                                        0.428439\n",
      "exploration/num steps total                           11000\n",
      "exploration/num paths total                             550\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0762588\n",
      "exploration/Rewards Std                                   0.0612162\n",
      "exploration/Rewards Max                                   0.0530134\n",
      "exploration/Rewards Min                                  -0.226374\n",
      "exploration/Returns Mean                                 -1.52518\n",
      "exploration/Returns Std                                   0.875395\n",
      "exploration/Returns Max                                  -0.610133\n",
      "exploration/Returns Min                                  -3.01908\n",
      "exploration/Actions Mean                                  0.00486841\n",
      "exploration/Actions Std                                   0.0970521\n",
      "exploration/Actions Max                                   0.317694\n",
      "exploration/Actions Min                                  -0.259397\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.52518\n",
      "exploration/env_infos/final/reward_dist Mean              0.171622\n",
      "exploration/env_infos/final/reward_dist Std               0.28904\n",
      "exploration/env_infos/final/reward_dist Max               0.746786\n",
      "exploration/env_infos/final/reward_dist Min               1.78777e-05\n",
      "exploration/env_infos/initial/reward_dist Mean            0.0056444\n",
      "exploration/env_infos/initial/reward_dist Std             0.00569794\n",
      "exploration/env_infos/initial/reward_dist Max             0.0127317\n",
      "exploration/env_infos/initial/reward_dist Min             2.51299e-05\n",
      "exploration/env_infos/reward_dist Mean                    0.21271\n",
      "exploration/env_infos/reward_dist Std                     0.274826\n",
      "exploration/env_infos/reward_dist Max                     0.982735\n",
      "exploration/env_infos/reward_dist Min                     1.58086e-06\n",
      "exploration/env_infos/final/reward_energy Mean           -0.127158\n",
      "exploration/env_infos/final/reward_energy Std             0.0940162\n",
      "exploration/env_infos/final/reward_energy Max            -0.0178786\n",
      "exploration/env_infos/final/reward_energy Min            -0.286261\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.201827\n",
      "exploration/env_infos/initial/reward_energy Std           0.0873548\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0946978\n",
      "exploration/env_infos/initial/reward_energy Min          -0.339247\n",
      "exploration/env_infos/reward_energy Mean                 -0.112314\n",
      "exploration/env_infos/reward_energy Std                   0.0791908\n",
      "exploration/env_infos/reward_energy Max                  -0.00653273\n",
      "exploration/env_infos/reward_energy Min                  -0.339247\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.0429079\n",
      "exploration/env_infos/final/end_effector_loc Std          0.205713\n",
      "exploration/env_infos/final/end_effector_loc Max          0.329409\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.286246\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.000790367\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00773508\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0153058\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0101638\n",
      "exploration/env_infos/end_effector_loc Mean               0.0191813\n",
      "exploration/env_infos/end_effector_loc Std                0.133763\n",
      "exploration/env_infos/end_effector_loc Max                0.329409\n",
      "exploration/env_infos/end_effector_loc Min               -0.286246\n",
      "evaluation/num steps total                           100000\n",
      "evaluation/num paths total                             5000\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0368138\n",
      "evaluation/Rewards Std                                    0.0791709\n",
      "evaluation/Rewards Max                                    0.165293\n",
      "evaluation/Rewards Min                                   -0.46463\n",
      "evaluation/Returns Mean                                  -0.736276\n",
      "evaluation/Returns Std                                    1.08921\n",
      "evaluation/Returns Max                                    2.73977\n",
      "evaluation/Returns Min                                   -2.90433\n",
      "evaluation/Actions Mean                                  -0.00278925\n",
      "evaluation/Actions Std                                    0.0810519\n",
      "evaluation/Actions Max                                    0.954338\n",
      "evaluation/Actions Min                                   -0.521498\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.736276\n",
      "evaluation/env_infos/final/reward_dist Mean               0.233475\n",
      "evaluation/env_infos/final/reward_dist Std                0.298353\n",
      "evaluation/env_infos/final/reward_dist Max                0.998135\n",
      "evaluation/env_infos/final/reward_dist Min                5.12212e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00749168\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0136073\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0582328\n",
      "evaluation/env_infos/initial/reward_dist Min              1.19151e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.251221\n",
      "evaluation/env_infos/reward_dist Std                      0.295792\n",
      "evaluation/env_infos/reward_dist Max                      0.998135\n",
      "evaluation/env_infos/reward_dist Min                      5.12212e-18\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0324157\n",
      "evaluation/env_infos/final/reward_energy Std              0.0330843\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00641784\n",
      "evaluation/env_infos/final/reward_energy Min             -0.192526\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.3341\n",
      "evaluation/env_infos/initial/reward_energy Std            0.227867\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0284777\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.956471\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0658236\n",
      "evaluation/env_infos/reward_energy Std                    0.0939235\n",
      "evaluation/env_infos/reward_energy Max                   -0.000473759\n",
      "evaluation/env_infos/reward_energy Min                   -0.956471\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0650979\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.257978\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.577137\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.766615\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000134525\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0142974\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0477169\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0260749\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.0305365\n",
      "evaluation/env_infos/end_effector_loc Std                 0.175891\n",
      "evaluation/env_infos/end_effector_loc Max                 0.577137\n",
      "evaluation/env_infos/end_effector_loc Min                -0.766615\n",
      "time/data storing (s)                                     0.0062566\n",
      "time/evaluation sampling (s)                              0.845254\n",
      "time/exploration sampling (s)                             0.125193\n",
      "time/logging (s)                                          0.0196255\n",
      "time/saving (s)                                           0.0292258\n",
      "time/training (s)                                        48.3167\n",
      "time/epoch (s)                                           49.3422\n",
      "time/total (s)                                         5027.7\n",
      "Epoch                                                    99\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[21081]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21c94778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21d15740). One of the two will be used. Which one is undefined.\n",
      "objc[21081]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21c94700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21d15768). One of the two will be used. Which one is undefined.\n",
      "objc[21081]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21c947a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21d157b8). One of the two will be used. Which one is undefined.\n",
      "objc[21081]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21c94818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21d15830). One of the two will be used. Which one is undefined.\n",
      "using sparse reward if specified\n",
      "2021-05-25 13:12:33.181742 PDT | Variant:\n",
      "2021-05-25 13:12:33.182406 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"pointreacherobs\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 20,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 20,\n",
      "    \"num_train_loops_per_epoch\": 5\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.97,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 2000,\n",
      "    \"latent_dim\": 6,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": true,\n",
      "    \"n_sampled_latents\": 100,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"sparse_reward\": null,\n",
      "    \"fixed_ratio\": null\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": true,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"horizon\": 20\n",
      "  }\n",
      "}\n",
      "pointreacher\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "2021-05-25 13:13:06.531896 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 0 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        1.02318\n",
      "trainer/QF2 Loss                                        1.0213\n",
      "trainer/Policy Loss                                    -1.37885\n",
      "trainer/Q1 Predictions Mean                            -0.00123041\n",
      "trainer/Q1 Predictions Std                              0.000628694\n",
      "trainer/Q1 Predictions Max                              0.00059533\n",
      "trainer/Q1 Predictions Min                             -0.00226844\n",
      "trainer/Q2 Predictions Mean                            -5.11984e-05\n",
      "trainer/Q2 Predictions Std                              0.00069396\n",
      "trainer/Q2 Predictions Max                              0.00126331\n",
      "trainer/Q2 Predictions Min                             -0.0017172\n",
      "trainer/Q Targets Mean                                  0.819907\n",
      "trainer/Q Targets Std                                   0.590703\n",
      "trainer/Q Targets Max                                   1.69958\n",
      "trainer/Q Targets Min                                  -1.35869\n",
      "trainer/Log Pis Mean                                   -1.3802\n",
      "trainer/Log Pis Std                                     0.289152\n",
      "trainer/Log Pis Max                                    -0.577523\n",
      "trainer/Log Pis Min                                    -2.60808\n",
      "trainer/Policy mu Mean                                  0.000991679\n",
      "trainer/Policy mu Std                                   0.000561408\n",
      "trainer/Policy mu Max                                   0.00220575\n",
      "trainer/Policy mu Min                                   5.95292e-05\n",
      "trainer/Policy log std Mean                             5.1369e-05\n",
      "trainer/Policy log std Std                              0.000675256\n",
      "trainer/Policy log std Max                              0.000954493\n",
      "trainer/Policy log std Min                             -0.00125532\n",
      "trainer/Alpha                                           0.997005\n",
      "trainer/Alpha Loss                                     -0\n",
      "exploration/num steps total                          1100\n",
      "exploration/num paths total                            55\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.782749\n",
      "exploration/Rewards Std                                 0.347327\n",
      "exploration/Rewards Max                                -0.144178\n",
      "exploration/Rewards Min                                -1.48172\n",
      "exploration/Returns Mean                              -15.655\n",
      "exploration/Returns Std                                 4.11611\n",
      "exploration/Returns Max                               -10.2897\n",
      "exploration/Returns Min                               -21.0432\n",
      "exploration/Actions Mean                                0.0664757\n",
      "exploration/Actions Std                                 0.591084\n",
      "exploration/Actions Max                                 0.98148\n",
      "exploration/Actions Min                                -0.95017\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -15.655\n",
      "exploration/env_infos/final/reward_dist Mean            9.06924e-46\n",
      "exploration/env_infos/final/reward_dist Std             1.81385e-45\n",
      "exploration/env_infos/final/reward_dist Max             4.53462e-45\n",
      "exploration/env_infos/final/reward_dist Min             5.62953e-138\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00583009\n",
      "exploration/env_infos/initial/reward_dist Std           0.0115506\n",
      "exploration/env_infos/initial/reward_dist Max           0.0289309\n",
      "exploration/env_infos/initial/reward_dist Min           2.88528e-08\n",
      "exploration/env_infos/reward_dist Mean                  0.00653357\n",
      "exploration/env_infos/reward_dist Std                   0.032089\n",
      "exploration/env_infos/reward_dist Max                   0.226219\n",
      "exploration/env_infos/reward_dist Min                   5.62953e-138\n",
      "exploration/env_infos/final/reward_energy Mean         -0.804141\n",
      "exploration/env_infos/final/reward_energy Std           0.313001\n",
      "exploration/env_infos/final/reward_energy Max          -0.183826\n",
      "exploration/env_infos/final/reward_energy Min          -1.03901\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.906971\n",
      "exploration/env_infos/initial/reward_energy Std         0.128033\n",
      "exploration/env_infos/initial/reward_energy Max        -0.743948\n",
      "exploration/env_infos/initial/reward_energy Min        -1.11742\n",
      "exploration/env_infos/reward_energy Mean               -0.792445\n",
      "exploration/env_infos/reward_energy Std                 0.282186\n",
      "exploration/env_infos/reward_energy Max                -0.183826\n",
      "exploration/env_infos/reward_energy Min                -1.25848\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.4564\n",
      "exploration/env_infos/final/end_effector_loc Std        0.64723\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.0224551\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0233347\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0474246\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.036915\n",
      "exploration/env_infos/end_effector_loc Mean             0.292628\n",
      "exploration/env_infos/end_effector_loc Std              0.485147\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           1000\n",
      "evaluation/num paths total                             50\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0712633\n",
      "evaluation/Rewards Std                                  0.0475803\n",
      "evaluation/Rewards Max                                  0.0475514\n",
      "evaluation/Rewards Min                                 -0.173638\n",
      "evaluation/Returns Mean                                -1.42527\n",
      "evaluation/Returns Std                                  0.950319\n",
      "evaluation/Returns Max                                  0.788764\n",
      "evaluation/Returns Min                                 -3.44079\n",
      "evaluation/Actions Mean                                 0.000994572\n",
      "evaluation/Actions Std                                  0.000557625\n",
      "evaluation/Actions Max                                  0.00203314\n",
      "evaluation/Actions Min                                  0.000153224\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.42527\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00403968\n",
      "evaluation/env_infos/final/reward_dist Std              0.00768942\n",
      "evaluation/env_infos/final/reward_dist Max              0.0353948\n",
      "evaluation/env_infos/final/reward_dist Min              3.75807e-07\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00313915\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00574652\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0262178\n",
      "evaluation/env_infos/initial/reward_dist Min            7.84187e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.00336929\n",
      "evaluation/env_infos/reward_dist Std                    0.00617529\n",
      "evaluation/env_infos/reward_dist Max                    0.0353948\n",
      "evaluation/env_infos/reward_dist Min                    3.75807e-07\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.00163784\n",
      "evaluation/env_infos/final/reward_energy Std            0.000254649\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00111555\n",
      "evaluation/env_infos/final/reward_energy Min           -0.00207608\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.00155191\n",
      "evaluation/env_infos/initial/reward_energy Std          0.000214815\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00115512\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.00197441\n",
      "evaluation/env_infos/reward_energy Mean                -0.00159509\n",
      "evaluation/env_infos/reward_energy Std                  0.0002365\n",
      "evaluation/env_infos/reward_energy Max                 -0.00111555\n",
      "evaluation/env_infos/reward_energy Min                 -0.00207608\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.0103731\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.00574269\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.0208968\n",
      "evaluation/env_infos/final/end_effector_loc Min         0.00181135\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      4.87051e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std       2.63823e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Max       9.79645e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Min       7.66118e-06\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00379042\n",
      "evaluation/env_infos/end_effector_loc Std               0.00422653\n",
      "evaluation/env_infos/end_effector_loc Max               0.0208968\n",
      "evaluation/env_infos/end_effector_loc Min               7.66118e-06\n",
      "time/data storing (s)                                   0.171095\n",
      "time/evaluation sampling (s)                            0.924758\n",
      "time/exploration sampling (s)                           0.117184\n",
      "time/logging (s)                                        0.0205708\n",
      "time/saving (s)                                         0.0783875\n",
      "time/training (s)                                      28.9167\n",
      "time/epoch (s)                                         30.2287\n",
      "time/total (s)                                         36.647\n",
      "Epoch                                                   0\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:13:45.634294 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 1 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.021879\n",
      "trainer/QF2 Loss                                        0.0270591\n",
      "trainer/Policy Loss                                    -1.45842\n",
      "trainer/Q1 Predictions Mean                             0.132824\n",
      "trainer/Q1 Predictions Std                              0.726922\n",
      "trainer/Q1 Predictions Max                              0.952736\n",
      "trainer/Q1 Predictions Min                             -2.0146\n",
      "trainer/Q2 Predictions Mean                             0.153286\n",
      "trainer/Q2 Predictions Std                              0.718707\n",
      "trainer/Q2 Predictions Max                              1.00797\n",
      "trainer/Q2 Predictions Min                             -1.91428\n",
      "trainer/Q Targets Mean                                  0.105628\n",
      "trainer/Q Targets Std                                   0.733789\n",
      "trainer/Q Targets Max                                   0.976877\n",
      "trainer/Q Targets Min                                  -2.05525\n",
      "trainer/Log Pis Mean                                   -1.29245\n",
      "trainer/Log Pis Std                                     0.388878\n",
      "trainer/Log Pis Max                                    -0.57036\n",
      "trainer/Log Pis Min                                    -3.75367\n",
      "trainer/Policy mu Mean                                  0.0285586\n",
      "trainer/Policy mu Std                                   0.046973\n",
      "trainer/Policy mu Max                                   0.132546\n",
      "trainer/Policy mu Min                                  -0.0972846\n",
      "trainer/Policy log std Mean                            -0.348636\n",
      "trainer/Policy log std Std                              0.150635\n",
      "trainer/Policy log std Max                             -0.143799\n",
      "trainer/Policy log std Min                             -0.745127\n",
      "trainer/Alpha                                           0.224015\n",
      "trainer/Alpha Loss                                     -4.91595\n",
      "exploration/num steps total                          1200\n",
      "exploration/num paths total                            60\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.437723\n",
      "exploration/Rewards Std                                 0.290415\n",
      "exploration/Rewards Max                                -0.0161116\n",
      "exploration/Rewards Min                                -1.19085\n",
      "exploration/Returns Mean                               -8.75447\n",
      "exploration/Returns Std                                 3.54643\n",
      "exploration/Returns Max                                -4.47049\n",
      "exploration/Returns Min                               -14.1604\n",
      "exploration/Actions Mean                                0.0310878\n",
      "exploration/Actions Std                                 0.468715\n",
      "exploration/Actions Max                                 0.967497\n",
      "exploration/Actions Min                                -0.967218\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -8.75447\n",
      "exploration/env_infos/final/reward_dist Mean            2.57636e-30\n",
      "exploration/env_infos/final/reward_dist Std             5.15272e-30\n",
      "exploration/env_infos/final/reward_dist Max             1.28818e-29\n",
      "exploration/env_infos/final/reward_dist Min             1.51013e-89\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00155307\n",
      "exploration/env_infos/initial/reward_dist Std           0.00288381\n",
      "exploration/env_infos/initial/reward_dist Max           0.00731896\n",
      "exploration/env_infos/initial/reward_dist Min           4.46296e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0550432\n",
      "exploration/env_infos/reward_dist Std                   0.177504\n",
      "exploration/env_infos/reward_dist Max                   0.831432\n",
      "exploration/env_infos/reward_dist Min                   1.51013e-89\n",
      "exploration/env_infos/final/reward_energy Mean         -0.673666\n",
      "exploration/env_infos/final/reward_energy Std           0.179282\n",
      "exploration/env_infos/final/reward_energy Max          -0.380676\n",
      "exploration/env_infos/final/reward_energy Min          -0.906077\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.58767\n",
      "exploration/env_infos/initial/reward_energy Std         0.186861\n",
      "exploration/env_infos/initial/reward_energy Max        -0.379047\n",
      "exploration/env_infos/initial/reward_energy Min        -0.871669\n",
      "exploration/env_infos/reward_energy Mean               -0.603278\n",
      "exploration/env_infos/reward_energy Std                 0.278166\n",
      "exploration/env_infos/reward_energy Max                -0.0943582\n",
      "exploration/env_infos/reward_energy Min                -1.22577\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.208593\n",
      "exploration/env_infos/final/end_effector_loc Std        0.725824\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.000391298\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0217988\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0318921\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0408166\n",
      "exploration/env_infos/end_effector_loc Mean             0.0905952\n",
      "exploration/env_infos/end_effector_loc Std              0.485053\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           2000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.114125\n",
      "evaluation/Rewards Std                                  0.0957268\n",
      "evaluation/Rewards Max                                  0.111657\n",
      "evaluation/Rewards Min                                 -0.724508\n",
      "evaluation/Returns Mean                                -2.2825\n",
      "evaluation/Returns Std                                  1.23398\n",
      "evaluation/Returns Max                                  0.521001\n",
      "evaluation/Returns Min                                 -5.83967\n",
      "evaluation/Actions Mean                                 0.024116\n",
      "evaluation/Actions Std                                  0.0465231\n",
      "evaluation/Actions Max                                  0.113284\n",
      "evaluation/Actions Min                                 -0.0982297\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.2825\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0260026\n",
      "evaluation/env_infos/final/reward_dist Std              0.0849142\n",
      "evaluation/env_infos/final/reward_dist Max              0.37666\n",
      "evaluation/env_infos/final/reward_dist Min              3.44775e-99\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00482087\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00818285\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0315676\n",
      "evaluation/env_infos/initial/reward_dist Min            1.03427e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0390308\n",
      "evaluation/env_infos/reward_dist Std                    0.113685\n",
      "evaluation/env_infos/reward_dist Max                    0.984581\n",
      "evaluation/env_infos/reward_dist Min                    3.44775e-99\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0726062\n",
      "evaluation/env_infos/final/reward_energy Std            0.0341126\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0213347\n",
      "evaluation/env_infos/final/reward_energy Min           -0.131482\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.0542799\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0339393\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00309216\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.135224\n",
      "evaluation/env_infos/reward_energy Mean                -0.0651809\n",
      "evaluation/env_infos/reward_energy Std                  0.0352622\n",
      "evaluation/env_infos/reward_energy Max                 -0.00309216\n",
      "evaluation/env_infos/reward_energy Min                 -0.136826\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.230477\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.458789\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.820571\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000924281\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00206602\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.00558739\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00304279\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0828569\n",
      "evaluation/env_infos/end_effector_loc Std               0.233594\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -0.820571\n",
      "time/data storing (s)                                   0.168608\n",
      "time/evaluation sampling (s)                            1.05364\n",
      "time/exploration sampling (s)                           0.115856\n",
      "time/logging (s)                                        0.0190396\n",
      "time/saving (s)                                         0.0266307\n",
      "time/training (s)                                      37.5326\n",
      "time/epoch (s)                                         38.9163\n",
      "time/total (s)                                         75.7469\n",
      "Epoch                                                   1\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:14:27.384871 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 2 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.0157411\n",
      "trainer/QF2 Loss                                        0.0205905\n",
      "trainer/Policy Loss                                    -0.289263\n",
      "trainer/Q1 Predictions Mean                            -0.341368\n",
      "trainer/Q1 Predictions Std                              1.10569\n",
      "trainer/Q1 Predictions Max                              0.759096\n",
      "trainer/Q1 Predictions Min                             -4.10044\n",
      "trainer/Q2 Predictions Mean                            -0.294782\n",
      "trainer/Q2 Predictions Std                              1.06268\n",
      "trainer/Q2 Predictions Max                              0.780594\n",
      "trainer/Q2 Predictions Min                             -3.87503\n",
      "trainer/Q Targets Mean                                 -0.351638\n",
      "trainer/Q Targets Std                                   1.09681\n",
      "trainer/Q Targets Max                                   0.788732\n",
      "trainer/Q Targets Min                                  -3.90439\n",
      "trainer/Log Pis Mean                                   -0.505664\n",
      "trainer/Log Pis Std                                     1.10721\n",
      "trainer/Log Pis Max                                     2.72575\n",
      "trainer/Log Pis Min                                    -5.33552\n",
      "trainer/Policy mu Mean                                  0.026246\n",
      "trainer/Policy mu Std                                   0.264029\n",
      "trainer/Policy mu Max                                   1.12429\n",
      "trainer/Policy mu Min                                  -1.57186\n",
      "trainer/Policy log std Mean                            -0.894428\n",
      "trainer/Policy log std Std                              0.521966\n",
      "trainer/Policy log std Max                             -0.157303\n",
      "trainer/Policy log std Min                             -2.14909\n",
      "trainer/Alpha                                           0.0564989\n",
      "trainer/Alpha Loss                                     -7.19392\n",
      "exploration/num steps total                          1300\n",
      "exploration/num paths total                            65\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.409046\n",
      "exploration/Rewards Std                                 0.169814\n",
      "exploration/Rewards Max                                -0.101315\n",
      "exploration/Rewards Min                                -0.955184\n",
      "exploration/Returns Mean                               -8.18093\n",
      "exploration/Returns Std                                 1.20494\n",
      "exploration/Returns Max                                -6.04047\n",
      "exploration/Returns Min                                -9.31192\n",
      "exploration/Actions Mean                               -0.0326369\n",
      "exploration/Actions Std                                 0.39914\n",
      "exploration/Actions Max                                 0.83389\n",
      "exploration/Actions Min                                -0.973917\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -8.18093\n",
      "exploration/env_infos/final/reward_dist Mean            7.44243e-10\n",
      "exploration/env_infos/final/reward_dist Std             1.48849e-09\n",
      "exploration/env_infos/final/reward_dist Max             3.72121e-09\n",
      "exploration/env_infos/final/reward_dist Min             9.59298e-91\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00310668\n",
      "exploration/env_infos/initial/reward_dist Std           0.00437623\n",
      "exploration/env_infos/initial/reward_dist Max           0.0113058\n",
      "exploration/env_infos/initial/reward_dist Min           1.25569e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0592213\n",
      "exploration/env_infos/reward_dist Std                   0.18015\n",
      "exploration/env_infos/reward_dist Max                   0.960072\n",
      "exploration/env_infos/reward_dist Min                   9.59298e-91\n",
      "exploration/env_infos/final/reward_energy Mean         -0.484182\n",
      "exploration/env_infos/final/reward_energy Std           0.178075\n",
      "exploration/env_infos/final/reward_energy Max          -0.193465\n",
      "exploration/env_infos/final/reward_energy Min          -0.717357\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.57261\n",
      "exploration/env_infos/initial/reward_energy Std         0.159814\n",
      "exploration/env_infos/initial/reward_energy Max        -0.347217\n",
      "exploration/env_infos/initial/reward_energy Min        -0.77121\n",
      "exploration/env_infos/reward_energy Mean               -0.505865\n",
      "exploration/env_infos/reward_energy Std                 0.254669\n",
      "exploration/env_infos/reward_energy Max                -0.0410072\n",
      "exploration/env_infos/reward_energy Min                -1.1053\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.217693\n",
      "exploration/env_infos/final/end_effector_loc Std        0.638154\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.000459856\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0210135\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0359652\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.03856\n",
      "exploration/env_infos/end_effector_loc Mean             0.222149\n",
      "exploration/env_infos/end_effector_loc Std              0.447824\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           3000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.187198\n",
      "evaluation/Rewards Std                                  0.164423\n",
      "evaluation/Rewards Max                                  0.145996\n",
      "evaluation/Rewards Min                                 -0.824948\n",
      "evaluation/Returns Mean                                -3.74396\n",
      "evaluation/Returns Std                                  1.93615\n",
      "evaluation/Returns Max                                  0.337633\n",
      "evaluation/Returns Min                                 -8.99521\n",
      "evaluation/Actions Mean                                -0.0287675\n",
      "evaluation/Actions Std                                  0.12341\n",
      "evaluation/Actions Max                                  0.295236\n",
      "evaluation/Actions Min                                 -0.481237\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.74396\n",
      "evaluation/env_infos/final/reward_dist Mean             4.02596e-07\n",
      "evaluation/env_infos/final/reward_dist Std              2.11882e-06\n",
      "evaluation/env_infos/final/reward_dist Max              1.46298e-05\n",
      "evaluation/env_infos/final/reward_dist Min              6.58472e-114\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00361696\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0067161\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0254002\n",
      "evaluation/env_infos/initial/reward_dist Min            1.09499e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.036205\n",
      "evaluation/env_infos/reward_dist Std                    0.126671\n",
      "evaluation/env_infos/reward_dist Max                    0.962577\n",
      "evaluation/env_infos/reward_dist Min                    6.58472e-114\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.187755\n",
      "evaluation/env_infos/final/reward_energy Std            0.11449\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0177185\n",
      "evaluation/env_infos/final/reward_energy Min           -0.478053\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.129904\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0787734\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0151984\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.328759\n",
      "evaluation/env_infos/reward_energy Mean                -0.146359\n",
      "evaluation/env_infos/reward_energy Std                  0.103413\n",
      "evaluation/env_infos/reward_energy Max                 -0.00120922\n",
      "evaluation/env_infos/reward_energy Min                 -0.538324\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.070563\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.629397\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00138697\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0051891\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0146213\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00865114\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0245234\n",
      "evaluation/env_infos/end_effector_loc Std               0.371774\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.180525\n",
      "time/evaluation sampling (s)                            1.05973\n",
      "time/exploration sampling (s)                           0.122498\n",
      "time/logging (s)                                        0.0192817\n",
      "time/saving (s)                                         0.0293437\n",
      "time/training (s)                                      40.2787\n",
      "time/epoch (s)                                         41.69\n",
      "time/total (s)                                        117.497\n",
      "Epoch                                                   2\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:15:09.867944 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 3 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.029441\n",
      "trainer/QF2 Loss                                        0.0364579\n",
      "trainer/Policy Loss                                     1.39748\n",
      "trainer/Q1 Predictions Mean                            -1.02111\n",
      "trainer/Q1 Predictions Std                              1.38674\n",
      "trainer/Q1 Predictions Max                              0.462104\n",
      "trainer/Q1 Predictions Min                             -5.96059\n",
      "trainer/Q2 Predictions Mean                            -1.06352\n",
      "trainer/Q2 Predictions Std                              1.3958\n",
      "trainer/Q2 Predictions Max                              0.477835\n",
      "trainer/Q2 Predictions Min                             -5.99442\n",
      "trainer/Q Targets Mean                                 -1.07266\n",
      "trainer/Q Targets Std                                   1.40578\n",
      "trainer/Q Targets Max                                   0.471801\n",
      "trainer/Q Targets Min                                  -5.92077\n",
      "trainer/Log Pis Mean                                    0.466328\n",
      "trainer/Log Pis Std                                     1.38997\n",
      "trainer/Log Pis Max                                     4.41614\n",
      "trainer/Log Pis Min                                    -4.11397\n",
      "trainer/Policy mu Mean                                 -0.00814241\n",
      "trainer/Policy mu Std                                   0.498167\n",
      "trainer/Policy mu Max                                   2.26084\n",
      "trainer/Policy mu Min                                  -3.4149\n",
      "trainer/Policy log std Mean                            -1.37729\n",
      "trainer/Policy log std Std                              0.627543\n",
      "trainer/Policy log std Max                             -0.0286426\n",
      "trainer/Policy log std Min                             -2.63298\n",
      "trainer/Alpha                                           0.0194543\n",
      "trainer/Alpha Loss                                     -6.03961\n",
      "exploration/num steps total                          1400\n",
      "exploration/num paths total                            70\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.422586\n",
      "exploration/Rewards Std                                 0.315085\n",
      "exploration/Rewards Max                                -0.0158758\n",
      "exploration/Rewards Min                                -1.37123\n",
      "exploration/Returns Mean                               -8.45171\n",
      "exploration/Returns Std                                 3.76678\n",
      "exploration/Returns Max                                -3.11728\n",
      "exploration/Returns Min                               -13.2358\n",
      "exploration/Actions Mean                               -0.0182026\n",
      "exploration/Actions Std                                 0.36965\n",
      "exploration/Actions Max                                 0.941873\n",
      "exploration/Actions Min                                -0.914941\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -8.45171\n",
      "exploration/env_infos/final/reward_dist Mean            3.83561e-20\n",
      "exploration/env_infos/final/reward_dist Std             7.67121e-20\n",
      "exploration/env_infos/final/reward_dist Max             1.9178e-19\n",
      "exploration/env_infos/final/reward_dist Min             9.58475e-140\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0146405\n",
      "exploration/env_infos/initial/reward_dist Std           0.0280997\n",
      "exploration/env_infos/initial/reward_dist Max           0.0708168\n",
      "exploration/env_infos/initial/reward_dist Min           1.0325e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0165409\n",
      "exploration/env_infos/reward_dist Std                   0.0758551\n",
      "exploration/env_infos/reward_dist Max                   0.522781\n",
      "exploration/env_infos/reward_dist Min                   1.26194e-146\n",
      "exploration/env_infos/final/reward_energy Mean         -0.58954\n",
      "exploration/env_infos/final/reward_energy Std           0.275046\n",
      "exploration/env_infos/final/reward_energy Max          -0.182032\n",
      "exploration/env_infos/final/reward_energy Min          -1.02011\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.531765\n",
      "exploration/env_infos/initial/reward_energy Std         0.347629\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0381182\n",
      "exploration/env_infos/initial/reward_energy Min        -0.976975\n",
      "exploration/env_infos/reward_energy Mean               -0.431464\n",
      "exploration/env_infos/reward_energy Std                 0.296283\n",
      "exploration/env_infos/reward_energy Max                -0.0189254\n",
      "exploration/env_infos/reward_energy Min                -1.06577\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.240482\n",
      "exploration/env_infos/final/end_effector_loc Std        0.735945\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00466591\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0219717\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0349611\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0454119\n",
      "exploration/env_infos/end_effector_loc Mean            -0.133567\n",
      "exploration/env_infos/end_effector_loc Std              0.535704\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           4000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.228346\n",
      "evaluation/Rewards Std                                  0.207126\n",
      "evaluation/Rewards Max                                  0.125679\n",
      "evaluation/Rewards Min                                 -0.965551\n",
      "evaluation/Returns Mean                                -4.56691\n",
      "evaluation/Returns Std                                  2.774\n",
      "evaluation/Returns Max                                 -0.341915\n",
      "evaluation/Returns Min                                -10.1888\n",
      "evaluation/Actions Mean                                -0.0296313\n",
      "evaluation/Actions Std                                  0.166511\n",
      "evaluation/Actions Max                                  0.873105\n",
      "evaluation/Actions Min                                 -0.624991\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -4.56691\n",
      "evaluation/env_infos/final/reward_dist Mean             0.000818653\n",
      "evaluation/env_infos/final/reward_dist Std              0.00326482\n",
      "evaluation/env_infos/final/reward_dist Max              0.0202191\n",
      "evaluation/env_infos/final/reward_dist Min              2.18825e-100\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00489768\n",
      "evaluation/env_infos/initial/reward_dist Std            0.014264\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0953997\n",
      "evaluation/env_infos/initial/reward_dist Min            1.00454e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0332141\n",
      "evaluation/env_infos/reward_dist Std                    0.122438\n",
      "evaluation/env_infos/reward_dist Max                    0.89894\n",
      "evaluation/env_infos/reward_dist Min                    6.37732e-102\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.227037\n",
      "evaluation/env_infos/final/reward_energy Std            0.168629\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0165859\n",
      "evaluation/env_infos/final/reward_energy Min           -0.711059\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.2753\n",
      "evaluation/env_infos/initial/reward_energy Std          0.244369\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0160728\n",
      "evaluation/env_infos/initial/reward_energy Min         -1.01278\n",
      "evaluation/env_infos/reward_energy Mean                -0.178916\n",
      "evaluation/env_infos/reward_energy Std                  0.158735\n",
      "evaluation/env_infos/reward_energy Max                 -0.00254854\n",
      "evaluation/env_infos/reward_energy Min                 -1.01278\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0241426\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.646796\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00411367\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0123475\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0436553\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0277295\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0225208\n",
      "evaluation/env_infos/end_effector_loc Std               0.423732\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.234775\n",
      "time/evaluation sampling (s)                            0.964641\n",
      "time/exploration sampling (s)                           0.121103\n",
      "time/logging (s)                                        0.0197056\n",
      "time/saving (s)                                         0.0280681\n",
      "time/training (s)                                      41.0409\n",
      "time/epoch (s)                                         42.4092\n",
      "time/total (s)                                        159.98\n",
      "Epoch                                                   3\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:15:53.066740 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 4 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.0113806\n",
      "trainer/QF2 Loss                                        0.0119479\n",
      "trainer/Policy Loss                                     2.63252\n",
      "trainer/Q1 Predictions Mean                            -1.44442\n",
      "trainer/Q1 Predictions Std                              1.49893\n",
      "trainer/Q1 Predictions Max                              0.30087\n",
      "trainer/Q1 Predictions Min                             -7.60727\n",
      "trainer/Q2 Predictions Mean                            -1.46048\n",
      "trainer/Q2 Predictions Std                              1.51141\n",
      "trainer/Q2 Predictions Max                              0.234084\n",
      "trainer/Q2 Predictions Min                             -7.55092\n",
      "trainer/Q Targets Mean                                 -1.46186\n",
      "trainer/Q Targets Std                                   1.52884\n",
      "trainer/Q Targets Max                                   0.311858\n",
      "trainer/Q Targets Min                                  -7.63921\n",
      "trainer/Log Pis Mean                                    1.32945\n",
      "trainer/Log Pis Std                                     1.86026\n",
      "trainer/Log Pis Max                                     9.82923\n",
      "trainer/Log Pis Min                                    -6.1304\n",
      "trainer/Policy mu Mean                                 -0.0812581\n",
      "trainer/Policy mu Std                                   0.730778\n",
      "trainer/Policy mu Max                                   3.43424\n",
      "trainer/Policy mu Min                                  -3.409\n",
      "trainer/Policy log std Mean                            -1.63542\n",
      "trainer/Policy log std Std                              0.739314\n",
      "trainer/Policy log std Max                             -0.0338542\n",
      "trainer/Policy log std Min                             -3.27202\n",
      "trainer/Alpha                                           0.010531\n",
      "trainer/Alpha Loss                                     -3.05271\n",
      "exploration/num steps total                          1500\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.283021\n",
      "exploration/Rewards Std                                 0.169159\n",
      "exploration/Rewards Max                                 0.0269444\n",
      "exploration/Rewards Min                                -0.833807\n",
      "exploration/Returns Mean                               -5.66043\n",
      "exploration/Returns Std                                 2.23631\n",
      "exploration/Returns Max                                -2.1773\n",
      "exploration/Returns Min                                -9.22774\n",
      "exploration/Actions Mean                               -0.070297\n",
      "exploration/Actions Std                                 0.235049\n",
      "exploration/Actions Max                                 0.522793\n",
      "exploration/Actions Min                                -0.942836\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -5.66043\n",
      "exploration/env_infos/final/reward_dist Mean            0.0274051\n",
      "exploration/env_infos/final/reward_dist Std             0.0548102\n",
      "exploration/env_infos/final/reward_dist Max             0.137025\n",
      "exploration/env_infos/final/reward_dist Min             3.655e-107\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00120265\n",
      "exploration/env_infos/initial/reward_dist Std           0.00164493\n",
      "exploration/env_infos/initial/reward_dist Max           0.00443065\n",
      "exploration/env_infos/initial/reward_dist Min           1.3371e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0798176\n",
      "exploration/env_infos/reward_dist Std                   0.191301\n",
      "exploration/env_infos/reward_dist Max                   0.938626\n",
      "exploration/env_infos/reward_dist Min                   3.655e-107\n",
      "exploration/env_infos/final/reward_energy Mean         -0.371038\n",
      "exploration/env_infos/final/reward_energy Std           0.139083\n",
      "exploration/env_infos/final/reward_energy Max          -0.196267\n",
      "exploration/env_infos/final/reward_energy Min          -0.622562\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.453462\n",
      "exploration/env_infos/initial/reward_energy Std         0.428575\n",
      "exploration/env_infos/initial/reward_energy Max        -0.134939\n",
      "exploration/env_infos/initial/reward_energy Min        -1.28795\n",
      "exploration/env_infos/reward_energy Mean               -0.273441\n",
      "exploration/env_infos/reward_energy Std                 0.213564\n",
      "exploration/env_infos/reward_energy Max                -0.0314251\n",
      "exploration/env_infos/reward_energy Min                -1.28795\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.294086\n",
      "exploration/env_infos/final/end_effector_loc Std        0.677109\n",
      "exploration/env_infos/final/end_effector_loc Max        0.847004\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00614153\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0211875\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0210827\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0471418\n",
      "exploration/env_infos/end_effector_loc Mean            -0.178905\n",
      "exploration/env_infos/end_effector_loc Std              0.472608\n",
      "exploration/env_infos/end_effector_loc Max              0.847004\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           5000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.220367\n",
      "evaluation/Rewards Std                                  0.206926\n",
      "evaluation/Rewards Max                                  0.0155983\n",
      "evaluation/Rewards Min                                 -1.02433\n",
      "evaluation/Returns Mean                                -4.40735\n",
      "evaluation/Returns Std                                  2.92968\n",
      "evaluation/Returns Max                                 -0.584877\n",
      "evaluation/Returns Min                                -13.9565\n",
      "evaluation/Actions Mean                                -0.029005\n",
      "evaluation/Actions Std                                  0.190331\n",
      "evaluation/Actions Max                                  0.95058\n",
      "evaluation/Actions Min                                 -0.88377\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -4.40735\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00576363\n",
      "evaluation/env_infos/final/reward_dist Std              0.0235129\n",
      "evaluation/env_infos/final/reward_dist Max              0.135635\n",
      "evaluation/env_infos/final/reward_dist Min              2.31651e-114\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.0124682\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0275005\n",
      "evaluation/env_infos/initial/reward_dist Max            0.1341\n",
      "evaluation/env_infos/initial/reward_dist Min            8.3841e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0393682\n",
      "evaluation/env_infos/reward_dist Std                    0.12006\n",
      "evaluation/env_infos/reward_dist Max                    0.958871\n",
      "evaluation/env_infos/reward_dist Min                    2.31651e-114\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.196888\n",
      "evaluation/env_infos/final/reward_energy Std            0.123868\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0282769\n",
      "evaluation/env_infos/final/reward_energy Min           -0.612013\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.369685\n",
      "evaluation/env_infos/initial/reward_energy Std          0.345921\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0282767\n",
      "evaluation/env_infos/initial/reward_energy Min         -1.19318\n",
      "evaluation/env_infos/reward_energy Mean                -0.198341\n",
      "evaluation/env_infos/reward_energy Std                  0.186535\n",
      "evaluation/env_infos/reward_energy Max                 -0.00313825\n",
      "evaluation/env_infos/reward_energy Min                 -1.19318\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.121158\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.680151\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000415784\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0178952\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.047529\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0430223\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0612721\n",
      "evaluation/env_infos/end_effector_loc Std               0.450396\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.176945\n",
      "time/evaluation sampling (s)                            0.932934\n",
      "time/exploration sampling (s)                           0.115033\n",
      "time/logging (s)                                        0.0196857\n",
      "time/saving (s)                                         0.0275964\n",
      "time/training (s)                                      41.8174\n",
      "time/epoch (s)                                         43.0895\n",
      "time/total (s)                                        203.178\n",
      "Epoch                                                   4\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:16:36.644664 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 5 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00740577\n",
      "trainer/QF2 Loss                                        0.0158123\n",
      "trainer/Policy Loss                                     3.95194\n",
      "trainer/Q1 Predictions Mean                            -1.94086\n",
      "trainer/Q1 Predictions Std                              1.72043\n",
      "trainer/Q1 Predictions Max                              0.153363\n",
      "trainer/Q1 Predictions Min                             -8.36648\n",
      "trainer/Q2 Predictions Mean                            -1.9484\n",
      "trainer/Q2 Predictions Std                              1.71277\n",
      "trainer/Q2 Predictions Max                              0.0451607\n",
      "trainer/Q2 Predictions Min                             -8.36347\n",
      "trainer/Q Targets Mean                                 -1.90147\n",
      "trainer/Q Targets Std                                   1.70703\n",
      "trainer/Q Targets Max                                   0.160402\n",
      "trainer/Q Targets Min                                  -8.21582\n",
      "trainer/Log Pis Mean                                    2.16776\n",
      "trainer/Log Pis Std                                     1.58874\n",
      "trainer/Log Pis Max                                     8.88206\n",
      "trainer/Log Pis Min                                    -1.88978\n",
      "trainer/Policy mu Mean                                 -0.0947256\n",
      "trainer/Policy mu Std                                   0.795725\n",
      "trainer/Policy mu Max                                   3.68586\n",
      "trainer/Policy mu Min                                  -3.43914\n",
      "trainer/Policy log std Mean                            -1.97792\n",
      "trainer/Policy log std Std                              0.765433\n",
      "trainer/Policy log std Max                             -0.184131\n",
      "trainer/Policy log std Min                             -3.28751\n",
      "trainer/Alpha                                           0.00761514\n",
      "trainer/Alpha Loss                                      0.818259\n",
      "exploration/num steps total                          1600\n",
      "exploration/num paths total                            80\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.159276\n",
      "exploration/Rewards Std                                 0.100169\n",
      "exploration/Rewards Max                                -0.0287647\n",
      "exploration/Rewards Min                                -0.726637\n",
      "exploration/Returns Mean                               -3.18553\n",
      "exploration/Returns Std                                 0.487434\n",
      "exploration/Returns Max                                -2.33611\n",
      "exploration/Returns Min                                -3.66985\n",
      "exploration/Actions Mean                               -0.00802018\n",
      "exploration/Actions Std                                 0.11133\n",
      "exploration/Actions Max                                 0.27973\n",
      "exploration/Actions Min                                -0.516436\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -3.18553\n",
      "exploration/env_infos/final/reward_dist Mean            0.00683225\n",
      "exploration/env_infos/final/reward_dist Std             0.00840213\n",
      "exploration/env_infos/final/reward_dist Max             0.018281\n",
      "exploration/env_infos/final/reward_dist Min             8.95204e-55\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00277095\n",
      "exploration/env_infos/initial/reward_dist Std           0.00194424\n",
      "exploration/env_infos/initial/reward_dist Max           0.00528572\n",
      "exploration/env_infos/initial/reward_dist Min           3.1364e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0730961\n",
      "exploration/env_infos/reward_dist Std                   0.188082\n",
      "exploration/env_infos/reward_dist Max                   0.938534\n",
      "exploration/env_infos/reward_dist Min                   8.95204e-55\n",
      "exploration/env_infos/final/reward_energy Mean         -0.222899\n",
      "exploration/env_infos/final/reward_energy Std           0.192655\n",
      "exploration/env_infos/final/reward_energy Max          -0.0565603\n",
      "exploration/env_infos/final/reward_energy Min          -0.586492\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.187092\n",
      "exploration/env_infos/initial/reward_energy Std         0.0834782\n",
      "exploration/env_infos/initial/reward_energy Max        -0.100998\n",
      "exploration/env_infos/initial/reward_energy Min        -0.3357\n",
      "exploration/env_infos/reward_energy Mean               -0.130872\n",
      "exploration/env_infos/reward_energy Std                 0.0882596\n",
      "exploration/env_infos/reward_energy Max                -0.0108835\n",
      "exploration/env_infos/reward_energy Min                -0.586492\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.130124\n",
      "exploration/env_infos/final/end_effector_loc Std        0.382337\n",
      "exploration/env_infos/final/end_effector_loc Max        0.413412\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00404253\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00601026\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00587219\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0129907\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0594147\n",
      "exploration/env_infos/end_effector_loc Std              0.212749\n",
      "exploration/env_infos/end_effector_loc Max              0.413412\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           6000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.173925\n",
      "evaluation/Rewards Std                                  0.180502\n",
      "evaluation/Rewards Max                                  0.0878243\n",
      "evaluation/Rewards Min                                 -0.959218\n",
      "evaluation/Returns Mean                                -3.4785\n",
      "evaluation/Returns Std                                  2.69288\n",
      "evaluation/Returns Max                                  0.260211\n",
      "evaluation/Returns Min                                -12.4609\n",
      "evaluation/Actions Mean                                -0.00409171\n",
      "evaluation/Actions Std                                  0.16468\n",
      "evaluation/Actions Max                                  0.97047\n",
      "evaluation/Actions Min                                 -0.852935\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.4785\n",
      "evaluation/env_infos/final/reward_dist Mean             0.026971\n",
      "evaluation/env_infos/final/reward_dist Std              0.107161\n",
      "evaluation/env_infos/final/reward_dist Max              0.662035\n",
      "evaluation/env_infos/final/reward_dist Min              4.15835e-122\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00487996\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00981165\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0348265\n",
      "evaluation/env_infos/initial/reward_dist Min            6.79745e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0390584\n",
      "evaluation/env_infos/reward_dist Std                    0.129036\n",
      "evaluation/env_infos/reward_dist Max                    0.952229\n",
      "evaluation/env_infos/reward_dist Min                    4.15835e-122\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.193333\n",
      "evaluation/env_infos/final/reward_energy Std            0.181464\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0185494\n",
      "evaluation/env_infos/final/reward_energy Min           -0.812151\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.253024\n",
      "evaluation/env_infos/initial/reward_energy Std          0.265274\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0405654\n",
      "evaluation/env_infos/initial/reward_energy Min         -1.14819\n",
      "evaluation/env_infos/reward_energy Mean                -0.155024\n",
      "evaluation/env_infos/reward_energy Std                  0.173896\n",
      "evaluation/env_infos/reward_energy Max                 -0.00166815\n",
      "evaluation/env_infos/reward_energy Min                 -1.14819\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.108467\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.519356\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean     -0.00132292\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0128934\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0485235\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0426468\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0681857\n",
      "evaluation/env_infos/end_effector_loc Std               0.34392\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.174104\n",
      "time/evaluation sampling (s)                            1.06405\n",
      "time/exploration sampling (s)                           0.124975\n",
      "time/logging (s)                                        0.0207629\n",
      "time/saving (s)                                         0.0272669\n",
      "time/training (s)                                      42.074\n",
      "time/epoch (s)                                         43.4852\n",
      "time/total (s)                                        246.756\n",
      "Epoch                                                   5\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"relabeled_latents\": np.array(self.relabeled_latents),\n",
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:404: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"relabeled_rewards\": np.array(self.relabeled_rewards)}\n",
      "2021-05-25 13:17:19.862811 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 6 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.006187\n",
      "trainer/QF2 Loss                                        0.00921116\n",
      "trainer/Policy Loss                                     3.9465\n",
      "trainer/Q1 Predictions Mean                            -1.94231\n",
      "trainer/Q1 Predictions Std                              1.57308\n",
      "trainer/Q1 Predictions Max                              0.09427\n",
      "trainer/Q1 Predictions Min                             -7.90892\n",
      "trainer/Q2 Predictions Mean                            -1.99057\n",
      "trainer/Q2 Predictions Std                              1.58073\n",
      "trainer/Q2 Predictions Max                             -0.00780889\n",
      "trainer/Q2 Predictions Min                             -7.98233\n",
      "trainer/Q Targets Mean                                 -1.95502\n",
      "trainer/Q Targets Std                                   1.5941\n",
      "trainer/Q Targets Max                                   0.0739021\n",
      "trainer/Q Targets Min                                  -7.97616\n",
      "trainer/Log Pis Mean                                    2.1276\n",
      "trainer/Log Pis Std                                     1.95976\n",
      "trainer/Log Pis Max                                    10.9105\n",
      "trainer/Log Pis Min                                    -3.33338\n",
      "trainer/Policy mu Mean                                 -0.158503\n",
      "trainer/Policy mu Std                                   0.903255\n",
      "trainer/Policy mu Max                                   3.59471\n",
      "trainer/Policy mu Min                                  -4.43751\n",
      "trainer/Policy log std Mean                            -1.95982\n",
      "trainer/Policy log std Std                              0.764829\n",
      "trainer/Policy log std Max                              0.274636\n",
      "trainer/Policy log std Min                             -3.26719\n",
      "trainer/Alpha                                           0.00796779\n",
      "trainer/Alpha Loss                                      0.616607\n",
      "exploration/num steps total                          1700\n",
      "exploration/num paths total                            85\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.19079\n",
      "exploration/Rewards Std                                 0.171507\n",
      "exploration/Rewards Max                                 0.0206439\n",
      "exploration/Rewards Min                                -0.694219\n",
      "exploration/Returns Mean                               -3.8158\n",
      "exploration/Returns Std                                 3.01828\n",
      "exploration/Returns Max                                -1.25859\n",
      "exploration/Returns Min                                -9.56534\n",
      "exploration/Actions Mean                                0.0299564\n",
      "exploration/Actions Std                                 0.262264\n",
      "exploration/Actions Max                                 0.849917\n",
      "exploration/Actions Min                                -0.910619\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -3.8158\n",
      "exploration/env_infos/final/reward_dist Mean            2.69932e-10\n",
      "exploration/env_infos/final/reward_dist Std             5.2362e-10\n",
      "exploration/env_infos/final/reward_dist Max             1.31686e-09\n",
      "exploration/env_infos/final/reward_dist Min             3.68334e-90\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0104042\n",
      "exploration/env_infos/initial/reward_dist Std           0.0136242\n",
      "exploration/env_infos/initial/reward_dist Max           0.0373511\n",
      "exploration/env_infos/initial/reward_dist Min           0.000232047\n",
      "exploration/env_infos/reward_dist Mean                  0.017989\n",
      "exploration/env_infos/reward_dist Std                   0.0895183\n",
      "exploration/env_infos/reward_dist Max                   0.826703\n",
      "exploration/env_infos/reward_dist Min                   3.68334e-90\n",
      "exploration/env_infos/final/reward_energy Mean         -0.208985\n",
      "exploration/env_infos/final/reward_energy Std           0.081818\n",
      "exploration/env_infos/final/reward_energy Max          -0.105604\n",
      "exploration/env_infos/final/reward_energy Min          -0.35299\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.335027\n",
      "exploration/env_infos/initial/reward_energy Std         0.27467\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0507106\n",
      "exploration/env_infos/initial/reward_energy Min        -0.861087\n",
      "exploration/env_infos/reward_energy Mean               -0.276258\n",
      "exploration/env_infos/reward_energy Std                 0.251081\n",
      "exploration/env_infos/reward_energy Max                -0.0166621\n",
      "exploration/env_infos/reward_energy Min                -1.17685\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.149022\n",
      "exploration/env_infos/final/end_effector_loc Std        0.550073\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.840687\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00584302\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0141586\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0424959\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0132118\n",
      "exploration/env_infos/end_effector_loc Mean             0.126093\n",
      "exploration/env_infos/end_effector_loc Std              0.380607\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -0.840687\n",
      "evaluation/num steps total                           7000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.166617\n",
      "evaluation/Rewards Std                                  0.156259\n",
      "evaluation/Rewards Max                                  0.126482\n",
      "evaluation/Rewards Min                                 -1.15441\n",
      "evaluation/Returns Mean                                -3.33234\n",
      "evaluation/Returns Std                                  2.07046\n",
      "evaluation/Returns Max                                 -0.214681\n",
      "evaluation/Returns Min                                -10.2653\n",
      "evaluation/Actions Mean                                -0.00391447\n",
      "evaluation/Actions Std                                  0.150941\n",
      "evaluation/Actions Max                                  0.923869\n",
      "evaluation/Actions Min                                 -0.862876\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.33234\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00751399\n",
      "evaluation/env_infos/final/reward_dist Std              0.0340839\n",
      "evaluation/env_infos/final/reward_dist Max              0.189144\n",
      "evaluation/env_infos/final/reward_dist Min              2.07727e-106\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00980837\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0239308\n",
      "evaluation/env_infos/initial/reward_dist Max            0.119083\n",
      "evaluation/env_infos/initial/reward_dist Min            1.3996e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0346297\n",
      "evaluation/env_infos/reward_dist Std                    0.116307\n",
      "evaluation/env_infos/reward_dist Max                    0.986718\n",
      "evaluation/env_infos/reward_dist Min                    2.07727e-106\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.212059\n",
      "evaluation/env_infos/final/reward_energy Std            0.191596\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00718469\n",
      "evaluation/env_infos/final/reward_energy Min           -0.847242\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.286709\n",
      "evaluation/env_infos/initial/reward_energy Std          0.327569\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0253584\n",
      "evaluation/env_infos/initial/reward_energy Min         -1.19948\n",
      "evaluation/env_infos/reward_energy Mean                -0.148942\n",
      "evaluation/env_infos/reward_energy Std                  0.153014\n",
      "evaluation/env_infos/reward_energy Max                 -0.00322036\n",
      "evaluation/env_infos/reward_energy Min                 -1.19948\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0318403\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.480243\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean     -0.00146598\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0153209\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0461935\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0431438\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0284525\n",
      "evaluation/env_infos/end_effector_loc Std               0.300096\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.16987\n",
      "time/evaluation sampling (s)                            0.921665\n",
      "time/exploration sampling (s)                           0.119099\n",
      "time/logging (s)                                        0.0185884\n",
      "time/saving (s)                                         0.0284867\n",
      "time/training (s)                                      41.85\n",
      "time/epoch (s)                                         43.1077\n",
      "time/total (s)                                        289.971\n",
      "Epoch                                                   6\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:18:03.496133 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 7 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00913595\n",
      "trainer/QF2 Loss                                        0.00714247\n",
      "trainer/Policy Loss                                     4.36166\n",
      "trainer/Q1 Predictions Mean                            -2.27862\n",
      "trainer/Q1 Predictions Std                              1.71949\n",
      "trainer/Q1 Predictions Max                              0.0695316\n",
      "trainer/Q1 Predictions Min                            -10.4014\n",
      "trainer/Q2 Predictions Mean                            -2.27913\n",
      "trainer/Q2 Predictions Std                              1.71884\n",
      "trainer/Q2 Predictions Max                              0.09634\n",
      "trainer/Q2 Predictions Min                            -10.2759\n",
      "trainer/Q Targets Mean                                 -2.24932\n",
      "trainer/Q Targets Std                                   1.7158\n",
      "trainer/Q Targets Max                                   0.171178\n",
      "trainer/Q Targets Min                                 -10.1194\n",
      "trainer/Log Pis Mean                                    2.24571\n",
      "trainer/Log Pis Std                                     1.74899\n",
      "trainer/Log Pis Max                                    11.1401\n",
      "trainer/Log Pis Min                                    -2.94202\n",
      "trainer/Policy mu Mean                                 -0.174283\n",
      "trainer/Policy mu Std                                   0.82556\n",
      "trainer/Policy mu Max                                   3.93356\n",
      "trainer/Policy mu Min                                  -3.36218\n",
      "trainer/Policy log std Mean                            -2.02971\n",
      "trainer/Policy log std Std                              0.828622\n",
      "trainer/Policy log std Max                              0.824064\n",
      "trainer/Policy log std Min                             -3.33344\n",
      "trainer/Alpha                                           0.0085107\n",
      "trainer/Alpha Loss                                      1.1712\n",
      "exploration/num steps total                          1800\n",
      "exploration/num paths total                            90\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.215886\n",
      "exploration/Rewards Std                                 0.163152\n",
      "exploration/Rewards Max                                -0.0275179\n",
      "exploration/Rewards Min                                -0.732701\n",
      "exploration/Returns Mean                               -4.31772\n",
      "exploration/Returns Std                                 2.1857\n",
      "exploration/Returns Max                                -2.06978\n",
      "exploration/Returns Min                                -8.30346\n",
      "exploration/Actions Mean                               -0.00303867\n",
      "exploration/Actions Std                                 0.154861\n",
      "exploration/Actions Max                                 0.449845\n",
      "exploration/Actions Min                                -0.522306\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -4.31772\n",
      "exploration/env_infos/final/reward_dist Mean            4.23815e-09\n",
      "exploration/env_infos/final/reward_dist Std             5.91701e-09\n",
      "exploration/env_infos/final/reward_dist Max             1.50865e-08\n",
      "exploration/env_infos/final/reward_dist Min             1.37007e-45\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000649137\n",
      "exploration/env_infos/initial/reward_dist Std           0.000788081\n",
      "exploration/env_infos/initial/reward_dist Max           0.00211786\n",
      "exploration/env_infos/initial/reward_dist Min           5.00967e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.000610044\n",
      "exploration/env_infos/reward_dist Std                   0.00126525\n",
      "exploration/env_infos/reward_dist Max                   0.00909753\n",
      "exploration/env_infos/reward_dist Min                   5.08668e-60\n",
      "exploration/env_infos/final/reward_energy Mean         -0.239287\n",
      "exploration/env_infos/final/reward_energy Std           0.10099\n",
      "exploration/env_infos/final/reward_energy Max          -0.164393\n",
      "exploration/env_infos/final/reward_energy Min          -0.437705\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.155083\n",
      "exploration/env_infos/initial/reward_energy Std         0.114566\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0507862\n",
      "exploration/env_infos/initial/reward_energy Min        -0.364306\n",
      "exploration/env_infos/reward_energy Mean               -0.174596\n",
      "exploration/env_infos/reward_energy Std                 0.132282\n",
      "exploration/env_infos/reward_energy Max                -0.00810666\n",
      "exploration/env_infos/reward_energy Min                -0.653739\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.116042\n",
      "exploration/env_infos/final/end_effector_loc Std        0.346154\n",
      "exploration/env_infos/final/end_effector_loc Max        0.347506\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.678809\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00379349\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00566388\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0029203\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0182\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0602949\n",
      "exploration/env_infos/end_effector_loc Std              0.231854\n",
      "exploration/env_infos/end_effector_loc Max              0.427477\n",
      "exploration/env_infos/end_effector_loc Min             -0.761812\n",
      "evaluation/num steps total                           8000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.17391\n",
      "evaluation/Rewards Std                                  0.145423\n",
      "evaluation/Rewards Max                                  0.0943324\n",
      "evaluation/Rewards Min                                 -0.985972\n",
      "evaluation/Returns Mean                                -3.47821\n",
      "evaluation/Returns Std                                  2.11806\n",
      "evaluation/Returns Max                                  0.450535\n",
      "evaluation/Returns Min                                 -8.66734\n",
      "evaluation/Actions Mean                                -0.0067924\n",
      "evaluation/Actions Std                                  0.126367\n",
      "evaluation/Actions Max                                  0.685385\n",
      "evaluation/Actions Min                                 -0.751719\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.47821\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0307757\n",
      "evaluation/env_infos/final/reward_dist Std              0.113279\n",
      "evaluation/env_infos/final/reward_dist Max              0.714939\n",
      "evaluation/env_infos/final/reward_dist Min              1.11664e-162\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00703713\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0141177\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0695961\n",
      "evaluation/env_infos/initial/reward_dist Min            8.28789e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0356799\n",
      "evaluation/env_infos/reward_dist Std                    0.123039\n",
      "evaluation/env_infos/reward_dist Max                    0.964844\n",
      "evaluation/env_infos/reward_dist Min                    1.11664e-162\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.154333\n",
      "evaluation/env_infos/final/reward_energy Std            0.0925979\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00771887\n",
      "evaluation/env_infos/final/reward_energy Min           -0.390418\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.254321\n",
      "evaluation/env_infos/initial/reward_energy Std          0.203453\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0171177\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.873863\n",
      "evaluation/env_infos/reward_energy Mean                -0.13361\n",
      "evaluation/env_infos/reward_energy Std                  0.11907\n",
      "evaluation/env_infos/reward_energy Max                 -0.00620593\n",
      "evaluation/env_infos/reward_energy Min                 -0.873863\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.113874\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.429234\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean     -0.000425695\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0115069\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0294609\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0375859\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0472127\n",
      "evaluation/env_infos/end_effector_loc Std               0.271916\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.173876\n",
      "time/evaluation sampling (s)                            0.93702\n",
      "time/exploration sampling (s)                           0.119902\n",
      "time/logging (s)                                        0.0201675\n",
      "time/saving (s)                                         0.0269935\n",
      "time/training (s)                                      42.2053\n",
      "time/epoch (s)                                         43.4833\n",
      "time/total (s)                                        333.605\n",
      "Epoch                                                   7\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:18:46.736000 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 8 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00533563\n",
      "trainer/QF2 Loss                                        0.00659788\n",
      "trainer/Policy Loss                                     4.42035\n",
      "trainer/Q1 Predictions Mean                            -2.28057\n",
      "trainer/Q1 Predictions Std                              1.62644\n",
      "trainer/Q1 Predictions Max                              0.0824078\n",
      "trainer/Q1 Predictions Min                             -9.12065\n",
      "trainer/Q2 Predictions Mean                            -2.22764\n",
      "trainer/Q2 Predictions Std                              1.60344\n",
      "trainer/Q2 Predictions Max                              0.122725\n",
      "trainer/Q2 Predictions Min                             -8.83307\n",
      "trainer/Q Targets Mean                                 -2.25968\n",
      "trainer/Q Targets Std                                   1.61937\n",
      "trainer/Q Targets Max                                   0.220764\n",
      "trainer/Q Targets Min                                  -9.13877\n",
      "trainer/Log Pis Mean                                    2.34288\n",
      "trainer/Log Pis Std                                     2.18089\n",
      "trainer/Log Pis Max                                    14.7293\n",
      "trainer/Log Pis Min                                    -4.16367\n",
      "trainer/Policy mu Mean                                 -0.157253\n",
      "trainer/Policy mu Std                                   0.953318\n",
      "trainer/Policy mu Max                                   4.30166\n",
      "trainer/Policy mu Min                                  -4.42701\n",
      "trainer/Policy log std Mean                            -2.01362\n",
      "trainer/Policy log std Std                              0.834473\n",
      "trainer/Policy log std Max                             -0.20653\n",
      "trainer/Policy log std Min                             -3.47713\n",
      "trainer/Alpha                                           0.00985431\n",
      "trainer/Alpha Loss                                      1.58431\n",
      "exploration/num steps total                          1900\n",
      "exploration/num paths total                            95\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.223974\n",
      "exploration/Rewards Std                                 0.141288\n",
      "exploration/Rewards Max                                -0.0394922\n",
      "exploration/Rewards Min                                -0.717659\n",
      "exploration/Returns Mean                               -4.47949\n",
      "exploration/Returns Std                                 1.46446\n",
      "exploration/Returns Max                                -2.92307\n",
      "exploration/Returns Min                                -7.16017\n",
      "exploration/Actions Mean                               -0.0112399\n",
      "exploration/Actions Std                                 0.205312\n",
      "exploration/Actions Max                                 0.740161\n",
      "exploration/Actions Min                                -0.87331\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -4.47949\n",
      "exploration/env_infos/final/reward_dist Mean            1.03656e-19\n",
      "exploration/env_infos/final/reward_dist Std             2.07311e-19\n",
      "exploration/env_infos/final/reward_dist Max             5.18279e-19\n",
      "exploration/env_infos/final/reward_dist Min             3.72565e-56\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00744965\n",
      "exploration/env_infos/initial/reward_dist Std           0.00512225\n",
      "exploration/env_infos/initial/reward_dist Max           0.013882\n",
      "exploration/env_infos/initial/reward_dist Min           2.40008e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0111772\n",
      "exploration/env_infos/reward_dist Std                   0.0443288\n",
      "exploration/env_infos/reward_dist Max                   0.320754\n",
      "exploration/env_infos/reward_dist Min                   3.44758e-56\n",
      "exploration/env_infos/final/reward_energy Mean         -0.200055\n",
      "exploration/env_infos/final/reward_energy Std           0.0717528\n",
      "exploration/env_infos/final/reward_energy Max          -0.127019\n",
      "exploration/env_infos/final/reward_energy Min          -0.308255\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.379102\n",
      "exploration/env_infos/initial/reward_energy Std         0.254928\n",
      "exploration/env_infos/initial/reward_energy Max        -0.167003\n",
      "exploration/env_infos/initial/reward_energy Min        -0.790057\n",
      "exploration/env_infos/reward_energy Mean               -0.220564\n",
      "exploration/env_infos/reward_energy Std                 0.189501\n",
      "exploration/env_infos/reward_energy Max                -0.0119804\n",
      "exploration/env_infos/reward_energy Min                -0.884617\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.0410105\n",
      "exploration/env_infos/final/end_effector_loc Std        0.518885\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.771769\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00587854\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0150441\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.037008\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.013816\n",
      "exploration/env_infos/end_effector_loc Mean             0.0577314\n",
      "exploration/env_infos/end_effector_loc Std              0.302228\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -0.771769\n",
      "evaluation/num steps total                           9000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.17266\n",
      "evaluation/Rewards Std                                  0.114783\n",
      "evaluation/Rewards Max                                  0.0694299\n",
      "evaluation/Rewards Min                                 -0.977339\n",
      "evaluation/Returns Mean                                -3.4532\n",
      "evaluation/Returns Std                                  1.63278\n",
      "evaluation/Returns Max                                 -0.609876\n",
      "evaluation/Returns Min                                 -8.09088\n",
      "evaluation/Actions Mean                                -0.0144938\n",
      "evaluation/Actions Std                                  0.135241\n",
      "evaluation/Actions Max                                  0.817857\n",
      "evaluation/Actions Min                                 -0.764278\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.4532\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0341713\n",
      "evaluation/env_infos/final/reward_dist Std              0.13621\n",
      "evaluation/env_infos/final/reward_dist Max              0.882031\n",
      "evaluation/env_infos/final/reward_dist Min              4.91068e-118\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00640933\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0145283\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0885174\n",
      "evaluation/env_infos/initial/reward_dist Min            4.48761e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0270263\n",
      "evaluation/env_infos/reward_dist Std                    0.123674\n",
      "evaluation/env_infos/reward_dist Max                    0.959137\n",
      "evaluation/env_infos/reward_dist Min                    2.58579e-118\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.136292\n",
      "evaluation/env_infos/final/reward_energy Std            0.0963622\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0205446\n",
      "evaluation/env_infos/final/reward_energy Min           -0.521737\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.252065\n",
      "evaluation/env_infos/initial/reward_energy Std          0.218089\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0197162\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.832727\n",
      "evaluation/env_infos/reward_energy Mean                -0.135048\n",
      "evaluation/env_infos/reward_energy Std                  0.136976\n",
      "evaluation/env_infos/reward_energy Max                 -0.00143426\n",
      "evaluation/env_infos/reward_energy Min                 -0.878671\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0312723\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.4489\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000623246\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.011768\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0408152\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0336095\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0123837\n",
      "evaluation/env_infos/end_effector_loc Std               0.282542\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.171167\n",
      "time/evaluation sampling (s)                            0.943539\n",
      "time/exploration sampling (s)                           0.118137\n",
      "time/logging (s)                                        0.0210369\n",
      "time/saving (s)                                         0.0315204\n",
      "time/training (s)                                      41.8207\n",
      "time/epoch (s)                                         43.1061\n",
      "time/total (s)                                        376.846\n",
      "Epoch                                                   8\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:19:30.878877 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 9 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00432969\n",
      "trainer/QF2 Loss                                         0.00949648\n",
      "trainer/Policy Loss                                      3.88852\n",
      "trainer/Q1 Predictions Mean                             -1.95893\n",
      "trainer/Q1 Predictions Std                               1.52931\n",
      "trainer/Q1 Predictions Max                               0.248303\n",
      "trainer/Q1 Predictions Min                              -7.67954\n",
      "trainer/Q2 Predictions Mean                             -1.95263\n",
      "trainer/Q2 Predictions Std                               1.51812\n",
      "trainer/Q2 Predictions Max                               0.200359\n",
      "trainer/Q2 Predictions Min                              -7.72436\n",
      "trainer/Q Targets Mean                                  -1.95022\n",
      "trainer/Q Targets Std                                    1.52448\n",
      "trainer/Q Targets Max                                    0.285175\n",
      "trainer/Q Targets Min                                   -7.60175\n",
      "trainer/Log Pis Mean                                     2.03785\n",
      "trainer/Log Pis Std                                      1.71557\n",
      "trainer/Log Pis Max                                      8.68068\n",
      "trainer/Log Pis Min                                     -5.26658\n",
      "trainer/Policy mu Mean                                  -0.112582\n",
      "trainer/Policy mu Std                                    0.679807\n",
      "trainer/Policy mu Max                                    3.07349\n",
      "trainer/Policy mu Min                                   -3.20926\n",
      "trainer/Policy log std Mean                             -2.10826\n",
      "trainer/Policy log std Std                               0.764556\n",
      "trainer/Policy log std Max                              -0.281164\n",
      "trainer/Policy log std Min                              -3.45967\n",
      "trainer/Alpha                                            0.0111275\n",
      "trainer/Alpha Loss                                       0.170275\n",
      "exploration/num steps total                           2000\n",
      "exploration/num paths total                            100\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.187784\n",
      "exploration/Rewards Std                                  0.114619\n",
      "exploration/Rewards Max                                  0.0296615\n",
      "exploration/Rewards Min                                 -0.576474\n",
      "exploration/Returns Mean                                -3.75567\n",
      "exploration/Returns Std                                  1.1919\n",
      "exploration/Returns Max                                 -1.99091\n",
      "exploration/Returns Min                                 -5.50404\n",
      "exploration/Actions Mean                                -0.00838535\n",
      "exploration/Actions Std                                  0.164762\n",
      "exploration/Actions Max                                  0.466892\n",
      "exploration/Actions Min                                 -0.634303\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.75567\n",
      "exploration/env_infos/final/reward_dist Mean             0.143828\n",
      "exploration/env_infos/final/reward_dist Std              0.287656\n",
      "exploration/env_infos/final/reward_dist Max              0.719141\n",
      "exploration/env_infos/final/reward_dist Min              1.72796e-32\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00133341\n",
      "exploration/env_infos/initial/reward_dist Std            0.00173258\n",
      "exploration/env_infos/initial/reward_dist Max            0.00440372\n",
      "exploration/env_infos/initial/reward_dist Min            1.15914e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0404029\n",
      "exploration/env_infos/reward_dist Std                    0.124964\n",
      "exploration/env_infos/reward_dist Max                    0.719141\n",
      "exploration/env_infos/reward_dist Min                    1.72796e-32\n",
      "exploration/env_infos/final/reward_energy Mean          -0.215428\n",
      "exploration/env_infos/final/reward_energy Std            0.136869\n",
      "exploration/env_infos/final/reward_energy Max           -0.0102378\n",
      "exploration/env_infos/final/reward_energy Min           -0.358182\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.256306\n",
      "exploration/env_infos/initial/reward_energy Std          0.120952\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0407215\n",
      "exploration/env_infos/initial/reward_energy Min         -0.385877\n",
      "exploration/env_infos/reward_energy Mean                -0.195565\n",
      "exploration/env_infos/reward_energy Std                  0.127231\n",
      "exploration/env_infos/reward_energy Max                 -0.00972723\n",
      "exploration/env_infos/reward_energy Min                 -0.696804\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00419778\n",
      "exploration/env_infos/final/end_effector_loc Std         0.277579\n",
      "exploration/env_infos/final/end_effector_loc Max         0.433277\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.658093\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00205885\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00980631\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0181713\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.01456\n",
      "exploration/env_infos/end_effector_loc Mean              0.0136436\n",
      "exploration/env_infos/end_effector_loc Std               0.15759\n",
      "exploration/env_infos/end_effector_loc Max               0.433277\n",
      "exploration/env_infos/end_effector_loc Min              -0.658093\n",
      "evaluation/num steps total                           10000\n",
      "evaluation/num paths total                             500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.172073\n",
      "evaluation/Rewards Std                                   0.16912\n",
      "evaluation/Rewards Max                                   0.133876\n",
      "evaluation/Rewards Min                                  -1.33916\n",
      "evaluation/Returns Mean                                 -3.44147\n",
      "evaluation/Returns Std                                   2.24701\n",
      "evaluation/Returns Max                                   0.635755\n",
      "evaluation/Returns Min                                  -9.63737\n",
      "evaluation/Actions Mean                                 -0.0133422\n",
      "evaluation/Actions Std                                   0.128601\n",
      "evaluation/Actions Max                                   0.827027\n",
      "evaluation/Actions Min                                  -0.745304\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -3.44147\n",
      "evaluation/env_infos/final/reward_dist Mean              0.00376458\n",
      "evaluation/env_infos/final/reward_dist Std               0.0206459\n",
      "evaluation/env_infos/final/reward_dist Max               0.145419\n",
      "evaluation/env_infos/final/reward_dist Min               4.56394e-183\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00546571\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0108334\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0451844\n",
      "evaluation/env_infos/initial/reward_dist Min             6.3523e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0320316\n",
      "evaluation/env_infos/reward_dist Std                     0.127326\n",
      "evaluation/env_infos/reward_dist Max                     0.988918\n",
      "evaluation/env_infos/reward_dist Min                     4.56394e-183\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.161763\n",
      "evaluation/env_infos/final/reward_energy Std             0.171641\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00583886\n",
      "evaluation/env_infos/final/reward_energy Min            -0.649548\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.307693\n",
      "evaluation/env_infos/initial/reward_energy Std           0.203542\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0426279\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.891255\n",
      "evaluation/env_infos/reward_energy Mean                 -0.136088\n",
      "evaluation/env_infos/reward_energy Std                   0.122117\n",
      "evaluation/env_infos/reward_energy Max                  -0.00369965\n",
      "evaluation/env_infos/reward_energy Min                  -0.891255\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.139958\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.482312\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000220228\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0130415\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0413513\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0372652\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.048171\n",
      "evaluation/env_infos/end_effector_loc Std                0.314972\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.177306\n",
      "time/evaluation sampling (s)                             1.21679\n",
      "time/exploration sampling (s)                            0.144775\n",
      "time/logging (s)                                         0.0182229\n",
      "time/saving (s)                                          0.0267277\n",
      "time/training (s)                                       42.2538\n",
      "time/epoch (s)                                          43.8376\n",
      "time/total (s)                                         420.985\n",
      "Epoch                                                    9\n",
      "---------------------------------------------------  ----------------\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 100 --env pointreacherobs\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 1 --env pointreacherobs\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env pointreacherobs\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env pointreacherobs --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d6f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
