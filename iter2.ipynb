{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc7553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[15508]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a241a4778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a24225740). One of the two will be used. Which one is undefined.\n",
      "objc[15508]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a241a4700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a24225768). One of the two will be used. Which one is undefined.\n",
      "objc[15508]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a241a47a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a242257b8). One of the two will be used. Which one is undefined.\n",
      "objc[15508]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a241a4818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a24225830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 10:24:55.667664 PDT | Variant:\n",
      "2021-05-25 10:24:55.668232 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"halfcheetahhard\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 4,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1\n",
      "}\n",
      "halfcheetah\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 10:25:26.013173 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  2000\n",
      "trainer/QF1 Loss                                      16.1022\n",
      "trainer/QF2 Loss                                      16.0024\n",
      "trainer/Policy Loss                                   -4.03239\n",
      "trainer/Q1 Predictions Mean                           -0.00868505\n",
      "trainer/Q1 Predictions Std                             0.00569892\n",
      "trainer/Q1 Predictions Max                             0.00276847\n",
      "trainer/Q1 Predictions Min                            -0.0293377\n",
      "trainer/Q2 Predictions Mean                            0.00402752\n",
      "trainer/Q2 Predictions Std                             0.00448522\n",
      "trainer/Q2 Predictions Max                             0.0155763\n",
      "trainer/Q2 Predictions Min                            -0.0079983\n",
      "trainer/Q Targets Mean                                 3.86128\n",
      "trainer/Q Targets Std                                  1.06061\n",
      "trainer/Q Targets Max                                  7.74404\n",
      "trainer/Q Targets Min                                  1.06643\n",
      "trainer/Log Pis Mean                                  -4.04116\n",
      "trainer/Log Pis Std                                    0.507125\n",
      "trainer/Log Pis Max                                   -2.36207\n",
      "trainer/Log Pis Min                                   -5.49049\n",
      "trainer/Policy mu Mean                                -0.000921182\n",
      "trainer/Policy mu Std                                  0.00241126\n",
      "trainer/Policy mu Max                                  0.00701357\n",
      "trainer/Policy mu Min                                 -0.00715128\n",
      "trainer/Policy log std Mean                           -0.00134487\n",
      "trainer/Policy log std Std                             0.00179297\n",
      "trainer/Policy log std Max                             0.00386933\n",
      "trainer/Policy log std Min                            -0.00782172\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                            2\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.180531\n",
      "exploration/Rewards Std                                0.575729\n",
      "exploration/Rewards Max                                1.67508\n",
      "exploration/Rewards Min                               -2.07869\n",
      "exploration/Returns Mean                            -180.531\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -180.531\n",
      "exploration/Returns Min                             -180.531\n",
      "exploration/Actions Mean                               0.00115617\n",
      "exploration/Actions Std                                0.627115\n",
      "exploration/Actions Max                                0.999103\n",
      "exploration/Actions Min                               -0.999803\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -180.531\n",
      "exploration/env_infos/final/reward_run Mean           -0.193592\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.193592\n",
      "exploration/env_infos/final/reward_run Min            -0.193592\n",
      "exploration/env_infos/initial/reward_run Mean         -0.17967\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.17967\n",
      "exploration/env_infos/initial/reward_run Min          -0.17967\n",
      "exploration/env_infos/reward_run Mean                 -0.0684477\n",
      "exploration/env_infos/reward_run Std                   0.705377\n",
      "exploration/env_infos/reward_run Max                   2.22754\n",
      "exploration/env_infos/reward_run Min                  -2.34565\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.11272\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.11272\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.11272\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.144624\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.144624\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.144624\n",
      "exploration/env_infos/reward_ctrl Mean                -0.235964\n",
      "exploration/env_infos/reward_ctrl Std                  0.0750503\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0181909\n",
      "exploration/env_infos/reward_ctrl Min                 -0.476008\n",
      "exploration/env_infos/final/height Mean                0.00225554\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                 0.00225554\n",
      "exploration/env_infos/final/height Min                 0.00225554\n",
      "exploration/env_infos/initial/height Mean             -0.0179407\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0179407\n",
      "exploration/env_infos/initial/height Min              -0.0179407\n",
      "exploration/env_infos/height Mean                     -0.0662649\n",
      "exploration/env_infos/height Std                       0.0781177\n",
      "exploration/env_infos/height Max                       0.161863\n",
      "exploration/env_infos/height Min                      -0.372448\n",
      "exploration/env_infos/final/reward_angular Mean       -2.32488\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -2.32488\n",
      "exploration/env_infos/final/reward_angular Min        -2.32488\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.113362\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.113362\n",
      "exploration/env_infos/initial/reward_angular Min      -0.113362\n",
      "exploration/env_infos/reward_angular Mean             -0.0205031\n",
      "exploration/env_infos/reward_angular Std               1.67749\n",
      "exploration/env_infos/reward_angular Max               5.52338\n",
      "exploration/env_infos/reward_angular Min              -6.63795\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0615665\n",
      "evaluation/Rewards Std                                 0.0479122\n",
      "evaluation/Rewards Max                                 1.31344\n",
      "evaluation/Rewards Min                                -1.96292\n",
      "evaluation/Returns Mean                              -61.5665\n",
      "evaluation/Returns Std                                38.0983\n",
      "evaluation/Returns Max                                -1.55696\n",
      "evaluation/Returns Min                              -127.68\n",
      "evaluation/Actions Mean                               -0.00032297\n",
      "evaluation/Actions Std                                 0.00121836\n",
      "evaluation/Actions Max                                 0.00379412\n",
      "evaluation/Actions Min                                -0.00324318\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -61.5665\n",
      "evaluation/env_infos/final/reward_run Mean             1.94289e-17\n",
      "evaluation/env_infos/final/reward_run Std              1.52365e-16\n",
      "evaluation/env_infos/final/reward_run Max              3.46945e-16\n",
      "evaluation/env_infos/final/reward_run Min             -5.55112e-16\n",
      "evaluation/env_infos/initial/reward_run Mean           0.0216643\n",
      "evaluation/env_infos/initial/reward_run Std            0.0908698\n",
      "evaluation/env_infos/initial/reward_run Max            0.207972\n",
      "evaluation/env_infos/initial/reward_run Min           -0.21485\n",
      "evaluation/env_infos/reward_run Mean                  -6.01134e-05\n",
      "evaluation/env_infos/reward_run Std                    0.0129475\n",
      "evaluation/env_infos/reward_run Max                    0.297686\n",
      "evaluation/env_infos/reward_run Min                   -0.382699\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -9.52177e-07\n",
      "evaluation/env_infos/final/reward_ctrl Std             9.0322e-08\n",
      "evaluation/env_infos/final/reward_ctrl Max            -8.07404e-07\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.10654e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -1.00856e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           1.02247e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -8.64679e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.19671e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -9.53235e-07\n",
      "evaluation/env_infos/reward_ctrl Std                   9.68369e-08\n",
      "evaluation/env_infos/reward_ctrl Max                  -5.59195e-07\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.16509e-06\n",
      "evaluation/env_infos/final/height Mean                -0.132891\n",
      "evaluation/env_infos/final/height Std                  3.5865e-05\n",
      "evaluation/env_infos/final/height Max                 -0.132829\n",
      "evaluation/env_infos/final/height Min                 -0.132952\n",
      "evaluation/env_infos/initial/height Mean              -0.00261261\n",
      "evaluation/env_infos/initial/height Std                0.0507168\n",
      "evaluation/env_infos/initial/height Max                0.0808508\n",
      "evaluation/env_infos/initial/height Min               -0.0863121\n",
      "evaluation/env_infos/height Mean                      -0.132403\n",
      "evaluation/env_infos/height Std                        0.00598515\n",
      "evaluation/env_infos/height Max                        0.0808508\n",
      "evaluation/env_infos/height Min                       -0.147987\n",
      "evaluation/env_infos/final/reward_angular Mean         1.80369e-16\n",
      "evaluation/env_infos/final/reward_angular Std          1.16466e-15\n",
      "evaluation/env_infos/final/reward_angular Max          2.20667e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -2.39561e-15\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.0462222\n",
      "evaluation/env_infos/initial/reward_angular Std        0.203028\n",
      "evaluation/env_infos/initial/reward_angular Max        0.61123\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.258986\n",
      "evaluation/env_infos/reward_angular Mean               0.00137134\n",
      "evaluation/env_infos/reward_angular Std                0.0441042\n",
      "evaluation/env_infos/reward_angular Max                2.04155\n",
      "evaluation/env_infos/reward_angular Min               -1.34125\n",
      "time/data storing (s)                                  0.0162352\n",
      "time/evaluation sampling (s)                          24.0442\n",
      "time/exploration sampling (s)                          1.24839\n",
      "time/logging (s)                                       0.240108\n",
      "time/saving (s)                                        0.0758232\n",
      "time/training (s)                                      3.81875\n",
      "time/epoch (s)                                        29.4435\n",
      "time/total (s)                                        33.292\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:25:55.894479 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  3000\n",
      "trainer/QF1 Loss                                       0.678998\n",
      "trainer/QF2 Loss                                       0.681364\n",
      "trainer/Policy Loss                                   -7.30679\n",
      "trainer/Q1 Predictions Mean                            3.26513\n",
      "trainer/Q1 Predictions Std                             0.577677\n",
      "trainer/Q1 Predictions Max                             4.89418\n",
      "trainer/Q1 Predictions Min                             2.13402\n",
      "trainer/Q2 Predictions Mean                            3.27674\n",
      "trainer/Q2 Predictions Std                             0.576639\n",
      "trainer/Q2 Predictions Max                             4.89072\n",
      "trainer/Q2 Predictions Min                             2.17989\n",
      "trainer/Q Targets Mean                                 3.28808\n",
      "trainer/Q Targets Std                                  0.923946\n",
      "trainer/Q Targets Max                                  8.23753\n",
      "trainer/Q Targets Min                                  0.520468\n",
      "trainer/Log Pis Mean                                  -4.08131\n",
      "trainer/Log Pis Std                                    0.34747\n",
      "trainer/Log Pis Max                                   -3.30788\n",
      "trainer/Log Pis Min                                   -6.72451\n",
      "trainer/Policy mu Mean                                -0.04255\n",
      "trainer/Policy mu Std                                  0.0315773\n",
      "trainer/Policy mu Max                                  0.0126763\n",
      "trainer/Policy mu Min                                 -0.157892\n",
      "trainer/Policy log std Mean                           -0.11093\n",
      "trainer/Policy log std Std                             0.0199102\n",
      "trainer/Policy log std Max                            -0.0667875\n",
      "trainer/Policy log std Min                            -0.194263\n",
      "trainer/Alpha                                          0.738516\n",
      "trainer/Alpha Loss                                    -3.02552\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                            3\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.212453\n",
      "exploration/Rewards Std                                0.470014\n",
      "exploration/Rewards Max                                1.69922\n",
      "exploration/Rewards Min                               -1.70513\n",
      "exploration/Returns Mean                            -212.453\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -212.453\n",
      "exploration/Returns Min                             -212.453\n",
      "exploration/Actions Mean                              -0.0328488\n",
      "exploration/Actions Std                                0.59676\n",
      "exploration/Actions Max                                0.995742\n",
      "exploration/Actions Min                               -0.998759\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -212.453\n",
      "exploration/env_infos/final/reward_run Mean           -0.210926\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.210926\n",
      "exploration/env_infos/final/reward_run Min            -0.210926\n",
      "exploration/env_infos/initial/reward_run Mean         -0.475005\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.475005\n",
      "exploration/env_infos/initial/reward_run Min          -0.475005\n",
      "exploration/env_infos/reward_run Mean                 -0.0793773\n",
      "exploration/env_infos/reward_run Std                   0.651602\n",
      "exploration/env_infos/reward_run Max                   1.81344\n",
      "exploration/env_infos/reward_run Min                  -2.30045\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.203473\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.203473\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.203473\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.2237\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.2237\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.2237\n",
      "exploration/env_infos/reward_ctrl Mean                -0.214321\n",
      "exploration/env_infos/reward_ctrl Std                  0.0715869\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0116574\n",
      "exploration/env_infos/reward_ctrl Min                 -0.496016\n",
      "exploration/env_infos/final/height Mean               -0.0337743\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0337743\n",
      "exploration/env_infos/final/height Min                -0.0337743\n",
      "exploration/env_infos/initial/height Mean              0.0839974\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0839974\n",
      "exploration/env_infos/initial/height Min               0.0839974\n",
      "exploration/env_infos/height Mean                     -0.0762302\n",
      "exploration/env_infos/height Std                       0.084257\n",
      "exploration/env_infos/height Max                       0.166745\n",
      "exploration/env_infos/height Min                      -0.327067\n",
      "exploration/env_infos/final/reward_angular Mean       -1.23031\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.23031\n",
      "exploration/env_infos/final/reward_angular Min        -1.23031\n",
      "exploration/env_infos/initial/reward_angular Mean      1.03283\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.03283\n",
      "exploration/env_infos/initial/reward_angular Min       1.03283\n",
      "exploration/env_infos/reward_angular Mean             -0.0116875\n",
      "exploration/env_infos/reward_angular Std               1.69053\n",
      "exploration/env_infos/reward_angular Max               5.89018\n",
      "exploration/env_infos/reward_angular Min              -5.68969\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0601582\n",
      "evaluation/Rewards Std                                 0.04255\n",
      "evaluation/Rewards Max                                 1.02703\n",
      "evaluation/Rewards Min                                -1.15343\n",
      "evaluation/Returns Mean                              -60.1582\n",
      "evaluation/Returns Std                                36.9065\n",
      "evaluation/Returns Max                                -0.695749\n",
      "evaluation/Returns Min                              -123.901\n",
      "evaluation/Actions Mean                               -0.0339672\n",
      "evaluation/Actions Std                                 0.0240189\n",
      "evaluation/Actions Max                                -0.00864913\n",
      "evaluation/Actions Min                                -0.0928537\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -60.1582\n",
      "evaluation/env_infos/final/reward_run Mean            -1.46515e-09\n",
      "evaluation/env_infos/final/reward_run Std              5.70146e-09\n",
      "evaluation/env_infos/final/reward_run Max              4.4929e-09\n",
      "evaluation/env_infos/final/reward_run Min             -2.11309e-08\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.0137592\n",
      "evaluation/env_infos/initial/reward_run Std            0.163507\n",
      "evaluation/env_infos/initial/reward_run Max            0.331616\n",
      "evaluation/env_infos/initial/reward_run Min           -0.387343\n",
      "evaluation/env_infos/reward_run Mean                  -0.000270794\n",
      "evaluation/env_infos/reward_run Std                    0.0171161\n",
      "evaluation/env_infos/reward_run Max                    0.352385\n",
      "evaluation/env_infos/reward_run Min                   -0.419241\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00103835\n",
      "evaluation/env_infos/final/reward_ctrl Std             5.82639e-05\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.000861242\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00111939\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00102597\n",
      "evaluation/env_infos/initial/reward_ctrl Std           6.66741e-05\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000879625\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00113864\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00103841\n",
      "evaluation/env_infos/reward_ctrl Std                   5.86909e-05\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000703211\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0012154\n",
      "evaluation/env_infos/final/height Mean                -0.129065\n",
      "evaluation/env_infos/final/height Std                  9.12761e-05\n",
      "evaluation/env_infos/final/height Max                 -0.128896\n",
      "evaluation/env_infos/final/height Min                 -0.129302\n",
      "evaluation/env_infos/initial/height Mean              -0.0138464\n",
      "evaluation/env_infos/initial/height Std                0.0516688\n",
      "evaluation/env_infos/initial/height Max                0.0824065\n",
      "evaluation/env_infos/initial/height Min               -0.093044\n",
      "evaluation/env_infos/height Mean                      -0.12866\n",
      "evaluation/env_infos/height Std                        0.00528021\n",
      "evaluation/env_infos/height Max                        0.0824065\n",
      "evaluation/env_infos/height Min                       -0.144338\n",
      "evaluation/env_infos/final/reward_angular Mean        -4.00339e-09\n",
      "evaluation/env_infos/final/reward_angular Std          2.78886e-08\n",
      "evaluation/env_infos/final/reward_angular Max          4.82978e-08\n",
      "evaluation/env_infos/final/reward_angular Min         -1.31216e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.297131\n",
      "evaluation/env_infos/initial/reward_angular Std        0.340036\n",
      "evaluation/env_infos/initial/reward_angular Max        1.20393\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.447319\n",
      "evaluation/env_infos/reward_angular Mean               0.00118776\n",
      "evaluation/env_infos/reward_angular Std                0.0409047\n",
      "evaluation/env_infos/reward_angular Max                1.98845\n",
      "evaluation/env_infos/reward_angular Min               -1.49289\n",
      "time/data storing (s)                                  0.0157651\n",
      "time/evaluation sampling (s)                          24.7408\n",
      "time/exploration sampling (s)                          1.13584\n",
      "time/logging (s)                                       0.235015\n",
      "time/saving (s)                                        0.0411126\n",
      "time/training (s)                                      3.45743\n",
      "time/epoch (s)                                        29.626\n",
      "time/total (s)                                        63.1674\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:26:24.701505 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                       0.404294\n",
      "trainer/QF2 Loss                                       0.403937\n",
      "trainer/Policy Loss                                   -7.04332\n",
      "trainer/Q1 Predictions Mean                            3.00057\n",
      "trainer/Q1 Predictions Std                             0.584367\n",
      "trainer/Q1 Predictions Max                             5.41582\n",
      "trainer/Q1 Predictions Min                             1.62855\n",
      "trainer/Q2 Predictions Mean                            3.01536\n",
      "trainer/Q2 Predictions Std                             0.567837\n",
      "trainer/Q2 Predictions Max                             5.29854\n",
      "trainer/Q2 Predictions Min                             1.62793\n",
      "trainer/Q Targets Mean                                 2.98985\n",
      "trainer/Q Targets Std                                  0.821935\n",
      "trainer/Q Targets Max                                  5.90788\n",
      "trainer/Q Targets Min                                  1.03996\n",
      "trainer/Log Pis Mean                                  -3.94463\n",
      "trainer/Log Pis Std                                    0.572421\n",
      "trainer/Log Pis Max                                   -1.57778\n",
      "trainer/Log Pis Min                                   -5.49842\n",
      "trainer/Policy mu Mean                                 0.0579283\n",
      "trainer/Policy mu Std                                  0.171058\n",
      "trainer/Policy mu Max                                  0.883871\n",
      "trainer/Policy mu Min                                 -0.193105\n",
      "trainer/Policy log std Mean                           -0.135114\n",
      "trainer/Policy log std Std                             0.0209456\n",
      "trainer/Policy log std Max                            -0.0761559\n",
      "trainer/Policy log std Min                            -0.225826\n",
      "trainer/Alpha                                          0.547506\n",
      "trainer/Alpha Loss                                    -5.96089\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                            4\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.24663\n",
      "exploration/Rewards Std                                0.581159\n",
      "exploration/Rewards Max                                1.74305\n",
      "exploration/Rewards Min                               -2.30054\n",
      "exploration/Returns Mean                            -246.63\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -246.63\n",
      "exploration/Returns Min                             -246.63\n",
      "exploration/Actions Mean                               0.00726707\n",
      "exploration/Actions Std                                0.594888\n",
      "exploration/Actions Max                                0.997216\n",
      "exploration/Actions Min                               -0.996298\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -246.63\n",
      "exploration/env_infos/final/reward_run Mean            0.116645\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.116645\n",
      "exploration/env_infos/final/reward_run Min             0.116645\n",
      "exploration/env_infos/initial/reward_run Mean          0.0616765\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.0616765\n",
      "exploration/env_infos/initial/reward_run Min           0.0616765\n",
      "exploration/env_infos/reward_run Mean                 -0.0188526\n",
      "exploration/env_infos/reward_run Std                   0.608925\n",
      "exploration/env_infos/reward_run Max                   1.75101\n",
      "exploration/env_infos/reward_run Min                  -2.38662\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.157908\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.157908\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.157908\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.160263\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.160263\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.160263\n",
      "exploration/env_infos/reward_ctrl Mean                -0.212367\n",
      "exploration/env_infos/reward_ctrl Std                  0.0734344\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0277211\n",
      "exploration/env_infos/reward_ctrl Min                 -0.42973\n",
      "exploration/env_infos/final/height Mean               -0.576053\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.576053\n",
      "exploration/env_infos/final/height Min                -0.576053\n",
      "exploration/env_infos/initial/height Mean             -0.0783618\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0783618\n",
      "exploration/env_infos/initial/height Min              -0.0783618\n",
      "exploration/env_infos/height Mean                     -0.329583\n",
      "exploration/env_infos/height Std                       0.236838\n",
      "exploration/env_infos/height Max                       0.166742\n",
      "exploration/env_infos/height Min                      -0.584316\n",
      "exploration/env_infos/final/reward_angular Mean       -0.00872479\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.00872479\n",
      "exploration/env_infos/final/reward_angular Min        -0.00872479\n",
      "exploration/env_infos/initial/reward_angular Mean      0.548147\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.548147\n",
      "exploration/env_infos/initial/reward_angular Min       0.548147\n",
      "exploration/env_infos/reward_angular Mean              0.0464629\n",
      "exploration/env_infos/reward_angular Std               1.41229\n",
      "exploration/env_infos/reward_angular Max               4.76513\n",
      "exploration/env_infos/reward_angular Min              -4.61424\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0631725\n",
      "evaluation/Rewards Std                                 0.0472528\n",
      "evaluation/Rewards Max                                 1.56586\n",
      "evaluation/Rewards Min                                -1.58833\n",
      "evaluation/Returns Mean                              -63.1725\n",
      "evaluation/Returns Std                                36.4916\n",
      "evaluation/Returns Max                                -3.80718\n",
      "evaluation/Returns Min                              -123.017\n",
      "evaluation/Actions Mean                                0.0722568\n",
      "evaluation/Actions Std                                 0.10843\n",
      "evaluation/Actions Max                                 0.471978\n",
      "evaluation/Actions Min                                -0.0753232\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -63.1725\n",
      "evaluation/env_infos/final/reward_run Mean            -1.47613e-09\n",
      "evaluation/env_infos/final/reward_run Std              3.34352e-08\n",
      "evaluation/env_infos/final/reward_run Max              5.64184e-08\n",
      "evaluation/env_infos/final/reward_run Min             -1.18418e-07\n",
      "evaluation/env_infos/initial/reward_run Mean           0.257676\n",
      "evaluation/env_infos/initial/reward_run Std            0.148793\n",
      "evaluation/env_infos/initial/reward_run Max            0.589325\n",
      "evaluation/env_infos/initial/reward_run Min           -0.06334\n",
      "evaluation/env_infos/reward_run Mean                  -0.000193345\n",
      "evaluation/env_infos/reward_run Std                    0.0219669\n",
      "evaluation/env_infos/reward_run Max                    0.721781\n",
      "evaluation/env_infos/reward_run Min                   -0.399405\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0101876\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00810318\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00079541\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.024873\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0156605\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0119967\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000774114\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0386431\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0101869\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00810298\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000716399\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0386431\n",
      "evaluation/env_infos/final/height Mean                -0.128694\n",
      "evaluation/env_infos/final/height Std                  0.000835328\n",
      "evaluation/env_infos/final/height Max                 -0.127758\n",
      "evaluation/env_infos/final/height Min                 -0.130298\n",
      "evaluation/env_infos/initial/height Mean              -0.0248016\n",
      "evaluation/env_infos/initial/height Std                0.0484514\n",
      "evaluation/env_infos/initial/height Max                0.0699546\n",
      "evaluation/env_infos/initial/height Min               -0.0921782\n",
      "evaluation/env_infos/height Mean                      -0.12822\n",
      "evaluation/env_infos/height Std                        0.00498882\n",
      "evaluation/env_infos/height Max                        0.0699546\n",
      "evaluation/env_infos/height Min                       -0.1444\n",
      "evaluation/env_infos/final/reward_angular Mean         1.16422e-08\n",
      "evaluation/env_infos/final/reward_angular Std          5.83257e-08\n",
      "evaluation/env_infos/final/reward_angular Max          2.49108e-07\n",
      "evaluation/env_infos/final/reward_angular Min         -7.37001e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.122197\n",
      "evaluation/env_infos/initial/reward_angular Std        0.477995\n",
      "evaluation/env_infos/initial/reward_angular Max        1.16753\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.903272\n",
      "evaluation/env_infos/reward_angular Mean               0.000826385\n",
      "evaluation/env_infos/reward_angular Std                0.0438158\n",
      "evaluation/env_infos/reward_angular Max                1.8137\n",
      "evaluation/env_infos/reward_angular Min               -1.36182\n",
      "time/data storing (s)                                  0.0151108\n",
      "time/evaluation sampling (s)                          23.7598\n",
      "time/exploration sampling (s)                          1.05998\n",
      "time/logging (s)                                       0.307348\n",
      "time/saving (s)                                        0.0277021\n",
      "time/training (s)                                      3.58953\n",
      "time/epoch (s)                                        28.7594\n",
      "time/total (s)                                        92.0462\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:26:55.923922 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   5000\n",
      "trainer/QF1 Loss                                        0.660069\n",
      "trainer/QF2 Loss                                        0.637088\n",
      "trainer/Policy Loss                                    -6.84251\n",
      "trainer/Q1 Predictions Mean                             2.79787\n",
      "trainer/Q1 Predictions Std                              0.677523\n",
      "trainer/Q1 Predictions Max                              4.76841\n",
      "trainer/Q1 Predictions Min                              0.851322\n",
      "trainer/Q2 Predictions Mean                             2.82682\n",
      "trainer/Q2 Predictions Std                              0.667765\n",
      "trainer/Q2 Predictions Max                              4.75247\n",
      "trainer/Q2 Predictions Min                              1.01812\n",
      "trainer/Q Targets Mean                                  3.04989\n",
      "trainer/Q Targets Std                                   0.920228\n",
      "trainer/Q Targets Max                                   8.43564\n",
      "trainer/Q Targets Min                                  -1.13101\n",
      "trainer/Log Pis Mean                                   -3.98263\n",
      "trainer/Log Pis Std                                     0.590446\n",
      "trainer/Log Pis Max                                    -1.90346\n",
      "trainer/Log Pis Min                                    -5.83776\n",
      "trainer/Policy mu Mean                                  0.0613373\n",
      "trainer/Policy mu Std                                   0.176744\n",
      "trainer/Policy mu Max                                   0.796341\n",
      "trainer/Policy mu Min                                  -0.44235\n",
      "trainer/Policy log std Mean                            -0.10363\n",
      "trainer/Policy log std Std                              0.0308055\n",
      "trainer/Policy log std Max                             -0.00845246\n",
      "trainer/Policy log std Min                             -0.267518\n",
      "trainer/Alpha                                           0.406715\n",
      "trainer/Alpha Loss                                     -8.95116\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                             5\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.0215991\n",
      "exploration/Rewards Std                                 0.515672\n",
      "exploration/Rewards Max                                 1.86304\n",
      "exploration/Rewards Min                                -1.51046\n",
      "exploration/Returns Mean                               21.5991\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                21.5991\n",
      "exploration/Returns Min                                21.5991\n",
      "exploration/Actions Mean                                0.0507274\n",
      "exploration/Actions Std                                 0.598589\n",
      "exploration/Actions Max                                 0.999479\n",
      "exploration/Actions Min                                -0.997621\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            21.5991\n",
      "exploration/env_infos/final/reward_run Mean             0.0147182\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0147182\n",
      "exploration/env_infos/final/reward_run Min              0.0147182\n",
      "exploration/env_infos/initial/reward_run Mean           0.440269\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.440269\n",
      "exploration/env_infos/initial/reward_run Min            0.440269\n",
      "exploration/env_infos/reward_run Mean                  -0.171141\n",
      "exploration/env_infos/reward_run Std                    0.683938\n",
      "exploration/env_infos/reward_run Max                    2.11781\n",
      "exploration/env_infos/reward_run Min                   -2.50504\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.175075\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.175075\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.175075\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.205215\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.205215\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.205215\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.216529\n",
      "exploration/env_infos/reward_ctrl Std                   0.0695732\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0481168\n",
      "exploration/env_infos/reward_ctrl Min                  -0.470154\n",
      "exploration/env_infos/final/height Mean                -0.148439\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.148439\n",
      "exploration/env_infos/final/height Min                 -0.148439\n",
      "exploration/env_infos/initial/height Mean              -0.0295987\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0295987\n",
      "exploration/env_infos/initial/height Min               -0.0295987\n",
      "exploration/env_infos/height Mean                      -0.0697129\n",
      "exploration/env_infos/height Std                        0.079027\n",
      "exploration/env_infos/height Max                        0.231921\n",
      "exploration/env_infos/height Min                       -0.32015\n",
      "exploration/env_infos/final/reward_angular Mean         0.688389\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.688389\n",
      "exploration/env_infos/final/reward_angular Min          0.688389\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.16192\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.16192\n",
      "exploration/env_infos/initial/reward_angular Min       -1.16192\n",
      "exploration/env_infos/reward_angular Mean              -0.0133176\n",
      "exploration/env_infos/reward_angular Std                1.68497\n",
      "exploration/env_infos/reward_angular Max                6.90645\n",
      "exploration/env_infos/reward_angular Min               -5.50978\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0638969\n",
      "evaluation/Rewards Std                                  0.0451971\n",
      "evaluation/Rewards Max                                  1.33637\n",
      "evaluation/Rewards Min                                 -0.747575\n",
      "evaluation/Returns Mean                               -63.8969\n",
      "evaluation/Returns Std                                 36.8872\n",
      "evaluation/Returns Max                                 -5.13772\n",
      "evaluation/Returns Min                               -123.338\n",
      "evaluation/Actions Mean                                 0.0973005\n",
      "evaluation/Actions Std                                  0.110045\n",
      "evaluation/Actions Max                                  0.438717\n",
      "evaluation/Actions Min                                 -0.0361656\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -63.8969\n",
      "evaluation/env_infos/final/reward_run Mean              1.43629e-11\n",
      "evaluation/env_infos/final/reward_run Std               2.57531e-08\n",
      "evaluation/env_infos/final/reward_run Max               8.06746e-08\n",
      "evaluation/env_infos/final/reward_run Min              -8.86879e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.225131\n",
      "evaluation/env_infos/initial/reward_run Std             0.164741\n",
      "evaluation/env_infos/initial/reward_run Max             0.731396\n",
      "evaluation/env_infos/initial/reward_run Min            -0.0662965\n",
      "evaluation/env_infos/reward_run Mean                   -0.000766692\n",
      "evaluation/env_infos/reward_run Std                     0.0226042\n",
      "evaluation/env_infos/reward_run Max                     0.844874\n",
      "evaluation/env_infos/reward_run Min                    -0.513954\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.012957\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00923473\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.000127909\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0293506\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0203973\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0121344\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.000182907\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0397603\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0129464\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0092331\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000108798\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0397603\n",
      "evaluation/env_infos/final/height Mean                 -0.128347\n",
      "evaluation/env_infos/final/height Std                   0.00104404\n",
      "evaluation/env_infos/final/height Max                  -0.127473\n",
      "evaluation/env_infos/final/height Min                  -0.131804\n",
      "evaluation/env_infos/initial/height Mean               -0.00598214\n",
      "evaluation/env_infos/initial/height Std                 0.0478423\n",
      "evaluation/env_infos/initial/height Max                 0.0707485\n",
      "evaluation/env_infos/initial/height Min                -0.0803713\n",
      "evaluation/env_infos/height Mean                       -0.127887\n",
      "evaluation/env_infos/height Std                         0.00562686\n",
      "evaluation/env_infos/height Max                         0.0707485\n",
      "evaluation/env_infos/height Min                        -0.142209\n",
      "evaluation/env_infos/final/reward_angular Mean          9.75646e-09\n",
      "evaluation/env_infos/final/reward_angular Std           4.298e-08\n",
      "evaluation/env_infos/final/reward_angular Max           1.63495e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -6.65047e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.344683\n",
      "evaluation/env_infos/initial/reward_angular Std         0.310808\n",
      "evaluation/env_infos/initial/reward_angular Max         0.302222\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.888375\n",
      "evaluation/env_infos/reward_angular Mean                5.41934e-05\n",
      "evaluation/env_infos/reward_angular Std                 0.0425479\n",
      "evaluation/env_infos/reward_angular Max                 1.63764\n",
      "evaluation/env_infos/reward_angular Min                -1.29989\n",
      "time/data storing (s)                                   0.0151406\n",
      "time/evaluation sampling (s)                           26.3482\n",
      "time/exploration sampling (s)                           1.02265\n",
      "time/logging (s)                                        0.247237\n",
      "time/saving (s)                                         0.0291334\n",
      "time/training (s)                                       3.3256\n",
      "time/epoch (s)                                         30.988\n",
      "time/total (s)                                        123.208\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:27:25.408745 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   6000\n",
      "trainer/QF1 Loss                                        0.34782\n",
      "trainer/QF2 Loss                                        0.374001\n",
      "trainer/Policy Loss                                    -6.9377\n",
      "trainer/Q1 Predictions Mean                             2.9726\n",
      "trainer/Q1 Predictions Std                              0.571176\n",
      "trainer/Q1 Predictions Max                              4.96146\n",
      "trainer/Q1 Predictions Min                              1.90839\n",
      "trainer/Q2 Predictions Mean                             2.98138\n",
      "trainer/Q2 Predictions Std                              0.552342\n",
      "trainer/Q2 Predictions Max                              4.72368\n",
      "trainer/Q2 Predictions Min                              1.93048\n",
      "trainer/Q Targets Mean                                  3.04122\n",
      "trainer/Q Targets Std                                   0.842551\n",
      "trainer/Q Targets Max                                   5.83423\n",
      "trainer/Q Targets Min                                   0.735378\n",
      "trainer/Log Pis Mean                                   -3.8756\n",
      "trainer/Log Pis Std                                     0.930922\n",
      "trainer/Log Pis Max                                    -0.6194\n",
      "trainer/Log Pis Min                                    -8.30373\n",
      "trainer/Policy mu Mean                                  0.087058\n",
      "trainer/Policy mu Std                                   0.230402\n",
      "trainer/Policy mu Max                                   0.844121\n",
      "trainer/Policy mu Min                                  -0.817405\n",
      "trainer/Policy log std Mean                            -0.103249\n",
      "trainer/Policy log std Std                              0.0241596\n",
      "trainer/Policy log std Max                             -0.0518442\n",
      "trainer/Policy log std Min                             -0.218388\n",
      "trainer/Alpha                                           0.302416\n",
      "trainer/Alpha Loss                                    -11.7815\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                             6\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.152268\n",
      "exploration/Rewards Std                                 0.785926\n",
      "exploration/Rewards Max                                 2.59785\n",
      "exploration/Rewards Min                                -3.1851\n",
      "exploration/Returns Mean                             -152.268\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -152.268\n",
      "exploration/Returns Min                              -152.268\n",
      "exploration/Actions Mean                                0.083698\n",
      "exploration/Actions Std                                 0.600483\n",
      "exploration/Actions Max                                 0.99743\n",
      "exploration/Actions Min                                -0.997158\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -152.268\n",
      "exploration/env_infos/final/reward_run Mean             1.46604\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.46604\n",
      "exploration/env_infos/final/reward_run Min              1.46604\n",
      "exploration/env_infos/initial/reward_run Mean           0.637536\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.637536\n",
      "exploration/env_infos/initial/reward_run Min            0.637536\n",
      "exploration/env_infos/reward_run Mean                  -0.0652612\n",
      "exploration/env_infos/reward_run Std                    0.732367\n",
      "exploration/env_infos/reward_run Max                    2.33612\n",
      "exploration/env_infos/reward_run Min                   -2.34585\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.306209\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.306209\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.306209\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.273272\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.273272\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.273272\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.220551\n",
      "exploration/env_infos/reward_ctrl Std                   0.0739784\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0238213\n",
      "exploration/env_infos/reward_ctrl Min                  -0.510183\n",
      "exploration/env_infos/final/height Mean                -0.101705\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.101705\n",
      "exploration/env_infos/final/height Min                 -0.101705\n",
      "exploration/env_infos/initial/height Mean              -0.0427249\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0427249\n",
      "exploration/env_infos/initial/height Min               -0.0427249\n",
      "exploration/env_infos/height Mean                      -0.0683865\n",
      "exploration/env_infos/height Std                        0.0896722\n",
      "exploration/env_infos/height Max                        0.239235\n",
      "exploration/env_infos/height Min                       -0.375183\n",
      "exploration/env_infos/final/reward_angular Mean         1.92289\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.92289\n",
      "exploration/env_infos/final/reward_angular Min          1.92289\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.93187\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.93187\n",
      "exploration/env_infos/initial/reward_angular Min       -1.93187\n",
      "exploration/env_infos/reward_angular Mean              -0.0190962\n",
      "exploration/env_infos/reward_angular Std                1.83357\n",
      "exploration/env_infos/reward_angular Max                6.72595\n",
      "exploration/env_infos/reward_angular Min               -5.07088\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0662701\n",
      "evaluation/Rewards Std                                  0.0488911\n",
      "evaluation/Rewards Max                                  1.37551\n",
      "evaluation/Rewards Min                                 -1.55438\n",
      "evaluation/Returns Mean                               -66.2701\n",
      "evaluation/Returns Std                                 37.2317\n",
      "evaluation/Returns Max                                 -7.82187\n",
      "evaluation/Returns Min                               -124.825\n",
      "evaluation/Actions Mean                                 0.118131\n",
      "evaluation/Actions Std                                  0.120476\n",
      "evaluation/Actions Max                                  0.512526\n",
      "evaluation/Actions Min                                 -0.242226\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -66.2701\n",
      "evaluation/env_infos/final/reward_run Mean              3.89846e-09\n",
      "evaluation/env_infos/final/reward_run Std               7.80762e-08\n",
      "evaluation/env_infos/final/reward_run Max               1.5864e-07\n",
      "evaluation/env_infos/final/reward_run Min              -2.75191e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.260463\n",
      "evaluation/env_infos/initial/reward_run Std             0.191691\n",
      "evaluation/env_infos/initial/reward_run Max             0.663177\n",
      "evaluation/env_infos/initial/reward_run Min            -0.127732\n",
      "evaluation/env_infos/reward_run Mean                   -0.00128514\n",
      "evaluation/env_infos/reward_run Std                     0.0336864\n",
      "evaluation/env_infos/reward_run Max                     0.846925\n",
      "evaluation/env_infos/reward_run Min                    -0.768794\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0170809\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0131514\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.000397432\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.041023\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0305576\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0197416\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.000368635\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0607829\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0170816\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0131796\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000230521\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0643334\n",
      "evaluation/env_infos/final/height Mean                 -0.129784\n",
      "evaluation/env_infos/final/height Std                   0.00139696\n",
      "evaluation/env_infos/final/height Max                  -0.127934\n",
      "evaluation/env_infos/final/height Min                  -0.134536\n",
      "evaluation/env_infos/initial/height Mean               -0.00297703\n",
      "evaluation/env_infos/initial/height Std                 0.0521741\n",
      "evaluation/env_infos/initial/height Max                 0.0847405\n",
      "evaluation/env_infos/initial/height Min                -0.0977625\n",
      "evaluation/env_infos/height Mean                       -0.129395\n",
      "evaluation/env_infos/height Std                         0.00599436\n",
      "evaluation/env_infos/height Max                         0.0847405\n",
      "evaluation/env_infos/height Min                        -0.160827\n",
      "evaluation/env_infos/final/reward_angular Mean         -2.3107e-08\n",
      "evaluation/env_infos/final/reward_angular Std           9.49464e-08\n",
      "evaluation/env_infos/final/reward_angular Max           1.31381e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -3.63647e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.482479\n",
      "evaluation/env_infos/initial/reward_angular Std         0.391506\n",
      "evaluation/env_infos/initial/reward_angular Max         0.752706\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.989899\n",
      "evaluation/env_infos/reward_angular Mean                4.84121e-05\n",
      "evaluation/env_infos/reward_angular Std                 0.0565949\n",
      "evaluation/env_infos/reward_angular Max                 2.49137\n",
      "evaluation/env_infos/reward_angular Min                -1.32524\n",
      "time/data storing (s)                                   0.015292\n",
      "time/evaluation sampling (s)                           24.1823\n",
      "time/exploration sampling (s)                           1.09973\n",
      "time/logging (s)                                        0.244341\n",
      "time/saving (s)                                         0.0269703\n",
      "time/training (s)                                       3.77265\n",
      "time/epoch (s)                                         29.3412\n",
      "time/total (s)                                        152.689\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:27:54.098445 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   7000\n",
      "trainer/QF1 Loss                                        0.302022\n",
      "trainer/QF2 Loss                                        0.309897\n",
      "trainer/Policy Loss                                    -7.11827\n",
      "trainer/Q1 Predictions Mean                             3.21751\n",
      "trainer/Q1 Predictions Std                              0.681575\n",
      "trainer/Q1 Predictions Max                              6.34929\n",
      "trainer/Q1 Predictions Min                              1.43128\n",
      "trainer/Q2 Predictions Mean                             3.20145\n",
      "trainer/Q2 Predictions Std                              0.670372\n",
      "trainer/Q2 Predictions Max                              6.0328\n",
      "trainer/Q2 Predictions Min                              1.60019\n",
      "trainer/Q Targets Mean                                  3.11226\n",
      "trainer/Q Targets Std                                   0.861615\n",
      "trainer/Q Targets Max                                   6.29185\n",
      "trainer/Q Targets Min                                   0.263009\n",
      "trainer/Log Pis Mean                                   -3.84321\n",
      "trainer/Log Pis Std                                     0.947173\n",
      "trainer/Log Pis Max                                    -0.9439\n",
      "trainer/Log Pis Min                                    -8.45313\n",
      "trainer/Policy mu Mean                                  0.0650433\n",
      "trainer/Policy mu Std                                   0.299663\n",
      "trainer/Policy mu Max                                   1.09032\n",
      "trainer/Policy mu Min                                  -0.618818\n",
      "trainer/Policy log std Mean                            -0.130071\n",
      "trainer/Policy log std Std                              0.0459318\n",
      "trainer/Policy log std Max                             -0.0310924\n",
      "trainer/Policy log std Min                             -0.31297\n",
      "trainer/Alpha                                           0.225194\n",
      "trainer/Alpha Loss                                    -14.6453\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                             7\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.133335\n",
      "exploration/Rewards Std                                 1.02307\n",
      "exploration/Rewards Max                                 2.71467\n",
      "exploration/Rewards Min                                -4.37094\n",
      "exploration/Returns Mean                             -133.335\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -133.335\n",
      "exploration/Returns Min                              -133.335\n",
      "exploration/Actions Mean                                0.0485146\n",
      "exploration/Actions Std                                 0.597525\n",
      "exploration/Actions Max                                 0.997636\n",
      "exploration/Actions Min                                -0.998246\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -133.335\n",
      "exploration/env_infos/final/reward_run Mean            -0.850672\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.850672\n",
      "exploration/env_infos/final/reward_run Min             -0.850672\n",
      "exploration/env_infos/initial/reward_run Mean           0.336019\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.336019\n",
      "exploration/env_infos/initial/reward_run Min            0.336019\n",
      "exploration/env_infos/reward_run Mean                  -0.196774\n",
      "exploration/env_infos/reward_run Std                    0.743209\n",
      "exploration/env_infos/reward_run Max                    2.02367\n",
      "exploration/env_infos/reward_run Min                   -2.59347\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.311993\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.311993\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.311993\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.195277\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.195277\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.195277\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.215634\n",
      "exploration/env_infos/reward_ctrl Std                   0.0727784\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0402273\n",
      "exploration/env_infos/reward_ctrl Min                  -0.4578\n",
      "exploration/env_infos/final/height Mean                -0.529436\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.529436\n",
      "exploration/env_infos/final/height Min                 -0.529436\n",
      "exploration/env_infos/initial/height Mean               0.0138918\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0138918\n",
      "exploration/env_infos/initial/height Min                0.0138918\n",
      "exploration/env_infos/height Mean                      -0.0872805\n",
      "exploration/env_infos/height Std                        0.146697\n",
      "exploration/env_infos/height Max                        0.329546\n",
      "exploration/env_infos/height Min                       -0.577221\n",
      "exploration/env_infos/final/reward_angular Mean        -1.59787\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.59787\n",
      "exploration/env_infos/final/reward_angular Min         -1.59787\n",
      "exploration/env_infos/initial/reward_angular Mean       0.335933\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.335933\n",
      "exploration/env_infos/initial/reward_angular Min        0.335933\n",
      "exploration/env_infos/reward_angular Mean               0.0452737\n",
      "exploration/env_infos/reward_angular Std                1.63253\n",
      "exploration/env_infos/reward_angular Max                6.67946\n",
      "exploration/env_infos/reward_angular Min               -4.11736\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0642839\n",
      "evaluation/Rewards Std                                  0.0442269\n",
      "evaluation/Rewards Max                                  1.14336\n",
      "evaluation/Rewards Min                                 -1.33236\n",
      "evaluation/Returns Mean                               -64.2839\n",
      "evaluation/Returns Std                                 35.0916\n",
      "evaluation/Returns Max                                -11.8819\n",
      "evaluation/Returns Min                               -124.56\n",
      "evaluation/Actions Mean                                 0.0867836\n",
      "evaluation/Actions Std                                  0.149971\n",
      "evaluation/Actions Max                                  0.660345\n",
      "evaluation/Actions Min                                 -0.195225\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -64.2839\n",
      "evaluation/env_infos/final/reward_run Mean             -2.28889e-09\n",
      "evaluation/env_infos/final/reward_run Std               1.07538e-07\n",
      "evaluation/env_infos/final/reward_run Max               1.95302e-07\n",
      "evaluation/env_infos/final/reward_run Min              -2.60385e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.342047\n",
      "evaluation/env_infos/initial/reward_run Std             0.21554\n",
      "evaluation/env_infos/initial/reward_run Max             0.669853\n",
      "evaluation/env_infos/initial/reward_run Min            -0.0732008\n",
      "evaluation/env_infos/reward_run Mean                   -0.000763219\n",
      "evaluation/env_infos/reward_run Std                     0.0320648\n",
      "evaluation/env_infos/reward_run Max                     0.697505\n",
      "evaluation/env_infos/reward_run Min                    -0.782973\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0180438\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0146508\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00234493\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0558265\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0338689\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.023959\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00153563\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0793432\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0180136\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0146364\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00119716\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0793432\n",
      "evaluation/env_infos/final/height Mean                 -0.124222\n",
      "evaluation/env_infos/final/height Std                   0.00369425\n",
      "evaluation/env_infos/final/height Max                  -0.119019\n",
      "evaluation/env_infos/final/height Min                  -0.135083\n",
      "evaluation/env_infos/initial/height Mean               -0.00864841\n",
      "evaluation/env_infos/initial/height Std                 0.0544218\n",
      "evaluation/env_infos/initial/height Max                 0.0888142\n",
      "evaluation/env_infos/initial/height Min                -0.106201\n",
      "evaluation/env_infos/height Mean                       -0.123863\n",
      "evaluation/env_infos/height Std                         0.00649149\n",
      "evaluation/env_infos/height Max                         0.0888142\n",
      "evaluation/env_infos/height Min                        -0.165325\n",
      "evaluation/env_infos/final/reward_angular Mean          4.43666e-10\n",
      "evaluation/env_infos/final/reward_angular Std           9.39996e-08\n",
      "evaluation/env_infos/final/reward_angular Max           1.9481e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -2.38687e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.424206\n",
      "evaluation/env_infos/initial/reward_angular Std         0.358403\n",
      "evaluation/env_infos/initial/reward_angular Max         0.538481\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.50301\n",
      "evaluation/env_infos/reward_angular Mean               -0.000451583\n",
      "evaluation/env_infos/reward_angular Std                 0.0407909\n",
      "evaluation/env_infos/reward_angular Max                 1.66636\n",
      "evaluation/env_infos/reward_angular Min                -1.50301\n",
      "time/data storing (s)                                   0.0152406\n",
      "time/evaluation sampling (s)                           23.4496\n",
      "time/exploration sampling (s)                           1.09141\n",
      "time/logging (s)                                        0.236815\n",
      "time/saving (s)                                         0.0281054\n",
      "time/training (s)                                       3.69812\n",
      "time/epoch (s)                                         28.5193\n",
      "time/total (s)                                        181.37\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:28:22.929369 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   8000\n",
      "trainer/QF1 Loss                                        0.311253\n",
      "trainer/QF2 Loss                                        0.305818\n",
      "trainer/Policy Loss                                    -6.5333\n",
      "trainer/Q1 Predictions Mean                             3.02075\n",
      "trainer/Q1 Predictions Std                              0.773871\n",
      "trainer/Q1 Predictions Max                              6.07187\n",
      "trainer/Q1 Predictions Min                              1.35424\n",
      "trainer/Q2 Predictions Mean                             3.04164\n",
      "trainer/Q2 Predictions Std                              0.753752\n",
      "trainer/Q2 Predictions Max                              6.04236\n",
      "trainer/Q2 Predictions Min                              1.46644\n",
      "trainer/Q Targets Mean                                  3.08249\n",
      "trainer/Q Targets Std                                   0.953505\n",
      "trainer/Q Targets Max                                   6.34373\n",
      "trainer/Q Targets Min                                   0.501439\n",
      "trainer/Log Pis Mean                                   -3.29842\n",
      "trainer/Log Pis Std                                     1.20823\n",
      "trainer/Log Pis Max                                     0.36734\n",
      "trainer/Log Pis Min                                    -6.5304\n",
      "trainer/Policy mu Mean                                  0.107687\n",
      "trainer/Policy mu Std                                   0.423264\n",
      "trainer/Policy mu Max                                   1.21434\n",
      "trainer/Policy mu Min                                  -1.2975\n",
      "trainer/Policy log std Mean                            -0.150765\n",
      "trainer/Policy log std Std                              0.0474662\n",
      "trainer/Policy log std Max                             -0.0623825\n",
      "trainer/Policy log std Min                             -0.415479\n",
      "trainer/Alpha                                           0.168506\n",
      "trainer/Alpha Loss                                    -16.5318\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                             8\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.132583\n",
      "exploration/Rewards Std                                 0.916441\n",
      "exploration/Rewards Max                                 2.5542\n",
      "exploration/Rewards Min                                -2.79809\n",
      "exploration/Returns Mean                             -132.583\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -132.583\n",
      "exploration/Returns Min                              -132.583\n",
      "exploration/Actions Mean                                0.163069\n",
      "exploration/Actions Std                                 0.584393\n",
      "exploration/Actions Max                                 0.998728\n",
      "exploration/Actions Min                                -0.993812\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -132.583\n",
      "exploration/env_infos/final/reward_run Mean             0.393565\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.393565\n",
      "exploration/env_infos/final/reward_run Min              0.393565\n",
      "exploration/env_infos/initial/reward_run Mean           0.102906\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.102906\n",
      "exploration/env_infos/initial/reward_run Min            0.102906\n",
      "exploration/env_infos/reward_run Mean                   0.110471\n",
      "exploration/env_infos/reward_run Std                    0.595749\n",
      "exploration/env_infos/reward_run Max                    1.7497\n",
      "exploration/env_infos/reward_run Min                   -2.15104\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.243049\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.243049\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.243049\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0494102\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0494102\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0494102\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.220864\n",
      "exploration/env_infos/reward_ctrl Std                   0.0726978\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0323928\n",
      "exploration/env_infos/reward_ctrl Min                  -0.445833\n",
      "exploration/env_infos/final/height Mean                -0.105356\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.105356\n",
      "exploration/env_infos/final/height Min                 -0.105356\n",
      "exploration/env_infos/initial/height Mean               0.0337205\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0337205\n",
      "exploration/env_infos/initial/height Min                0.0337205\n",
      "exploration/env_infos/height Mean                      -0.0906307\n",
      "exploration/env_infos/height Std                        0.0956253\n",
      "exploration/env_infos/height Max                        0.219754\n",
      "exploration/env_infos/height Min                       -0.357882\n",
      "exploration/env_infos/final/reward_angular Mean        -2.25245\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.25245\n",
      "exploration/env_infos/final/reward_angular Min         -2.25245\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.504082\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.504082\n",
      "exploration/env_infos/initial/reward_angular Min       -0.504082\n",
      "exploration/env_infos/reward_angular Mean               0.000221663\n",
      "exploration/env_infos/reward_angular Std                1.63122\n",
      "exploration/env_infos/reward_angular Max                4.7788\n",
      "exploration/env_infos/reward_angular Min               -4.81696\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0741872\n",
      "evaluation/Rewards Std                                  0.0540667\n",
      "evaluation/Rewards Max                                  1.38595\n",
      "evaluation/Rewards Min                                 -0.611058\n",
      "evaluation/Returns Mean                               -74.1872\n",
      "evaluation/Returns Std                                 34.4883\n",
      "evaluation/Returns Max                                 -1.82452\n",
      "evaluation/Returns Min                               -141.575\n",
      "evaluation/Actions Mean                                 0.13231\n",
      "evaluation/Actions Std                                  0.256171\n",
      "evaluation/Actions Max                                  0.725726\n",
      "evaluation/Actions Min                                 -0.795668\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -74.1872\n",
      "evaluation/env_infos/final/reward_run Mean              3.39553e-08\n",
      "evaluation/env_infos/final/reward_run Std               1.74019e-07\n",
      "evaluation/env_infos/final/reward_run Max               5.82459e-07\n",
      "evaluation/env_infos/final/reward_run Min              -3.0088e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.306446\n",
      "evaluation/env_infos/initial/reward_run Std             0.245852\n",
      "evaluation/env_infos/initial/reward_run Max             0.656701\n",
      "evaluation/env_infos/initial/reward_run Min            -0.209952\n",
      "evaluation/env_infos/reward_run Mean                    0.000176292\n",
      "evaluation/env_infos/reward_run Std                     0.0354188\n",
      "evaluation/env_infos/reward_run Max                     0.841495\n",
      "evaluation/env_infos/reward_run Min                    -0.555329\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0498743\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0199702\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0133719\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0756444\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0734294\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0259681\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0218386\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.116788\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0498778\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0199948\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0025786\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.130834\n",
      "evaluation/env_infos/final/height Mean                 -0.127698\n",
      "evaluation/env_infos/final/height Std                   0.0273535\n",
      "evaluation/env_infos/final/height Max                  -0.101489\n",
      "evaluation/env_infos/final/height Min                  -0.240458\n",
      "evaluation/env_infos/initial/height Mean               -0.0104864\n",
      "evaluation/env_infos/initial/height Std                 0.0560078\n",
      "evaluation/env_infos/initial/height Max                 0.0808589\n",
      "evaluation/env_infos/initial/height Min                -0.0924165\n",
      "evaluation/env_infos/height Mean                       -0.12735\n",
      "evaluation/env_infos/height Std                         0.0280139\n",
      "evaluation/env_infos/height Max                         0.0808589\n",
      "evaluation/env_infos/height Min                        -0.317924\n",
      "evaluation/env_infos/final/reward_angular Mean         -1.35412e-08\n",
      "evaluation/env_infos/final/reward_angular Std           1.40932e-07\n",
      "evaluation/env_infos/final/reward_angular Max           3.54589e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -2.22701e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.297336\n",
      "evaluation/env_infos/initial/reward_angular Std         0.888196\n",
      "evaluation/env_infos/initial/reward_angular Max         1.53201\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.97585\n",
      "evaluation/env_infos/reward_angular Mean                0.000735248\n",
      "evaluation/env_infos/reward_angular Std                 0.0651814\n",
      "evaluation/env_infos/reward_angular Max                 1.85842\n",
      "evaluation/env_infos/reward_angular Min                -1.97585\n",
      "time/data storing (s)                                   0.0143119\n",
      "time/evaluation sampling (s)                           23.6739\n",
      "time/exploration sampling (s)                           1.05523\n",
      "time/logging (s)                                        0.230204\n",
      "time/saving (s)                                         0.0297492\n",
      "time/training (s)                                       3.67364\n",
      "time/epoch (s)                                         28.677\n",
      "time/total (s)                                        210.194\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:28:55.042511 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   9000\n",
      "trainer/QF1 Loss                                        0.366858\n",
      "trainer/QF2 Loss                                        0.390279\n",
      "trainer/Policy Loss                                    -6.50662\n",
      "trainer/Q1 Predictions Mean                             3.05197\n",
      "trainer/Q1 Predictions Std                              0.858733\n",
      "trainer/Q1 Predictions Max                              6.50648\n",
      "trainer/Q1 Predictions Min                              1.22083\n",
      "trainer/Q2 Predictions Mean                             2.98844\n",
      "trainer/Q2 Predictions Std                              0.839669\n",
      "trainer/Q2 Predictions Max                              6.3676\n",
      "trainer/Q2 Predictions Min                              0.98963\n",
      "trainer/Q Targets Mean                                  3.00652\n",
      "trainer/Q Targets Std                                   1.06152\n",
      "trainer/Q Targets Max                                   8.19994\n",
      "trainer/Q Targets Min                                   0.430129\n",
      "trainer/Log Pis Mean                                   -3.36148\n",
      "trainer/Log Pis Std                                     1.24214\n",
      "trainer/Log Pis Max                                     0.865553\n",
      "trainer/Log Pis Min                                    -6.99672\n",
      "trainer/Policy mu Mean                                  0.098193\n",
      "trainer/Policy mu Std                                   0.429879\n",
      "trainer/Policy mu Max                                   1.13697\n",
      "trainer/Policy mu Min                                  -1.46031\n",
      "trainer/Policy log std Mean                            -0.150965\n",
      "trainer/Policy log std Std                              0.0671134\n",
      "trainer/Policy log std Max                              0.00279409\n",
      "trainer/Policy log std Min                             -0.411634\n",
      "trainer/Alpha                                           0.127143\n",
      "trainer/Alpha Loss                                    -19.2816\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                             9\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.331082\n",
      "exploration/Rewards Std                                 0.39926\n",
      "exploration/Rewards Max                                 1.22621\n",
      "exploration/Rewards Min                                -1.91359\n",
      "exploration/Returns Mean                             -331.082\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -331.082\n",
      "exploration/Returns Min                              -331.082\n",
      "exploration/Actions Mean                                0.0636036\n",
      "exploration/Actions Std                                 0.601142\n",
      "exploration/Actions Max                                 0.998308\n",
      "exploration/Actions Min                                -0.996476\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -331.082\n",
      "exploration/env_infos/final/reward_run Mean             0.701903\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.701903\n",
      "exploration/env_infos/final/reward_run Min              0.701903\n",
      "exploration/env_infos/initial/reward_run Mean           0.522772\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.522772\n",
      "exploration/env_infos/initial/reward_run Min            0.522772\n",
      "exploration/env_infos/reward_run Mean                   0.060462\n",
      "exploration/env_infos/reward_run Std                    0.563869\n",
      "exploration/env_infos/reward_run Max                    1.74384\n",
      "exploration/env_infos/reward_run Min                   -2.11935\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.313563\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.313563\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.313563\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.333007\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.333007\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.333007\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.21925\n",
      "exploration/env_infos/reward_ctrl Std                   0.0752098\n",
      "exploration/env_infos/reward_ctrl Max                  -0.025331\n",
      "exploration/env_infos/reward_ctrl Min                  -0.47343\n",
      "exploration/env_infos/final/height Mean                -0.517642\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.517642\n",
      "exploration/env_infos/final/height Min                 -0.517642\n",
      "exploration/env_infos/initial/height Mean              -0.0193259\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0193259\n",
      "exploration/env_infos/initial/height Min               -0.0193259\n",
      "exploration/env_infos/height Mean                      -0.392447\n",
      "exploration/env_infos/height Std                        0.223643\n",
      "exploration/env_infos/height Max                        0.20079\n",
      "exploration/env_infos/height Min                       -0.583908\n",
      "exploration/env_infos/final/reward_angular Mean         1.65226\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.65226\n",
      "exploration/env_infos/final/reward_angular Min          1.65226\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.67577\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.67577\n",
      "exploration/env_infos/initial/reward_angular Min       -1.67577\n",
      "exploration/env_infos/reward_angular Mean               0.0367101\n",
      "exploration/env_infos/reward_angular Std                1.36133\n",
      "exploration/env_infos/reward_angular Max                5.69306\n",
      "exploration/env_infos/reward_angular Min               -4.10553\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0893393\n",
      "evaluation/Rewards Std                                  0.0639081\n",
      "evaluation/Rewards Max                                  1.86875\n",
      "evaluation/Rewards Min                                 -0.894307\n",
      "evaluation/Returns Mean                               -89.3393\n",
      "evaluation/Returns Std                                 38.273\n",
      "evaluation/Returns Max                                 -4.731\n",
      "evaluation/Returns Min                               -160.043\n",
      "evaluation/Actions Mean                                 0.109719\n",
      "evaluation/Actions Std                                  0.341522\n",
      "evaluation/Actions Max                                  0.795235\n",
      "evaluation/Actions Min                                 -0.856884\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -89.3393\n",
      "evaluation/env_infos/final/reward_run Mean              0.00139953\n",
      "evaluation/env_infos/final/reward_run Std               0.0315586\n",
      "evaluation/env_infos/final/reward_run Max               0.0928237\n",
      "evaluation/env_infos/final/reward_run Min              -0.114559\n",
      "evaluation/env_infos/initial/reward_run Mean            0.154453\n",
      "evaluation/env_infos/initial/reward_run Std             0.335763\n",
      "evaluation/env_infos/initial/reward_run Max             0.831824\n",
      "evaluation/env_infos/initial/reward_run Min            -0.483821\n",
      "evaluation/env_infos/reward_run Mean                    0.00040014\n",
      "evaluation/env_infos/reward_run Std                     0.0490861\n",
      "evaluation/env_infos/reward_run Max                     0.962737\n",
      "evaluation/env_infos/reward_run Min                    -0.815609\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0772719\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0355419\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0336026\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.147406\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0928768\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0323537\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0398058\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.150094\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0772053\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0356049\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0193105\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.16944\n",
      "evaluation/env_infos/final/height Mean                 -0.141981\n",
      "evaluation/env_infos/final/height Std                   0.038087\n",
      "evaluation/env_infos/final/height Max                  -0.103836\n",
      "evaluation/env_infos/final/height Min                  -0.301845\n",
      "evaluation/env_infos/initial/height Mean               -0.00274296\n",
      "evaluation/env_infos/initial/height Std                 0.0556386\n",
      "evaluation/env_infos/initial/height Max                 0.0798893\n",
      "evaluation/env_infos/initial/height Min                -0.102651\n",
      "evaluation/env_infos/height Mean                       -0.141593\n",
      "evaluation/env_infos/height Std                         0.0384638\n",
      "evaluation/env_infos/height Max                         0.0798893\n",
      "evaluation/env_infos/height Min                        -0.391286\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0130875\n",
      "evaluation/env_infos/final/reward_angular Std           0.0627234\n",
      "evaluation/env_infos/final/reward_angular Max           0.260424\n",
      "evaluation/env_infos/final/reward_angular Min          -0.0944246\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.221766\n",
      "evaluation/env_infos/initial/reward_angular Std         1.16648\n",
      "evaluation/env_infos/initial/reward_angular Max         2.09881\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.03103\n",
      "evaluation/env_infos/reward_angular Mean                0.000934048\n",
      "evaluation/env_infos/reward_angular Std                 0.099894\n",
      "evaluation/env_infos/reward_angular Max                 2.19899\n",
      "evaluation/env_infos/reward_angular Min                -2.03103\n",
      "time/data storing (s)                                   0.0139589\n",
      "time/evaluation sampling (s)                           25.6578\n",
      "time/exploration sampling (s)                           1.01289\n",
      "time/logging (s)                                        0.236044\n",
      "time/saving (s)                                         0.0283792\n",
      "time/training (s)                                       5.01963\n",
      "time/epoch (s)                                         31.9687\n",
      "time/total (s)                                        242.312\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:29:25.640503 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.396492\n",
      "trainer/QF2 Loss                                        0.45555\n",
      "trainer/Policy Loss                                    -5.69391\n",
      "trainer/Q1 Predictions Mean                             2.85443\n",
      "trainer/Q1 Predictions Std                              0.866309\n",
      "trainer/Q1 Predictions Max                              5.13464\n",
      "trainer/Q1 Predictions Min                              0.855442\n",
      "trainer/Q2 Predictions Mean                             2.85276\n",
      "trainer/Q2 Predictions Std                              0.88765\n",
      "trainer/Q2 Predictions Max                              5.44078\n",
      "trainer/Q2 Predictions Min                              0.872045\n",
      "trainer/Q Targets Mean                                  2.98283\n",
      "trainer/Q Targets Std                                   1.03583\n",
      "trainer/Q Targets Max                                   6.37942\n",
      "trainer/Q Targets Min                                   0.154292\n",
      "trainer/Log Pis Mean                                   -2.64767\n",
      "trainer/Log Pis Std                                     1.83803\n",
      "trainer/Log Pis Max                                     3.66269\n",
      "trainer/Log Pis Min                                    -8.15564\n",
      "trainer/Policy mu Mean                                  0.17921\n",
      "trainer/Policy mu Std                                   0.589758\n",
      "trainer/Policy mu Max                                   1.83659\n",
      "trainer/Policy mu Min                                  -1.90115\n",
      "trainer/Policy log std Mean                            -0.193133\n",
      "trainer/Policy log std Std                              0.0955556\n",
      "trainer/Policy log std Max                              0.0207221\n",
      "trainer/Policy log std Min                             -0.609458\n",
      "trainer/Alpha                                           0.0964551\n",
      "trainer/Alpha Loss                                    -20.201\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            10\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.279427\n",
      "exploration/Rewards Std                                 0.629328\n",
      "exploration/Rewards Max                                 2.97378\n",
      "exploration/Rewards Min                                -2.31236\n",
      "exploration/Returns Mean                             -279.427\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -279.427\n",
      "exploration/Returns Min                              -279.427\n",
      "exploration/Actions Mean                               -0.093557\n",
      "exploration/Actions Std                                 0.606431\n",
      "exploration/Actions Max                                 0.997263\n",
      "exploration/Actions Min                                -0.998393\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -279.427\n",
      "exploration/env_infos/final/reward_run Mean            -0.959611\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.959611\n",
      "exploration/env_infos/final/reward_run Min             -0.959611\n",
      "exploration/env_infos/initial/reward_run Mean           0.871612\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.871612\n",
      "exploration/env_infos/initial/reward_run Min            0.871612\n",
      "exploration/env_infos/reward_run Mean                  -0.0163227\n",
      "exploration/env_infos/reward_run Std                    0.605483\n",
      "exploration/env_infos/reward_run Max                    2.59884\n",
      "exploration/env_infos/reward_run Min                   -1.8465\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.219076\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.219076\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.219076\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.409534\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.409534\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.409534\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.225907\n",
      "exploration/env_infos/reward_ctrl Std                   0.0760969\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0128957\n",
      "exploration/env_infos/reward_ctrl Min                  -0.482815\n",
      "exploration/env_infos/final/height Mean                -0.559632\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.559632\n",
      "exploration/env_infos/final/height Min                 -0.559632\n",
      "exploration/env_infos/initial/height Mean               0.0795178\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0795178\n",
      "exploration/env_infos/initial/height Min                0.0795178\n",
      "exploration/env_infos/height Mean                      -0.473104\n",
      "exploration/env_infos/height Std                        0.146229\n",
      "exploration/env_infos/height Max                        0.222094\n",
      "exploration/env_infos/height Min                       -0.58475\n",
      "exploration/env_infos/final/reward_angular Mean         0.477173\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.477173\n",
      "exploration/env_infos/final/reward_angular Min          0.477173\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.198617\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.198617\n",
      "exploration/env_infos/initial/reward_angular Min       -0.198617\n",
      "exploration/env_infos/reward_angular Mean               0.0685486\n",
      "exploration/env_infos/reward_angular Std                1.50792\n",
      "exploration/env_infos/reward_angular Max                6.55913\n",
      "exploration/env_infos/reward_angular Min               -4.80212\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.118042\n",
      "evaluation/Rewards Std                                  0.094573\n",
      "evaluation/Rewards Max                                  1.88904\n",
      "evaluation/Rewards Min                                 -1.03839\n",
      "evaluation/Returns Mean                              -118.042\n",
      "evaluation/Returns Std                                 57.6869\n",
      "evaluation/Returns Max                                -16.541\n",
      "evaluation/Returns Min                               -191.75\n",
      "evaluation/Actions Mean                                 0.155618\n",
      "evaluation/Actions Std                                  0.452071\n",
      "evaluation/Actions Max                                  0.949207\n",
      "evaluation/Actions Min                                 -0.925411\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -118.042\n",
      "evaluation/env_infos/final/reward_run Mean             -0.00718322\n",
      "evaluation/env_infos/final/reward_run Std               0.0889627\n",
      "evaluation/env_infos/final/reward_run Max               0.21272\n",
      "evaluation/env_infos/final/reward_run Min              -0.3923\n",
      "evaluation/env_infos/initial/reward_run Mean            0.121926\n",
      "evaluation/env_infos/initial/reward_run Std             0.42735\n",
      "evaluation/env_infos/initial/reward_run Max             0.690743\n",
      "evaluation/env_infos/initial/reward_run Min            -0.658931\n",
      "evaluation/env_infos/reward_run Mean                    0.00119972\n",
      "evaluation/env_infos/reward_run Std                     0.0913823\n",
      "evaluation/env_infos/reward_run Max                     0.888153\n",
      "evaluation/env_infos/reward_run Min                    -0.964507\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.138539\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.057508\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0511635\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.234922\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.1441\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0448074\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0754737\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.220267\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.137151\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0577214\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0169002\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.277262\n",
      "evaluation/env_infos/final/height Mean                 -0.161097\n",
      "evaluation/env_infos/final/height Std                   0.0653437\n",
      "evaluation/env_infos/final/height Max                  -0.0797708\n",
      "evaluation/env_infos/final/height Min                  -0.390148\n",
      "evaluation/env_infos/initial/height Mean               -0.00271014\n",
      "evaluation/env_infos/initial/height Std                 0.0510249\n",
      "evaluation/env_infos/initial/height Max                 0.0745272\n",
      "evaluation/env_infos/initial/height Min                -0.0878226\n",
      "evaluation/env_infos/height Mean                       -0.158104\n",
      "evaluation/env_infos/height Std                         0.0591385\n",
      "evaluation/env_infos/height Max                         0.0745272\n",
      "evaluation/env_infos/height Min                        -0.420088\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0457603\n",
      "evaluation/env_infos/final/reward_angular Std           0.155593\n",
      "evaluation/env_infos/final/reward_angular Max           0.612036\n",
      "evaluation/env_infos/final/reward_angular Min          -1.32812e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.451353\n",
      "evaluation/env_infos/initial/reward_angular Std         1.11919\n",
      "evaluation/env_infos/initial/reward_angular Max         1.63531\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.28871\n",
      "evaluation/env_infos/reward_angular Mean               -0.000442958\n",
      "evaluation/env_infos/reward_angular Std                 0.261367\n",
      "evaluation/env_infos/reward_angular Max                 1.86197\n",
      "evaluation/env_infos/reward_angular Min                -2.28871\n",
      "time/data storing (s)                                   0.0154268\n",
      "time/evaluation sampling (s)                           24.9184\n",
      "time/exploration sampling (s)                           1.06343\n",
      "time/logging (s)                                        0.237683\n",
      "time/saving (s)                                         0.0272585\n",
      "time/training (s)                                       4.17277\n",
      "time/epoch (s)                                         30.435\n",
      "time/total (s)                                        272.911\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:29:53.601068 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  11000\n",
      "trainer/QF1 Loss                                        0.440484\n",
      "trainer/QF2 Loss                                        0.4617\n",
      "trainer/Policy Loss                                    -5.41309\n",
      "trainer/Q1 Predictions Mean                             2.98194\n",
      "trainer/Q1 Predictions Std                              1.02691\n",
      "trainer/Q1 Predictions Max                              5.8588\n",
      "trainer/Q1 Predictions Min                              1.14559\n",
      "trainer/Q2 Predictions Mean                             2.92538\n",
      "trainer/Q2 Predictions Std                              1.02364\n",
      "trainer/Q2 Predictions Max                              6.17431\n",
      "trainer/Q2 Predictions Min                              1.08548\n",
      "trainer/Q Targets Mean                                  2.86781\n",
      "trainer/Q Targets Std                                   1.19284\n",
      "trainer/Q Targets Max                                   7.07663\n",
      "trainer/Q Targets Min                                  -0.692508\n",
      "trainer/Log Pis Mean                                   -2.21857\n",
      "trainer/Log Pis Std                                     1.96479\n",
      "trainer/Log Pis Max                                     3.85707\n",
      "trainer/Log Pis Min                                    -8.13553\n",
      "trainer/Policy mu Mean                                  0.134139\n",
      "trainer/Policy mu Std                                   0.648911\n",
      "trainer/Policy mu Max                                   1.97391\n",
      "trainer/Policy mu Min                                  -1.85443\n",
      "trainer/Policy log std Mean                            -0.22119\n",
      "trainer/Policy log std Std                              0.115862\n",
      "trainer/Policy log std Max                              0.033157\n",
      "trainer/Policy log std Min                             -0.708952\n",
      "trainer/Alpha                                           0.0738915\n",
      "trainer/Alpha Loss                                    -21.3893\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            11\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.150566\n",
      "exploration/Rewards Std                                 0.616007\n",
      "exploration/Rewards Max                                 1.96489\n",
      "exploration/Rewards Min                                -2.85181\n",
      "exploration/Returns Mean                             -150.566\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -150.566\n",
      "exploration/Returns Min                              -150.566\n",
      "exploration/Actions Mean                               -0.0873956\n",
      "exploration/Actions Std                                 0.582745\n",
      "exploration/Actions Max                                 0.995202\n",
      "exploration/Actions Min                                -0.997828\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -150.566\n",
      "exploration/env_infos/final/reward_run Mean            -0.504067\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.504067\n",
      "exploration/env_infos/final/reward_run Min             -0.504067\n",
      "exploration/env_infos/initial/reward_run Mean           0.203928\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.203928\n",
      "exploration/env_infos/initial/reward_run Min            0.203928\n",
      "exploration/env_infos/reward_run Mean                  -0.136499\n",
      "exploration/env_infos/reward_run Std                    0.680133\n",
      "exploration/env_infos/reward_run Max                    1.80255\n",
      "exploration/env_infos/reward_run Min                   -2.34348\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.233995\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.233995\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.233995\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.153462\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.153462\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.153462\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.208338\n",
      "exploration/env_infos/reward_ctrl Std                   0.0701441\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0163894\n",
      "exploration/env_infos/reward_ctrl Min                  -0.441796\n",
      "exploration/env_infos/final/height Mean                 0.00595382\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.00595382\n",
      "exploration/env_infos/final/height Min                  0.00595382\n",
      "exploration/env_infos/initial/height Mean              -0.00228377\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00228377\n",
      "exploration/env_infos/initial/height Min               -0.00228377\n",
      "exploration/env_infos/height Mean                      -0.0780336\n",
      "exploration/env_infos/height Std                        0.068569\n",
      "exploration/env_infos/height Max                        0.13261\n",
      "exploration/env_infos/height Min                       -0.266723\n",
      "exploration/env_infos/final/reward_angular Mean         1.48418\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.48418\n",
      "exploration/env_infos/final/reward_angular Min          1.48418\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.423474\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.423474\n",
      "exploration/env_infos/initial/reward_angular Min       -0.423474\n",
      "exploration/env_infos/reward_angular Mean              -0.0170365\n",
      "exploration/env_infos/reward_angular Std                1.62545\n",
      "exploration/env_infos/reward_angular Max                5.54911\n",
      "exploration/env_infos/reward_angular Min               -6.75835\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.132298\n",
      "evaluation/Rewards Std                                  0.283416\n",
      "evaluation/Rewards Max                                  2.91883\n",
      "evaluation/Rewards Min                                 -2.33025\n",
      "evaluation/Returns Mean                              -132.298\n",
      "evaluation/Returns Std                                 62.2532\n",
      "evaluation/Returns Max                                -35.3564\n",
      "evaluation/Returns Min                               -231.123\n",
      "evaluation/Actions Mean                                 0.114633\n",
      "evaluation/Actions Std                                  0.504517\n",
      "evaluation/Actions Max                                  0.982149\n",
      "evaluation/Actions Min                                 -0.947904\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -132.298\n",
      "evaluation/env_infos/final/reward_run Mean             -0.014843\n",
      "evaluation/env_infos/final/reward_run Std               0.111602\n",
      "evaluation/env_infos/final/reward_run Max               0.0807284\n",
      "evaluation/env_infos/final/reward_run Min              -0.551441\n",
      "evaluation/env_infos/initial/reward_run Mean            0.129341\n",
      "evaluation/env_infos/initial/reward_run Std             0.521533\n",
      "evaluation/env_infos/initial/reward_run Max             0.978732\n",
      "evaluation/env_infos/initial/reward_run Min            -0.628902\n",
      "evaluation/env_infos/reward_run Mean                    0.00284097\n",
      "evaluation/env_infos/reward_run Std                     0.130655\n",
      "evaluation/env_infos/reward_run Max                     1.12333\n",
      "evaluation/env_infos/reward_run Min                    -0.883157\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.166003\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0825414\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0427064\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.315\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.161476\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0662182\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0521294\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.290327\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.160607\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0775172\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0218503\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.334434\n",
      "evaluation/env_infos/final/height Mean                 -0.176591\n",
      "evaluation/env_infos/final/height Std                   0.0662451\n",
      "evaluation/env_infos/final/height Max                  -0.0875225\n",
      "evaluation/env_infos/final/height Min                  -0.356538\n",
      "evaluation/env_infos/initial/height Mean               -0.0176979\n",
      "evaluation/env_infos/initial/height Std                 0.0577566\n",
      "evaluation/env_infos/initial/height Max                 0.0751301\n",
      "evaluation/env_infos/initial/height Min                -0.0998241\n",
      "evaluation/env_infos/height Mean                       -0.172652\n",
      "evaluation/env_infos/height Std                         0.0693058\n",
      "evaluation/env_infos/height Max                         0.0751301\n",
      "evaluation/env_infos/height Min                        -0.451593\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0203959\n",
      "evaluation/env_infos/final/reward_angular Std           0.176406\n",
      "evaluation/env_infos/final/reward_angular Max           0.245222\n",
      "evaluation/env_infos/final/reward_angular Min          -0.849754\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.271615\n",
      "evaluation/env_infos/initial/reward_angular Std         1.44724\n",
      "evaluation/env_infos/initial/reward_angular Max         2.15185\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.42146\n",
      "evaluation/env_infos/reward_angular Mean                0.000354565\n",
      "evaluation/env_infos/reward_angular Std                 0.387838\n",
      "evaluation/env_infos/reward_angular Max                 2.46579\n",
      "evaluation/env_infos/reward_angular Min                -3.51152\n",
      "time/data storing (s)                                   0.0152336\n",
      "time/evaluation sampling (s)                           22.5513\n",
      "time/exploration sampling (s)                           1.05932\n",
      "time/logging (s)                                        0.236578\n",
      "time/saving (s)                                         0.0269645\n",
      "time/training (s)                                       3.90319\n",
      "time/epoch (s)                                         27.7926\n",
      "time/total (s)                                        300.869\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:22.796134 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 10 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.347352\n",
      "trainer/QF2 Loss                                        0.363084\n",
      "trainer/Policy Loss                                    -4.36417\n",
      "trainer/Q1 Predictions Mean                             2.80703\n",
      "trainer/Q1 Predictions Std                              1.07711\n",
      "trainer/Q1 Predictions Max                              7.37976\n",
      "trainer/Q1 Predictions Min                              0.798434\n",
      "trainer/Q2 Predictions Mean                             2.83167\n",
      "trainer/Q2 Predictions Std                              1.09118\n",
      "trainer/Q2 Predictions Max                              7.30804\n",
      "trainer/Q2 Predictions Min                              0.780649\n",
      "trainer/Q Targets Mean                                  2.7231\n",
      "trainer/Q Targets Std                                   1.24174\n",
      "trainer/Q Targets Max                                   8.40074\n",
      "trainer/Q Targets Min                                  -0.567778\n",
      "trainer/Log Pis Mean                                   -1.29696\n",
      "trainer/Log Pis Std                                     2.75919\n",
      "trainer/Log Pis Max                                     7.30614\n",
      "trainer/Log Pis Min                                    -6.36145\n",
      "trainer/Policy mu Mean                                  0.0708576\n",
      "trainer/Policy mu Std                                   0.839867\n",
      "trainer/Policy mu Max                                   2.76601\n",
      "trainer/Policy mu Min                                  -2.7229\n",
      "trainer/Policy log std Mean                            -0.279531\n",
      "trainer/Policy log std Std                              0.157806\n",
      "trainer/Policy log std Max                             -0.0415836\n",
      "trainer/Policy log std Min                             -1.02691\n",
      "trainer/Alpha                                           0.0572572\n",
      "trainer/Alpha Loss                                    -20.8534\n",
      "exploration/num steps total                         12000\n",
      "exploration/num paths total                            12\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.256502\n",
      "exploration/Rewards Std                                 0.486424\n",
      "exploration/Rewards Max                                 1.39869\n",
      "exploration/Rewards Min                                -1.94202\n",
      "exploration/Returns Mean                             -256.502\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -256.502\n",
      "exploration/Returns Min                              -256.502\n",
      "exploration/Actions Mean                               -0.00579013\n",
      "exploration/Actions Std                                 0.671616\n",
      "exploration/Actions Max                                 0.998928\n",
      "exploration/Actions Min                                -0.999132\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -256.502\n",
      "exploration/env_infos/final/reward_run Mean             0.228027\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.228027\n",
      "exploration/env_infos/final/reward_run Min              0.228027\n",
      "exploration/env_infos/initial/reward_run Mean          -0.151863\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.151863\n",
      "exploration/env_infos/initial/reward_run Min           -0.151863\n",
      "exploration/env_infos/reward_run Mean                   0.0734391\n",
      "exploration/env_infos/reward_run Std                    0.450262\n",
      "exploration/env_infos/reward_run Max                    1.75293\n",
      "exploration/env_infos/reward_run Min                   -1.28313\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.417611\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.417611\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.417611\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.302015\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.302015\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.302015\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.270661\n",
      "exploration/env_infos/reward_ctrl Std                   0.0727398\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0306649\n",
      "exploration/env_infos/reward_ctrl Min                  -0.464515\n",
      "exploration/env_infos/final/height Mean                -0.130841\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.130841\n",
      "exploration/env_infos/final/height Min                 -0.130841\n",
      "exploration/env_infos/initial/height Mean              -0.0507589\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0507589\n",
      "exploration/env_infos/initial/height Min               -0.0507589\n",
      "exploration/env_infos/height Mean                      -0.141916\n",
      "exploration/env_infos/height Std                        0.0414614\n",
      "exploration/env_infos/height Max                       -0.00716628\n",
      "exploration/env_infos/height Min                       -0.275951\n",
      "exploration/env_infos/final/reward_angular Mean        -0.0739635\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.0739635\n",
      "exploration/env_infos/final/reward_angular Min         -0.0739635\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.547677\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.547677\n",
      "exploration/env_infos/initial/reward_angular Min       -0.547677\n",
      "exploration/env_infos/reward_angular Mean              -0.0240182\n",
      "exploration/env_infos/reward_angular Std                1.07733\n",
      "exploration/env_infos/reward_angular Max                3.45054\n",
      "exploration/env_infos/reward_angular Min               -3.65822\n",
      "evaluation/num steps total                         275000\n",
      "evaluation/num paths total                            275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.191591\n",
      "evaluation/Rewards Std                                  0.138817\n",
      "evaluation/Rewards Max                                  2.30994\n",
      "evaluation/Rewards Min                                 -1.5459\n",
      "evaluation/Returns Mean                              -191.591\n",
      "evaluation/Returns Std                                 86.7916\n",
      "evaluation/Returns Max                                -50.5056\n",
      "evaluation/Returns Min                               -356.541\n",
      "evaluation/Actions Mean                                -0.0594011\n",
      "evaluation/Actions Std                                  0.632702\n",
      "evaluation/Actions Max                                  0.996176\n",
      "evaluation/Actions Min                                 -0.990727\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -191.591\n",
      "evaluation/env_infos/final/reward_run Mean             -0.00499401\n",
      "evaluation/env_infos/final/reward_run Std               0.0338081\n",
      "evaluation/env_infos/final/reward_run Max               0.0410231\n",
      "evaluation/env_infos/final/reward_run Min              -0.165877\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.0765445\n",
      "evaluation/env_infos/initial/reward_run Std             0.622454\n",
      "evaluation/env_infos/initial/reward_run Max             0.903393\n",
      "evaluation/env_infos/initial/reward_run Min            -0.986574\n",
      "evaluation/env_infos/reward_run Mean                   -0.00132038\n",
      "evaluation/env_infos/reward_run Std                     0.125599\n",
      "evaluation/env_infos/reward_run Max                     1.39884\n",
      "evaluation/env_infos/reward_run Min                    -2.03435\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.240693\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0949835\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0633274\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.414877\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.226589\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.072847\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0565931\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.397624\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.242304\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0930547\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00592707\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.433391\n",
      "evaluation/env_infos/final/height Mean                 -0.255507\n",
      "evaluation/env_infos/final/height Std                   0.111019\n",
      "evaluation/env_infos/final/height Max                  -0.086954\n",
      "evaluation/env_infos/final/height Min                  -0.444274\n",
      "evaluation/env_infos/initial/height Mean               -0.00853817\n",
      "evaluation/env_infos/initial/height Std                 0.0564785\n",
      "evaluation/env_infos/initial/height Max                 0.0803523\n",
      "evaluation/env_infos/initial/height Min                -0.0876324\n",
      "evaluation/env_infos/height Mean                       -0.246829\n",
      "evaluation/env_infos/height Std                         0.110982\n",
      "evaluation/env_infos/height Max                         0.0836824\n",
      "evaluation/env_infos/height Min                        -0.589453\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.144693\n",
      "evaluation/env_infos/final/reward_angular Std           0.704218\n",
      "evaluation/env_infos/final/reward_angular Max           3.14399e-05\n",
      "evaluation/env_infos/final/reward_angular Min          -3.59457\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.123777\n",
      "evaluation/env_infos/initial/reward_angular Std         1.68156\n",
      "evaluation/env_infos/initial/reward_angular Max         2.46396\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.40719\n",
      "evaluation/env_infos/reward_angular Mean                0.00622323\n",
      "evaluation/env_infos/reward_angular Std                 0.321124\n",
      "evaluation/env_infos/reward_angular Max                 4.37271\n",
      "evaluation/env_infos/reward_angular Min                -4.97917\n",
      "time/data storing (s)                                   0.0150893\n",
      "time/evaluation sampling (s)                           23.2413\n",
      "time/exploration sampling (s)                           1.14123\n",
      "time/logging (s)                                        0.231701\n",
      "time/saving (s)                                         0.0279358\n",
      "time/training (s)                                       4.35625\n",
      "time/epoch (s)                                         29.0135\n",
      "time/total (s)                                        330.059\n",
      "Epoch                                                  10\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:52.114202 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 11 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  13000\n",
      "trainer/QF1 Loss                                        0.62186\n",
      "trainer/QF2 Loss                                        0.641404\n",
      "trainer/Policy Loss                                    -3.34762\n",
      "trainer/Q1 Predictions Mean                             2.80112\n",
      "trainer/Q1 Predictions Std                              1.17322\n",
      "trainer/Q1 Predictions Max                              6.5256\n",
      "trainer/Q1 Predictions Min                              0.309959\n",
      "trainer/Q2 Predictions Mean                             2.76899\n",
      "trainer/Q2 Predictions Std                              1.14927\n",
      "trainer/Q2 Predictions Max                              6.87711\n",
      "trainer/Q2 Predictions Min                              0.422154\n",
      "trainer/Q Targets Mean                                  2.85016\n",
      "trainer/Q Targets Std                                   1.4288\n",
      "trainer/Q Targets Max                                   6.62139\n",
      "trainer/Q Targets Min                                  -1.42041\n",
      "trainer/Log Pis Mean                                   -0.223176\n",
      "trainer/Log Pis Std                                     3.0246\n",
      "trainer/Log Pis Max                                     8.99264\n",
      "trainer/Log Pis Min                                    -6.29994\n",
      "trainer/Policy mu Mean                                  0.0890124\n",
      "trainer/Policy mu Std                                   0.949379\n",
      "trainer/Policy mu Max                                   2.22471\n",
      "trainer/Policy mu Min                                  -3.30748\n",
      "trainer/Policy log std Mean                            -0.33648\n",
      "trainer/Policy log std Std                              0.144646\n",
      "trainer/Policy log std Max                             -0.00875626\n",
      "trainer/Policy log std Min                             -0.886248\n",
      "trainer/Alpha                                           0.0454195\n",
      "trainer/Alpha Loss                                    -19.2273\n",
      "exploration/num steps total                         13000\n",
      "exploration/num paths total                            13\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.21107\n",
      "exploration/Rewards Std                                 0.683479\n",
      "exploration/Rewards Max                                 2.70811\n",
      "exploration/Rewards Min                                -2.71554\n",
      "exploration/Returns Mean                             -211.07\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -211.07\n",
      "exploration/Returns Min                              -211.07\n",
      "exploration/Actions Mean                                0.0340567\n",
      "exploration/Actions Std                                 0.667242\n",
      "exploration/Actions Max                                 0.99835\n",
      "exploration/Actions Min                                -0.999235\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -211.07\n",
      "exploration/env_infos/final/reward_run Mean            -0.273459\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.273459\n",
      "exploration/env_infos/final/reward_run Min             -0.273459\n",
      "exploration/env_infos/initial/reward_run Mean           0.572769\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.572769\n",
      "exploration/env_infos/initial/reward_run Min            0.572769\n",
      "exploration/env_infos/reward_run Mean                  -0.0137146\n",
      "exploration/env_infos/reward_run Std                    0.75633\n",
      "exploration/env_infos/reward_run Max                    2.65295\n",
      "exploration/env_infos/reward_run Min                   -2.37686\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.29803\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.29803\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.29803\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.285346\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.285346\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.285346\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.267823\n",
      "exploration/env_infos/reward_ctrl Std                   0.0832693\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0393823\n",
      "exploration/env_infos/reward_ctrl Min                  -0.552954\n",
      "exploration/env_infos/final/height Mean                -0.350062\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.350062\n",
      "exploration/env_infos/final/height Min                 -0.350062\n",
      "exploration/env_infos/initial/height Mean              -0.0576912\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0576912\n",
      "exploration/env_infos/initial/height Min               -0.0576912\n",
      "exploration/env_infos/height Mean                      -0.429672\n",
      "exploration/env_infos/height Std                        0.150929\n",
      "exploration/env_infos/height Max                        0.189633\n",
      "exploration/env_infos/height Min                       -0.589262\n",
      "exploration/env_infos/final/reward_angular Mean        -3.43719\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -3.43719\n",
      "exploration/env_infos/final/reward_angular Min         -3.43719\n",
      "exploration/env_infos/initial/reward_angular Mean       1.53315\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.53315\n",
      "exploration/env_infos/initial/reward_angular Min        1.53315\n",
      "exploration/env_infos/reward_angular Mean               0.103856\n",
      "exploration/env_infos/reward_angular Std                1.97122\n",
      "exploration/env_infos/reward_angular Max                7.26266\n",
      "exploration/env_infos/reward_angular Min               -6.98633\n",
      "evaluation/num steps total                         300000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.173491\n",
      "evaluation/Rewards Std                                  0.475206\n",
      "evaluation/Rewards Max                                  3.91446\n",
      "evaluation/Rewards Min                                 -3.28242\n",
      "evaluation/Returns Mean                              -173.491\n",
      "evaluation/Returns Std                                 86.3477\n",
      "evaluation/Returns Max                                -35.9942\n",
      "evaluation/Returns Min                               -332.804\n",
      "evaluation/Actions Mean                                -0.0668547\n",
      "evaluation/Actions Std                                  0.634571\n",
      "evaluation/Actions Max                                  0.982637\n",
      "evaluation/Actions Min                                 -0.998535\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -173.491\n",
      "evaluation/env_infos/final/reward_run Mean              0.0954628\n",
      "evaluation/env_infos/final/reward_run Std               0.233269\n",
      "evaluation/env_infos/final/reward_run Max               0.876388\n",
      "evaluation/env_infos/final/reward_run Min              -0.0447034\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.22386\n",
      "evaluation/env_infos/initial/reward_run Std             0.576193\n",
      "evaluation/env_infos/initial/reward_run Max             0.745999\n",
      "evaluation/env_infos/initial/reward_run Min            -0.931447\n",
      "evaluation/env_infos/reward_run Mean                    0.0836933\n",
      "evaluation/env_infos/reward_run Std                     0.299153\n",
      "evaluation/env_infos/reward_run Max                     1.89961\n",
      "evaluation/env_infos/reward_run Min                    -1.326\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.242221\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0877073\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.109499\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.411052\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.243753\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0770816\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0586705\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.378439\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.24429\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0856868\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0320418\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.435262\n",
      "evaluation/env_infos/final/height Mean                 -0.181629\n",
      "evaluation/env_infos/final/height Std                   0.0756955\n",
      "evaluation/env_infos/final/height Max                  -0.0740405\n",
      "evaluation/env_infos/final/height Min                  -0.395312\n",
      "evaluation/env_infos/initial/height Mean               -0.00711958\n",
      "evaluation/env_infos/initial/height Std                 0.0528391\n",
      "evaluation/env_infos/initial/height Max                 0.0865466\n",
      "evaluation/env_infos/initial/height Min                -0.0863864\n",
      "evaluation/env_infos/height Mean                       -0.181878\n",
      "evaluation/env_infos/height Std                         0.074944\n",
      "evaluation/env_infos/height Max                         0.0865466\n",
      "evaluation/env_infos/height Min                        -0.450342\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0816831\n",
      "evaluation/env_infos/final/reward_angular Std           0.416494\n",
      "evaluation/env_infos/final/reward_angular Max           1.09898\n",
      "evaluation/env_infos/final/reward_angular Min          -1.20715\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.323237\n",
      "evaluation/env_infos/initial/reward_angular Std         1.60142\n",
      "evaluation/env_infos/initial/reward_angular Max         2.84788\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.33052\n",
      "evaluation/env_infos/reward_angular Mean               -0.00663392\n",
      "evaluation/env_infos/reward_angular Std                 0.608382\n",
      "evaluation/env_infos/reward_angular Max                 3.9897\n",
      "evaluation/env_infos/reward_angular Min                -4.72252\n",
      "time/data storing (s)                                   0.0152453\n",
      "time/evaluation sampling (s)                           23.6584\n",
      "time/exploration sampling (s)                           1.01504\n",
      "time/logging (s)                                        0.230469\n",
      "time/saving (s)                                         0.0277838\n",
      "time/training (s)                                       4.18998\n",
      "time/epoch (s)                                         29.1369\n",
      "time/total (s)                                        359.374\n",
      "Epoch                                                  11\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:31:20.721531 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 12 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.549177\n",
      "trainer/QF2 Loss                                        0.507934\n",
      "trainer/Policy Loss                                    -2.88674\n",
      "trainer/Q1 Predictions Mean                             2.95582\n",
      "trainer/Q1 Predictions Std                              1.28181\n",
      "trainer/Q1 Predictions Max                              7.07628\n",
      "trainer/Q1 Predictions Min                              0.628578\n",
      "trainer/Q2 Predictions Mean                             2.96642\n",
      "trainer/Q2 Predictions Std                              1.28397\n",
      "trainer/Q2 Predictions Max                              7.48487\n",
      "trainer/Q2 Predictions Min                              0.638272\n",
      "trainer/Q Targets Mean                                  2.80434\n",
      "trainer/Q Targets Std                                   1.42437\n",
      "trainer/Q Targets Max                                   8.18657\n",
      "trainer/Q Targets Min                                   0.0962379\n",
      "trainer/Log Pis Mean                                    0.38509\n",
      "trainer/Log Pis Std                                     3.02855\n",
      "trainer/Log Pis Max                                     9.63037\n",
      "trainer/Log Pis Min                                   -10.8615\n",
      "trainer/Policy mu Mean                                  0.249563\n",
      "trainer/Policy mu Std                                   0.994607\n",
      "trainer/Policy mu Max                                   2.68983\n",
      "trainer/Policy mu Min                                  -2.30509\n",
      "trainer/Policy log std Mean                            -0.380714\n",
      "trainer/Policy log std Std                              0.192551\n",
      "trainer/Policy log std Max                              0.0118805\n",
      "trainer/Policy log std Min                             -1.0596\n",
      "trainer/Alpha                                           0.0368297\n",
      "trainer/Alpha Loss                                    -18.5266\n",
      "exploration/num steps total                         14000\n",
      "exploration/num paths total                            14\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0890633\n",
      "exploration/Rewards Std                                 0.481711\n",
      "exploration/Rewards Max                                 1.64833\n",
      "exploration/Rewards Min                                -1.68039\n",
      "exploration/Returns Mean                              -89.0633\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -89.0633\n",
      "exploration/Returns Min                               -89.0633\n",
      "exploration/Actions Mean                                0.477336\n",
      "exploration/Actions Std                                 0.533965\n",
      "exploration/Actions Max                                 0.997706\n",
      "exploration/Actions Min                                -0.994714\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -89.0633\n",
      "exploration/env_infos/final/reward_run Mean             0.0189792\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0189792\n",
      "exploration/env_infos/final/reward_run Min              0.0189792\n",
      "exploration/env_infos/initial/reward_run Mean           0.410402\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.410402\n",
      "exploration/env_infos/initial/reward_run Min            0.410402\n",
      "exploration/env_infos/reward_run Mean                  -0.00369599\n",
      "exploration/env_infos/reward_run Std                    0.252821\n",
      "exploration/env_infos/reward_run Max                    1.12982\n",
      "exploration/env_infos/reward_run Min                   -0.974741\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.349309\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.349309\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.349309\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.328851\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.328851\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.328851\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.307781\n",
      "exploration/env_infos/reward_ctrl Std                   0.0700938\n",
      "exploration/env_infos/reward_ctrl Max                  -0.118948\n",
      "exploration/env_infos/reward_ctrl Min                  -0.48654\n",
      "exploration/env_infos/final/height Mean                -0.344528\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.344528\n",
      "exploration/env_infos/final/height Min                 -0.344528\n",
      "exploration/env_infos/initial/height Mean              -0.0258072\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0258072\n",
      "exploration/env_infos/initial/height Min               -0.0258072\n",
      "exploration/env_infos/height Mean                      -0.324168\n",
      "exploration/env_infos/height Std                        0.0507742\n",
      "exploration/env_infos/height Max                       -0.0258072\n",
      "exploration/env_infos/height Min                       -0.441793\n",
      "exploration/env_infos/final/reward_angular Mean        -0.684543\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.684543\n",
      "exploration/env_infos/final/reward_angular Min         -0.684543\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.62545\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.62545\n",
      "exploration/env_infos/initial/reward_angular Min       -1.62545\n",
      "exploration/env_infos/reward_angular Mean              -0.0012423\n",
      "exploration/env_infos/reward_angular Std                0.80034\n",
      "exploration/env_infos/reward_angular Max                2.4117\n",
      "exploration/env_infos/reward_angular Min               -2.90075\n",
      "evaluation/num steps total                         325000\n",
      "evaluation/num paths total                            325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.2256\n",
      "evaluation/Rewards Std                                  0.387904\n",
      "evaluation/Rewards Max                                  3.12191\n",
      "evaluation/Rewards Min                                 -2.882\n",
      "evaluation/Returns Mean                              -225.6\n",
      "evaluation/Returns Std                                 96.2777\n",
      "evaluation/Returns Max                                -44.7712\n",
      "evaluation/Returns Min                               -473.946\n",
      "evaluation/Actions Mean                                -0.00787126\n",
      "evaluation/Actions Std                                  0.664645\n",
      "evaluation/Actions Max                                  0.997176\n",
      "evaluation/Actions Min                                 -0.993918\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -225.6\n",
      "evaluation/env_infos/final/reward_run Mean              0.0119646\n",
      "evaluation/env_infos/final/reward_run Std               0.107058\n",
      "evaluation/env_infos/final/reward_run Max               0.412049\n",
      "evaluation/env_infos/final/reward_run Min              -0.253395\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.190175\n",
      "evaluation/env_infos/initial/reward_run Std             0.564526\n",
      "evaluation/env_infos/initial/reward_run Max             0.99687\n",
      "evaluation/env_infos/initial/reward_run Min            -0.88276\n",
      "evaluation/env_infos/reward_run Mean                    0.0438064\n",
      "evaluation/env_infos/reward_run Std                     0.202783\n",
      "evaluation/env_infos/reward_run Max                     1.40322\n",
      "evaluation/env_infos/reward_run Min                    -1.72366\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.270911\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0754336\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.113904\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.401494\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.270615\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0849165\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0767672\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.461197\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.265089\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0828373\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0193374\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.461197\n",
      "evaluation/env_infos/final/height Mean                 -0.294315\n",
      "evaluation/env_infos/final/height Std                   0.137262\n",
      "evaluation/env_infos/final/height Max                  -0.127651\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.00418314\n",
      "evaluation/env_infos/initial/height Std                 0.0581023\n",
      "evaluation/env_infos/initial/height Max                 0.0962184\n",
      "evaluation/env_infos/initial/height Min                -0.106613\n",
      "evaluation/env_infos/height Mean                       -0.287356\n",
      "evaluation/env_infos/height Std                         0.136773\n",
      "evaluation/env_infos/height Max                         0.233334\n",
      "evaluation/env_infos/height Min                        -0.578619\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0227311\n",
      "evaluation/env_infos/final/reward_angular Std           0.516879\n",
      "evaluation/env_infos/final/reward_angular Max           1.81372\n",
      "evaluation/env_infos/final/reward_angular Min          -1.32046\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.285508\n",
      "evaluation/env_infos/initial/reward_angular Std         1.76829\n",
      "evaluation/env_infos/initial/reward_angular Max         3.28255\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.0145\n",
      "evaluation/env_infos/reward_angular Mean                0.00700043\n",
      "evaluation/env_infos/reward_angular Std                 0.646872\n",
      "evaluation/env_infos/reward_angular Max                 5.37882\n",
      "evaluation/env_infos/reward_angular Min                -4.79748\n",
      "time/data storing (s)                                   0.0153795\n",
      "time/evaluation sampling (s)                           22.8627\n",
      "time/exploration sampling (s)                           1.12505\n",
      "time/logging (s)                                        0.234299\n",
      "time/saving (s)                                         0.0296286\n",
      "time/training (s)                                       4.15822\n",
      "time/epoch (s)                                         28.4253\n",
      "time/total (s)                                        387.985\n",
      "Epoch                                                  12\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:31:50.344071 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 13 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  15000\n",
      "trainer/QF1 Loss                                        0.577147\n",
      "trainer/QF2 Loss                                        0.62556\n",
      "trainer/Policy Loss                                    -1.66617\n",
      "trainer/Q1 Predictions Mean                             2.75146\n",
      "trainer/Q1 Predictions Std                              1.40217\n",
      "trainer/Q1 Predictions Max                              7.60927\n",
      "trainer/Q1 Predictions Min                              0.247626\n",
      "trainer/Q2 Predictions Mean                             2.73843\n",
      "trainer/Q2 Predictions Std                              1.3904\n",
      "trainer/Q2 Predictions Max                              7.39643\n",
      "trainer/Q2 Predictions Min                             -0.369382\n",
      "trainer/Q Targets Mean                                  2.75468\n",
      "trainer/Q Targets Std                                   1.66063\n",
      "trainer/Q Targets Max                                  10.5376\n",
      "trainer/Q Targets Min                                  -0.707128\n",
      "trainer/Log Pis Mean                                    1.50785\n",
      "trainer/Log Pis Std                                     4.60254\n",
      "trainer/Log Pis Max                                    22.8324\n",
      "trainer/Log Pis Min                                    -7.13137\n",
      "trainer/Policy mu Mean                                 -0.056557\n",
      "trainer/Policy mu Std                                   1.16269\n",
      "trainer/Policy mu Max                                   3.78647\n",
      "trainer/Policy mu Min                                  -3.88354\n",
      "trainer/Policy log std Mean                            -0.460428\n",
      "trainer/Policy log std Std                              0.21314\n",
      "trainer/Policy log std Max                             -0.0139964\n",
      "trainer/Policy log std Min                             -1.27508\n",
      "trainer/Alpha                                           0.0306378\n",
      "trainer/Alpha Loss                                    -15.6494\n",
      "exploration/num steps total                         15000\n",
      "exploration/num paths total                            15\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0278168\n",
      "exploration/Rewards Std                                 0.670919\n",
      "exploration/Rewards Max                                 2.3349\n",
      "exploration/Rewards Min                                -1.7654\n",
      "exploration/Returns Mean                              -27.8168\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -27.8168\n",
      "exploration/Returns Min                               -27.8168\n",
      "exploration/Actions Mean                                0.119004\n",
      "exploration/Actions Std                                 0.618165\n",
      "exploration/Actions Max                                 0.996166\n",
      "exploration/Actions Min                                -0.996919\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -27.8168\n",
      "exploration/env_infos/final/reward_run Mean            -0.254252\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.254252\n",
      "exploration/env_infos/final/reward_run Min             -0.254252\n",
      "exploration/env_infos/initial/reward_run Mean          -0.212611\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.212611\n",
      "exploration/env_infos/initial/reward_run Min           -0.212611\n",
      "exploration/env_infos/reward_run Mean                  -0.000383858\n",
      "exploration/env_infos/reward_run Std                    0.410062\n",
      "exploration/env_infos/reward_run Max                    1.33245\n",
      "exploration/env_infos/reward_run Min                   -1.1609\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.189041\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.189041\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.189041\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.283946\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.283946\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.283946\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.237774\n",
      "exploration/env_infos/reward_ctrl Std                   0.0768719\n",
      "exploration/env_infos/reward_ctrl Max                  -0.022046\n",
      "exploration/env_infos/reward_ctrl Min                  -0.4591\n",
      "exploration/env_infos/final/height Mean                -0.31289\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.31289\n",
      "exploration/env_infos/final/height Min                 -0.31289\n",
      "exploration/env_infos/initial/height Mean              -0.0158694\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0158694\n",
      "exploration/env_infos/initial/height Min               -0.0158694\n",
      "exploration/env_infos/height Mean                      -0.250668\n",
      "exploration/env_infos/height Std                        0.0908399\n",
      "exploration/env_infos/height Max                        0.0562123\n",
      "exploration/env_infos/height Min                       -0.451531\n",
      "exploration/env_infos/final/reward_angular Mean         0.181549\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.181549\n",
      "exploration/env_infos/final/reward_angular Min          0.181549\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.91928\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.91928\n",
      "exploration/env_infos/initial/reward_angular Min       -0.91928\n",
      "exploration/env_infos/reward_angular Mean              -0.022187\n",
      "exploration/env_infos/reward_angular Std                1.13255\n",
      "exploration/env_infos/reward_angular Max                3.61859\n",
      "exploration/env_infos/reward_angular Min               -3.80514\n",
      "evaluation/num steps total                         350000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.206684\n",
      "evaluation/Rewards Std                                  0.773467\n",
      "evaluation/Rewards Max                                  8.00889\n",
      "evaluation/Rewards Min                                 -7.146\n",
      "evaluation/Returns Mean                              -206.684\n",
      "evaluation/Returns Std                                 94.7858\n",
      "evaluation/Returns Max                                -36.1494\n",
      "evaluation/Returns Min                               -382.976\n",
      "evaluation/Actions Mean                                -0.333514\n",
      "evaluation/Actions Std                                  0.617423\n",
      "evaluation/Actions Max                                  0.998597\n",
      "evaluation/Actions Min                                 -0.998637\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -206.684\n",
      "evaluation/env_infos/final/reward_run Mean             -0.120722\n",
      "evaluation/env_infos/final/reward_run Std               0.322446\n",
      "evaluation/env_infos/final/reward_run Max               0.364791\n",
      "evaluation/env_infos/final/reward_run Min              -0.999243\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.250662\n",
      "evaluation/env_infos/initial/reward_run Std             0.583556\n",
      "evaluation/env_infos/initial/reward_run Max             0.753415\n",
      "evaluation/env_infos/initial/reward_run Min            -1.04045\n",
      "evaluation/env_infos/reward_run Mean                    0.0177647\n",
      "evaluation/env_infos/reward_run Std                     0.478559\n",
      "evaluation/env_infos/reward_run Max                     3.66692\n",
      "evaluation/env_infos/reward_run Min                    -2.52735\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.301612\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0905276\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.171039\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.485625\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.266347\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0822944\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.109426\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.38028\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.295465\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0823838\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0469743\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.543992\n",
      "evaluation/env_infos/final/height Mean                 -0.279483\n",
      "evaluation/env_infos/final/height Std                   0.118237\n",
      "evaluation/env_infos/final/height Max                  -0.123444\n",
      "evaluation/env_infos/final/height Min                  -0.556518\n",
      "evaluation/env_infos/initial/height Mean                0.00952989\n",
      "evaluation/env_infos/initial/height Std                 0.0541721\n",
      "evaluation/env_infos/initial/height Max                 0.0946882\n",
      "evaluation/env_infos/initial/height Min                -0.0811334\n",
      "evaluation/env_infos/height Mean                       -0.249891\n",
      "evaluation/env_infos/height Std                         0.114869\n",
      "evaluation/env_infos/height Max                         0.381994\n",
      "evaluation/env_infos/height Min                        -0.593067\n",
      "evaluation/env_infos/final/reward_angular Mean          0.144601\n",
      "evaluation/env_infos/final/reward_angular Std           1.34018\n",
      "evaluation/env_infos/final/reward_angular Max           5.01241\n",
      "evaluation/env_infos/final/reward_angular Min          -3.38418\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.966388\n",
      "evaluation/env_infos/initial/reward_angular Std         1.32543\n",
      "evaluation/env_infos/initial/reward_angular Max         3.97346\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.76722\n",
      "evaluation/env_infos/reward_angular Mean                0.0265442\n",
      "evaluation/env_infos/reward_angular Std                 1.34927\n",
      "evaluation/env_infos/reward_angular Max                 9.71043\n",
      "evaluation/env_infos/reward_angular Min                -8.77405\n",
      "time/data storing (s)                                   0.0148438\n",
      "time/evaluation sampling (s)                           23.3212\n",
      "time/exploration sampling (s)                           0.990002\n",
      "time/logging (s)                                        0.237899\n",
      "time/saving (s)                                         0.028773\n",
      "time/training (s)                                       4.83684\n",
      "time/epoch (s)                                         29.4296\n",
      "time/total (s)                                        417.61\n",
      "Epoch                                                  13\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:32:18.908806 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 14 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.491169\n",
      "trainer/QF2 Loss                                        0.458346\n",
      "trainer/Policy Loss                                    -1.63522\n",
      "trainer/Q1 Predictions Mean                             2.81469\n",
      "trainer/Q1 Predictions Std                              1.43423\n",
      "trainer/Q1 Predictions Max                              7.34389\n",
      "trainer/Q1 Predictions Min                              0.062531\n",
      "trainer/Q2 Predictions Mean                             2.76964\n",
      "trainer/Q2 Predictions Std                              1.38745\n",
      "trainer/Q2 Predictions Max                              6.82272\n",
      "trainer/Q2 Predictions Min                             -0.336188\n",
      "trainer/Q Targets Mean                                  2.77112\n",
      "trainer/Q Targets Std                                   1.59817\n",
      "trainer/Q Targets Max                                   8.37291\n",
      "trainer/Q Targets Min                                  -1.05192\n",
      "trainer/Log Pis Mean                                    1.45395\n",
      "trainer/Log Pis Std                                     3.70353\n",
      "trainer/Log Pis Max                                    13.3107\n",
      "trainer/Log Pis Min                                    -7.61755\n",
      "trainer/Policy mu Mean                                  0.165463\n",
      "trainer/Policy mu Std                                   1.14121\n",
      "trainer/Policy mu Max                                   3.29922\n",
      "trainer/Policy mu Min                                  -3.00593\n",
      "trainer/Policy log std Mean                            -0.453192\n",
      "trainer/Policy log std Std                              0.20574\n",
      "trainer/Policy log std Max                              0.162698\n",
      "trainer/Policy log std Min                             -1.26236\n",
      "trainer/Alpha                                           0.0260164\n",
      "trainer/Alpha Loss                                    -16.581\n",
      "exploration/num steps total                         16000\n",
      "exploration/num paths total                            16\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.198728\n",
      "exploration/Rewards Std                                 0.603783\n",
      "exploration/Rewards Max                                 1.67508\n",
      "exploration/Rewards Min                                -2.18602\n",
      "exploration/Returns Mean                             -198.728\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -198.728\n",
      "exploration/Returns Min                              -198.728\n",
      "exploration/Actions Mean                                0.22944\n",
      "exploration/Actions Std                                 0.632989\n",
      "exploration/Actions Max                                 0.998196\n",
      "exploration/Actions Min                                -0.997529\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -198.728\n",
      "exploration/env_infos/final/reward_run Mean            -0.383831\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.383831\n",
      "exploration/env_infos/final/reward_run Min             -0.383831\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0264874\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0264874\n",
      "exploration/env_infos/initial/reward_run Min           -0.0264874\n",
      "exploration/env_infos/reward_run Mean                   0.0154074\n",
      "exploration/env_infos/reward_run Std                    0.366945\n",
      "exploration/env_infos/reward_run Max                    1.3258\n",
      "exploration/env_infos/reward_run Min                   -1.1516\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.267353\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.267353\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.267353\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.333143\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.333143\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.333143\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.271991\n",
      "exploration/env_infos/reward_ctrl Std                   0.0730298\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0324728\n",
      "exploration/env_infos/reward_ctrl Min                  -0.499507\n",
      "exploration/env_infos/final/height Mean                -0.365404\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.365404\n",
      "exploration/env_infos/final/height Min                 -0.365404\n",
      "exploration/env_infos/initial/height Mean               0.0401786\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0401786\n",
      "exploration/env_infos/initial/height Min                0.0401786\n",
      "exploration/env_infos/height Mean                      -0.30245\n",
      "exploration/env_infos/height Std                        0.0626837\n",
      "exploration/env_infos/height Max                        0.0401786\n",
      "exploration/env_infos/height Min                       -0.428221\n",
      "exploration/env_infos/final/reward_angular Mean        -1.23734\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.23734\n",
      "exploration/env_infos/final/reward_angular Min         -1.23734\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.05504\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.05504\n",
      "exploration/env_infos/initial/reward_angular Min       -1.05504\n",
      "exploration/env_infos/reward_angular Mean              -0.0220392\n",
      "exploration/env_infos/reward_angular Std                1.04637\n",
      "exploration/env_infos/reward_angular Max                4.38598\n",
      "exploration/env_infos/reward_angular Min               -2.97391\n",
      "evaluation/num steps total                         375000\n",
      "evaluation/num paths total                            375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.183352\n",
      "evaluation/Rewards Std                                  0.614014\n",
      "evaluation/Rewards Max                                  3.62665\n",
      "evaluation/Rewards Min                                 -3.60486\n",
      "evaluation/Returns Mean                              -183.352\n",
      "evaluation/Returns Std                                105.76\n",
      "evaluation/Returns Max                                -27.0104\n",
      "evaluation/Returns Min                               -458.42\n",
      "evaluation/Actions Mean                                -0.195748\n",
      "evaluation/Actions Std                                  0.633004\n",
      "evaluation/Actions Max                                  0.996212\n",
      "evaluation/Actions Min                                 -0.998923\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -183.352\n",
      "evaluation/env_infos/final/reward_run Mean              0.00131965\n",
      "evaluation/env_infos/final/reward_run Std               0.206563\n",
      "evaluation/env_infos/final/reward_run Max               0.537152\n",
      "evaluation/env_infos/final/reward_run Min              -0.76737\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.274162\n",
      "evaluation/env_infos/initial/reward_run Std             0.487451\n",
      "evaluation/env_infos/initial/reward_run Max             0.795283\n",
      "evaluation/env_infos/initial/reward_run Min            -0.823373\n",
      "evaluation/env_infos/reward_run Mean                    0.00844339\n",
      "evaluation/env_infos/reward_run Std                     0.297454\n",
      "evaluation/env_infos/reward_run Max                     2.03182\n",
      "evaluation/env_infos/reward_run Min                    -2.58392\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.279832\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.090413\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0994533\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.460147\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.284042\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0674658\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0960828\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.38284\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.263407\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0928557\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.006108\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.501945\n",
      "evaluation/env_infos/final/height Mean                 -0.211148\n",
      "evaluation/env_infos/final/height Std                   0.11202\n",
      "evaluation/env_infos/final/height Max                   0.00985826\n",
      "evaluation/env_infos/final/height Min                  -0.519416\n",
      "evaluation/env_infos/initial/height Mean               -0.0127214\n",
      "evaluation/env_infos/initial/height Std                 0.0573272\n",
      "evaluation/env_infos/initial/height Max                 0.0921092\n",
      "evaluation/env_infos/initial/height Min                -0.112815\n",
      "evaluation/env_infos/height Mean                       -0.207064\n",
      "evaluation/env_infos/height Std                         0.0994032\n",
      "evaluation/env_infos/height Max                         0.143831\n",
      "evaluation/env_infos/height Min                        -0.594904\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0558546\n",
      "evaluation/env_infos/final/reward_angular Std           0.792966\n",
      "evaluation/env_infos/final/reward_angular Max           2.21868\n",
      "evaluation/env_infos/final/reward_angular Min          -1.59726\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.336481\n",
      "evaluation/env_infos/initial/reward_angular Std         1.28115\n",
      "evaluation/env_infos/initial/reward_angular Max         3.20401\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.73395\n",
      "evaluation/env_infos/reward_angular Mean                0.00131921\n",
      "evaluation/env_infos/reward_angular Std                 0.97358\n",
      "evaluation/env_infos/reward_angular Max                 6.75604\n",
      "evaluation/env_infos/reward_angular Min                -6.30449\n",
      "time/data storing (s)                                   0.0150071\n",
      "time/evaluation sampling (s)                           22.8385\n",
      "time/exploration sampling (s)                           1.15528\n",
      "time/logging (s)                                        0.234431\n",
      "time/saving (s)                                         0.0289808\n",
      "time/training (s)                                       4.05982\n",
      "time/epoch (s)                                         28.3321\n",
      "time/total (s)                                        446.17\n",
      "Epoch                                                  14\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:32:47.556131 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 15 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  17000\n",
      "trainer/QF1 Loss                                        0.487767\n",
      "trainer/QF2 Loss                                        0.510808\n",
      "trainer/Policy Loss                                     0.40068\n",
      "trainer/Q1 Predictions Mean                             2.67481\n",
      "trainer/Q1 Predictions Std                              1.50279\n",
      "trainer/Q1 Predictions Max                             10.7883\n",
      "trainer/Q1 Predictions Min                              0.0572906\n",
      "trainer/Q2 Predictions Mean                             2.74056\n",
      "trainer/Q2 Predictions Std                              1.53948\n",
      "trainer/Q2 Predictions Max                             10.7192\n",
      "trainer/Q2 Predictions Min                             -0.0677769\n",
      "trainer/Q Targets Mean                                  2.65408\n",
      "trainer/Q Targets Std                                   1.66346\n",
      "trainer/Q Targets Max                                  12.4668\n",
      "trainer/Q Targets Min                                  -1.2054\n",
      "trainer/Log Pis Mean                                    3.5104\n",
      "trainer/Log Pis Std                                     4.37045\n",
      "trainer/Log Pis Max                                    19.69\n",
      "trainer/Log Pis Min                                    -5.54057\n",
      "trainer/Policy mu Mean                                 -0.310188\n",
      "trainer/Policy mu Std                                   1.28518\n",
      "trainer/Policy mu Max                                   3.09698\n",
      "trainer/Policy mu Min                                  -3.96767\n",
      "trainer/Policy log std Mean                            -0.606224\n",
      "trainer/Policy log std Std                              0.236122\n",
      "trainer/Policy log std Max                             -0.0941598\n",
      "trainer/Policy log std Min                             -1.56565\n",
      "trainer/Alpha                                           0.0225654\n",
      "trainer/Alpha Loss                                     -9.43609\n",
      "exploration/num steps total                         17000\n",
      "exploration/num paths total                            17\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.174255\n",
      "exploration/Rewards Std                                 0.117138\n",
      "exploration/Rewards Max                                 0.535287\n",
      "exploration/Rewards Min                                -0.625845\n",
      "exploration/Returns Mean                             -174.255\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -174.255\n",
      "exploration/Returns Min                              -174.255\n",
      "exploration/Actions Mean                               -0.574097\n",
      "exploration/Actions Std                                 0.575534\n",
      "exploration/Actions Max                                 0.989937\n",
      "exploration/Actions Min                                -0.99706\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -174.255\n",
      "exploration/env_infos/final/reward_run Mean            -0.309725\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.309725\n",
      "exploration/env_infos/final/reward_run Min             -0.309725\n",
      "exploration/env_infos/initial/reward_run Mean          -0.833147\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.833147\n",
      "exploration/env_infos/initial/reward_run Min           -0.833147\n",
      "exploration/env_infos/reward_run Mean                  -0.0765004\n",
      "exploration/env_infos/reward_run Std                    0.169522\n",
      "exploration/env_infos/reward_run Max                    0.517506\n",
      "exploration/env_infos/reward_run Min                   -1.0301\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.340832\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.340832\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.340832\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.446242\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.446242\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.446242\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.396496\n",
      "exploration/env_infos/reward_ctrl Std                   0.0565271\n",
      "exploration/env_infos/reward_ctrl Max                  -0.16857\n",
      "exploration/env_infos/reward_ctrl Min                  -0.552172\n",
      "exploration/env_infos/final/height Mean                -0.227902\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.227902\n",
      "exploration/env_infos/final/height Min                 -0.227902\n",
      "exploration/env_infos/initial/height Mean              -0.00420185\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00420185\n",
      "exploration/env_infos/initial/height Min               -0.00420185\n",
      "exploration/env_infos/height Mean                      -0.210703\n",
      "exploration/env_infos/height Std                        0.0185301\n",
      "exploration/env_infos/height Max                       -0.00420185\n",
      "exploration/env_infos/height Min                       -0.254181\n",
      "exploration/env_infos/final/reward_angular Mean         0.713645\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.713645\n",
      "exploration/env_infos/final/reward_angular Min          0.713645\n",
      "exploration/env_infos/initial/reward_angular Mean       0.947772\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.947772\n",
      "exploration/env_infos/initial/reward_angular Min        0.947772\n",
      "exploration/env_infos/reward_angular Mean               0.0161348\n",
      "exploration/env_infos/reward_angular Std                0.610436\n",
      "exploration/env_infos/reward_angular Max                2.65306\n",
      "exploration/env_infos/reward_angular Min               -2.51692\n",
      "evaluation/num steps total                         400000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.236651\n",
      "evaluation/Rewards Std                                  0.414026\n",
      "evaluation/Rewards Max                                  4.032\n",
      "evaluation/Rewards Min                                 -3.69819\n",
      "evaluation/Returns Mean                              -236.651\n",
      "evaluation/Returns Std                                 93.0383\n",
      "evaluation/Returns Max                                -58.0871\n",
      "evaluation/Returns Min                               -480.477\n",
      "evaluation/Actions Mean                                -0.311994\n",
      "evaluation/Actions Std                                  0.709237\n",
      "evaluation/Actions Max                                  0.999776\n",
      "evaluation/Actions Min                                 -0.999909\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -236.651\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0373786\n",
      "evaluation/env_infos/final/reward_run Std               0.360317\n",
      "evaluation/env_infos/final/reward_run Max               1.21381\n",
      "evaluation/env_infos/final/reward_run Min              -0.901827\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.190386\n",
      "evaluation/env_infos/initial/reward_run Std             0.493886\n",
      "evaluation/env_infos/initial/reward_run Max             1.06073\n",
      "evaluation/env_infos/initial/reward_run Min            -1.10514\n",
      "evaluation/env_infos/reward_run Mean                    0.0436491\n",
      "evaluation/env_infos/reward_run Std                     0.378365\n",
      "evaluation/env_infos/reward_run Max                     2.81998\n",
      "evaluation/env_infos/reward_run Min                    -2.20625\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.364523\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.101952\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.114489\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.502703\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.321136\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0938392\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.140246\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.485837\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.360215\n",
      "evaluation/env_infos/reward_ctrl Std                    0.109766\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0300755\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.556528\n",
      "evaluation/env_infos/final/height Mean                 -0.24595\n",
      "evaluation/env_infos/final/height Std                   0.101514\n",
      "evaluation/env_infos/final/height Max                  -0.133139\n",
      "evaluation/env_infos/final/height Min                  -0.541593\n",
      "evaluation/env_infos/initial/height Mean               -0.00326818\n",
      "evaluation/env_infos/initial/height Std                 0.0553187\n",
      "evaluation/env_infos/initial/height Max                 0.090568\n",
      "evaluation/env_infos/initial/height Min                -0.112105\n",
      "evaluation/env_infos/height Mean                       -0.239686\n",
      "evaluation/env_infos/height Std                         0.0979602\n",
      "evaluation/env_infos/height Max                         0.192323\n",
      "evaluation/env_infos/height Min                        -0.597291\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.131632\n",
      "evaluation/env_infos/final/reward_angular Std           1.0435\n",
      "evaluation/env_infos/final/reward_angular Max           1.96469\n",
      "evaluation/env_infos/final/reward_angular Min          -4.68451\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.655111\n",
      "evaluation/env_infos/initial/reward_angular Std         0.742244\n",
      "evaluation/env_infos/initial/reward_angular Max         1.9156\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.740101\n",
      "evaluation/env_infos/reward_angular Mean                0.0069941\n",
      "evaluation/env_infos/reward_angular Std                 0.759907\n",
      "evaluation/env_infos/reward_angular Max                 6.41049\n",
      "evaluation/env_infos/reward_angular Min                -5.70705\n",
      "time/data storing (s)                                   0.0155025\n",
      "time/evaluation sampling (s)                           22.9195\n",
      "time/exploration sampling (s)                           1.20098\n",
      "time/logging (s)                                        0.234405\n",
      "time/saving (s)                                         0.0293477\n",
      "time/training (s)                                       4.03771\n",
      "time/epoch (s)                                         28.4374\n",
      "time/total (s)                                        474.816\n",
      "Epoch                                                  15\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:33:15.722673 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 16 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.572728\n",
      "trainer/QF2 Loss                                        0.548747\n",
      "trainer/Policy Loss                                    -0.510618\n",
      "trainer/Q1 Predictions Mean                             2.78083\n",
      "trainer/Q1 Predictions Std                              1.57291\n",
      "trainer/Q1 Predictions Max                              8.13888\n",
      "trainer/Q1 Predictions Min                              0.110418\n",
      "trainer/Q2 Predictions Mean                             2.82488\n",
      "trainer/Q2 Predictions Std                              1.56997\n",
      "trainer/Q2 Predictions Max                              7.53182\n",
      "trainer/Q2 Predictions Min                              0.112843\n",
      "trainer/Q Targets Mean                                  2.73158\n",
      "trainer/Q Targets Std                                   1.73712\n",
      "trainer/Q Targets Max                                   8.08794\n",
      "trainer/Q Targets Min                                  -1.10875\n",
      "trainer/Log Pis Mean                                    2.70775\n",
      "trainer/Log Pis Std                                     4.16994\n",
      "trainer/Log Pis Max                                    21.2366\n",
      "trainer/Log Pis Min                                    -5.51415\n",
      "trainer/Policy mu Mean                                  0.0960156\n",
      "trainer/Policy mu Std                                   1.25513\n",
      "trainer/Policy mu Max                                   3.81087\n",
      "trainer/Policy mu Min                                  -2.8442\n",
      "trainer/Policy log std Mean                            -0.573248\n",
      "trainer/Policy log std Std                              0.23115\n",
      "trainer/Policy log std Max                              0.102688\n",
      "trainer/Policy log std Min                             -1.45299\n",
      "trainer/Alpha                                           0.0198029\n",
      "trainer/Alpha Loss                                    -12.9079\n",
      "exploration/num steps total                         18000\n",
      "exploration/num paths total                            18\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.226407\n",
      "exploration/Rewards Std                                 1.35289\n",
      "exploration/Rewards Max                                 4.17049\n",
      "exploration/Rewards Min                                -3.93198\n",
      "exploration/Returns Mean                             -226.407\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -226.407\n",
      "exploration/Returns Min                              -226.407\n",
      "exploration/Actions Mean                                0.193894\n",
      "exploration/Actions Std                                 0.711934\n",
      "exploration/Actions Max                                 0.999989\n",
      "exploration/Actions Min                                -0.999867\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -226.407\n",
      "exploration/env_infos/final/reward_run Mean             0.144763\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.144763\n",
      "exploration/env_infos/final/reward_run Min              0.144763\n",
      "exploration/env_infos/initial/reward_run Mean           0.787944\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.787944\n",
      "exploration/env_infos/initial/reward_run Min            0.787944\n",
      "exploration/env_infos/reward_run Mean                  -0.121743\n",
      "exploration/env_infos/reward_run Std                    0.949961\n",
      "exploration/env_infos/reward_run Max                    2.65537\n",
      "exploration/env_infos/reward_run Min                   -2.60802\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.153985\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.153985\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.153985\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.321866\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.321866\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.321866\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.326667\n",
      "exploration/env_infos/reward_ctrl Std                   0.104799\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0559081\n",
      "exploration/env_infos/reward_ctrl Min                  -0.586648\n",
      "exploration/env_infos/final/height Mean                -0.357671\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.357671\n",
      "exploration/env_infos/final/height Min                 -0.357671\n",
      "exploration/env_infos/initial/height Mean              -0.0153677\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0153677\n",
      "exploration/env_infos/initial/height Min               -0.0153677\n",
      "exploration/env_infos/height Mean                      -0.37967\n",
      "exploration/env_infos/height Std                        0.158577\n",
      "exploration/env_infos/height Max                        0.2181\n",
      "exploration/env_infos/height Min                       -0.587845\n",
      "exploration/env_infos/final/reward_angular Mean         2.58583\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          2.58583\n",
      "exploration/env_infos/final/reward_angular Min          2.58583\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.237863\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.237863\n",
      "exploration/env_infos/initial/reward_angular Min       -0.237863\n",
      "exploration/env_infos/reward_angular Mean               0.0246491\n",
      "exploration/env_infos/reward_angular Std                2.5916\n",
      "exploration/env_infos/reward_angular Max                8.05246\n",
      "exploration/env_infos/reward_angular Min               -6.41686\n",
      "evaluation/num steps total                         425000\n",
      "evaluation/num paths total                            425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.202419\n",
      "evaluation/Rewards Std                                  1.17097\n",
      "evaluation/Rewards Max                                  7.88136\n",
      "evaluation/Rewards Min                                 -7.46194\n",
      "evaluation/Returns Mean                              -202.419\n",
      "evaluation/Returns Std                                 95.963\n",
      "evaluation/Returns Max                                 -3.57365\n",
      "evaluation/Returns Min                               -382.119\n",
      "evaluation/Actions Mean                                -0.10722\n",
      "evaluation/Actions Std                                  0.708642\n",
      "evaluation/Actions Max                                  0.999966\n",
      "evaluation/Actions Min                                 -0.999944\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -202.419\n",
      "evaluation/env_infos/final/reward_run Mean              0.0364216\n",
      "evaluation/env_infos/final/reward_run Std               0.703934\n",
      "evaluation/env_infos/final/reward_run Max               2.49418\n",
      "evaluation/env_infos/final/reward_run Min              -1.66633\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.105231\n",
      "evaluation/env_infos/initial/reward_run Std             0.388031\n",
      "evaluation/env_infos/initial/reward_run Max             0.604375\n",
      "evaluation/env_infos/initial/reward_run Min            -0.786328\n",
      "evaluation/env_infos/reward_run Mean                   -0.00133738\n",
      "evaluation/env_infos/reward_run Std                     0.597259\n",
      "evaluation/env_infos/reward_run Max                     4.30194\n",
      "evaluation/env_infos/reward_run Min                    -3.12226\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.309136\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.101272\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0825489\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.576061\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.247989\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.074812\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0837564\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.357023\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.308202\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0983888\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.011505\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596092\n",
      "evaluation/env_infos/final/height Mean                 -0.274526\n",
      "evaluation/env_infos/final/height Std                   0.119382\n",
      "evaluation/env_infos/final/height Max                  -0.092741\n",
      "evaluation/env_infos/final/height Min                  -0.520601\n",
      "evaluation/env_infos/initial/height Mean               -0.0125183\n",
      "evaluation/env_infos/initial/height Std                 0.05196\n",
      "evaluation/env_infos/initial/height Max                 0.0891309\n",
      "evaluation/env_infos/initial/height Min                -0.101204\n",
      "evaluation/env_infos/height Mean                       -0.261873\n",
      "evaluation/env_infos/height Std                         0.13271\n",
      "evaluation/env_infos/height Max                         0.392002\n",
      "evaluation/env_infos/height Min                        -0.596167\n",
      "evaluation/env_infos/final/reward_angular Mean          0.413185\n",
      "evaluation/env_infos/final/reward_angular Std           2.13542\n",
      "evaluation/env_infos/final/reward_angular Max           6.9724\n",
      "evaluation/env_infos/final/reward_angular Min          -6.68166\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.397276\n",
      "evaluation/env_infos/initial/reward_angular Std         0.93673\n",
      "evaluation/env_infos/initial/reward_angular Max         3.20156\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.35309\n",
      "evaluation/env_infos/reward_angular Mean                0.0418981\n",
      "evaluation/env_infos/reward_angular Std                 1.5876\n",
      "evaluation/env_infos/reward_angular Max                 8.8914\n",
      "evaluation/env_infos/reward_angular Min                -8.22886\n",
      "time/data storing (s)                                   0.0149804\n",
      "time/evaluation sampling (s)                           22.7664\n",
      "time/exploration sampling (s)                           1.00831\n",
      "time/logging (s)                                        0.230746\n",
      "time/saving (s)                                         0.0274281\n",
      "time/training (s)                                       3.89168\n",
      "time/epoch (s)                                         27.9396\n",
      "time/total (s)                                        502.978\n",
      "Epoch                                                  16\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:33:43.858560 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 17 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  19000\n",
      "trainer/QF1 Loss                                        0.40942\n",
      "trainer/QF2 Loss                                        0.422275\n",
      "trainer/Policy Loss                                     0.525523\n",
      "trainer/Q1 Predictions Mean                             2.64444\n",
      "trainer/Q1 Predictions Std                              1.56305\n",
      "trainer/Q1 Predictions Max                              7.86381\n",
      "trainer/Q1 Predictions Min                              0.102036\n",
      "trainer/Q2 Predictions Mean                             2.59539\n",
      "trainer/Q2 Predictions Std                              1.55554\n",
      "trainer/Q2 Predictions Max                              7.73058\n",
      "trainer/Q2 Predictions Min                              0.126768\n",
      "trainer/Q Targets Mean                                  2.62273\n",
      "trainer/Q Targets Std                                   1.67199\n",
      "trainer/Q Targets Max                                   8.18254\n",
      "trainer/Q Targets Min                                  -0.761314\n",
      "trainer/Log Pis Mean                                    3.57829\n",
      "trainer/Log Pis Std                                     4.42207\n",
      "trainer/Log Pis Max                                    17.8167\n",
      "trainer/Log Pis Min                                    -5.87204\n",
      "trainer/Policy mu Mean                                 -0.31434\n",
      "trainer/Policy mu Std                                   1.26253\n",
      "trainer/Policy mu Max                                   3.4465\n",
      "trainer/Policy mu Min                                  -3.70878\n",
      "trainer/Policy log std Mean                            -0.691003\n",
      "trainer/Policy log std Std                              0.285043\n",
      "trainer/Policy log std Max                              0.0829471\n",
      "trainer/Policy log std Min                             -1.86183\n",
      "trainer/Alpha                                           0.0177091\n",
      "trainer/Alpha Loss                                     -9.76584\n",
      "exploration/num steps total                         19000\n",
      "exploration/num paths total                            19\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.205947\n",
      "exploration/Rewards Std                                 0.19006\n",
      "exploration/Rewards Max                                 0.358404\n",
      "exploration/Rewards Min                                -0.945016\n",
      "exploration/Returns Mean                             -205.947\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -205.947\n",
      "exploration/Returns Min                              -205.947\n",
      "exploration/Actions Mean                               -0.224399\n",
      "exploration/Actions Std                                 0.5285\n",
      "exploration/Actions Max                                 0.992105\n",
      "exploration/Actions Min                                -0.991862\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -205.947\n",
      "exploration/env_infos/final/reward_run Mean            -0.00496155\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.00496155\n",
      "exploration/env_infos/final/reward_run Min             -0.00496155\n",
      "exploration/env_infos/initial/reward_run Mean          -0.335281\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.335281\n",
      "exploration/env_infos/initial/reward_run Min           -0.335281\n",
      "exploration/env_infos/reward_run Mean                  -0.0232906\n",
      "exploration/env_infos/reward_run Std                    0.465421\n",
      "exploration/env_infos/reward_run Max                    1.61714\n",
      "exploration/env_infos/reward_run Min                   -1.41515\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.144383\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.144383\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.144383\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.209083\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.209083\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.209083\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.1978\n",
      "exploration/env_infos/reward_ctrl Std                   0.0685047\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0232006\n",
      "exploration/env_infos/reward_ctrl Min                  -0.460132\n",
      "exploration/env_infos/final/height Mean                -0.165387\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.165387\n",
      "exploration/env_infos/final/height Min                 -0.165387\n",
      "exploration/env_infos/initial/height Mean               0.0792449\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0792449\n",
      "exploration/env_infos/initial/height Min                0.0792449\n",
      "exploration/env_infos/height Mean                      -0.125752\n",
      "exploration/env_infos/height Std                        0.0410335\n",
      "exploration/env_infos/height Max                        0.0792449\n",
      "exploration/env_infos/height Min                       -0.247189\n",
      "exploration/env_infos/final/reward_angular Mean         0.236228\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.236228\n",
      "exploration/env_infos/final/reward_angular Min          0.236228\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.0708612\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.0708612\n",
      "exploration/env_infos/initial/reward_angular Min       -0.0708612\n",
      "exploration/env_infos/reward_angular Mean               0.00178092\n",
      "exploration/env_infos/reward_angular Std                0.932945\n",
      "exploration/env_infos/reward_angular Max                3.74919\n",
      "exploration/env_infos/reward_angular Min               -2.51673\n",
      "evaluation/num steps total                         450000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.246189\n",
      "evaluation/Rewards Std                                  0.918351\n",
      "evaluation/Rewards Max                                  7.93945\n",
      "evaluation/Rewards Min                                 -5.80787\n",
      "evaluation/Returns Mean                              -246.189\n",
      "evaluation/Returns Std                                138.129\n",
      "evaluation/Returns Max                                -47.1048\n",
      "evaluation/Returns Min                               -599.196\n",
      "evaluation/Actions Mean                                -0.177067\n",
      "evaluation/Actions Std                                  0.727333\n",
      "evaluation/Actions Max                                  0.999988\n",
      "evaluation/Actions Min                                 -0.999984\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -246.189\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0307523\n",
      "evaluation/env_infos/final/reward_run Std               0.33355\n",
      "evaluation/env_infos/final/reward_run Max               1.03044\n",
      "evaluation/env_infos/final/reward_run Min              -0.777504\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.340481\n",
      "evaluation/env_infos/initial/reward_run Std             0.447354\n",
      "evaluation/env_infos/initial/reward_run Max             0.772796\n",
      "evaluation/env_infos/initial/reward_run Min            -0.939478\n",
      "evaluation/env_infos/reward_run Mean                   -0.115995\n",
      "evaluation/env_infos/reward_run Std                     0.652391\n",
      "evaluation/env_infos/reward_run Max                     3.43078\n",
      "evaluation/env_infos/reward_run Min                    -3.82307\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.340883\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0881029\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.1068\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.466216\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.266148\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.107382\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0491941\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.394352\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.336219\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0955403\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00827807\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.588391\n",
      "evaluation/env_infos/final/height Mean                 -0.242305\n",
      "evaluation/env_infos/final/height Std                   0.124716\n",
      "evaluation/env_infos/final/height Max                  -0.118559\n",
      "evaluation/env_infos/final/height Min                  -0.574803\n",
      "evaluation/env_infos/initial/height Mean               -0.00721206\n",
      "evaluation/env_infos/initial/height Std                 0.0524488\n",
      "evaluation/env_infos/initial/height Max                 0.0951835\n",
      "evaluation/env_infos/initial/height Min                -0.107119\n",
      "evaluation/env_infos/height Mean                       -0.237119\n",
      "evaluation/env_infos/height Std                         0.137578\n",
      "evaluation/env_infos/height Max                         0.386115\n",
      "evaluation/env_infos/height Min                        -0.593109\n",
      "evaluation/env_infos/final/reward_angular Mean          0.415198\n",
      "evaluation/env_infos/final/reward_angular Std           1.293\n",
      "evaluation/env_infos/final/reward_angular Max           4.31146\n",
      "evaluation/env_infos/final/reward_angular Min          -2.51902\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.676895\n",
      "evaluation/env_infos/initial/reward_angular Std         0.704883\n",
      "evaluation/env_infos/initial/reward_angular Max         1.69465\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.21557\n",
      "evaluation/env_infos/reward_angular Mean                0.0380127\n",
      "evaluation/env_infos/reward_angular Std                 1.2408\n",
      "evaluation/env_infos/reward_angular Max                 9.12125\n",
      "evaluation/env_infos/reward_angular Min                -5.97525\n",
      "time/data storing (s)                                   0.0144383\n",
      "time/evaluation sampling (s)                           22.2264\n",
      "time/exploration sampling (s)                           1.10894\n",
      "time/logging (s)                                        0.232391\n",
      "time/saving (s)                                         0.0270515\n",
      "time/training (s)                                       4.31019\n",
      "time/epoch (s)                                         27.9194\n",
      "time/total (s)                                        531.114\n",
      "Epoch                                                  17\n",
      "-------------------------------------------------  ---------------\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 100 --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 1 --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env halfcheetahhard --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44350bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
