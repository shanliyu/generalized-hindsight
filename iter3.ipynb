{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54d162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[15518]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa9778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a740). One of the two will be used. Which one is undefined.\n",
      "objc[15518]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa9700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a768). One of the two will be used. Which one is undefined.\n",
      "objc[15518]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa97a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a7b8). One of the two will be used. Which one is undefined.\n",
      "objc[15518]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa9818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a830). One of the two will be used. Which one is undefined.\n",
      "using sparse reward if specified\n",
      "2021-05-25 10:24:59.424982 PDT | Variant:\n",
      "2021-05-25 10:24:59.425577 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"pointreacherobs\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 20,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 20,\n",
      "    \"num_train_loops_per_epoch\": 5\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.97,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 2000,\n",
      "    \"latent_dim\": 6,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"sparse_reward\": null,\n",
      "    \"fixed_ratio\": null\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"horizon\": 20\n",
      "  }\n",
      "}\n",
      "pointreacher\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "2021-05-25 10:25:34.660135 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 0 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   1100\n",
      "trainer/QF1 Loss                                        0.576872\n",
      "trainer/QF2 Loss                                        0.57548\n",
      "trainer/Policy Loss                                    -1.37884\n",
      "trainer/Q1 Predictions Mean                            -0.00126527\n",
      "trainer/Q1 Predictions Std                              0.000605492\n",
      "trainer/Q1 Predictions Max                              0.000269961\n",
      "trainer/Q1 Predictions Min                             -0.00227255\n",
      "trainer/Q2 Predictions Mean                            -4.35855e-06\n",
      "trainer/Q2 Predictions Std                              0.000609121\n",
      "trainer/Q2 Predictions Max                              0.00133012\n",
      "trainer/Q2 Predictions Min                             -0.00163013\n",
      "trainer/Q Targets Mean                                  0.514032\n",
      "trainer/Q Targets Std                                   0.557929\n",
      "trainer/Q Targets Max                                   1.83678\n",
      "trainer/Q Targets Min                                  -1.35869\n",
      "trainer/Log Pis Mean                                   -1.38018\n",
      "trainer/Log Pis Std                                     0.289063\n",
      "trainer/Log Pis Max                                    -0.577882\n",
      "trainer/Log Pis Min                                    -2.60453\n",
      "trainer/Policy mu Mean                                  0.000988936\n",
      "trainer/Policy mu Std                                   0.00058491\n",
      "trainer/Policy mu Max                                   0.00221213\n",
      "trainer/Policy mu Min                                   1.31949e-05\n",
      "trainer/Policy log std Mean                             4.21378e-05\n",
      "trainer/Policy log std Std                              0.000645919\n",
      "trainer/Policy log std Max                              0.000916166\n",
      "trainer/Policy log std Min                             -0.00127692\n",
      "trainer/Alpha                                           0.997005\n",
      "trainer/Alpha Loss                                     -0\n",
      "exploration/num steps total                          1100\n",
      "exploration/num paths total                            55\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.674579\n",
      "exploration/Rewards Std                                 0.290019\n",
      "exploration/Rewards Max                                -0.105313\n",
      "exploration/Rewards Min                                -1.25229\n",
      "exploration/Returns Mean                              -13.4916\n",
      "exploration/Returns Std                                 1.35362\n",
      "exploration/Returns Max                               -11.0651\n",
      "exploration/Returns Min                               -14.8536\n",
      "exploration/Actions Mean                                0.0632062\n",
      "exploration/Actions Std                                 0.578647\n",
      "exploration/Actions Max                                 0.98148\n",
      "exploration/Actions Min                                -0.941187\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -13.4916\n",
      "exploration/env_infos/final/reward_dist Mean            3.94634e-51\n",
      "exploration/env_infos/final/reward_dist Std             7.89266e-51\n",
      "exploration/env_infos/final/reward_dist Max             1.97317e-50\n",
      "exploration/env_infos/final/reward_dist Min             4.19542e-102\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0344088\n",
      "exploration/env_infos/initial/reward_dist Std           0.0428659\n",
      "exploration/env_infos/initial/reward_dist Max           0.106806\n",
      "exploration/env_infos/initial/reward_dist Min           9.56386e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0187599\n",
      "exploration/env_infos/reward_dist Std                   0.0898929\n",
      "exploration/env_infos/reward_dist Max                   0.836662\n",
      "exploration/env_infos/reward_dist Min                   4.19542e-102\n",
      "exploration/env_infos/final/reward_energy Mean         -0.787813\n",
      "exploration/env_infos/final/reward_energy Std           0.299654\n",
      "exploration/env_infos/final/reward_energy Max          -0.19223\n",
      "exploration/env_infos/final/reward_energy Min          -1.00022\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.87809\n",
      "exploration/env_infos/initial/reward_energy Std         0.131384\n",
      "exploration/env_infos/initial/reward_energy Max        -0.719892\n",
      "exploration/env_infos/initial/reward_energy Min        -1.09336\n",
      "exploration/env_infos/reward_energy Mean               -0.775129\n",
      "exploration/env_infos/reward_energy Std                 0.277183\n",
      "exploration/env_infos/reward_energy Max                -0.19223\n",
      "exploration/env_infos/reward_energy Min                -1.25569\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.412974\n",
      "exploration/env_infos/final/end_effector_loc Std        0.646101\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.0216553\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.022725\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0471042\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0352084\n",
      "exploration/env_infos/end_effector_loc Mean             0.273985\n",
      "exploration/env_infos/end_effector_loc Std              0.485792\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           1000\n",
      "evaluation/num paths total                             50\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0780623\n",
      "evaluation/Rewards Std                                  0.0473164\n",
      "evaluation/Rewards Max                                  0.0425092\n",
      "evaluation/Rewards Min                                 -0.150422\n",
      "evaluation/Returns Mean                                -1.56125\n",
      "evaluation/Returns Std                                  0.945244\n",
      "evaluation/Returns Max                                  0.807234\n",
      "evaluation/Returns Min                                 -2.97967\n",
      "evaluation/Actions Mean                                 0.00098014\n",
      "evaluation/Actions Std                                  0.000587866\n",
      "evaluation/Actions Max                                  0.0020782\n",
      "evaluation/Actions Min                                  0.000173299\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.56125\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00593428\n",
      "evaluation/env_infos/final/reward_dist Std              0.00912983\n",
      "evaluation/env_infos/final/reward_dist Max              0.0441983\n",
      "evaluation/env_infos/final/reward_dist Min              5.15938e-07\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00533279\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00750724\n",
      "evaluation/env_infos/initial/reward_dist Max            0.02745\n",
      "evaluation/env_infos/initial/reward_dist Min            9.54696e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.00547818\n",
      "evaluation/env_infos/reward_dist Std                    0.0078623\n",
      "evaluation/env_infos/reward_dist Max                    0.0441983\n",
      "evaluation/env_infos/reward_dist Min                    5.15938e-07\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.00163802\n",
      "evaluation/env_infos/final/reward_energy Std            0.000251158\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00111856\n",
      "evaluation/env_infos/final/reward_energy Min           -0.00210778\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.00156036\n",
      "evaluation/env_infos/initial/reward_energy Std          0.00021699\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00111835\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.00196039\n",
      "evaluation/env_infos/reward_energy Mean                -0.00159921\n",
      "evaluation/env_infos/reward_energy Std                  0.000234643\n",
      "evaluation/env_infos/reward_energy Max                 -0.00111835\n",
      "evaluation/env_infos/reward_energy Min                 -0.00210778\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.0102276\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.00607673\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.0207555\n",
      "evaluation/env_infos/final/end_effector_loc Min         0.00191566\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      4.80703e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std       2.81336e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Max       9.61769e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Min       8.66496e-06\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00373819\n",
      "evaluation/env_infos/end_effector_loc Std               0.0043006\n",
      "evaluation/env_infos/end_effector_loc Max               0.0207555\n",
      "evaluation/env_infos/end_effector_loc Min               8.66496e-06\n",
      "time/data storing (s)                                   0.00299722\n",
      "time/evaluation sampling (s)                            1.39765\n",
      "time/exploration sampling (s)                           0.122225\n",
      "time/logging (s)                                        0.0222268\n",
      "time/saving (s)                                         0.0734678\n",
      "time/training (s)                                      32.3707\n",
      "time/epoch (s)                                         33.9893\n",
      "time/total (s)                                         38.19\n",
      "Epoch                                                   0\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:26:17.494277 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 1 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   1200\n",
      "trainer/QF1 Loss                                        0.0142788\n",
      "trainer/QF2 Loss                                        0.0179015\n",
      "trainer/Policy Loss                                    -0.759393\n",
      "trainer/Q1 Predictions Mean                            -0.46276\n",
      "trainer/Q1 Predictions Std                              0.624373\n",
      "trainer/Q1 Predictions Max                              0.586792\n",
      "trainer/Q1 Predictions Min                             -2.04044\n",
      "trainer/Q2 Predictions Mean                            -0.537618\n",
      "trainer/Q2 Predictions Std                              0.622405\n",
      "trainer/Q2 Predictions Max                              0.567538\n",
      "trainer/Q2 Predictions Min                             -2.17126\n",
      "trainer/Q Targets Mean                                 -0.486009\n",
      "trainer/Q Targets Std                                   0.654017\n",
      "trainer/Q Targets Max                                   0.673842\n",
      "trainer/Q Targets Min                                  -2.31864\n",
      "trainer/Log Pis Mean                                   -1.18884\n",
      "trainer/Log Pis Std                                     0.472486\n",
      "trainer/Log Pis Max                                    -0.544822\n",
      "trainer/Log Pis Min                                    -3.78867\n",
      "trainer/Policy mu Mean                                  0.0285259\n",
      "trainer/Policy mu Std                                   0.0346983\n",
      "trainer/Policy mu Max                                   0.134678\n",
      "trainer/Policy mu Min                                  -0.0757775\n",
      "trainer/Policy log std Mean                            -0.526448\n",
      "trainer/Policy log std Std                              0.0696464\n",
      "trainer/Policy log std Max                             -0.366739\n",
      "trainer/Policy log std Min                             -0.693374\n",
      "trainer/Alpha                                           0.225887\n",
      "trainer/Alpha Loss                                     -4.73488\n",
      "exploration/num steps total                          1200\n",
      "exploration/num paths total                            60\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.517996\n",
      "exploration/Rewards Std                                 0.284947\n",
      "exploration/Rewards Max                                 0.0111953\n",
      "exploration/Rewards Min                                -1.26828\n",
      "exploration/Returns Mean                              -10.3599\n",
      "exploration/Returns Std                                 2.90097\n",
      "exploration/Returns Max                                -6.89275\n",
      "exploration/Returns Min                               -15.7143\n",
      "exploration/Actions Mean                                0.046027\n",
      "exploration/Actions Std                                 0.385387\n",
      "exploration/Actions Max                                 0.915877\n",
      "exploration/Actions Min                                -0.852987\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -10.3599\n",
      "exploration/env_infos/final/reward_dist Mean            1.0713e-53\n",
      "exploration/env_infos/final/reward_dist Std             2.14259e-53\n",
      "exploration/env_infos/final/reward_dist Max             5.35648e-53\n",
      "exploration/env_infos/final/reward_dist Min             1.19818e-151\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0103099\n",
      "exploration/env_infos/initial/reward_dist Std           0.00620404\n",
      "exploration/env_infos/initial/reward_dist Max           0.017284\n",
      "exploration/env_infos/initial/reward_dist Min           2.62818e-07\n",
      "exploration/env_infos/reward_dist Mean                  0.0169753\n",
      "exploration/env_infos/reward_dist Std                   0.049046\n",
      "exploration/env_infos/reward_dist Max                   0.271489\n",
      "exploration/env_infos/reward_dist Min                   1.19818e-151\n",
      "exploration/env_infos/final/reward_energy Mean         -0.484143\n",
      "exploration/env_infos/final/reward_energy Std           0.217856\n",
      "exploration/env_infos/final/reward_energy Max          -0.152269\n",
      "exploration/env_infos/final/reward_energy Min          -0.809446\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.473428\n",
      "exploration/env_infos/initial/reward_energy Std         0.250453\n",
      "exploration/env_infos/initial/reward_energy Max        -0.136648\n",
      "exploration/env_infos/initial/reward_energy Min        -0.769096\n",
      "exploration/env_infos/reward_energy Mean               -0.491715\n",
      "exploration/env_infos/reward_energy Std                 0.243927\n",
      "exploration/env_infos/reward_energy Max                -0.0138491\n",
      "exploration/env_infos/reward_energy Min                -1.14363\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.281292\n",
      "exploration/env_infos/final/end_effector_loc Std        0.753392\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.000124701\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0189357\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0364533\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0353364\n",
      "exploration/env_infos/end_effector_loc Mean             0.131406\n",
      "exploration/env_infos/end_effector_loc Std              0.468006\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           2000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.129317\n",
      "evaluation/Rewards Std                                  0.0950763\n",
      "evaluation/Rewards Max                                  0.165276\n",
      "evaluation/Rewards Min                                 -0.637119\n",
      "evaluation/Returns Mean                                -2.58635\n",
      "evaluation/Returns Std                                  1.38028\n",
      "evaluation/Returns Max                                  1.35225\n",
      "evaluation/Returns Min                                 -5.58551\n",
      "evaluation/Actions Mean                                 0.0325454\n",
      "evaluation/Actions Std                                  0.0245784\n",
      "evaluation/Actions Max                                  0.123077\n",
      "evaluation/Actions Min                                 -0.057515\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.58635\n",
      "evaluation/env_infos/final/reward_dist Mean             2.69966e-07\n",
      "evaluation/env_infos/final/reward_dist Std              1.09715e-06\n",
      "evaluation/env_infos/final/reward_dist Max              5.76264e-06\n",
      "evaluation/env_infos/final/reward_dist Min              3.98986e-74\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00497559\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00811029\n",
      "evaluation/env_infos/initial/reward_dist Max            0.027829\n",
      "evaluation/env_infos/initial/reward_dist Min            1.12831e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0435771\n",
      "evaluation/env_infos/reward_dist Std                    0.143624\n",
      "evaluation/env_infos/reward_dist Max                    0.954145\n",
      "evaluation/env_infos/reward_dist Min                    3.98986e-74\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0490002\n",
      "evaluation/env_infos/final/reward_energy Std            0.00727559\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0370384\n",
      "evaluation/env_infos/final/reward_energy Min           -0.0729301\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.0621672\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0211793\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0401044\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.133877\n",
      "evaluation/env_infos/reward_energy Mean                -0.0556139\n",
      "evaluation/env_infos/reward_energy Std                  0.015287\n",
      "evaluation/env_infos/reward_energy Max                 -0.0370384\n",
      "evaluation/env_infos/reward_energy Min                 -0.134243\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.356476\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.266035\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.969306\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.581257\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00178086\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00149003\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.00614425\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00287575\n",
      "evaluation/env_infos/end_effector_loc Mean              0.133145\n",
      "evaluation/env_infos/end_effector_loc Std               0.172783\n",
      "evaluation/env_infos/end_effector_loc Max               0.969306\n",
      "evaluation/env_infos/end_effector_loc Min              -0.581257\n",
      "time/data storing (s)                                   0.00308579\n",
      "time/evaluation sampling (s)                            1.13693\n",
      "time/exploration sampling (s)                           0.127787\n",
      "time/logging (s)                                        0.0202216\n",
      "time/saving (s)                                         0.0366551\n",
      "time/training (s)                                      41.3416\n",
      "time/epoch (s)                                         42.6663\n",
      "time/total (s)                                         81.0213\n",
      "Epoch                                                   1\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:27:04.151054 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 2 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                   1300\r\n",
      "trainer/QF1 Loss                                        0.016673\r\n",
      "trainer/QF2 Loss                                        0.0196761\r\n",
      "trainer/Policy Loss                                     1.37269\r\n",
      "trainer/Q1 Predictions Mean                            -1.49769\r\n",
      "trainer/Q1 Predictions Std                              0.976716\r\n",
      "trainer/Q1 Predictions Max                              0.332782\r\n",
      "trainer/Q1 Predictions Min                             -4.23755\r\n",
      "trainer/Q2 Predictions Mean                            -1.54115\r\n",
      "trainer/Q2 Predictions Std                              0.980129\r\n",
      "trainer/Q2 Predictions Max                              0.280805\r\n",
      "trainer/Q2 Predictions Min                             -4.01748\r\n",
      "trainer/Q Targets Mean                                 -1.5193\r\n",
      "trainer/Q Targets Std                                   1.02966\r\n",
      "trainer/Q Targets Max                                   0.350677\r\n",
      "trainer/Q Targets Min                                  -4.41199\r\n",
      "trainer/Log Pis Mean                                    0.132319\r\n",
      "trainer/Log Pis Std                                     1.14895\r\n",
      "trainer/Log Pis Max                                     3.76002\r\n",
      "trainer/Log Pis Min                                    -6.51631\r\n",
      "trainer/Policy mu Mean                                 -0.0163182\r\n",
      "trainer/Policy mu Std                                   0.341392\r\n",
      "trainer/Policy mu Max                                   1.57785\r\n",
      "trainer/Policy mu Min                                  -1.65983\r\n",
      "trainer/Policy log std Mean                            -1.28305\r\n",
      "trainer/Policy log std Std                              0.384718\r\n",
      "trainer/Policy log std Max                             -0.30159\r\n",
      "trainer/Policy log std Min                             -2.16013\r\n",
      "trainer/Alpha                                           0.0639025\r\n",
      "trainer/Alpha Loss                                     -5.133\r\n",
      "exploration/num steps total                          1300\r\n",
      "exploration/num paths total                            65\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.317213\r\n",
      "exploration/Rewards Std                                 0.206701\r\n",
      "exploration/Rewards Max                                -0.0785534\r\n",
      "exploration/Rewards Min                                -1.08456\r\n",
      "exploration/Returns Mean                               -6.34426\r\n",
      "exploration/Returns Std                                 3.03259\r\n",
      "exploration/Returns Max                                -4.30967\r\n",
      "exploration/Returns Min                               -12.3355\r\n",
      "exploration/Actions Mean                                0.0176146\r\n",
      "exploration/Actions Std                                 0.249469\r\n",
      "exploration/Actions Max                                 0.793571\r\n",
      "exploration/Actions Min                                -0.562318\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -6.34426\r\n",
      "exploration/env_infos/final/reward_dist Mean            1.10525e-05\r\n",
      "exploration/env_infos/final/reward_dist Std             2.2105e-05\r\n",
      "exploration/env_infos/final/reward_dist Max             5.52624e-05\r\n",
      "exploration/env_infos/final/reward_dist Min             8.32703e-100\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00917166\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.0115859\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0301845\r\n",
      "exploration/env_infos/initial/reward_dist Min           1.11096e-05\r\n",
      "exploration/env_infos/reward_dist Mean                  0.0291417\r\n",
      "exploration/env_infos/reward_dist Std                   0.101679\r\n",
      "exploration/env_infos/reward_dist Max                   0.609998\r\n",
      "exploration/env_infos/reward_dist Min                   1.44801e-101\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.447668\r\n",
      "exploration/env_infos/final/reward_energy Std           0.173098\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.170972\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.6067\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.416838\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.144682\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.210439\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.655991\r\n",
      "exploration/env_infos/reward_energy Mean               -0.307407\r\n",
      "exploration/env_infos/reward_energy Std                 0.174901\r\n",
      "exploration/env_infos/reward_energy Max                -0.0300016\r\n",
      "exploration/env_infos/reward_energy Min                -0.815759\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.324347\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.475971\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.857327\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.902187\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00309135\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0152906\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0318791\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0214156\r\n",
      "exploration/env_infos/end_effector_loc Mean             0.143115\r\n",
      "exploration/env_infos/end_effector_loc Std              0.335973\r\n",
      "exploration/env_infos/end_effector_loc Max              0.857327\r\n",
      "exploration/env_infos/end_effector_loc Min             -1\r\n",
      "evaluation/num steps total                           3000\r\n",
      "evaluation/num paths total                            150\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.171235\r\n",
      "evaluation/Rewards Std                                  0.156176\r\n",
      "evaluation/Rewards Max                                  0.0822294\r\n",
      "evaluation/Rewards Min                                 -0.979425\r\n",
      "evaluation/Returns Mean                                -3.4247\r\n",
      "evaluation/Returns Std                                  2.37878\r\n",
      "evaluation/Returns Max                                 -1.03687\r\n",
      "evaluation/Returns Min                                -12.9465\r\n",
      "evaluation/Actions Mean                                -0.00332473\r\n",
      "evaluation/Actions Std                                  0.113027\r\n",
      "evaluation/Actions Max                                  0.655579\r\n",
      "evaluation/Actions Min                                 -0.545786\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -3.4247\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0091053\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.0369815\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.234501\r\n",
      "evaluation/env_infos/final/reward_dist Min              4.91896e-119\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00550878\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0120953\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0659473\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.36827e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0420782\r\n",
      "evaluation/env_infos/reward_dist Std                    0.136436\r\n",
      "evaluation/env_infos/reward_dist Max                    0.985674\r\n",
      "evaluation/env_infos/reward_dist Min                    1.56268e-124\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.189778\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.121088\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00870755\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.55283\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.157609\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.140839\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00895795\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.66551\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.121254\r\n",
      "evaluation/env_infos/reward_energy Std                  0.104257\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00162313\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.66551\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.123052\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.380354\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.51224\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00333402\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00668802\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.032779\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0145551\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0941151\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.253196\r\n",
      "evaluation/env_infos/end_effector_loc Max               1\r\n",
      "evaluation/env_infos/end_effector_loc Min              -0.71139\r\n",
      "time/data storing (s)                                   0.00349065\r\n",
      "time/evaluation sampling (s)                            1.06136\r\n",
      "time/exploration sampling (s)                           0.139693\r\n",
      "time/logging (s)                                        0.0188483\r\n",
      "time/saving (s)                                         0.0263298\r\n",
      "time/training (s)                                      45.341\r\n",
      "time/epoch (s)                                         46.5907\r\n",
      "time/total (s)                                        127.676\r\n",
      "Epoch                                                   2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 10:27:50.188074 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 3 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1400\n",
      "trainer/QF1 Loss                                        0.00992432\n",
      "trainer/QF2 Loss                                        0.0117483\n",
      "trainer/Policy Loss                                     3.07273\n",
      "trainer/Q1 Predictions Mean                            -2.17948\n",
      "trainer/Q1 Predictions Std                              1.31205\n",
      "trainer/Q1 Predictions Max                             -0.0172831\n",
      "trainer/Q1 Predictions Min                             -7.29756\n",
      "trainer/Q2 Predictions Mean                            -2.1591\n",
      "trainer/Q2 Predictions Std                              1.29779\n",
      "trainer/Q2 Predictions Max                             -0.00159588\n",
      "trainer/Q2 Predictions Min                             -7.20492\n",
      "trainer/Q Targets Mean                                 -2.18751\n",
      "trainer/Q Targets Std                                   1.31597\n",
      "trainer/Q Targets Max                                   0.0423553\n",
      "trainer/Q Targets Min                                  -7.1498\n",
      "trainer/Log Pis Mean                                    1.20588\n",
      "trainer/Log Pis Std                                     1.36652\n",
      "trainer/Log Pis Max                                     6.7834\n",
      "trainer/Log Pis Min                                    -3.3873\n",
      "trainer/Policy mu Mean                                 -0.0163686\n",
      "trainer/Policy mu Std                                   0.633887\n",
      "trainer/Policy mu Max                                   2.41305\n",
      "trainer/Policy mu Min                                  -2.83405\n",
      "trainer/Policy log std Mean                            -1.68811\n",
      "trainer/Policy log std Std                              0.555395\n",
      "trainer/Policy log std Max                              0.351206\n",
      "trainer/Policy log std Min                             -2.70888\n",
      "trainer/Alpha                                           0.0278039\n",
      "trainer/Alpha Loss                                     -2.84399\n",
      "exploration/num steps total                          1400\n",
      "exploration/num paths total                            70\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.218513\n",
      "exploration/Rewards Std                                 0.0870036\n",
      "exploration/Rewards Max                                -0.0625078\n",
      "exploration/Rewards Min                                -0.527785\n",
      "exploration/Returns Mean                               -4.37025\n",
      "exploration/Returns Std                                 0.745795\n",
      "exploration/Returns Max                                -3.18479\n",
      "exploration/Returns Min                                -5.5153\n",
      "exploration/Actions Mean                               -0.00675753\n",
      "exploration/Actions Std                                 0.191701\n",
      "exploration/Actions Max                                 0.585023\n",
      "exploration/Actions Min                                -0.698324\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -4.37025\n",
      "exploration/env_infos/final/reward_dist Mean            1.95884e-07\n",
      "exploration/env_infos/final/reward_dist Std             3.91767e-07\n",
      "exploration/env_infos/final/reward_dist Max             9.79418e-07\n",
      "exploration/env_infos/final/reward_dist Min             4.13074e-31\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0065823\n",
      "exploration/env_infos/initial/reward_dist Std           0.00953403\n",
      "exploration/env_infos/initial/reward_dist Max           0.0246377\n",
      "exploration/env_infos/initial/reward_dist Min           5.58389e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0741105\n",
      "exploration/env_infos/reward_dist Std                   0.213269\n",
      "exploration/env_infos/reward_dist Max                   0.996782\n",
      "exploration/env_infos/reward_dist Min                   4.13074e-31\n",
      "exploration/env_infos/final/reward_energy Mean         -0.34174\n",
      "exploration/env_infos/final/reward_energy Std           0.133296\n",
      "exploration/env_infos/final/reward_energy Max          -0.180776\n",
      "exploration/env_infos/final/reward_energy Min          -0.5564\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.194765\n",
      "exploration/env_infos/initial/reward_energy Std         0.0711915\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0959558\n",
      "exploration/env_infos/initial/reward_energy Min        -0.266792\n",
      "exploration/env_infos/reward_energy Mean               -0.227697\n",
      "exploration/env_infos/reward_energy Std                 0.147458\n",
      "exploration/env_infos/reward_energy Max                -0.0210123\n",
      "exploration/env_infos/reward_energy Min                -0.725702\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0748444\n",
      "exploration/env_infos/final/end_effector_loc Std        0.382551\n",
      "exploration/env_infos/final/end_effector_loc Max        0.524773\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.723146\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00169161\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00713376\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0111471\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0120533\n",
      "exploration/env_infos/end_effector_loc Mean             0.00363569\n",
      "exploration/env_infos/end_effector_loc Std              0.223757\n",
      "exploration/env_infos/end_effector_loc Max              0.524773\n",
      "exploration/env_infos/end_effector_loc Min             -0.723146\n",
      "evaluation/num steps total                           4000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.154526\n",
      "evaluation/Rewards Std                                  0.126546\n",
      "evaluation/Rewards Max                                  0.0852592\n",
      "evaluation/Rewards Min                                 -1.04159\n",
      "evaluation/Returns Mean                                -3.09053\n",
      "evaluation/Returns Std                                  1.68053\n",
      "evaluation/Returns Max                                 -0.385755\n",
      "evaluation/Returns Min                                 -9.44947\n",
      "evaluation/Actions Mean                                -0.0106644\n",
      "evaluation/Actions Std                                  0.115393\n",
      "evaluation/Actions Max                                  0.447187\n",
      "evaluation/Actions Min                                 -0.590926\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.09053\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0139149\n",
      "evaluation/env_infos/final/reward_dist Std              0.0439649\n",
      "evaluation/env_infos/final/reward_dist Max              0.234655\n",
      "evaluation/env_infos/final/reward_dist Min              1.7802e-77\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00607394\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00989201\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0349882\n",
      "evaluation/env_infos/initial/reward_dist Min            6.16823e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0783719\n",
      "evaluation/env_infos/reward_dist Std                    0.188876\n",
      "evaluation/env_infos/reward_dist Max                    0.983201\n",
      "evaluation/env_infos/reward_dist Min                    1.7802e-77\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.194703\n",
      "evaluation/env_infos/final/reward_energy Std            0.119711\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0241036\n",
      "evaluation/env_infos/final/reward_energy Min           -0.594926\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.175934\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0973403\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0299292\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.542958\n",
      "evaluation/env_infos/reward_energy Mean                -0.136909\n",
      "evaluation/env_infos/reward_energy Std                  0.0900816\n",
      "evaluation/env_infos/reward_energy Max                 -0.00539068\n",
      "evaluation/env_infos/reward_energy Min                 -0.594926\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.111028\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.366704\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.688137\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000338744\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00710071\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0223594\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.018419\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0352607\n",
      "evaluation/env_infos/end_effector_loc Std               0.245378\n",
      "evaluation/env_infos/end_effector_loc Max               0.688137\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00297755\n",
      "time/evaluation sampling (s)                            0.949526\n",
      "time/exploration sampling (s)                           0.12669\n",
      "time/logging (s)                                        0.0193505\n",
      "time/saving (s)                                         0.0291837\n",
      "time/training (s)                                      44.8391\n",
      "time/epoch (s)                                         45.9668\n",
      "time/total (s)                                        173.713\n",
      "Epoch                                                   3\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:28:37.803683 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 4 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1500\n",
      "trainer/QF1 Loss                                        0.00980622\n",
      "trainer/QF2 Loss                                        0.00970217\n",
      "trainer/Policy Loss                                     4.19382\n",
      "trainer/Q1 Predictions Mean                            -2.77101\n",
      "trainer/Q1 Predictions Std                              1.67244\n",
      "trainer/Q1 Predictions Max                             -0.181449\n",
      "trainer/Q1 Predictions Min                             -9.09773\n",
      "trainer/Q2 Predictions Mean                            -2.83605\n",
      "trainer/Q2 Predictions Std                              1.68902\n",
      "trainer/Q2 Predictions Max                             -0.220586\n",
      "trainer/Q2 Predictions Min                             -9.08909\n",
      "trainer/Q Targets Mean                                 -2.80172\n",
      "trainer/Q Targets Std                                   1.67376\n",
      "trainer/Q Targets Max                                  -0.304971\n",
      "trainer/Q Targets Min                                  -9.10335\n",
      "trainer/Log Pis Mean                                    1.73946\n",
      "trainer/Log Pis Std                                     1.47903\n",
      "trainer/Log Pis Max                                     8.6428\n",
      "trainer/Log Pis Min                                    -5.90367\n",
      "trainer/Policy mu Mean                                 -0.0116877\n",
      "trainer/Policy mu Std                                   0.768037\n",
      "trainer/Policy mu Max                                   3.12088\n",
      "trainer/Policy mu Min                                  -2.7154\n",
      "trainer/Policy log std Mean                            -1.85284\n",
      "trainer/Policy log std Std                              0.609531\n",
      "trainer/Policy log std Max                             -0.248283\n",
      "trainer/Policy log std Min                             -2.81579\n",
      "trainer/Alpha                                           0.0179338\n",
      "trainer/Alpha Loss                                     -1.04753\n",
      "exploration/num steps total                          1500\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.185039\n",
      "exploration/Rewards Std                                 0.0864499\n",
      "exploration/Rewards Max                                -0.00210195\n",
      "exploration/Rewards Min                                -0.502919\n",
      "exploration/Returns Mean                               -3.70077\n",
      "exploration/Returns Std                                 0.686651\n",
      "exploration/Returns Max                                -2.5347\n",
      "exploration/Returns Min                                -4.67839\n",
      "exploration/Actions Mean                               -0.0191028\n",
      "exploration/Actions Std                                 0.119402\n",
      "exploration/Actions Max                                 0.34221\n",
      "exploration/Actions Min                                -0.37382\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -3.70077\n",
      "exploration/env_infos/final/reward_dist Mean            2.69802e-06\n",
      "exploration/env_infos/final/reward_dist Std             3.4599e-06\n",
      "exploration/env_infos/final/reward_dist Max             8.42812e-06\n",
      "exploration/env_infos/final/reward_dist Min             4.61265e-28\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0120991\n",
      "exploration/env_infos/initial/reward_dist Std           0.0239685\n",
      "exploration/env_infos/initial/reward_dist Max           0.0600351\n",
      "exploration/env_infos/initial/reward_dist Min           9.77122e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0578942\n",
      "exploration/env_infos/reward_dist Std                   0.155278\n",
      "exploration/env_infos/reward_dist Max                   0.794904\n",
      "exploration/env_infos/reward_dist Min                   4.61265e-28\n",
      "exploration/env_infos/final/reward_energy Mean         -0.241985\n",
      "exploration/env_infos/final/reward_energy Std           0.0910221\n",
      "exploration/env_infos/final/reward_energy Max          -0.153321\n",
      "exploration/env_infos/final/reward_energy Min          -0.405928\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.192135\n",
      "exploration/env_infos/initial/reward_energy Std         0.0863032\n",
      "exploration/env_infos/initial/reward_energy Max        -0.124289\n",
      "exploration/env_infos/initial/reward_energy Min        -0.346944\n",
      "exploration/env_infos/reward_energy Mean               -0.150374\n",
      "exploration/env_infos/reward_energy Std                 0.0814307\n",
      "exploration/env_infos/reward_energy Max                -0.0108661\n",
      "exploration/env_infos/reward_energy Min                -0.405928\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0977437\n",
      "exploration/env_infos/final/end_effector_loc Std        0.328272\n",
      "exploration/env_infos/final/end_effector_loc Max        0.412813\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.550653\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00133301\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00732653\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0171105\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0098357\n",
      "exploration/env_infos/end_effector_loc Mean            -0.00177883\n",
      "exploration/env_infos/end_effector_loc Std              0.193416\n",
      "exploration/env_infos/end_effector_loc Max              0.412813\n",
      "exploration/env_infos/end_effector_loc Min             -0.550653\n",
      "evaluation/num steps total                           5000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.138405\n",
      "evaluation/Rewards Std                                  0.127079\n",
      "evaluation/Rewards Max                                  0.175818\n",
      "evaluation/Rewards Min                                 -0.875634\n",
      "evaluation/Returns Mean                                -2.7681\n",
      "evaluation/Returns Std                                  1.6903\n",
      "evaluation/Returns Max                                  0.848121\n",
      "evaluation/Returns Min                                 -7.44648\n",
      "evaluation/Actions Mean                                -0.0132224\n",
      "evaluation/Actions Std                                  0.123843\n",
      "evaluation/Actions Max                                  0.558661\n",
      "evaluation/Actions Min                                 -0.756\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.7681\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0327722\n",
      "evaluation/env_infos/final/reward_dist Std              0.0957473\n",
      "evaluation/env_infos/final/reward_dist Max              0.495844\n",
      "evaluation/env_infos/final/reward_dist Min              1.7421e-111\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.0056543\n",
      "evaluation/env_infos/initial/reward_dist Std            0.009642\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0369354\n",
      "evaluation/env_infos/initial/reward_dist Min            9.7795e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0708348\n",
      "evaluation/env_infos/reward_dist Std                    0.177649\n",
      "evaluation/env_infos/reward_dist Max                    0.998196\n",
      "evaluation/env_infos/reward_dist Min                    1.7421e-111\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.226136\n",
      "evaluation/env_infos/final/reward_energy Std            0.142231\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0337407\n",
      "evaluation/env_infos/final/reward_energy Min           -0.616076\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.24447\n",
      "evaluation/env_infos/initial/reward_energy Std          0.185361\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0357208\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.872488\n",
      "evaluation/env_infos/reward_energy Mean                -0.139616\n",
      "evaluation/env_infos/reward_energy Std                  0.107384\n",
      "evaluation/env_infos/reward_energy Max                 -0.00332395\n",
      "evaluation/env_infos/reward_energy Min                 -0.872488\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.120011\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.356042\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.965892\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000624074\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0108289\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.027933\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0378\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0319308\n",
      "evaluation/env_infos/end_effector_loc Std               0.22982\n",
      "evaluation/env_infos/end_effector_loc Max               0.965892\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00349711\n",
      "time/evaluation sampling (s)                            1.34777\n",
      "time/exploration sampling (s)                           0.132789\n",
      "time/logging (s)                                        0.026126\n",
      "time/saving (s)                                         0.0350646\n",
      "time/training (s)                                      45.9903\n",
      "time/epoch (s)                                         47.5356\n",
      "time/total (s)                                        221.335\n",
      "Epoch                                                   4\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:29:26.335015 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 5 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   1600\r\n",
      "trainer/QF1 Loss                                        0.00391405\r\n",
      "trainer/QF2 Loss                                        0.00828227\r\n",
      "trainer/Policy Loss                                     4.53958\r\n",
      "trainer/Q1 Predictions Mean                            -2.71977\r\n",
      "trainer/Q1 Predictions Std                              1.61255\r\n",
      "trainer/Q1 Predictions Max                             -0.24123\r\n",
      "trainer/Q1 Predictions Min                            -10.4015\r\n",
      "trainer/Q2 Predictions Mean                            -2.6909\r\n",
      "trainer/Q2 Predictions Std                              1.61907\r\n",
      "trainer/Q2 Predictions Max                             -0.162658\r\n",
      "trainer/Q2 Predictions Min                            -10.2839\r\n",
      "trainer/Q Targets Mean                                 -2.72431\r\n",
      "trainer/Q Targets Std                                   1.61548\r\n",
      "trainer/Q Targets Max                                  -0.134168\r\n",
      "trainer/Q Targets Min                                 -10.3591\r\n",
      "trainer/Log Pis Mean                                    2.15081\r\n",
      "trainer/Log Pis Std                                     1.18245\r\n",
      "trainer/Log Pis Max                                     5.55259\r\n",
      "trainer/Log Pis Min                                    -4.33486\r\n",
      "trainer/Policy mu Mean                                 -0.0691641\r\n",
      "trainer/Policy mu Std                                   0.567645\r\n",
      "trainer/Policy mu Max                                   2.31754\r\n",
      "trainer/Policy mu Min                                  -2.55865\r\n",
      "trainer/Policy log std Mean                            -2.14175\r\n",
      "trainer/Policy log std Std                              0.568775\r\n",
      "trainer/Policy log std Max                             -0.37912\r\n",
      "trainer/Policy log std Min                             -3.00872\r\n",
      "trainer/Alpha                                           0.0152722\r\n",
      "trainer/Alpha Loss                                      0.630623\r\n",
      "exploration/num steps total                          1600\r\n",
      "exploration/num paths total                            80\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.159711\r\n",
      "exploration/Rewards Std                                 0.077504\r\n",
      "exploration/Rewards Max                                -0.0124729\r\n",
      "exploration/Rewards Min                                -0.407509\r\n",
      "exploration/Returns Mean                               -3.19422\r\n",
      "exploration/Returns Std                                 1.14011\r\n",
      "exploration/Returns Max                                -1.35609\r\n",
      "exploration/Returns Min                                -4.27307\r\n",
      "exploration/Actions Mean                               -0.0130702\r\n",
      "exploration/Actions Std                                 0.182171\r\n",
      "exploration/Actions Max                                 0.616261\r\n",
      "exploration/Actions Min                                -0.672497\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -3.19422\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.129612\r\n",
      "exploration/env_infos/final/reward_dist Std             0.259203\r\n",
      "exploration/env_infos/final/reward_dist Max             0.648019\r\n",
      "exploration/env_infos/final/reward_dist Min             7.32844e-39\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00216965\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.00427531\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0107201\r\n",
      "exploration/env_infos/initial/reward_dist Min           4.88192e-06\r\n",
      "exploration/env_infos/reward_dist Mean                  0.128057\r\n",
      "exploration/env_infos/reward_dist Std                   0.281165\r\n",
      "exploration/env_infos/reward_dist Max                   0.961812\r\n",
      "exploration/env_infos/reward_dist Min                   7.32844e-39\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.245481\r\n",
      "exploration/env_infos/final/reward_energy Std           0.0895232\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.107677\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.347745\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.177843\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.156465\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0534861\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.480436\r\n",
      "exploration/env_infos/reward_energy Mean               -0.202719\r\n",
      "exploration/env_infos/reward_energy Std                 0.16006\r\n",
      "exploration/env_infos/reward_energy Max                -0.00457819\r\n",
      "exploration/env_infos/reward_energy Min                -0.768262\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0926429\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.236094\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.2314\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.48896\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00360691\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00755822\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00599825\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0230505\r\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0116781\r\n",
      "exploration/env_infos/end_effector_loc Std              0.137859\r\n",
      "exploration/env_infos/end_effector_loc Max              0.266246\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.48896\r\n",
      "evaluation/num steps total                           6000\r\n",
      "evaluation/num paths total                            300\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.140115\r\n",
      "evaluation/Rewards Std                                  0.0954639\r\n",
      "evaluation/Rewards Max                                  0.122094\r\n",
      "evaluation/Rewards Min                                 -0.676226\r\n",
      "evaluation/Returns Mean                                -2.8023\r\n",
      "evaluation/Returns Std                                  1.18033\r\n",
      "evaluation/Returns Max                                 -0.53575\r\n",
      "evaluation/Returns Min                                 -5.67409\r\n",
      "evaluation/Actions Mean                                -0.0217011\r\n",
      "evaluation/Actions Std                                  0.0957455\r\n",
      "evaluation/Actions Max                                  0.397837\r\n",
      "evaluation/Actions Min                                 -0.409855\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -2.8023\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0370594\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.138308\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.783803\r\n",
      "evaluation/env_infos/final/reward_dist Min              3.64493e-74\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00665165\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00947028\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0324627\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.2328e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0515727\r\n",
      "evaluation/env_infos/reward_dist Std                    0.134432\r\n",
      "evaluation/env_infos/reward_dist Max                    0.9591\r\n",
      "evaluation/env_infos/reward_dist Min                    3.64493e-74\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.187431\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.118135\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.043561\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.528155\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.153871\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0687537\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0442669\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.364019\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.114251\r\n",
      "evaluation/env_infos/reward_energy Std                  0.0788854\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00217171\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.528155\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.201088\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.29125\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.319776\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00126295\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00582315\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0111058\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0180466\r\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0514303\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.188284\r\n",
      "evaluation/env_infos/end_effector_loc Max               0.333243\r\n",
      "evaluation/env_infos/end_effector_loc Min              -1\r\n",
      "time/data storing (s)                                   0.00319928\r\n",
      "time/evaluation sampling (s)                            1.33085\r\n",
      "time/exploration sampling (s)                           0.144429\r\n",
      "time/logging (s)                                        0.020892\r\n",
      "time/saving (s)                                         0.0299084\r\n",
      "time/training (s)                                      46.882\r\n",
      "time/epoch (s)                                         48.4113\r\n",
      "time/total (s)                                        269.86\r\n",
      "Epoch                                                   5\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:11.294383 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 6 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1700\n",
      "trainer/QF1 Loss                                        0.00401628\n",
      "trainer/QF2 Loss                                        0.00301326\n",
      "trainer/Policy Loss                                     4.5679\n",
      "trainer/Q1 Predictions Mean                            -2.94904\n",
      "trainer/Q1 Predictions Std                              1.78239\n",
      "trainer/Q1 Predictions Max                             -0.145669\n",
      "trainer/Q1 Predictions Min                             -9.25414\n",
      "trainer/Q2 Predictions Mean                            -2.95907\n",
      "trainer/Q2 Predictions Std                              1.78011\n",
      "trainer/Q2 Predictions Max                             -0.138948\n",
      "trainer/Q2 Predictions Min                             -9.21313\n",
      "trainer/Q Targets Mean                                 -2.96176\n",
      "trainer/Q Targets Std                                   1.78064\n",
      "trainer/Q Targets Max                                  -0.134168\n",
      "trainer/Q Targets Min                                  -9.25124\n",
      "trainer/Log Pis Mean                                    1.98999\n",
      "trainer/Log Pis Std                                     1.43828\n",
      "trainer/Log Pis Max                                     7.88639\n",
      "trainer/Log Pis Min                                    -3.32601\n",
      "trainer/Policy mu Mean                                 -0.00859346\n",
      "trainer/Policy mu Std                                   0.609074\n",
      "trainer/Policy mu Max                                   2.67305\n",
      "trainer/Policy mu Min                                  -2.60096\n",
      "trainer/Policy log std Mean                            -2.12876\n",
      "trainer/Policy log std Std                              0.567733\n",
      "trainer/Policy log std Max                             -0.163518\n",
      "trainer/Policy log std Min                             -3.13718\n",
      "trainer/Alpha                                           0.0148503\n",
      "trainer/Alpha Loss                                     -0.0421255\n",
      "exploration/num steps total                          1700\n",
      "exploration/num paths total                            85\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.12011\n",
      "exploration/Rewards Std                                 0.0995193\n",
      "exploration/Rewards Max                                 0.0391944\n",
      "exploration/Rewards Min                                -0.515167\n",
      "exploration/Returns Mean                               -2.40221\n",
      "exploration/Returns Std                                 1.31784\n",
      "exploration/Returns Max                                -0.940018\n",
      "exploration/Returns Min                                -4.38468\n",
      "exploration/Actions Mean                               -0.0201889\n",
      "exploration/Actions Std                                 0.124215\n",
      "exploration/Actions Max                                 0.457947\n",
      "exploration/Actions Min                                -0.424974\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.40221\n",
      "exploration/env_infos/final/reward_dist Mean            0.0403122\n",
      "exploration/env_infos/final/reward_dist Std             0.0806241\n",
      "exploration/env_infos/final/reward_dist Max             0.20156\n",
      "exploration/env_infos/final/reward_dist Min             2.46859e-46\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0120158\n",
      "exploration/env_infos/initial/reward_dist Std           0.0158164\n",
      "exploration/env_infos/initial/reward_dist Max           0.0433042\n",
      "exploration/env_infos/initial/reward_dist Min           4.82377e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0555058\n",
      "exploration/env_infos/reward_dist Std                   0.0871416\n",
      "exploration/env_infos/reward_dist Max                   0.48784\n",
      "exploration/env_infos/reward_dist Min                   2.46859e-46\n",
      "exploration/env_infos/final/reward_energy Mean         -0.207155\n",
      "exploration/env_infos/final/reward_energy Std           0.0950714\n",
      "exploration/env_infos/final/reward_energy Max          -0.102552\n",
      "exploration/env_infos/final/reward_energy Min          -0.366865\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.229679\n",
      "exploration/env_infos/initial/reward_energy Std         0.161923\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0247831\n",
      "exploration/env_infos/initial/reward_energy Min        -0.483169\n",
      "exploration/env_infos/reward_energy Mean               -0.153621\n",
      "exploration/env_infos/reward_energy Std                 0.0898589\n",
      "exploration/env_infos/reward_energy Max                -0.0215799\n",
      "exploration/env_infos/reward_energy Min                -0.483169\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.185485\n",
      "exploration/env_infos/final/end_effector_loc Std        0.318867\n",
      "exploration/env_infos/final/end_effector_loc Max        0.246597\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.711116\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00150753\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00982047\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.012737\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0212487\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0659243\n",
      "exploration/env_infos/end_effector_loc Std              0.195697\n",
      "exploration/env_infos/end_effector_loc Max              0.252024\n",
      "exploration/env_infos/end_effector_loc Min             -0.711116\n",
      "evaluation/num steps total                           7000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.126556\n",
      "evaluation/Rewards Std                                  0.122697\n",
      "evaluation/Rewards Max                                  0.107346\n",
      "evaluation/Rewards Min                                 -0.939904\n",
      "evaluation/Returns Mean                                -2.53112\n",
      "evaluation/Returns Std                                  1.3597\n",
      "evaluation/Returns Max                                 -0.361605\n",
      "evaluation/Returns Min                                 -8.06351\n",
      "evaluation/Actions Mean                                -0.0139378\n",
      "evaluation/Actions Std                                  0.102267\n",
      "evaluation/Actions Max                                  0.43149\n",
      "evaluation/Actions Min                                 -0.801056\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.53112\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0116841\n",
      "evaluation/env_infos/final/reward_dist Std              0.0428651\n",
      "evaluation/env_infos/final/reward_dist Max              0.245459\n",
      "evaluation/env_infos/final/reward_dist Min              1.55703e-63\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00737524\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0124404\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0593836\n",
      "evaluation/env_infos/initial/reward_dist Min            3.99796e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0540835\n",
      "evaluation/env_infos/reward_dist Std                    0.146721\n",
      "evaluation/env_infos/reward_dist Max                    0.960647\n",
      "evaluation/env_infos/reward_dist Min                    2.778e-70\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.217906\n",
      "evaluation/env_infos/final/reward_energy Std            0.184873\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0116104\n",
      "evaluation/env_infos/final/reward_energy Min           -0.878679\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.154152\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0884744\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0229355\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.448848\n",
      "evaluation/env_infos/reward_energy Mean                -0.107257\n",
      "evaluation/env_infos/reward_energy Std                  0.0990024\n",
      "evaluation/env_infos/reward_energy Max                 -0.00113453\n",
      "evaluation/env_infos/reward_energy Min                 -0.878679\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.023272\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.328776\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.518937\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.0025474\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00574448\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0215745\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0149535\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0251593\n",
      "evaluation/env_infos/end_effector_loc Std               0.194369\n",
      "evaluation/env_infos/end_effector_loc Max               0.715969\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00296628\n",
      "time/evaluation sampling (s)                            1.01306\n",
      "time/exploration sampling (s)                           0.118467\n",
      "time/logging (s)                                        0.0265779\n",
      "time/saving (s)                                         0.0293528\n",
      "time/training (s)                                      43.6627\n",
      "time/epoch (s)                                         44.8531\n",
      "time/total (s)                                        314.825\n",
      "Epoch                                                   6\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:58.238699 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 7 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1800\n",
      "trainer/QF1 Loss                                        0.00532094\n",
      "trainer/QF2 Loss                                        0.00913572\n",
      "trainer/Policy Loss                                     4.66646\n",
      "trainer/Q1 Predictions Mean                            -3.1223\n",
      "trainer/Q1 Predictions Std                              1.9577\n",
      "trainer/Q1 Predictions Max                             -0.148908\n",
      "trainer/Q1 Predictions Min                            -10.6049\n",
      "trainer/Q2 Predictions Mean                            -3.04868\n",
      "trainer/Q2 Predictions Std                              1.9379\n",
      "trainer/Q2 Predictions Max                             -0.103933\n",
      "trainer/Q2 Predictions Min                            -10.6075\n",
      "trainer/Q Targets Mean                                 -3.09267\n",
      "trainer/Q Targets Std                                   1.95351\n",
      "trainer/Q Targets Max                                  -0.114265\n",
      "trainer/Q Targets Min                                 -10.6191\n",
      "trainer/Log Pis Mean                                    1.91905\n",
      "trainer/Log Pis Std                                     1.22466\n",
      "trainer/Log Pis Max                                     7.41546\n",
      "trainer/Log Pis Min                                    -3.28219\n",
      "trainer/Policy mu Mean                                 -0.0383501\n",
      "trainer/Policy mu Std                                   0.657596\n",
      "trainer/Policy mu Max                                   2.85599\n",
      "trainer/Policy mu Min                                  -2.87545\n",
      "trainer/Policy log std Mean                            -2.07488\n",
      "trainer/Policy log std Std                              0.596162\n",
      "trainer/Policy log std Max                             -0.0168958\n",
      "trainer/Policy log std Min                             -2.9021\n",
      "trainer/Alpha                                           0.014825\n",
      "trainer/Alpha Loss                                     -0.340906\n",
      "exploration/num steps total                          1800\n",
      "exploration/num paths total                            90\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.109248\n",
      "exploration/Rewards Std                                 0.078963\n",
      "exploration/Rewards Max                                 0.00457018\n",
      "exploration/Rewards Min                                -0.349884\n",
      "exploration/Returns Mean                               -2.18496\n",
      "exploration/Returns Std                                 1.01039\n",
      "exploration/Returns Max                                -1.00419\n",
      "exploration/Returns Min                                -4.03704\n",
      "exploration/Actions Mean                                0.00175343\n",
      "exploration/Actions Std                                 0.136674\n",
      "exploration/Actions Max                                 0.361814\n",
      "exploration/Actions Min                                -0.312836\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.18496\n",
      "exploration/env_infos/final/reward_dist Mean            0.0328673\n",
      "exploration/env_infos/final/reward_dist Std             0.054723\n",
      "exploration/env_infos/final/reward_dist Max             0.140989\n",
      "exploration/env_infos/final/reward_dist Min             4.04595e-10\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00521036\n",
      "exploration/env_infos/initial/reward_dist Std           0.00504233\n",
      "exploration/env_infos/initial/reward_dist Max           0.0125878\n",
      "exploration/env_infos/initial/reward_dist Min           5.22559e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0718827\n",
      "exploration/env_infos/reward_dist Std                   0.116213\n",
      "exploration/env_infos/reward_dist Max                   0.51901\n",
      "exploration/env_infos/reward_dist Min                   4.04595e-10\n",
      "exploration/env_infos/final/reward_energy Mean         -0.192104\n",
      "exploration/env_infos/final/reward_energy Std           0.117009\n",
      "exploration/env_infos/final/reward_energy Max          -0.0606609\n",
      "exploration/env_infos/final/reward_energy Min          -0.355326\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.184392\n",
      "exploration/env_infos/initial/reward_energy Std         0.109897\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0346794\n",
      "exploration/env_infos/initial/reward_energy Min        -0.367841\n",
      "exploration/env_infos/reward_energy Mean               -0.165589\n",
      "exploration/env_infos/reward_energy Std                 0.0997282\n",
      "exploration/env_infos/reward_energy Max                -0.0136825\n",
      "exploration/env_infos/reward_energy Min                -0.401459\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.00647823\n",
      "exploration/env_infos/final/end_effector_loc Std        0.250561\n",
      "exploration/env_infos/final/end_effector_loc Max        0.417808\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.369255\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.000362005\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00758064\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00967479\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0156418\n",
      "exploration/env_infos/end_effector_loc Mean             0.00333751\n",
      "exploration/env_infos/end_effector_loc Std              0.154675\n",
      "exploration/env_infos/end_effector_loc Max              0.417808\n",
      "exploration/env_infos/end_effector_loc Min             -0.369255\n",
      "evaluation/num steps total                           8000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.120225\n",
      "evaluation/Rewards Std                                  0.114706\n",
      "evaluation/Rewards Max                                  0.10838\n",
      "evaluation/Rewards Min                                 -0.824555\n",
      "evaluation/Returns Mean                                -2.40449\n",
      "evaluation/Returns Std                                  1.31461\n",
      "evaluation/Returns Max                                 -0.224426\n",
      "evaluation/Returns Min                                 -5.54946\n",
      "evaluation/Actions Mean                                -0.012673\n",
      "evaluation/Actions Std                                  0.0939277\n",
      "evaluation/Actions Max                                  0.474986\n",
      "evaluation/Actions Min                                 -0.631368\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.40449\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0359109\n",
      "evaluation/env_infos/final/reward_dist Std              0.126213\n",
      "evaluation/env_infos/final/reward_dist Max              0.759091\n",
      "evaluation/env_infos/final/reward_dist Min              4.1122e-70\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00473938\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00780024\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0308037\n",
      "evaluation/env_infos/initial/reward_dist Min            8.80153e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0778644\n",
      "evaluation/env_infos/reward_dist Std                    0.183512\n",
      "evaluation/env_infos/reward_dist Max                    0.993649\n",
      "evaluation/env_infos/reward_dist Min                    4.1122e-70\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.194139\n",
      "evaluation/env_infos/final/reward_energy Std            0.166278\n",
      "evaluation/env_infos/final/reward_energy Max           -0.011186\n",
      "evaluation/env_infos/final/reward_energy Min           -0.631476\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.138763\n",
      "evaluation/env_infos/initial/reward_energy Std          0.073323\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0173869\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.401362\n",
      "evaluation/env_infos/reward_energy Mean                -0.100189\n",
      "evaluation/env_infos/reward_energy Std                  0.0890407\n",
      "evaluation/env_infos/reward_energy Max                 -0.0037521\n",
      "evaluation/env_infos/reward_energy Min                 -0.631476\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0413599\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.33914\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.483354\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00209752\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00513709\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0132112\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0181754\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00788367\n",
      "evaluation/env_infos/end_effector_loc Std               0.191619\n",
      "evaluation/env_infos/end_effector_loc Max               0.483354\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00345626\n",
      "time/evaluation sampling (s)                            1.41541\n",
      "time/exploration sampling (s)                           0.128368\n",
      "time/logging (s)                                        0.0210347\n",
      "time/saving (s)                                         0.0292165\n",
      "time/training (s)                                      45.2144\n",
      "time/epoch (s)                                         46.8119\n",
      "time/total (s)                                        361.761\n",
      "Epoch                                                   7\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:31:44.198167 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 8 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1900\n",
      "trainer/QF1 Loss                                        0.00315309\n",
      "trainer/QF2 Loss                                        0.0186633\n",
      "trainer/Policy Loss                                     4.69278\n",
      "trainer/Q1 Predictions Mean                            -3.04336\n",
      "trainer/Q1 Predictions Std                              1.76928\n",
      "trainer/Q1 Predictions Max                             -0.101967\n",
      "trainer/Q1 Predictions Min                            -10.13\n",
      "trainer/Q2 Predictions Mean                            -3.15417\n",
      "trainer/Q2 Predictions Std                              1.78896\n",
      "trainer/Q2 Predictions Max                             -0.116019\n",
      "trainer/Q2 Predictions Min                            -10.1045\n",
      "trainer/Q Targets Mean                                 -3.05003\n",
      "trainer/Q Targets Std                                   1.77127\n",
      "trainer/Q Targets Max                                  -0.119584\n",
      "trainer/Q Targets Min                                 -10.2059\n",
      "trainer/Log Pis Mean                                    1.89864\n",
      "trainer/Log Pis Std                                     1.52933\n",
      "trainer/Log Pis Max                                     7.42613\n",
      "trainer/Log Pis Min                                    -2.84766\n",
      "trainer/Policy mu Mean                                  0.00674611\n",
      "trainer/Policy mu Std                                   0.796232\n",
      "trainer/Policy mu Max                                   2.85768\n",
      "trainer/Policy mu Min                                  -2.94379\n",
      "trainer/Policy log std Mean                            -1.96356\n",
      "trainer/Policy log std Std                              0.660782\n",
      "trainer/Policy log std Max                              0.0122131\n",
      "trainer/Policy log std Min                             -3.01445\n",
      "trainer/Alpha                                           0.0155872\n",
      "trainer/Alpha Loss                                     -0.421793\n",
      "exploration/num steps total                          1900\n",
      "exploration/num paths total                            95\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.120469\n",
      "exploration/Rewards Std                                 0.0981408\n",
      "exploration/Rewards Max                                 0.028481\n",
      "exploration/Rewards Min                                -0.542977\n",
      "exploration/Returns Mean                               -2.40938\n",
      "exploration/Returns Std                                 0.874631\n",
      "exploration/Returns Max                                -1.33071\n",
      "exploration/Returns Min                                -3.75844\n",
      "exploration/Actions Mean                               -0.00631211\n",
      "exploration/Actions Std                                 0.187973\n",
      "exploration/Actions Max                                 0.727745\n",
      "exploration/Actions Min                                -0.506436\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.40938\n",
      "exploration/env_infos/final/reward_dist Mean            0.000490911\n",
      "exploration/env_infos/final/reward_dist Std             0.0009812\n",
      "exploration/env_infos/final/reward_dist Max             0.00245331\n",
      "exploration/env_infos/final/reward_dist Min             1.22721e-35\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000109025\n",
      "exploration/env_infos/initial/reward_dist Std           0.000112665\n",
      "exploration/env_infos/initial/reward_dist Max           0.000274696\n",
      "exploration/env_infos/initial/reward_dist Min           1.33551e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0641289\n",
      "exploration/env_infos/reward_dist Std                   0.181745\n",
      "exploration/env_infos/reward_dist Max                   0.867845\n",
      "exploration/env_infos/reward_dist Min                   1.22721e-35\n",
      "exploration/env_infos/final/reward_energy Mean         -0.244852\n",
      "exploration/env_infos/final/reward_energy Std           0.0872895\n",
      "exploration/env_infos/final/reward_energy Max          -0.0963312\n",
      "exploration/env_infos/final/reward_energy Min          -0.369723\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.454919\n",
      "exploration/env_infos/initial/reward_energy Std         0.176031\n",
      "exploration/env_infos/initial/reward_energy Max        -0.249878\n",
      "exploration/env_infos/initial/reward_energy Min        -0.728648\n",
      "exploration/env_infos/reward_energy Mean               -0.224167\n",
      "exploration/env_infos/reward_energy Std                 0.143166\n",
      "exploration/env_infos/reward_energy Max                -0.0209205\n",
      "exploration/env_infos/reward_energy Min                -0.728648\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0597196\n",
      "exploration/env_infos/final/end_effector_loc Std        0.401409\n",
      "exploration/env_infos/final/end_effector_loc Max        0.6004\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.768194\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00411278\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0167484\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0363872\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0164456\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0123202\n",
      "exploration/env_infos/end_effector_loc Std              0.265136\n",
      "exploration/env_infos/end_effector_loc Max              0.6004\n",
      "exploration/env_infos/end_effector_loc Min             -0.768194\n",
      "evaluation/num steps total                           9000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.121668\n",
      "evaluation/Rewards Std                                  0.117023\n",
      "evaluation/Rewards Max                                  0.104391\n",
      "evaluation/Rewards Min                                 -0.792242\n",
      "evaluation/Returns Mean                                -2.43336\n",
      "evaluation/Returns Std                                  1.28797\n",
      "evaluation/Returns Max                                 -0.0298396\n",
      "evaluation/Returns Min                                 -5.82313\n",
      "evaluation/Actions Mean                                -0.0164039\n",
      "evaluation/Actions Std                                  0.0997085\n",
      "evaluation/Actions Max                                  0.528124\n",
      "evaluation/Actions Min                                 -0.608067\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.43336\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0783531\n",
      "evaluation/env_infos/final/reward_dist Std              0.210993\n",
      "evaluation/env_infos/final/reward_dist Max              0.959972\n",
      "evaluation/env_infos/final/reward_dist Min              5.13075e-81\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00538389\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00752556\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0352487\n",
      "evaluation/env_infos/initial/reward_dist Min            1.07641e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0828767\n",
      "evaluation/env_infos/reward_dist Std                    0.177923\n",
      "evaluation/env_infos/reward_dist Max                    0.98926\n",
      "evaluation/env_infos/reward_dist Min                    5.13075e-81\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.144679\n",
      "evaluation/env_infos/final/reward_energy Std            0.0979144\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00811962\n",
      "evaluation/env_infos/final/reward_energy Min           -0.464694\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.233305\n",
      "evaluation/env_infos/initial/reward_energy Std          0.141318\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0240861\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.632523\n",
      "evaluation/env_infos/reward_energy Mean                -0.112431\n",
      "evaluation/env_infos/reward_energy Std                  0.08821\n",
      "evaluation/env_infos/reward_energy Max                 -0.00260803\n",
      "evaluation/env_infos/reward_energy Min                 -0.632523\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0991857\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.393903\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.771833\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00200743\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00943255\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0264062\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0304034\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0129488\n",
      "evaluation/env_infos/end_effector_loc Std               0.236235\n",
      "evaluation/env_infos/end_effector_loc Max               0.771833\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00310569\n",
      "time/evaluation sampling (s)                            0.985649\n",
      "time/exploration sampling (s)                           0.136818\n",
      "time/logging (s)                                        0.0185161\n",
      "time/saving (s)                                         0.0318406\n",
      "time/training (s)                                      44.6417\n",
      "time/epoch (s)                                         45.8176\n",
      "time/total (s)                                        407.717\n",
      "Epoch                                                   8\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:32:30.837165 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 9 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00445208\n",
      "trainer/QF2 Loss                                         0.00415612\n",
      "trainer/Policy Loss                                      4.46808\n",
      "trainer/Q1 Predictions Mean                             -2.91392\n",
      "trainer/Q1 Predictions Std                               1.97332\n",
      "trainer/Q1 Predictions Max                              -0.0731378\n",
      "trainer/Q1 Predictions Min                             -11.0758\n",
      "trainer/Q2 Predictions Mean                             -2.88326\n",
      "trainer/Q2 Predictions Std                               1.95605\n",
      "trainer/Q2 Predictions Max                              -0.10773\n",
      "trainer/Q2 Predictions Min                             -10.884\n",
      "trainer/Q Targets Mean                                  -2.88772\n",
      "trainer/Q Targets Std                                    1.95442\n",
      "trainer/Q Targets Max                                   -0.111562\n",
      "trainer/Q Targets Min                                  -11.063\n",
      "trainer/Log Pis Mean                                     1.88253\n",
      "trainer/Log Pis Std                                      1.37722\n",
      "trainer/Log Pis Max                                      7.06818\n",
      "trainer/Log Pis Min                                     -5.68848\n",
      "trainer/Policy mu Mean                                  -0.0142511\n",
      "trainer/Policy mu Std                                    0.634605\n",
      "trainer/Policy mu Max                                    2.53393\n",
      "trainer/Policy mu Min                                   -2.73537\n",
      "trainer/Policy log std Mean                             -2.06168\n",
      "trainer/Policy log std Std                               0.583922\n",
      "trainer/Policy log std Max                              -0.228722\n",
      "trainer/Policy log std Min                              -3.07149\n",
      "trainer/Alpha                                            0.015224\n",
      "trainer/Alpha Loss                                      -0.491583\n",
      "exploration/num steps total                           2000\n",
      "exploration/num paths total                            100\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117089\n",
      "exploration/Rewards Std                                  0.0956642\n",
      "exploration/Rewards Max                                  0.0709816\n",
      "exploration/Rewards Min                                 -0.41412\n",
      "exploration/Returns Mean                                -2.34178\n",
      "exploration/Returns Std                                  0.93993\n",
      "exploration/Returns Max                                 -1.16824\n",
      "exploration/Returns Min                                 -3.80301\n",
      "exploration/Actions Mean                                -0.000832796\n",
      "exploration/Actions Std                                  0.16664\n",
      "exploration/Actions Max                                  0.473452\n",
      "exploration/Actions Min                                 -0.678519\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.34178\n",
      "exploration/env_infos/final/reward_dist Mean             0.0788667\n",
      "exploration/env_infos/final/reward_dist Std              0.157733\n",
      "exploration/env_infos/final/reward_dist Max              0.394332\n",
      "exploration/env_infos/final/reward_dist Min              3.88658e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0134992\n",
      "exploration/env_infos/initial/reward_dist Std            0.0267896\n",
      "exploration/env_infos/initial/reward_dist Max            0.0670783\n",
      "exploration/env_infos/initial/reward_dist Min            2.26571e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.132425\n",
      "exploration/env_infos/reward_dist Std                    0.205354\n",
      "exploration/env_infos/reward_dist Max                    0.8765\n",
      "exploration/env_infos/reward_dist Min                    9.70504e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.299579\n",
      "exploration/env_infos/final/reward_energy Std            0.21723\n",
      "exploration/env_infos/final/reward_energy Max           -0.0449317\n",
      "exploration/env_infos/final/reward_energy Min           -0.682126\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.41637\n",
      "exploration/env_infos/initial/reward_energy Std          0.21694\n",
      "exploration/env_infos/initial/reward_energy Max         -0.141798\n",
      "exploration/env_infos/initial/reward_energy Min         -0.799736\n",
      "exploration/env_infos/reward_energy Mean                -0.196532\n",
      "exploration/env_infos/reward_energy Std                  0.130056\n",
      "exploration/env_infos/reward_energy Max                 -0.0124541\n",
      "exploration/env_infos/reward_energy Min                 -0.799736\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0561878\n",
      "exploration/env_infos/final/end_effector_loc Std         0.39595\n",
      "exploration/env_infos/final/end_effector_loc Max         0.499235\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.617098\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00108787\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0165635\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0228931\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0327848\n",
      "exploration/env_infos/end_effector_loc Mean              0.0335449\n",
      "exploration/env_infos/end_effector_loc Std               0.25367\n",
      "exploration/env_infos/end_effector_loc Max               0.499235\n",
      "exploration/env_infos/end_effector_loc Min              -0.617098\n",
      "evaluation/num steps total                           10000\n",
      "evaluation/num paths total                             500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.109354\n",
      "evaluation/Rewards Std                                   0.080017\n",
      "evaluation/Rewards Max                                   0.0688032\n",
      "evaluation/Rewards Min                                  -0.717519\n",
      "evaluation/Returns Mean                                 -2.18708\n",
      "evaluation/Returns Std                                   0.944662\n",
      "evaluation/Returns Max                                  -0.131174\n",
      "evaluation/Returns Min                                  -5.55097\n",
      "evaluation/Actions Mean                                 -0.0136973\n",
      "evaluation/Actions Std                                   0.0880017\n",
      "evaluation/Actions Max                                   0.735795\n",
      "evaluation/Actions Min                                  -0.797453\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.18708\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0363625\n",
      "evaluation/env_infos/final/reward_dist Std               0.118915\n",
      "evaluation/env_infos/final/reward_dist Max               0.610875\n",
      "evaluation/env_infos/final/reward_dist Min               4.08377e-65\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00587902\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0188641\n",
      "evaluation/env_infos/initial/reward_dist Max             0.131242\n",
      "evaluation/env_infos/initial/reward_dist Min             1.76679e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0692563\n",
      "evaluation/env_infos/reward_dist Std                     0.171646\n",
      "evaluation/env_infos/reward_dist Max                     0.9802\n",
      "evaluation/env_infos/reward_dist Min                     4.08377e-65\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.139345\n",
      "evaluation/env_infos/final/reward_energy Std             0.122126\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0109747\n",
      "evaluation/env_infos/final/reward_energy Min            -0.575587\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.212727\n",
      "evaluation/env_infos/initial/reward_energy Std           0.185945\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.030087\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.04139\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0912323\n",
      "evaluation/env_infos/reward_energy Std                   0.086836\n",
      "evaluation/env_infos/reward_energy Max                  -0.00263502\n",
      "evaluation/env_infos/reward_energy Min                  -1.04139\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0561189\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.333355\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.740545\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00146743\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00988088\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0367897\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0398726\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0039207\n",
      "evaluation/env_infos/end_effector_loc Std                0.190851\n",
      "evaluation/env_infos/end_effector_loc Max                0.740545\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00299985\n",
      "time/evaluation sampling (s)                             0.976468\n",
      "time/exploration sampling (s)                            0.118854\n",
      "time/logging (s)                                         0.0203661\n",
      "time/saving (s)                                          0.0399637\n",
      "time/training (s)                                       45.3413\n",
      "time/epoch (s)                                          46.4999\n",
      "time/total (s)                                         454.357\n",
      "Epoch                                                    9\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:33:16.836145 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 10 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00612566\n",
      "trainer/QF2 Loss                                         0.00450927\n",
      "trainer/Policy Loss                                      4.73966\n",
      "trainer/Q1 Predictions Mean                             -2.95715\n",
      "trainer/Q1 Predictions Std                               1.90926\n",
      "trainer/Q1 Predictions Max                              -0.161132\n",
      "trainer/Q1 Predictions Min                              -9.46225\n",
      "trainer/Q2 Predictions Mean                             -2.95376\n",
      "trainer/Q2 Predictions Std                               1.9247\n",
      "trainer/Q2 Predictions Max                              -0.174487\n",
      "trainer/Q2 Predictions Min                              -9.79856\n",
      "trainer/Q Targets Mean                                  -2.94762\n",
      "trainer/Q Targets Std                                    1.92437\n",
      "trainer/Q Targets Max                                   -0.153345\n",
      "trainer/Q Targets Min                                   -9.82373\n",
      "trainer/Log Pis Mean                                     2.04306\n",
      "trainer/Log Pis Std                                      1.46884\n",
      "trainer/Log Pis Max                                      9.24999\n",
      "trainer/Log Pis Min                                     -2.4602\n",
      "trainer/Policy mu Mean                                  -0.0136412\n",
      "trainer/Policy mu Std                                    0.706992\n",
      "trainer/Policy mu Max                                    2.57877\n",
      "trainer/Policy mu Min                                   -3.37981\n",
      "trainer/Policy log std Mean                             -2.08289\n",
      "trainer/Policy log std Std                               0.592796\n",
      "trainer/Policy log std Max                              -0.181075\n",
      "trainer/Policy log std Min                              -3.24415\n",
      "trainer/Alpha                                            0.0158679\n",
      "trainer/Alpha Loss                                       0.178438\n",
      "exploration/num steps total                           2100\n",
      "exploration/num paths total                            105\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.211701\n",
      "exploration/Rewards Std                                  0.0934085\n",
      "exploration/Rewards Max                                 -0.0372659\n",
      "exploration/Rewards Min                                 -0.556664\n",
      "exploration/Returns Mean                                -4.23401\n",
      "exploration/Returns Std                                  1.21427\n",
      "exploration/Returns Max                                 -2.7763\n",
      "exploration/Returns Min                                 -6.18214\n",
      "exploration/Actions Mean                                 0.00843026\n",
      "exploration/Actions Std                                  0.175229\n",
      "exploration/Actions Max                                  0.747791\n",
      "exploration/Actions Min                                 -0.840261\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -4.23401\n",
      "exploration/env_infos/final/reward_dist Mean             5.10512e-08\n",
      "exploration/env_infos/final/reward_dist Std              1.0208e-07\n",
      "exploration/env_infos/final/reward_dist Max              2.55211e-07\n",
      "exploration/env_infos/final/reward_dist Min              5.44369e-44\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000241264\n",
      "exploration/env_infos/initial/reward_dist Std            0.000357503\n",
      "exploration/env_infos/initial/reward_dist Max            0.000950018\n",
      "exploration/env_infos/initial/reward_dist Min            7.80915e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0147121\n",
      "exploration/env_infos/reward_dist Std                    0.0792127\n",
      "exploration/env_infos/reward_dist Max                    0.642368\n",
      "exploration/env_infos/reward_dist Min                    2.06964e-45\n",
      "exploration/env_infos/final/reward_energy Mean          -0.233899\n",
      "exploration/env_infos/final/reward_energy Std            0.046255\n",
      "exploration/env_infos/final/reward_energy Max           -0.189089\n",
      "exploration/env_infos/final/reward_energy Min           -0.29248\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.180868\n",
      "exploration/env_infos/initial/reward_energy Std          0.0902192\n",
      "exploration/env_infos/initial/reward_energy Max         -0.04204\n",
      "exploration/env_infos/initial/reward_energy Min         -0.322384\n",
      "exploration/env_infos/reward_energy Mean                -0.193262\n",
      "exploration/env_infos/reward_energy Std                  0.15557\n",
      "exploration/env_infos/reward_energy Max                 -0.0153834\n",
      "exploration/env_infos/reward_energy Min                 -0.944226\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00325038\n",
      "exploration/env_infos/final/end_effector_loc Std         0.420076\n",
      "exploration/env_infos/final/end_effector_loc Max         0.494579\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.991038\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000785262\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00710275\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00849682\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0159193\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00370282\n",
      "exploration/env_infos/end_effector_loc Std               0.287445\n",
      "exploration/env_infos/end_effector_loc Max               0.503013\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           11000\n",
      "evaluation/num paths total                             550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.120884\n",
      "evaluation/Rewards Std                                   0.0901638\n",
      "evaluation/Rewards Max                                   0.108132\n",
      "evaluation/Rewards Min                                  -0.718647\n",
      "evaluation/Returns Mean                                 -2.41767\n",
      "evaluation/Returns Std                                   1.2018\n",
      "evaluation/Returns Max                                   0.582752\n",
      "evaluation/Returns Min                                  -5.47631\n",
      "evaluation/Actions Mean                                 -0.00884906\n",
      "evaluation/Actions Std                                   0.0849369\n",
      "evaluation/Actions Max                                   0.60225\n",
      "evaluation/Actions Min                                  -0.58994\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.41767\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0513876\n",
      "evaluation/env_infos/final/reward_dist Std               0.162926\n",
      "evaluation/env_infos/final/reward_dist Max               0.988065\n",
      "evaluation/env_infos/final/reward_dist Min               6.14719e-68\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00744284\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0156932\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0817627\n",
      "evaluation/env_infos/initial/reward_dist Min             1.13791e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0557876\n",
      "evaluation/env_infos/reward_dist Std                     0.159761\n",
      "evaluation/env_infos/reward_dist Max                     0.988065\n",
      "evaluation/env_infos/reward_dist Min                     6.14719e-68\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.119466\n",
      "evaluation/env_infos/final/reward_energy Std             0.0836574\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0171798\n",
      "evaluation/env_infos/final/reward_energy Min            -0.433978\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.233227\n",
      "evaluation/env_infos/initial/reward_energy Std           0.165023\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0168539\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.768105\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0881556\n",
      "evaluation/env_infos/reward_energy Std                   0.0825455\n",
      "evaluation/env_infos/reward_energy Max                  -0.000673391\n",
      "evaluation/env_infos/reward_energy Min                  -0.768105\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0366146\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.332479\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.641184\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000604364\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0100831\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0301125\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.029497\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00377817\n",
      "evaluation/env_infos/end_effector_loc Std                0.206309\n",
      "evaluation/env_infos/end_effector_loc Max                0.641184\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00298404\n",
      "time/evaluation sampling (s)                             0.999527\n",
      "time/exploration sampling (s)                            0.120004\n",
      "time/logging (s)                                         0.0196417\n",
      "time/saving (s)                                          0.0301854\n",
      "time/training (s)                                       44.6641\n",
      "time/epoch (s)                                          45.8364\n",
      "time/total (s)                                         500.355\n",
      "Epoch                                                   10\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:05.145095 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 11 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00576653\n",
      "trainer/QF2 Loss                                         0.00529006\n",
      "trainer/Policy Loss                                      4.54779\n",
      "trainer/Q1 Predictions Mean                             -2.77117\n",
      "trainer/Q1 Predictions Std                               1.86587\n",
      "trainer/Q1 Predictions Max                              -0.130918\n",
      "trainer/Q1 Predictions Min                             -11.2886\n",
      "trainer/Q2 Predictions Mean                             -2.74842\n",
      "trainer/Q2 Predictions Std                               1.85531\n",
      "trainer/Q2 Predictions Max                              -0.108681\n",
      "trainer/Q2 Predictions Min                             -11.2334\n",
      "trainer/Q Targets Mean                                  -2.73754\n",
      "trainer/Q Targets Std                                    1.84606\n",
      "trainer/Q Targets Max                                   -0.0946199\n",
      "trainer/Q Targets Min                                  -11.3446\n",
      "trainer/Log Pis Mean                                     2.07449\n",
      "trainer/Log Pis Std                                      1.58767\n",
      "trainer/Log Pis Max                                      9.11468\n",
      "trainer/Log Pis Min                                     -2.71209\n",
      "trainer/Policy mu Mean                                  -0.0443209\n",
      "trainer/Policy mu Std                                    0.793992\n",
      "trainer/Policy mu Max                                    3.44733\n",
      "trainer/Policy mu Min                                   -3.32761\n",
      "trainer/Policy log std Mean                             -2.00017\n",
      "trainer/Policy log std Std                               0.599633\n",
      "trainer/Policy log std Max                              -0.0475062\n",
      "trainer/Policy log std Min                              -2.97489\n",
      "trainer/Alpha                                            0.016235\n",
      "trainer/Alpha Loss                                       0.306914\n",
      "exploration/num steps total                           2200\n",
      "exploration/num paths total                            110\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.183434\n",
      "exploration/Rewards Std                                  0.109815\n",
      "exploration/Rewards Max                                  0.0568666\n",
      "exploration/Rewards Min                                 -0.583449\n",
      "exploration/Returns Mean                                -3.66867\n",
      "exploration/Returns Std                                  0.671211\n",
      "exploration/Returns Max                                 -2.63071\n",
      "exploration/Returns Min                                 -4.27238\n",
      "exploration/Actions Mean                                -0.00890265\n",
      "exploration/Actions Std                                  0.113645\n",
      "exploration/Actions Max                                  0.288116\n",
      "exploration/Actions Min                                 -0.30274\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.66867\n",
      "exploration/env_infos/final/reward_dist Mean             0.033112\n",
      "exploration/env_infos/final/reward_dist Std              0.0546735\n",
      "exploration/env_infos/final/reward_dist Max              0.140758\n",
      "exploration/env_infos/final/reward_dist Min              1.42662e-40\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00919615\n",
      "exploration/env_infos/initial/reward_dist Std            0.0110347\n",
      "exploration/env_infos/initial/reward_dist Max            0.027808\n",
      "exploration/env_infos/initial/reward_dist Min            6.24952e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0686175\n",
      "exploration/env_infos/reward_dist Std                    0.114971\n",
      "exploration/env_infos/reward_dist Max                    0.435792\n",
      "exploration/env_infos/reward_dist Min                    1.42662e-40\n",
      "exploration/env_infos/final/reward_energy Mean          -0.178713\n",
      "exploration/env_infos/final/reward_energy Std            0.0543983\n",
      "exploration/env_infos/final/reward_energy Max           -0.0905265\n",
      "exploration/env_infos/final/reward_energy Min           -0.243733\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.215577\n",
      "exploration/env_infos/initial/reward_energy Std          0.0491593\n",
      "exploration/env_infos/initial/reward_energy Max         -0.155156\n",
      "exploration/env_infos/initial/reward_energy Min         -0.30568\n",
      "exploration/env_infos/reward_energy Mean                -0.141997\n",
      "exploration/env_infos/reward_energy Std                  0.0763271\n",
      "exploration/env_infos/reward_energy Max                 -0.0265819\n",
      "exploration/env_infos/reward_energy Min                 -0.360992\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.11665\n",
      "exploration/env_infos/final/end_effector_loc Std         0.385239\n",
      "exploration/env_infos/final/end_effector_loc Max         0.362007\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.842267\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00209792\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00753068\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0104526\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0149354\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0550128\n",
      "exploration/env_infos/end_effector_loc Std               0.206879\n",
      "exploration/env_infos/end_effector_loc Max               0.362007\n",
      "exploration/env_infos/end_effector_loc Min              -0.842267\n",
      "evaluation/num steps total                           12000\n",
      "evaluation/num paths total                             600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.107939\n",
      "evaluation/Rewards Std                                   0.0729217\n",
      "evaluation/Rewards Max                                   0.0645711\n",
      "evaluation/Rewards Min                                  -0.62767\n",
      "evaluation/Returns Mean                                 -2.15878\n",
      "evaluation/Returns Std                                   0.967839\n",
      "evaluation/Returns Max                                  -0.598675\n",
      "evaluation/Returns Min                                  -5.50918\n",
      "evaluation/Actions Mean                                  0.000196849\n",
      "evaluation/Actions Std                                   0.0938852\n",
      "evaluation/Actions Max                                   0.474783\n",
      "evaluation/Actions Min                                  -0.975392\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.15878\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0126502\n",
      "evaluation/env_infos/final/reward_dist Std               0.0580807\n",
      "evaluation/env_infos/final/reward_dist Max               0.401314\n",
      "evaluation/env_infos/final/reward_dist Min               9.40453e-47\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00976908\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0329858\n",
      "evaluation/env_infos/initial/reward_dist Max             0.22407\n",
      "evaluation/env_infos/initial/reward_dist Min             2.34212e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0557189\n",
      "evaluation/env_infos/reward_dist Std                     0.142974\n",
      "evaluation/env_infos/reward_dist Max                     0.972434\n",
      "evaluation/env_infos/reward_dist Min                     9.40453e-47\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.118539\n",
      "evaluation/env_infos/final/reward_energy Std             0.0847105\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0212853\n",
      "evaluation/env_infos/final/reward_energy Min            -0.412104\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.248573\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255942\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0150159\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.28634\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0874845\n",
      "evaluation/env_infos/reward_energy Std                   0.099877\n",
      "evaluation/env_infos/reward_energy Max                  -0.00165591\n",
      "evaluation/env_infos/reward_energy Min                  -1.28634\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0446976\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27984\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.662738\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.964706\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00106255\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0125694\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0237391\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0487696\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0179977\n",
      "evaluation/env_infos/end_effector_loc Std                0.175489\n",
      "evaluation/env_infos/end_effector_loc Max                0.662738\n",
      "evaluation/env_infos/end_effector_loc Min               -0.964706\n",
      "time/data storing (s)                                    0.0029602\n",
      "time/evaluation sampling (s)                             0.995818\n",
      "time/exploration sampling (s)                            0.132013\n",
      "time/logging (s)                                         0.0240012\n",
      "time/saving (s)                                          0.0349898\n",
      "time/training (s)                                       46.9489\n",
      "time/epoch (s)                                          48.1387\n",
      "time/total (s)                                         548.668\n",
      "Epoch                                                   11\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:52.614512 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 12 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00317505\n",
      "trainer/QF2 Loss                                         0.00356741\n",
      "trainer/Policy Loss                                      4.224\n",
      "trainer/Q1 Predictions Mean                             -2.44865\n",
      "trainer/Q1 Predictions Std                               1.56015\n",
      "trainer/Q1 Predictions Max                              -0.0263941\n",
      "trainer/Q1 Predictions Min                              -9.22536\n",
      "trainer/Q2 Predictions Mean                             -2.45026\n",
      "trainer/Q2 Predictions Std                               1.55578\n",
      "trainer/Q2 Predictions Max                              -0.0634723\n",
      "trainer/Q2 Predictions Min                              -9.20636\n",
      "trainer/Q Targets Mean                                  -2.46091\n",
      "trainer/Q Targets Std                                    1.56521\n",
      "trainer/Q Targets Max                                   -0.0572312\n",
      "trainer/Q Targets Min                                   -9.26282\n",
      "trainer/Log Pis Mean                                     2.00327\n",
      "trainer/Log Pis Std                                      1.21642\n",
      "trainer/Log Pis Max                                      7.88299\n",
      "trainer/Log Pis Min                                     -2.20284\n",
      "trainer/Policy mu Mean                                  -0.00195495\n",
      "trainer/Policy mu Std                                    0.537883\n",
      "trainer/Policy mu Max                                    2.93236\n",
      "trainer/Policy mu Min                                   -2.59237\n",
      "trainer/Policy log std Mean                             -2.15926\n",
      "trainer/Policy log std Std                               0.529236\n",
      "trainer/Policy log std Max                              -0.392954\n",
      "trainer/Policy log std Min                              -3.06677\n",
      "trainer/Alpha                                            0.0164994\n",
      "trainer/Alpha Loss                                       0.0134078\n",
      "exploration/num steps total                           2300\n",
      "exploration/num paths total                            115\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.131759\n",
      "exploration/Rewards Std                                  0.0715171\n",
      "exploration/Rewards Max                                  0.0031933\n",
      "exploration/Rewards Min                                 -0.4202\n",
      "exploration/Returns Mean                                -2.63519\n",
      "exploration/Returns Std                                  0.544396\n",
      "exploration/Returns Max                                 -1.78086\n",
      "exploration/Returns Min                                 -3.45244\n",
      "exploration/Actions Mean                                -0.00353127\n",
      "exploration/Actions Std                                  0.122522\n",
      "exploration/Actions Max                                  0.581829\n",
      "exploration/Actions Min                                 -0.349097\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.63519\n",
      "exploration/env_infos/final/reward_dist Mean             0.004765\n",
      "exploration/env_infos/final/reward_dist Std              0.00936162\n",
      "exploration/env_infos/final/reward_dist Max              0.0234869\n",
      "exploration/env_infos/final/reward_dist Min              8.48965e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00470963\n",
      "exploration/env_infos/initial/reward_dist Std            0.00768251\n",
      "exploration/env_infos/initial/reward_dist Max            0.0199204\n",
      "exploration/env_infos/initial/reward_dist Min            6.01152e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0420192\n",
      "exploration/env_infos/reward_dist Std                    0.113035\n",
      "exploration/env_infos/reward_dist Max                    0.570829\n",
      "exploration/env_infos/reward_dist Min                    8.48965e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.129404\n",
      "exploration/env_infos/final/reward_energy Std            0.0663869\n",
      "exploration/env_infos/final/reward_energy Max           -0.0778513\n",
      "exploration/env_infos/final/reward_energy Min           -0.259631\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.214067\n",
      "exploration/env_infos/initial/reward_energy Std          0.120652\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0838295\n",
      "exploration/env_infos/initial/reward_energy Min         -0.411506\n",
      "exploration/env_infos/reward_energy Mean                -0.143218\n",
      "exploration/env_infos/reward_energy Std                  0.097657\n",
      "exploration/env_infos/reward_energy Max                 -0.00381842\n",
      "exploration/env_infos/reward_energy Min                 -0.589576\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0781184\n",
      "exploration/env_infos/final/end_effector_loc Std         0.267225\n",
      "exploration/env_infos/final/end_effector_loc Max         0.335246\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.405207\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000212404\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00868514\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.014911\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0141776\n",
      "exploration/env_infos/end_effector_loc Mean             -0.047834\n",
      "exploration/env_infos/end_effector_loc Std               0.179718\n",
      "exploration/env_infos/end_effector_loc Max               0.335246\n",
      "exploration/env_infos/end_effector_loc Min              -0.493902\n",
      "evaluation/num steps total                           13000\n",
      "evaluation/num paths total                             650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.11331\n",
      "evaluation/Rewards Std                                   0.0805047\n",
      "evaluation/Rewards Max                                   0.089257\n",
      "evaluation/Rewards Min                                  -0.596522\n",
      "evaluation/Returns Mean                                 -2.26621\n",
      "evaluation/Returns Std                                   1.08585\n",
      "evaluation/Returns Max                                   0.051371\n",
      "evaluation/Returns Min                                  -4.6357\n",
      "evaluation/Actions Mean                                 -0.00543072\n",
      "evaluation/Actions Std                                   0.080599\n",
      "evaluation/Actions Max                                   0.58027\n",
      "evaluation/Actions Min                                  -0.531828\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.26621\n",
      "evaluation/env_infos/final/reward_dist Mean              0.016623\n",
      "evaluation/env_infos/final/reward_dist Std               0.0590321\n",
      "evaluation/env_infos/final/reward_dist Max               0.394556\n",
      "evaluation/env_infos/final/reward_dist Min               3.77082e-97\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0056598\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00928809\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0354464\n",
      "evaluation/env_infos/initial/reward_dist Min             1.17022e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.054205\n",
      "evaluation/env_infos/reward_dist Std                     0.142844\n",
      "evaluation/env_infos/reward_dist Max                     0.994045\n",
      "evaluation/env_infos/reward_dist Min                     3.77082e-97\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.11998\n",
      "evaluation/env_infos/final/reward_energy Std             0.0784046\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0183616\n",
      "evaluation/env_infos/final/reward_energy Min            -0.349532\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.217207\n",
      "evaluation/env_infos/initial/reward_energy Std           0.132593\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0133179\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.616938\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0876509\n",
      "evaluation/env_infos/reward_energy Std                   0.0732715\n",
      "evaluation/env_infos/reward_energy Max                  -0.00223285\n",
      "evaluation/env_infos/reward_energy Min                  -0.616938\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0213813\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.364162\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.750014\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000851428\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00895683\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0290135\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0265914\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0184845\n",
      "evaluation/env_infos/end_effector_loc Std                0.211441\n",
      "evaluation/env_infos/end_effector_loc Max                0.750014\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00287573\n",
      "time/evaluation sampling (s)                             1.51933\n",
      "time/exploration sampling (s)                            0.120694\n",
      "time/logging (s)                                         0.0205859\n",
      "time/saving (s)                                          0.0283214\n",
      "time/training (s)                                       45.4317\n",
      "time/epoch (s)                                          47.1235\n",
      "time/total (s)                                         596.132\n",
      "Epoch                                                   12\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:35:40.069844 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 13 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00537421\n",
      "trainer/QF2 Loss                                         0.00195803\n",
      "trainer/Policy Loss                                      4.03001\n",
      "trainer/Q1 Predictions Mean                             -2.32261\n",
      "trainer/Q1 Predictions Std                               1.358\n",
      "trainer/Q1 Predictions Max                              -0.0723802\n",
      "trainer/Q1 Predictions Min                              -8.56189\n",
      "trainer/Q2 Predictions Mean                             -2.35368\n",
      "trainer/Q2 Predictions Std                               1.36607\n",
      "trainer/Q2 Predictions Max                              -0.094653\n",
      "trainer/Q2 Predictions Min                              -8.55066\n",
      "trainer/Q Targets Mean                                  -2.36893\n",
      "trainer/Q Targets Std                                    1.37124\n",
      "trainer/Q Targets Max                                   -0.119584\n",
      "trainer/Q Targets Min                                   -8.51335\n",
      "trainer/Log Pis Mean                                     1.88783\n",
      "trainer/Log Pis Std                                      1.27279\n",
      "trainer/Log Pis Max                                      5.90759\n",
      "trainer/Log Pis Min                                     -4.09385\n",
      "trainer/Policy mu Mean                                   0.00487049\n",
      "trainer/Policy mu Std                                    0.467628\n",
      "trainer/Policy mu Max                                    2.49156\n",
      "trainer/Policy mu Min                                   -2.47192\n",
      "trainer/Policy log std Mean                             -2.18588\n",
      "trainer/Policy log std Std                               0.553875\n",
      "trainer/Policy log std Max                              -0.303618\n",
      "trainer/Policy log std Min                              -3.17569\n",
      "trainer/Alpha                                            0.0167315\n",
      "trainer/Alpha Loss                                      -0.458823\n",
      "exploration/num steps total                           2400\n",
      "exploration/num paths total                            120\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.113472\n",
      "exploration/Rewards Std                                  0.132479\n",
      "exploration/Rewards Max                                  0.10711\n",
      "exploration/Rewards Min                                 -0.519395\n",
      "exploration/Returns Mean                                -2.26944\n",
      "exploration/Returns Std                                  2.00007\n",
      "exploration/Returns Max                                 -0.609371\n",
      "exploration/Returns Min                                 -6.13734\n",
      "exploration/Actions Mean                                -0.0153735\n",
      "exploration/Actions Std                                  0.158891\n",
      "exploration/Actions Max                                  0.35151\n",
      "exploration/Actions Min                                 -0.490659\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.26944\n",
      "exploration/env_infos/final/reward_dist Mean             0.0238925\n",
      "exploration/env_infos/final/reward_dist Std              0.044568\n",
      "exploration/env_infos/final/reward_dist Max              0.112911\n",
      "exploration/env_infos/final/reward_dist Min              2.22024e-44\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00283851\n",
      "exploration/env_infos/initial/reward_dist Std            0.00434978\n",
      "exploration/env_infos/initial/reward_dist Max            0.0113688\n",
      "exploration/env_infos/initial/reward_dist Min            7.41113e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.237012\n",
      "exploration/env_infos/reward_dist Std                    0.321921\n",
      "exploration/env_infos/reward_dist Max                    0.931536\n",
      "exploration/env_infos/reward_dist Min                    2.22024e-44\n",
      "exploration/env_infos/final/reward_energy Mean          -0.31262\n",
      "exploration/env_infos/final/reward_energy Std            0.167229\n",
      "exploration/env_infos/final/reward_energy Max           -0.123241\n",
      "exploration/env_infos/final/reward_energy Min           -0.589717\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.34263\n",
      "exploration/env_infos/initial/reward_energy Std          0.104192\n",
      "exploration/env_infos/initial/reward_energy Max         -0.190269\n",
      "exploration/env_infos/initial/reward_energy Min         -0.488087\n",
      "exploration/env_infos/reward_energy Mean                -0.196466\n",
      "exploration/env_infos/reward_energy Std                  0.111207\n",
      "exploration/env_infos/reward_energy Max                 -0.0182278\n",
      "exploration/env_infos/reward_energy Min                 -0.589717\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0682677\n",
      "exploration/env_infos/final/end_effector_loc Std         0.278402\n",
      "exploration/env_infos/final/end_effector_loc Max         0.354944\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.60749\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0023988\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0124322\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0175755\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0227185\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0125012\n",
      "exploration/env_infos/end_effector_loc Std               0.197948\n",
      "exploration/env_infos/end_effector_loc Max               0.395989\n",
      "exploration/env_infos/end_effector_loc Min              -0.60749\n",
      "evaluation/num steps total                           14000\n",
      "evaluation/num paths total                             700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.141833\n",
      "evaluation/Rewards Std                                   0.139444\n",
      "evaluation/Rewards Max                                   0.113222\n",
      "evaluation/Rewards Min                                  -0.852967\n",
      "evaluation/Returns Mean                                 -2.83665\n",
      "evaluation/Returns Std                                   1.7884\n",
      "evaluation/Returns Max                                  -0.407308\n",
      "evaluation/Returns Min                                  -7.11624\n",
      "evaluation/Actions Mean                                 -0.0176875\n",
      "evaluation/Actions Std                                   0.112689\n",
      "evaluation/Actions Max                                   0.460985\n",
      "evaluation/Actions Min                                  -0.965456\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.83665\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0410933\n",
      "evaluation/env_infos/final/reward_dist Std               0.120438\n",
      "evaluation/env_infos/final/reward_dist Max               0.64469\n",
      "evaluation/env_infos/final/reward_dist Min               3.17996e-71\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00902278\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0176072\n",
      "evaluation/env_infos/initial/reward_dist Max             0.106044\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0219e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0799971\n",
      "evaluation/env_infos/reward_dist Std                     0.17262\n",
      "evaluation/env_infos/reward_dist Max                     0.993073\n",
      "evaluation/env_infos/reward_dist Min                     3.17996e-71\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.172793\n",
      "evaluation/env_infos/final/reward_energy Std             0.176303\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00893808\n",
      "evaluation/env_infos/final/reward_energy Min            -0.716867\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.291787\n",
      "evaluation/env_infos/initial/reward_energy Std           0.252017\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0272597\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.23423\n",
      "evaluation/env_infos/reward_energy Mean                 -0.104271\n",
      "evaluation/env_infos/reward_energy Std                   0.123089\n",
      "evaluation/env_infos/reward_energy Max                  -0.00196708\n",
      "evaluation/env_infos/reward_energy Min                  -1.23423\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.113795\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.381246\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.872424\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.921956\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0018764\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0135016\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0230492\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0482728\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.040054\n",
      "evaluation/env_infos/end_effector_loc Std                0.246911\n",
      "evaluation/env_infos/end_effector_loc Max                0.872424\n",
      "evaluation/env_infos/end_effector_loc Min               -0.921956\n",
      "time/data storing (s)                                    0.00295392\n",
      "time/evaluation sampling (s)                             0.972253\n",
      "time/exploration sampling (s)                            0.13327\n",
      "time/logging (s)                                         0.0212546\n",
      "time/saving (s)                                          0.0287246\n",
      "time/training (s)                                       46.1097\n",
      "time/epoch (s)                                          47.2682\n",
      "time/total (s)                                         643.588\n",
      "Epoch                                                   13\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:36:26.274199 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 14 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00354493\n",
      "trainer/QF2 Loss                                         0.00852515\n",
      "trainer/Policy Loss                                      4.14236\n",
      "trainer/Q1 Predictions Mean                             -2.31629\n",
      "trainer/Q1 Predictions Std                               1.62204\n",
      "trainer/Q1 Predictions Max                              -0.102295\n",
      "trainer/Q1 Predictions Min                             -11.0259\n",
      "trainer/Q2 Predictions Mean                             -2.28972\n",
      "trainer/Q2 Predictions Std                               1.62082\n",
      "trainer/Q2 Predictions Max                              -0.095188\n",
      "trainer/Q2 Predictions Min                             -10.8642\n",
      "trainer/Q Targets Mean                                  -2.35058\n",
      "trainer/Q Targets Std                                    1.62666\n",
      "trainer/Q Targets Max                                   -0.0946199\n",
      "trainer/Q Targets Min                                  -11.1702\n",
      "trainer/Log Pis Mean                                     2.04076\n",
      "trainer/Log Pis Std                                      1.41477\n",
      "trainer/Log Pis Max                                      9.44729\n",
      "trainer/Log Pis Min                                     -4.11472\n",
      "trainer/Policy mu Mean                                  -0.000519301\n",
      "trainer/Policy mu Std                                    0.591826\n",
      "trainer/Policy mu Max                                    3.58411\n",
      "trainer/Policy mu Min                                   -3.15038\n",
      "trainer/Policy log std Mean                             -2.19905\n",
      "trainer/Policy log std Std                               0.571105\n",
      "trainer/Policy log std Max                               0.267343\n",
      "trainer/Policy log std Min                              -3.24373\n",
      "trainer/Alpha                                            0.0169097\n",
      "trainer/Alpha Loss                                       0.166235\n",
      "exploration/num steps total                           2500\n",
      "exploration/num paths total                            125\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0992958\n",
      "exploration/Rewards Std                                  0.0523542\n",
      "exploration/Rewards Max                                  6.9293e-05\n",
      "exploration/Rewards Min                                 -0.265651\n",
      "exploration/Returns Mean                                -1.98592\n",
      "exploration/Returns Std                                  0.168447\n",
      "exploration/Returns Max                                 -1.75153\n",
      "exploration/Returns Min                                 -2.26618\n",
      "exploration/Actions Mean                                 0.0094125\n",
      "exploration/Actions Std                                  0.110942\n",
      "exploration/Actions Max                                  0.34979\n",
      "exploration/Actions Min                                 -0.30868\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.98592\n",
      "exploration/env_infos/final/reward_dist Mean             0.0149397\n",
      "exploration/env_infos/final/reward_dist Std              0.0293737\n",
      "exploration/env_infos/final/reward_dist Max              0.0736822\n",
      "exploration/env_infos/final/reward_dist Min              1.03509e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0124829\n",
      "exploration/env_infos/initial/reward_dist Std            0.0138044\n",
      "exploration/env_infos/initial/reward_dist Max            0.0353458\n",
      "exploration/env_infos/initial/reward_dist Min            1.89945e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.13013\n",
      "exploration/env_infos/reward_dist Std                    0.200411\n",
      "exploration/env_infos/reward_dist Max                    0.952369\n",
      "exploration/env_infos/reward_dist Min                    1.03509e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0966569\n",
      "exploration/env_infos/final/reward_energy Std            0.0371921\n",
      "exploration/env_infos/final/reward_energy Max           -0.0510761\n",
      "exploration/env_infos/final/reward_energy Min           -0.158774\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.261763\n",
      "exploration/env_infos/initial/reward_energy Std          0.0588845\n",
      "exploration/env_infos/initial/reward_energy Max         -0.207782\n",
      "exploration/env_infos/initial/reward_energy Min         -0.370234\n",
      "exploration/env_infos/reward_energy Mean                -0.137738\n",
      "exploration/env_infos/reward_energy Std                  0.0762989\n",
      "exploration/env_infos/reward_energy Max                 -0.0123593\n",
      "exploration/env_infos/reward_energy Min                 -0.370234\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.131153\n",
      "exploration/env_infos/final/end_effector_loc Std         0.222824\n",
      "exploration/env_infos/final/end_effector_loc Max         0.421921\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.253324\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00254492\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00913823\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0174895\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0117221\n",
      "exploration/env_infos/end_effector_loc Mean              0.0779194\n",
      "exploration/env_infos/end_effector_loc Std               0.159239\n",
      "exploration/env_infos/end_effector_loc Max               0.421921\n",
      "exploration/env_infos/end_effector_loc Min              -0.272167\n",
      "evaluation/num steps total                           15000\n",
      "evaluation/num paths total                             750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0651951\n",
      "evaluation/Rewards Std                                   0.0671266\n",
      "evaluation/Rewards Max                                   0.120919\n",
      "evaluation/Rewards Min                                  -0.457957\n",
      "evaluation/Returns Mean                                 -1.3039\n",
      "evaluation/Returns Std                                   0.828431\n",
      "evaluation/Returns Max                                   0.492935\n",
      "evaluation/Returns Min                                  -3.027\n",
      "evaluation/Actions Mean                                  0.00805109\n",
      "evaluation/Actions Std                                   0.069193\n",
      "evaluation/Actions Max                                   0.447872\n",
      "evaluation/Actions Min                                  -0.888165\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.3039\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0835301\n",
      "evaluation/env_infos/final/reward_dist Std               0.201805\n",
      "evaluation/env_infos/final/reward_dist Max               0.894993\n",
      "evaluation/env_infos/final/reward_dist Min               2.30216e-28\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00669398\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0148527\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0835262\n",
      "evaluation/env_infos/initial/reward_dist Min             2.16964e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.146945\n",
      "evaluation/env_infos/reward_dist Std                     0.236651\n",
      "evaluation/env_infos/reward_dist Max                     0.997651\n",
      "evaluation/env_infos/reward_dist Min                     2.30216e-28\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.105141\n",
      "evaluation/env_infos/final/reward_energy Std             0.0485803\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0234537\n",
      "evaluation/env_infos/final/reward_energy Min            -0.251173\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.19599\n",
      "evaluation/env_infos/initial/reward_energy Std           0.149576\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0491944\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.05276\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0745109\n",
      "evaluation/env_infos/reward_energy Std                   0.0644446\n",
      "evaluation/env_infos/reward_energy Max                  -0.00219685\n",
      "evaluation/env_infos/reward_energy Min                  -1.05276\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0751944\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.298578\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.744223\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.800687\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000479177\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00870354\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0197703\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0444083\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0306339\n",
      "evaluation/env_infos/end_effector_loc Std                0.169931\n",
      "evaluation/env_infos/end_effector_loc Max                0.744223\n",
      "evaluation/env_infos/end_effector_loc Min               -0.800687\n",
      "time/data storing (s)                                    0.00300905\n",
      "time/evaluation sampling (s)                             1.08299\n",
      "time/exploration sampling (s)                            0.12979\n",
      "time/logging (s)                                         0.0197886\n",
      "time/saving (s)                                          0.0278049\n",
      "time/training (s)                                       44.7187\n",
      "time/epoch (s)                                          45.982\n",
      "time/total (s)                                         689.79\n",
      "Epoch                                                   14\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:37:15.380528 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 15 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00947299\r\n",
      "trainer/QF2 Loss                                         0.00308359\r\n",
      "trainer/Policy Loss                                      3.80736\r\n",
      "trainer/Q1 Predictions Mean                             -1.98889\r\n",
      "trainer/Q1 Predictions Std                               1.50386\r\n",
      "trainer/Q1 Predictions Max                               0.040041\r\n",
      "trainer/Q1 Predictions Min                              -8.6679\r\n",
      "trainer/Q2 Predictions Mean                             -2.00543\r\n",
      "trainer/Q2 Predictions Std                               1.50119\r\n",
      "trainer/Q2 Predictions Max                               0.00657093\r\n",
      "trainer/Q2 Predictions Min                              -8.69265\r\n",
      "trainer/Q Targets Mean                                  -2.02206\r\n",
      "trainer/Q Targets Std                                    1.51506\r\n",
      "trainer/Q Targets Max                                   -0.00576274\r\n",
      "trainer/Q Targets Min                                   -8.81367\r\n",
      "trainer/Log Pis Mean                                     1.94306\r\n",
      "trainer/Log Pis Std                                      1.46155\r\n",
      "trainer/Log Pis Max                                      7.6618\r\n",
      "trainer/Log Pis Min                                     -4.12302\r\n",
      "trainer/Policy mu Mean                                  -0.0299617\r\n",
      "trainer/Policy mu Std                                    0.573395\r\n",
      "trainer/Policy mu Max                                    2.85293\r\n",
      "trainer/Policy mu Min                                   -3.0388\r\n",
      "trainer/Policy log std Mean                             -2.18362\r\n",
      "trainer/Policy log std Std                               0.603666\r\n",
      "trainer/Policy log std Max                              -0.196057\r\n",
      "trainer/Policy log std Min                              -3.25162\r\n",
      "trainer/Alpha                                            0.0178759\r\n",
      "trainer/Alpha Loss                                      -0.229084\r\n",
      "exploration/num steps total                           2600\r\n",
      "exploration/num paths total                            130\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.145228\r\n",
      "exploration/Rewards Std                                  0.0450999\r\n",
      "exploration/Rewards Max                                 -0.0601713\r\n",
      "exploration/Rewards Min                                 -0.285053\r\n",
      "exploration/Returns Mean                                -2.90456\r\n",
      "exploration/Returns Std                                  0.219997\r\n",
      "exploration/Returns Max                                 -2.55286\r\n",
      "exploration/Returns Min                                 -3.16032\r\n",
      "exploration/Actions Mean                                 0.0106482\r\n",
      "exploration/Actions Std                                  0.0956965\r\n",
      "exploration/Actions Max                                  0.232541\r\n",
      "exploration/Actions Min                                 -0.26368\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.90456\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.015703\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0251714\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0649381\r\n",
      "exploration/env_infos/final/reward_dist Min              1.11619e-21\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00101802\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00145204\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00383731\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.74231e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.065356\r\n",
      "exploration/env_infos/reward_dist Std                    0.142251\r\n",
      "exploration/env_infos/reward_dist Max                    0.758304\r\n",
      "exploration/env_infos/reward_dist Min                    1.11619e-21\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.169584\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0858512\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0441417\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.286725\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.155508\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0762767\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0747444\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.279184\r\n",
      "exploration/env_infos/reward_energy Mean                -0.121667\r\n",
      "exploration/env_infos/reward_energy Std                  0.0611518\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0200404\r\n",
      "exploration/env_infos/reward_energy Min                 -0.286725\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.214847\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.234917\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.538943\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.12317\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000820742\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00606855\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0100372\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00959922\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0977765\r\n",
      "exploration/env_infos/end_effector_loc Std               0.165075\r\n",
      "exploration/env_infos/end_effector_loc Max               0.538943\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.163314\r\n",
      "evaluation/num steps total                           16000\r\n",
      "evaluation/num paths total                             800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0744945\r\n",
      "evaluation/Rewards Std                                   0.0822976\r\n",
      "evaluation/Rewards Max                                   0.183006\r\n",
      "evaluation/Rewards Min                                  -0.81635\r\n",
      "evaluation/Returns Mean                                 -1.48989\r\n",
      "evaluation/Returns Std                                   1.22768\r\n",
      "evaluation/Returns Max                                   1.21746\r\n",
      "evaluation/Returns Min                                  -4.36623\r\n",
      "evaluation/Actions Mean                                 -0.00288425\r\n",
      "evaluation/Actions Std                                   0.0896244\r\n",
      "evaluation/Actions Max                                   0.681598\r\n",
      "evaluation/Actions Min                                  -0.963146\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.48989\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0982139\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.179896\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.620801\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.11665e-90\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00543677\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111258\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0505848\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.27344e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.133422\r\n",
      "evaluation/env_infos/reward_dist Std                     0.233219\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996337\r\n",
      "evaluation/env_infos/reward_dist Min                     3.11665e-90\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0856777\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.076248\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00787643\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.337661\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.268818\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.239762\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0497387\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.13845\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0826603\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0961717\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00343799\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.13845\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0137987\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.301057\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.605947\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000130705\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127346\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0340799\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0481573\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00591288\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.188143\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.605947\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00313563\r\n",
      "time/evaluation sampling (s)                             0.958558\r\n",
      "time/exploration sampling (s)                            0.1377\r\n",
      "time/logging (s)                                         0.0196128\r\n",
      "time/saving (s)                                          0.0297937\r\n",
      "time/training (s)                                       47.7398\r\n",
      "time/epoch (s)                                          48.8886\r\n",
      "time/total (s)                                         738.894\r\n",
      "Epoch                                                   15\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:38:03.742903 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 16 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00364263\n",
      "trainer/QF2 Loss                                         0.00332528\n",
      "trainer/Policy Loss                                      3.88451\n",
      "trainer/Q1 Predictions Mean                             -1.92774\n",
      "trainer/Q1 Predictions Std                               1.35645\n",
      "trainer/Q1 Predictions Max                              -0.0201662\n",
      "trainer/Q1 Predictions Min                              -8.8972\n",
      "trainer/Q2 Predictions Mean                             -1.94057\n",
      "trainer/Q2 Predictions Std                               1.36417\n",
      "trainer/Q2 Predictions Max                              -0.0133821\n",
      "trainer/Q2 Predictions Min                              -9.0084\n",
      "trainer/Q Targets Mean                                  -1.95292\n",
      "trainer/Q Targets Std                                    1.3628\n",
      "trainer/Q Targets Max                                   -0.0231312\n",
      "trainer/Q Targets Min                                   -8.98498\n",
      "trainer/Log Pis Mean                                     2.06366\n",
      "trainer/Log Pis Std                                      1.39188\n",
      "trainer/Log Pis Max                                      7.54606\n",
      "trainer/Log Pis Min                                     -3.20364\n",
      "trainer/Policy mu Mean                                  -0.0310648\n",
      "trainer/Policy mu Std                                    0.532819\n",
      "trainer/Policy mu Max                                    2.96365\n",
      "trainer/Policy mu Min                                   -3.29314\n",
      "trainer/Policy log std Mean                             -2.26724\n",
      "trainer/Policy log std Std                               0.592817\n",
      "trainer/Policy log std Max                              -0.101008\n",
      "trainer/Policy log std Min                              -3.32593\n",
      "trainer/Alpha                                            0.0182089\n",
      "trainer/Alpha Loss                                       0.254999\n",
      "exploration/num steps total                           2700\n",
      "exploration/num paths total                            135\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.155666\n",
      "exploration/Rewards Std                                  0.0732162\n",
      "exploration/Rewards Max                                 -0.0239469\n",
      "exploration/Rewards Min                                 -0.354925\n",
      "exploration/Returns Mean                                -3.11332\n",
      "exploration/Returns Std                                  0.878222\n",
      "exploration/Returns Max                                 -1.71933\n",
      "exploration/Returns Min                                 -4.29106\n",
      "exploration/Actions Mean                                 0.00247802\n",
      "exploration/Actions Std                                  0.11933\n",
      "exploration/Actions Max                                  0.575437\n",
      "exploration/Actions Min                                 -0.495197\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.11332\n",
      "exploration/env_infos/final/reward_dist Mean             0.170989\n",
      "exploration/env_infos/final/reward_dist Std              0.336779\n",
      "exploration/env_infos/final/reward_dist Max              0.844499\n",
      "exploration/env_infos/final/reward_dist Min              6.75078e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00745628\n",
      "exploration/env_infos/initial/reward_dist Std            0.0123423\n",
      "exploration/env_infos/initial/reward_dist Max            0.0319307\n",
      "exploration/env_infos/initial/reward_dist Min            0.000181903\n",
      "exploration/env_infos/reward_dist Mean                   0.11577\n",
      "exploration/env_infos/reward_dist Std                    0.262172\n",
      "exploration/env_infos/reward_dist Max                    0.998944\n",
      "exploration/env_infos/reward_dist Min                    6.75078e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.10089\n",
      "exploration/env_infos/final/reward_energy Std            0.0689294\n",
      "exploration/env_infos/final/reward_energy Max           -0.0185651\n",
      "exploration/env_infos/final/reward_energy Min           -0.200434\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.265584\n",
      "exploration/env_infos/initial/reward_energy Std          0.226792\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0786856\n",
      "exploration/env_infos/initial/reward_energy Min         -0.712\n",
      "exploration/env_infos/reward_energy Mean                -0.137754\n",
      "exploration/env_infos/reward_energy Std                  0.097547\n",
      "exploration/env_infos/reward_energy Max                 -0.015592\n",
      "exploration/env_infos/reward_energy Min                 -0.712\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.110411\n",
      "exploration/env_infos/final/end_effector_loc Std         0.238506\n",
      "exploration/env_infos/final/end_effector_loc Max         0.480511\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.257732\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00729579\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00996162\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0287718\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00331793\n",
      "exploration/env_infos/end_effector_loc Mean              0.0827868\n",
      "exploration/env_infos/end_effector_loc Std               0.123895\n",
      "exploration/env_infos/end_effector_loc Max               0.480511\n",
      "exploration/env_infos/end_effector_loc Min              -0.257732\n",
      "evaluation/num steps total                           17000\n",
      "evaluation/num paths total                             850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0904323\n",
      "evaluation/Rewards Std                                   0.10273\n",
      "evaluation/Rewards Max                                   0.151495\n",
      "evaluation/Rewards Min                                  -0.893192\n",
      "evaluation/Returns Mean                                 -1.80865\n",
      "evaluation/Returns Std                                   1.5718\n",
      "evaluation/Returns Max                                   1.25263\n",
      "evaluation/Returns Min                                  -7.44473\n",
      "evaluation/Actions Mean                                  0.0042963\n",
      "evaluation/Actions Std                                   0.0750943\n",
      "evaluation/Actions Max                                   0.590102\n",
      "evaluation/Actions Min                                  -0.825016\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.80865\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0720422\n",
      "evaluation/env_infos/final/reward_dist Std               0.229787\n",
      "evaluation/env_infos/final/reward_dist Max               0.978556\n",
      "evaluation/env_infos/final/reward_dist Min               2.24021e-104\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00864416\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0158566\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0767524\n",
      "evaluation/env_infos/initial/reward_dist Min             1.94032e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.118339\n",
      "evaluation/env_infos/reward_dist Std                     0.230631\n",
      "evaluation/env_infos/reward_dist Max                     0.992252\n",
      "evaluation/env_infos/reward_dist Min                     2.24021e-104\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0882389\n",
      "evaluation/env_infos/final/reward_energy Std             0.0664704\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00674174\n",
      "evaluation/env_infos/final/reward_energy Min            -0.36752\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.213732\n",
      "evaluation/env_infos/initial/reward_energy Std           0.18317\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0316562\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.03267\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0761563\n",
      "evaluation/env_infos/reward_energy Std                   0.0742661\n",
      "evaluation/env_infos/reward_energy Max                  -0.000825669\n",
      "evaluation/env_infos/reward_energy Min                  -1.03267\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0811854\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.341554\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.839028\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.001867\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00977523\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0295051\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0412508\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0367432\n",
      "evaluation/env_infos/end_effector_loc Std                0.20107\n",
      "evaluation/env_infos/end_effector_loc Max                0.839028\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00295026\n",
      "time/evaluation sampling (s)                             0.995473\n",
      "time/exploration sampling (s)                            0.127965\n",
      "time/logging (s)                                         0.0228965\n",
      "time/saving (s)                                          0.0285141\n",
      "time/training (s)                                       46.9488\n",
      "time/epoch (s)                                          48.1266\n",
      "time/total (s)                                         787.259\n",
      "Epoch                                                   16\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:38:51.603925 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 17 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00134535\n",
      "trainer/QF2 Loss                                         0.00185492\n",
      "trainer/Policy Loss                                      3.79255\n",
      "trainer/Q1 Predictions Mean                             -1.92636\n",
      "trainer/Q1 Predictions Std                               1.23702\n",
      "trainer/Q1 Predictions Max                              -0.143558\n",
      "trainer/Q1 Predictions Min                              -6.7644\n",
      "trainer/Q2 Predictions Mean                             -1.92472\n",
      "trainer/Q2 Predictions Std                               1.23606\n",
      "trainer/Q2 Predictions Max                              -0.12905\n",
      "trainer/Q2 Predictions Min                              -6.71593\n",
      "trainer/Q Targets Mean                                  -1.91933\n",
      "trainer/Q Targets Std                                    1.23681\n",
      "trainer/Q Targets Max                                   -0.125169\n",
      "trainer/Q Targets Min                                   -6.75863\n",
      "trainer/Log Pis Mean                                     1.97943\n",
      "trainer/Log Pis Std                                      1.23391\n",
      "trainer/Log Pis Max                                      6.04687\n",
      "trainer/Log Pis Min                                     -2.26705\n",
      "trainer/Policy mu Mean                                  -0.0348583\n",
      "trainer/Policy mu Std                                    0.422634\n",
      "trainer/Policy mu Max                                    2.74933\n",
      "trainer/Policy mu Min                                   -2.5356\n",
      "trainer/Policy log std Mean                             -2.25391\n",
      "trainer/Policy log std Std                               0.517431\n",
      "trainer/Policy log std Max                              -0.132871\n",
      "trainer/Policy log std Min                              -3.09875\n",
      "trainer/Alpha                                            0.0184581\n",
      "trainer/Alpha Loss                                      -0.0821298\n",
      "exploration/num steps total                           2800\n",
      "exploration/num paths total                            140\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.105289\n",
      "exploration/Rewards Std                                  0.0546284\n",
      "exploration/Rewards Max                                 -0.0203557\n",
      "exploration/Rewards Min                                 -0.3092\n",
      "exploration/Returns Mean                                -2.10578\n",
      "exploration/Returns Std                                  0.543837\n",
      "exploration/Returns Max                                 -1.49298\n",
      "exploration/Returns Min                                 -2.94156\n",
      "exploration/Actions Mean                                -0.00807327\n",
      "exploration/Actions Std                                  0.140173\n",
      "exploration/Actions Max                                  0.43857\n",
      "exploration/Actions Min                                 -0.401733\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.10578\n",
      "exploration/env_infos/final/reward_dist Mean             0.0364483\n",
      "exploration/env_infos/final/reward_dist Std              0.0463598\n",
      "exploration/env_infos/final/reward_dist Max              0.119194\n",
      "exploration/env_infos/final/reward_dist Min              9.84639e-22\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103968\n",
      "exploration/env_infos/initial/reward_dist Std            0.0127951\n",
      "exploration/env_infos/initial/reward_dist Max            0.0280293\n",
      "exploration/env_infos/initial/reward_dist Min            1.82944e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0785548\n",
      "exploration/env_infos/reward_dist Std                    0.17422\n",
      "exploration/env_infos/reward_dist Max                    0.825281\n",
      "exploration/env_infos/reward_dist Min                    9.84639e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.283046\n",
      "exploration/env_infos/final/reward_energy Std            0.137616\n",
      "exploration/env_infos/final/reward_energy Max           -0.0864816\n",
      "exploration/env_infos/final/reward_energy Min           -0.464948\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.247942\n",
      "exploration/env_infos/initial/reward_energy Std          0.116682\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0871623\n",
      "exploration/env_infos/initial/reward_energy Min         -0.428858\n",
      "exploration/env_infos/reward_energy Mean                -0.171317\n",
      "exploration/env_infos/reward_energy Std                  0.100389\n",
      "exploration/env_infos/reward_energy Max                 -0.0246617\n",
      "exploration/env_infos/reward_energy Min                 -0.464948\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0579131\n",
      "exploration/env_infos/final/end_effector_loc Std         0.246385\n",
      "exploration/env_infos/final/end_effector_loc Max         0.282337\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.687232\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00223741\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00942636\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0188638\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0133595\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00539084\n",
      "exploration/env_infos/end_effector_loc Std               0.138298\n",
      "exploration/env_infos/end_effector_loc Max               0.298093\n",
      "exploration/env_infos/end_effector_loc Min              -0.687232\n",
      "evaluation/num steps total                           18000\n",
      "evaluation/num paths total                             900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.090208\n",
      "evaluation/Rewards Std                                   0.0776281\n",
      "evaluation/Rewards Max                                   0.0977076\n",
      "evaluation/Rewards Min                                  -0.589263\n",
      "evaluation/Returns Mean                                 -1.80416\n",
      "evaluation/Returns Std                                   1.14835\n",
      "evaluation/Returns Max                                   0.840507\n",
      "evaluation/Returns Min                                  -4.51516\n",
      "evaluation/Actions Mean                                 -0.00156521\n",
      "evaluation/Actions Std                                   0.0943222\n",
      "evaluation/Actions Max                                   0.818999\n",
      "evaluation/Actions Min                                  -0.914876\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.80416\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0836764\n",
      "evaluation/env_infos/final/reward_dist Std               0.200721\n",
      "evaluation/env_infos/final/reward_dist Max               0.903356\n",
      "evaluation/env_infos/final/reward_dist Min               1.15563e-64\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705409\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165747\n",
      "evaluation/env_infos/initial/reward_dist Max             0.104594\n",
      "evaluation/env_infos/initial/reward_dist Min             1.84652e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0898322\n",
      "evaluation/env_infos/reward_dist Std                     0.183387\n",
      "evaluation/env_infos/reward_dist Max                     0.987575\n",
      "evaluation/env_infos/reward_dist Min                     1.15563e-64\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0884292\n",
      "evaluation/env_infos/final/reward_energy Std             0.0764876\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0108143\n",
      "evaluation/env_infos/final/reward_energy Min            -0.351331\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.268741\n",
      "evaluation/env_infos/initial/reward_energy Std           0.305338\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0132435\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.10874\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0823987\n",
      "evaluation/env_infos/reward_energy Std                   0.104922\n",
      "evaluation/env_infos/reward_energy Max                  -0.00172516\n",
      "evaluation/env_infos/reward_energy Min                  -1.10874\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0180693\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.287308\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.590674\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.91359\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000114944\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0143806\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0409499\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0457438\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0144328\n",
      "evaluation/env_infos/end_effector_loc Std                0.188083\n",
      "evaluation/env_infos/end_effector_loc Max                0.590674\n",
      "evaluation/env_infos/end_effector_loc Min               -0.91359\n",
      "time/data storing (s)                                    0.00331784\n",
      "time/evaluation sampling (s)                             1.06589\n",
      "time/exploration sampling (s)                            0.151207\n",
      "time/logging (s)                                         0.0216172\n",
      "time/saving (s)                                          0.0285518\n",
      "time/training (s)                                       46.3353\n",
      "time/epoch (s)                                          47.6059\n",
      "time/total (s)                                         835.118\n",
      "Epoch                                                   17\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:39:40.020896 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 18 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00234868\n",
      "trainer/QF2 Loss                                         0.00141051\n",
      "trainer/Policy Loss                                      3.85093\n",
      "trainer/Q1 Predictions Mean                             -1.87584\n",
      "trainer/Q1 Predictions Std                               1.35617\n",
      "trainer/Q1 Predictions Max                              -0.0242751\n",
      "trainer/Q1 Predictions Min                              -8.4857\n",
      "trainer/Q2 Predictions Mean                             -1.88257\n",
      "trainer/Q2 Predictions Std                               1.35924\n",
      "trainer/Q2 Predictions Max                              -0.0286584\n",
      "trainer/Q2 Predictions Min                              -8.47534\n",
      "trainer/Q Targets Mean                                  -1.88788\n",
      "trainer/Q Targets Std                                    1.36053\n",
      "trainer/Q Targets Max                                   -0.00576274\n",
      "trainer/Q Targets Min                                   -8.54988\n",
      "trainer/Log Pis Mean                                     2.06299\n",
      "trainer/Log Pis Std                                      1.18116\n",
      "trainer/Log Pis Max                                      9.20379\n",
      "trainer/Log Pis Min                                     -2.44228\n",
      "trainer/Policy mu Mean                                  -0.041886\n",
      "trainer/Policy mu Std                                    0.365306\n",
      "trainer/Policy mu Max                                    3.03972\n",
      "trainer/Policy mu Min                                   -2.89623\n",
      "trainer/Policy log std Mean                             -2.34255\n",
      "trainer/Policy log std Std                               0.473543\n",
      "trainer/Policy log std Max                              -0.212367\n",
      "trainer/Policy log std Min                              -3.16668\n",
      "trainer/Alpha                                            0.0190544\n",
      "trainer/Alpha Loss                                       0.249439\n",
      "exploration/num steps total                           2900\n",
      "exploration/num paths total                            145\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.123431\n",
      "exploration/Rewards Std                                  0.0563293\n",
      "exploration/Rewards Max                                  0.0334956\n",
      "exploration/Rewards Min                                 -0.298208\n",
      "exploration/Returns Mean                                -2.46862\n",
      "exploration/Returns Std                                  0.755072\n",
      "exploration/Returns Max                                 -0.978008\n",
      "exploration/Returns Min                                 -2.96509\n",
      "exploration/Actions Mean                                -0.0048313\n",
      "exploration/Actions Std                                  0.107572\n",
      "exploration/Actions Max                                  0.336576\n",
      "exploration/Actions Min                                 -0.26348\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.46862\n",
      "exploration/env_infos/final/reward_dist Mean             0.00497595\n",
      "exploration/env_infos/final/reward_dist Std              0.00981316\n",
      "exploration/env_infos/final/reward_dist Max              0.0246017\n",
      "exploration/env_infos/final/reward_dist Min              1.31031e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00791899\n",
      "exploration/env_infos/initial/reward_dist Std            0.0132966\n",
      "exploration/env_infos/initial/reward_dist Max            0.0344729\n",
      "exploration/env_infos/initial/reward_dist Min            1.63266e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0789486\n",
      "exploration/env_infos/reward_dist Std                    0.194042\n",
      "exploration/env_infos/reward_dist Max                    0.892121\n",
      "exploration/env_infos/reward_dist Min                    4.17754e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.128683\n",
      "exploration/env_infos/final/reward_energy Std            0.0465194\n",
      "exploration/env_infos/final/reward_energy Max           -0.0750233\n",
      "exploration/env_infos/final/reward_energy Min           -0.18857\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.241831\n",
      "exploration/env_infos/initial/reward_energy Std          0.0964385\n",
      "exploration/env_infos/initial/reward_energy Max         -0.097135\n",
      "exploration/env_infos/initial/reward_energy Min         -0.369216\n",
      "exploration/env_infos/reward_energy Mean                -0.133601\n",
      "exploration/env_infos/reward_energy Std                  0.0730825\n",
      "exploration/env_infos/reward_energy Max                 -0.00744318\n",
      "exploration/env_infos/reward_energy Min                 -0.369216\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0642819\n",
      "exploration/env_infos/final/end_effector_loc Std         0.193274\n",
      "exploration/env_infos/final/end_effector_loc Max         0.331154\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.296752\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00299784\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00870294\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0168288\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00991487\n",
      "exploration/env_infos/end_effector_loc Mean              0.0537275\n",
      "exploration/env_infos/end_effector_loc Std               0.141439\n",
      "exploration/env_infos/end_effector_loc Max               0.331154\n",
      "exploration/env_infos/end_effector_loc Min              -0.296752\n",
      "evaluation/num steps total                           19000\n",
      "evaluation/num paths total                             950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0947729\n",
      "evaluation/Rewards Std                                   0.0785104\n",
      "evaluation/Rewards Max                                   0.0890514\n",
      "evaluation/Rewards Min                                  -0.496971\n",
      "evaluation/Returns Mean                                 -1.89546\n",
      "evaluation/Returns Std                                   1.04921\n",
      "evaluation/Returns Max                                  -0.056208\n",
      "evaluation/Returns Min                                  -4.94838\n",
      "evaluation/Actions Mean                                 -0.00483825\n",
      "evaluation/Actions Std                                   0.0755955\n",
      "evaluation/Actions Max                                   0.477069\n",
      "evaluation/Actions Min                                  -0.831519\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.89546\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0397\n",
      "evaluation/env_infos/final/reward_dist Std               0.122278\n",
      "evaluation/env_infos/final/reward_dist Max               0.518291\n",
      "evaluation/env_infos/final/reward_dist Min               1.04003e-39\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00687103\n",
      "evaluation/env_infos/initial/reward_dist Std             0.011469\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0442391\n",
      "evaluation/env_infos/initial/reward_dist Min             8.31955e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.078358\n",
      "evaluation/env_infos/reward_dist Std                     0.153832\n",
      "evaluation/env_infos/reward_dist Max                     0.939358\n",
      "evaluation/env_infos/reward_dist Min                     1.04003e-39\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0955743\n",
      "evaluation/env_infos/final/reward_energy Std             0.0728919\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0140328\n",
      "evaluation/env_infos/final/reward_energy Min            -0.275318\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.192148\n",
      "evaluation/env_infos/initial/reward_energy Std           0.197052\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00276471\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.02152\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0741009\n",
      "evaluation/env_infos/reward_energy Std                   0.0773643\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178216\n",
      "evaluation/env_infos/reward_energy Min                  -1.02152\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0190025\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.354451\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.752581\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.820331\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00100173\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00967907\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0238535\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0415759\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00587165\n",
      "evaluation/env_infos/end_effector_loc Std                0.205025\n",
      "evaluation/env_infos/end_effector_loc Max                0.752581\n",
      "evaluation/env_infos/end_effector_loc Min               -0.820331\n",
      "time/data storing (s)                                    0.00300187\n",
      "time/evaluation sampling (s)                             1.01949\n",
      "time/exploration sampling (s)                            0.136509\n",
      "time/logging (s)                                         0.0193517\n",
      "time/saving (s)                                          0.0295015\n",
      "time/training (s)                                       46.9456\n",
      "time/epoch (s)                                          48.1535\n",
      "time/total (s)                                         883.532\n",
      "Epoch                                                   18\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:40:28.051037 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 19 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00251724\n",
      "trainer/QF2 Loss                                         0.00149648\n",
      "trainer/Policy Loss                                      3.6917\n",
      "trainer/Q1 Predictions Mean                             -1.82294\n",
      "trainer/Q1 Predictions Std                               1.23015\n",
      "trainer/Q1 Predictions Max                              -0.00687152\n",
      "trainer/Q1 Predictions Min                              -8.12665\n",
      "trainer/Q2 Predictions Mean                             -1.83408\n",
      "trainer/Q2 Predictions Std                               1.23274\n",
      "trainer/Q2 Predictions Max                              -0.0346256\n",
      "trainer/Q2 Predictions Min                              -8.09983\n",
      "trainer/Q Targets Mean                                  -1.84844\n",
      "trainer/Q Targets Std                                    1.24047\n",
      "trainer/Q Targets Max                                   -0.0657975\n",
      "trainer/Q Targets Min                                   -8.158\n",
      "trainer/Log Pis Mean                                     1.9549\n",
      "trainer/Log Pis Std                                      1.23686\n",
      "trainer/Log Pis Max                                      4.65368\n",
      "trainer/Log Pis Min                                     -4.7722\n",
      "trainer/Policy mu Mean                                  -0.0485406\n",
      "trainer/Policy mu Std                                    0.397637\n",
      "trainer/Policy mu Max                                    2.35706\n",
      "trainer/Policy mu Min                                   -2.70976\n",
      "trainer/Policy log std Mean                             -2.28258\n",
      "trainer/Policy log std Std                               0.519785\n",
      "trainer/Policy log std Max                               0.39747\n",
      "trainer/Policy log std Min                              -3.1943\n",
      "trainer/Alpha                                            0.0191408\n",
      "trainer/Alpha Loss                                      -0.178329\n",
      "exploration/num steps total                           3000\n",
      "exploration/num paths total                            150\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.161702\n",
      "exploration/Rewards Std                                  0.0863981\n",
      "exploration/Rewards Max                                  0.0152004\n",
      "exploration/Rewards Min                                 -0.390631\n",
      "exploration/Returns Mean                                -3.23404\n",
      "exploration/Returns Std                                  1.12267\n",
      "exploration/Returns Max                                 -2.06462\n",
      "exploration/Returns Min                                 -5.19172\n",
      "exploration/Actions Mean                                -0.00669403\n",
      "exploration/Actions Std                                  0.177658\n",
      "exploration/Actions Max                                  0.743394\n",
      "exploration/Actions Min                                 -0.631316\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.23404\n",
      "exploration/env_infos/final/reward_dist Mean             0.0313882\n",
      "exploration/env_infos/final/reward_dist Std              0.0378835\n",
      "exploration/env_infos/final/reward_dist Max              0.0838394\n",
      "exploration/env_infos/final/reward_dist Min              1.48617e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00101911\n",
      "exploration/env_infos/initial/reward_dist Std            0.00130908\n",
      "exploration/env_infos/initial/reward_dist Max            0.0032901\n",
      "exploration/env_infos/initial/reward_dist Min            3.75583e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.102914\n",
      "exploration/env_infos/reward_dist Std                    0.219232\n",
      "exploration/env_infos/reward_dist Max                    0.895757\n",
      "exploration/env_infos/reward_dist Min                    1.48617e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.256606\n",
      "exploration/env_infos/final/reward_energy Std            0.148254\n",
      "exploration/env_infos/final/reward_energy Max           -0.102855\n",
      "exploration/env_infos/final/reward_energy Min           -0.458756\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.401437\n",
      "exploration/env_infos/initial/reward_energy Std          0.331354\n",
      "exploration/env_infos/initial/reward_energy Max         -0.125458\n",
      "exploration/env_infos/initial/reward_energy Min         -1.03444\n",
      "exploration/env_infos/reward_energy Mean                -0.19052\n",
      "exploration/env_infos/reward_energy Std                  0.164064\n",
      "exploration/env_infos/reward_energy Max                 -0.0191299\n",
      "exploration/env_infos/reward_energy Min                 -1.03444\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0317379\n",
      "exploration/env_infos/final/end_effector_loc Std         0.278544\n",
      "exploration/env_infos/final/end_effector_loc Max         0.307163\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.482666\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0049213\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0177331\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0371697\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0202071\n",
      "exploration/env_infos/end_effector_loc Mean              0.00809683\n",
      "exploration/env_infos/end_effector_loc Std               0.179612\n",
      "exploration/env_infos/end_effector_loc Max               0.311814\n",
      "exploration/env_infos/end_effector_loc Min              -0.482666\n",
      "evaluation/num steps total                           20000\n",
      "evaluation/num paths total                            1000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0873154\n",
      "evaluation/Rewards Std                                   0.075299\n",
      "evaluation/Rewards Max                                   0.153521\n",
      "evaluation/Rewards Min                                  -0.351075\n",
      "evaluation/Returns Mean                                 -1.74631\n",
      "evaluation/Returns Std                                   1.12527\n",
      "evaluation/Returns Max                                   1.65978\n",
      "evaluation/Returns Min                                  -4.49939\n",
      "evaluation/Actions Mean                                 -0.00544523\n",
      "evaluation/Actions Std                                   0.0647317\n",
      "evaluation/Actions Max                                   0.615275\n",
      "evaluation/Actions Min                                  -0.573434\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.74631\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0339472\n",
      "evaluation/env_infos/final/reward_dist Std               0.106366\n",
      "evaluation/env_infos/final/reward_dist Max               0.618989\n",
      "evaluation/env_infos/final/reward_dist Min               1.33749e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00379805\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00699191\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0306269\n",
      "evaluation/env_infos/initial/reward_dist Min             2.39838e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0969367\n",
      "evaluation/env_infos/reward_dist Std                     0.19606\n",
      "evaluation/env_infos/reward_dist Max                     0.998722\n",
      "evaluation/env_infos/reward_dist Min                     1.33749e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.07739\n",
      "evaluation/env_infos/final/reward_energy Std             0.0547344\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00413626\n",
      "evaluation/env_infos/final/reward_energy Min            -0.233816\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.181832\n",
      "evaluation/env_infos/initial/reward_energy Std           0.146164\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.019499\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.802407\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0664638\n",
      "evaluation/env_infos/reward_energy Std                   0.0634212\n",
      "evaluation/env_infos/reward_energy Max                  -0.000204311\n",
      "evaluation/env_infos/reward_energy Min                  -0.802407\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0184727\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.316183\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.621443\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.666319\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00175271\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00805987\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0307637\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0286717\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0247948\n",
      "evaluation/env_infos/end_effector_loc Std                0.184259\n",
      "evaluation/env_infos/end_effector_loc Max                0.621443\n",
      "evaluation/env_infos/end_effector_loc Min               -0.666319\n",
      "time/data storing (s)                                    0.00299726\n",
      "time/evaluation sampling (s)                             0.974664\n",
      "time/exploration sampling (s)                            0.128159\n",
      "time/logging (s)                                         0.0286003\n",
      "time/saving (s)                                          0.0303682\n",
      "time/training (s)                                       46.6086\n",
      "time/epoch (s)                                          47.7733\n",
      "time/total (s)                                         931.57\n",
      "Epoch                                                   19\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:41:16.799378 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 20 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00248698\r\n",
      "trainer/QF2 Loss                                         0.00261965\r\n",
      "trainer/Policy Loss                                      3.59663\r\n",
      "trainer/Q1 Predictions Mean                             -1.69227\r\n",
      "trainer/Q1 Predictions Std                               1.01348\r\n",
      "trainer/Q1 Predictions Max                              -0.0219329\r\n",
      "trainer/Q1 Predictions Min                              -5.05338\r\n",
      "trainer/Q2 Predictions Mean                             -1.66368\r\n",
      "trainer/Q2 Predictions Std                               1.01852\r\n",
      "trainer/Q2 Predictions Max                              -0.00909691\r\n",
      "trainer/Q2 Predictions Min                              -5.00154\r\n",
      "trainer/Q Targets Mean                                  -1.69052\r\n",
      "trainer/Q Targets Std                                    1.02921\r\n",
      "trainer/Q Targets Max                                   -0.0287947\r\n",
      "trainer/Q Targets Min                                   -5.08097\r\n",
      "trainer/Log Pis Mean                                     1.95785\r\n",
      "trainer/Log Pis Std                                      1.32828\r\n",
      "trainer/Log Pis Max                                      4.61581\r\n",
      "trainer/Log Pis Min                                     -3.44465\r\n",
      "trainer/Policy mu Mean                                  -0.0179185\r\n",
      "trainer/Policy mu Std                                    0.210716\r\n",
      "trainer/Policy mu Max                                    1.41571\r\n",
      "trainer/Policy mu Min                                   -1.55029\r\n",
      "trainer/Policy log std Mean                             -2.36776\r\n",
      "trainer/Policy log std Std                               0.439399\r\n",
      "trainer/Policy log std Max                              -0.652599\r\n",
      "trainer/Policy log std Min                              -3.33175\r\n",
      "trainer/Alpha                                            0.0201313\r\n",
      "trainer/Alpha Loss                                      -0.16461\r\n",
      "exploration/num steps total                           3100\r\n",
      "exploration/num paths total                            155\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.134735\r\n",
      "exploration/Rewards Std                                  0.0527813\r\n",
      "exploration/Rewards Max                                 -0.0101755\r\n",
      "exploration/Rewards Min                                 -0.259321\r\n",
      "exploration/Returns Mean                                -2.69469\r\n",
      "exploration/Returns Std                                  0.6064\r\n",
      "exploration/Returns Max                                 -1.88303\r\n",
      "exploration/Returns Min                                 -3.51046\r\n",
      "exploration/Actions Mean                                -0.00356104\r\n",
      "exploration/Actions Std                                  0.103819\r\n",
      "exploration/Actions Max                                  0.332155\r\n",
      "exploration/Actions Min                                 -0.3876\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.69469\r\n",
      "exploration/env_infos/final/reward_dist Mean             7.12171e-06\r\n",
      "exploration/env_infos/final/reward_dist Std              1.38087e-05\r\n",
      "exploration/env_infos/final/reward_dist Max              3.47335e-05\r\n",
      "exploration/env_infos/final/reward_dist Min              1.18478e-42\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0010101\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00125727\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00342967\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.7014e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0190489\r\n",
      "exploration/env_infos/reward_dist Std                    0.0428231\r\n",
      "exploration/env_infos/reward_dist Max                    0.220602\r\n",
      "exploration/env_infos/reward_dist Min                    1.18478e-42\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.176152\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0620387\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0785922\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.231599\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.117049\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0317396\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0673138\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.144941\r\n",
      "exploration/env_infos/reward_energy Mean                -0.123626\r\n",
      "exploration/env_infos/reward_energy Std                  0.0793651\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0132256\r\n",
      "exploration/env_infos/reward_energy Min                 -0.407985\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.000963422\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.371752\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.373629\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.760319\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00238737\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00356166\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00641923\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00344389\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0199854\r\n",
      "exploration/env_infos/end_effector_loc Std               0.201887\r\n",
      "exploration/env_infos/end_effector_loc Max               0.373629\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.760319\r\n",
      "evaluation/num steps total                           21000\r\n",
      "evaluation/num paths total                            1050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0841271\r\n",
      "evaluation/Rewards Std                                   0.0677043\r\n",
      "evaluation/Rewards Max                                   0.147359\r\n",
      "evaluation/Rewards Min                                  -0.582624\r\n",
      "evaluation/Returns Mean                                 -1.68254\r\n",
      "evaluation/Returns Std                                   0.865894\r\n",
      "evaluation/Returns Max                                   0.48494\r\n",
      "evaluation/Returns Min                                  -3.58603\r\n",
      "evaluation/Actions Mean                                 -0.00691527\r\n",
      "evaluation/Actions Std                                   0.0484629\r\n",
      "evaluation/Actions Max                                   0.399415\r\n",
      "evaluation/Actions Min                                  -0.284991\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.68254\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0640186\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.160954\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.939318\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.20436e-44\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0045537\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00767161\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0303989\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.15282e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0884866\r\n",
      "evaluation/env_infos/reward_dist Std                     0.179034\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996489\r\n",
      "evaluation/env_infos/reward_dist Min                     3.20436e-44\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.081909\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0760277\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00972662\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.404483\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.110375\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.0686613\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.023516\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.318258\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0533209\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0441569\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00188164\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.404483\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0451065\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.301534\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.454465\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.848303\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00026901\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00458791\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0158921\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0142496\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0102572\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.164542\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.454465\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.848303\r\n",
      "time/data storing (s)                                    0.00309027\r\n",
      "time/evaluation sampling (s)                             1.18336\r\n",
      "time/exploration sampling (s)                            0.12645\r\n",
      "time/logging (s)                                         0.0244806\r\n",
      "time/saving (s)                                          0.0319922\r\n",
      "time/training (s)                                       47.0561\r\n",
      "time/epoch (s)                                          48.4255\r\n",
      "time/total (s)                                         980.313\r\n",
      "Epoch                                                   20\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:42:07.689044 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 21 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00210275\n",
      "trainer/QF2 Loss                                         0.00304153\n",
      "trainer/Policy Loss                                      3.9314\n",
      "trainer/Q1 Predictions Mean                             -1.69986\n",
      "trainer/Q1 Predictions Std                               1.01105\n",
      "trainer/Q1 Predictions Max                              -0.0352082\n",
      "trainer/Q1 Predictions Min                              -6.33899\n",
      "trainer/Q2 Predictions Mean                             -1.68502\n",
      "trainer/Q2 Predictions Std                               1.01375\n",
      "trainer/Q2 Predictions Max                              -0.0391836\n",
      "trainer/Q2 Predictions Min                              -6.33102\n",
      "trainer/Q Targets Mean                                  -1.69475\n",
      "trainer/Q Targets Std                                    1.01455\n",
      "trainer/Q Targets Max                                   -0.0376383\n",
      "trainer/Q Targets Min                                   -6.50395\n",
      "trainer/Log Pis Mean                                     2.26102\n",
      "trainer/Log Pis Std                                      1.31855\n",
      "trainer/Log Pis Max                                      5.40832\n",
      "trainer/Log Pis Min                                     -3.51674\n",
      "trainer/Policy mu Mean                                  -0.0103715\n",
      "trainer/Policy mu Std                                    0.202554\n",
      "trainer/Policy mu Max                                    1.59305\n",
      "trainer/Policy mu Min                                   -2.39728\n",
      "trainer/Policy log std Mean                             -2.51204\n",
      "trainer/Policy log std Std                               0.392728\n",
      "trainer/Policy log std Max                              -0.461271\n",
      "trainer/Policy log std Min                              -3.3921\n",
      "trainer/Alpha                                            0.0202628\n",
      "trainer/Alpha Loss                                       1.01795\n",
      "exploration/num steps total                           3200\n",
      "exploration/num paths total                            160\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0569692\n",
      "exploration/Rewards Std                                  0.106616\n",
      "exploration/Rewards Max                                  0.153634\n",
      "exploration/Rewards Min                                 -0.339855\n",
      "exploration/Returns Mean                                -1.13938\n",
      "exploration/Returns Std                                  1.49792\n",
      "exploration/Returns Max                                  1.61441\n",
      "exploration/Returns Min                                 -2.55416\n",
      "exploration/Actions Mean                                 0.00352316\n",
      "exploration/Actions Std                                  0.111657\n",
      "exploration/Actions Max                                  0.355426\n",
      "exploration/Actions Min                                 -0.324545\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.13938\n",
      "exploration/env_infos/final/reward_dist Mean             0.0727321\n",
      "exploration/env_infos/final/reward_dist Std              0.127377\n",
      "exploration/env_infos/final/reward_dist Max              0.326748\n",
      "exploration/env_infos/final/reward_dist Min              1.0774e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00814386\n",
      "exploration/env_infos/initial/reward_dist Std            0.0145627\n",
      "exploration/env_infos/initial/reward_dist Max            0.0372108\n",
      "exploration/env_infos/initial/reward_dist Min            2.39896e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.159732\n",
      "exploration/env_infos/reward_dist Std                    0.23699\n",
      "exploration/env_infos/reward_dist Max                    0.894881\n",
      "exploration/env_infos/reward_dist Min                    1.0774e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.124808\n",
      "exploration/env_infos/final/reward_energy Std            0.0751601\n",
      "exploration/env_infos/final/reward_energy Max           -0.0393795\n",
      "exploration/env_infos/final/reward_energy Min           -0.230109\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.222566\n",
      "exploration/env_infos/initial/reward_energy Std          0.104817\n",
      "exploration/env_infos/initial/reward_energy Max         -0.117339\n",
      "exploration/env_infos/initial/reward_energy Min         -0.416374\n",
      "exploration/env_infos/reward_energy Mean                -0.13365\n",
      "exploration/env_infos/reward_energy Std                  0.0842442\n",
      "exploration/env_infos/reward_energy Max                 -0.00431652\n",
      "exploration/env_infos/reward_energy Min                 -0.416374\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00459556\n",
      "exploration/env_infos/final/end_effector_loc Std         0.29192\n",
      "exploration/env_infos/final/end_effector_loc Max         0.278122\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.636882\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00243703\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00834947\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00893417\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0162272\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0148786\n",
      "exploration/env_infos/end_effector_loc Std               0.189972\n",
      "exploration/env_infos/end_effector_loc Max               0.278122\n",
      "exploration/env_infos/end_effector_loc Min              -0.636882\n",
      "evaluation/num steps total                           22000\n",
      "evaluation/num paths total                            1100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0626847\n",
      "evaluation/Rewards Std                                   0.0745355\n",
      "evaluation/Rewards Max                                   0.111089\n",
      "evaluation/Rewards Min                                  -0.545762\n",
      "evaluation/Returns Mean                                 -1.25369\n",
      "evaluation/Returns Std                                   1.12259\n",
      "evaluation/Returns Max                                   1.22594\n",
      "evaluation/Returns Min                                  -3.49241\n",
      "evaluation/Actions Mean                                 -0.0077482\n",
      "evaluation/Actions Std                                   0.0558542\n",
      "evaluation/Actions Max                                   0.365591\n",
      "evaluation/Actions Min                                  -0.74091\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.25369\n",
      "evaluation/env_infos/final/reward_dist Mean              0.167529\n",
      "evaluation/env_infos/final/reward_dist Std               0.224108\n",
      "evaluation/env_infos/final/reward_dist Max               0.994827\n",
      "evaluation/env_infos/final/reward_dist Min               5.36533e-57\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00521255\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00933152\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0409575\n",
      "evaluation/env_infos/initial/reward_dist Min             1.33828e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.137158\n",
      "evaluation/env_infos/reward_dist Std                     0.212616\n",
      "evaluation/env_infos/reward_dist Max                     0.998779\n",
      "evaluation/env_infos/reward_dist Min                     5.36533e-57\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0706683\n",
      "evaluation/env_infos/final/reward_energy Std             0.0542872\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00334885\n",
      "evaluation/env_infos/final/reward_energy Min            -0.229215\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.157231\n",
      "evaluation/env_infos/initial/reward_energy Std           0.15346\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0164047\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.7706\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0547942\n",
      "evaluation/env_infos/reward_energy Std                   0.05794\n",
      "evaluation/env_infos/reward_energy Max                  -0.000380304\n",
      "evaluation/env_infos/reward_energy Min                  -0.7706\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0698358\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.266896\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.5753\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.919514\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0011473\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00768266\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0182796\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0370455\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0240066\n",
      "evaluation/env_infos/end_effector_loc Std                0.156785\n",
      "evaluation/env_infos/end_effector_loc Max                0.5753\n",
      "evaluation/env_infos/end_effector_loc Min               -0.919514\n",
      "time/data storing (s)                                    0.00304472\n",
      "time/evaluation sampling (s)                             1.19425\n",
      "time/exploration sampling (s)                            0.140406\n",
      "time/logging (s)                                         0.0201398\n",
      "time/saving (s)                                          0.0280863\n",
      "time/training (s)                                       49.1754\n",
      "time/epoch (s)                                          50.5614\n",
      "time/total (s)                                        1031.2\n",
      "Epoch                                                   21\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:42:55.654567 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 22 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00208177\n",
      "trainer/QF2 Loss                                         0.00075504\n",
      "trainer/Policy Loss                                      3.67203\n",
      "trainer/Q1 Predictions Mean                             -1.64912\n",
      "trainer/Q1 Predictions Std                               1.02217\n",
      "trainer/Q1 Predictions Max                              -0.0100421\n",
      "trainer/Q1 Predictions Min                              -6.28763\n",
      "trainer/Q2 Predictions Mean                             -1.63301\n",
      "trainer/Q2 Predictions Std                               1.00303\n",
      "trainer/Q2 Predictions Max                              -0.0400837\n",
      "trainer/Q2 Predictions Min                              -6.16132\n",
      "trainer/Q Targets Mean                                  -1.63218\n",
      "trainer/Q Targets Std                                    1.0066\n",
      "trainer/Q Targets Max                                   -0.0595577\n",
      "trainer/Q Targets Min                                   -6.2031\n",
      "trainer/Log Pis Mean                                     2.04664\n",
      "trainer/Log Pis Std                                      1.28886\n",
      "trainer/Log Pis Max                                      5.20349\n",
      "trainer/Log Pis Min                                     -2.11795\n",
      "trainer/Policy mu Mean                                   0.00162103\n",
      "trainer/Policy mu Std                                    0.246505\n",
      "trainer/Policy mu Max                                    2.37764\n",
      "trainer/Policy mu Min                                   -2.10731\n",
      "trainer/Policy log std Mean                             -2.39335\n",
      "trainer/Policy log std Std                               0.441233\n",
      "trainer/Policy log std Max                              -0.248241\n",
      "trainer/Policy log std Min                              -3.21867\n",
      "trainer/Alpha                                            0.0200744\n",
      "trainer/Alpha Loss                                       0.182321\n",
      "exploration/num steps total                           3300\n",
      "exploration/num paths total                            165\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0914819\n",
      "exploration/Rewards Std                                  0.0905991\n",
      "exploration/Rewards Max                                  0.0907386\n",
      "exploration/Rewards Min                                 -0.390972\n",
      "exploration/Returns Mean                                -1.82964\n",
      "exploration/Returns Std                                  1.2724\n",
      "exploration/Returns Max                                  0.109419\n",
      "exploration/Returns Min                                 -3.28081\n",
      "exploration/Actions Mean                                 0.00345199\n",
      "exploration/Actions Std                                  0.0972037\n",
      "exploration/Actions Max                                  0.330593\n",
      "exploration/Actions Min                                 -0.250378\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.82964\n",
      "exploration/env_infos/final/reward_dist Mean             0.0707797\n",
      "exploration/env_infos/final/reward_dist Std              0.137737\n",
      "exploration/env_infos/final/reward_dist Max              0.346211\n",
      "exploration/env_infos/final/reward_dist Min              3.72994e-35\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0150057\n",
      "exploration/env_infos/initial/reward_dist Std            0.0182832\n",
      "exploration/env_infos/initial/reward_dist Max            0.0434135\n",
      "exploration/env_infos/initial/reward_dist Min            1.13222e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.163333\n",
      "exploration/env_infos/reward_dist Std                    0.252853\n",
      "exploration/env_infos/reward_dist Max                    0.980687\n",
      "exploration/env_infos/reward_dist Min                    3.72994e-35\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0887638\n",
      "exploration/env_infos/final/reward_energy Std            0.0347684\n",
      "exploration/env_infos/final/reward_energy Max           -0.0281889\n",
      "exploration/env_infos/final/reward_energy Min           -0.121169\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.20304\n",
      "exploration/env_infos/initial/reward_energy Std          0.0760875\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0987474\n",
      "exploration/env_infos/initial/reward_energy Min         -0.330613\n",
      "exploration/env_infos/reward_energy Mean                -0.119357\n",
      "exploration/env_infos/reward_energy Std                  0.0683724\n",
      "exploration/env_infos/reward_energy Max                 -0.0116462\n",
      "exploration/env_infos/reward_energy Min                 -0.350358\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.166991\n",
      "exploration/env_infos/final/end_effector_loc Std         0.315079\n",
      "exploration/env_infos/final/end_effector_loc Max         0.803847\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.306483\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00346441\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00683858\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0165297\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00982475\n",
      "exploration/env_infos/end_effector_loc Mean              0.0858867\n",
      "exploration/env_infos/end_effector_loc Std               0.173915\n",
      "exploration/env_infos/end_effector_loc Max               0.803847\n",
      "exploration/env_infos/end_effector_loc Min              -0.306483\n",
      "evaluation/num steps total                           23000\n",
      "evaluation/num paths total                            1150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0823196\n",
      "evaluation/Rewards Std                                   0.0681208\n",
      "evaluation/Rewards Max                                   0.127273\n",
      "evaluation/Rewards Min                                  -0.391238\n",
      "evaluation/Returns Mean                                 -1.64639\n",
      "evaluation/Returns Std                                   1.14259\n",
      "evaluation/Returns Max                                   1.50547\n",
      "evaluation/Returns Min                                  -4.2164\n",
      "evaluation/Actions Mean                                 -0.00348344\n",
      "evaluation/Actions Std                                   0.0552982\n",
      "evaluation/Actions Max                                   0.394784\n",
      "evaluation/Actions Min                                  -0.822481\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.64639\n",
      "evaluation/env_infos/final/reward_dist Mean              0.18094\n",
      "evaluation/env_infos/final/reward_dist Std               0.31012\n",
      "evaluation/env_infos/final/reward_dist Max               0.979428\n",
      "evaluation/env_infos/final/reward_dist Min               2.35261e-22\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00445483\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00894614\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0436059\n",
      "evaluation/env_infos/initial/reward_dist Min             1.39161e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.122351\n",
      "evaluation/env_infos/reward_dist Std                     0.221854\n",
      "evaluation/env_infos/reward_dist Max                     0.999277\n",
      "evaluation/env_infos/reward_dist Min                     2.35261e-22\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0586889\n",
      "evaluation/env_infos/final/reward_energy Std             0.0636247\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0119675\n",
      "evaluation/env_infos/final/reward_energy Min            -0.345051\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.154708\n",
      "evaluation/env_infos/initial/reward_energy Std           0.147983\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0217239\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.830936\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0523917\n",
      "evaluation/env_infos/reward_energy Std                   0.058268\n",
      "evaluation/env_infos/reward_energy Max                  -0.000632169\n",
      "evaluation/env_infos/reward_energy Min                  -0.830936\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00954474\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.252928\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.749336\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.585671\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00049892\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00755267\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0197392\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0411241\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0129256\n",
      "evaluation/env_infos/end_effector_loc Std                0.149668\n",
      "evaluation/env_infos/end_effector_loc Max                0.749336\n",
      "evaluation/env_infos/end_effector_loc Min               -0.585671\n",
      "time/data storing (s)                                    0.00303623\n",
      "time/evaluation sampling (s)                             1.03329\n",
      "time/exploration sampling (s)                            0.125712\n",
      "time/logging (s)                                         0.0198914\n",
      "time/saving (s)                                          0.0274667\n",
      "time/training (s)                                       46.4538\n",
      "time/epoch (s)                                          47.6632\n",
      "time/total (s)                                        1079.16\n",
      "Epoch                                                   22\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:43:42.909944 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 23 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000845264\r\n",
      "trainer/QF2 Loss                                         0.000738536\r\n",
      "trainer/Policy Loss                                      3.24365\r\n",
      "trainer/Q1 Predictions Mean                             -1.47428\r\n",
      "trainer/Q1 Predictions Std                               0.926854\r\n",
      "trainer/Q1 Predictions Max                               0.0180639\r\n",
      "trainer/Q1 Predictions Min                              -3.9728\r\n",
      "trainer/Q2 Predictions Mean                             -1.46816\r\n",
      "trainer/Q2 Predictions Std                               0.923123\r\n",
      "trainer/Q2 Predictions Max                               0.0499661\r\n",
      "trainer/Q2 Predictions Min                              -3.9255\r\n",
      "trainer/Q Targets Mean                                  -1.46658\r\n",
      "trainer/Q Targets Std                                    0.92405\r\n",
      "trainer/Q Targets Max                                    0.0203456\r\n",
      "trainer/Q Targets Min                                   -3.91251\r\n",
      "trainer/Log Pis Mean                                     1.7836\r\n",
      "trainer/Log Pis Std                                      1.25738\r\n",
      "trainer/Log Pis Max                                      4.06054\r\n",
      "trainer/Log Pis Min                                     -6.73606\r\n",
      "trainer/Policy mu Mean                                   0.00486339\r\n",
      "trainer/Policy mu Std                                    0.214854\r\n",
      "trainer/Policy mu Max                                    2.09769\r\n",
      "trainer/Policy mu Min                                   -0.540247\r\n",
      "trainer/Policy log std Mean                             -2.29203\r\n",
      "trainer/Policy log std Std                               0.464893\r\n",
      "trainer/Policy log std Max                              -0.62013\r\n",
      "trainer/Policy log std Min                              -3.0852\r\n",
      "trainer/Alpha                                            0.021282\r\n",
      "trainer/Alpha Loss                                      -0.832993\r\n",
      "exploration/num steps total                           3400\r\n",
      "exploration/num paths total                            170\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.121694\r\n",
      "exploration/Rewards Std                                  0.0714952\r\n",
      "exploration/Rewards Max                                 -0.0272096\r\n",
      "exploration/Rewards Min                                 -0.376006\r\n",
      "exploration/Returns Mean                                -2.43387\r\n",
      "exploration/Returns Std                                  0.759646\r\n",
      "exploration/Returns Max                                 -1.61762\r\n",
      "exploration/Returns Min                                 -3.60402\r\n",
      "exploration/Actions Mean                                -0.00160453\r\n",
      "exploration/Actions Std                                  0.148671\r\n",
      "exploration/Actions Max                                  0.573589\r\n",
      "exploration/Actions Min                                 -0.392601\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.43387\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0357856\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0382002\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0975672\r\n",
      "exploration/env_infos/final/reward_dist Min              1.33244e-36\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00778208\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0106843\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.027118\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.49011e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.136185\r\n",
      "exploration/env_infos/reward_dist Std                    0.209742\r\n",
      "exploration/env_infos/reward_dist Max                    0.944358\r\n",
      "exploration/env_infos/reward_dist Min                    1.33244e-36\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.133502\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0319811\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.079353\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.172768\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.332912\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.159562\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0907183\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.579413\r\n",
      "exploration/env_infos/reward_energy Mean                -0.184057\r\n",
      "exploration/env_infos/reward_energy Std                  0.101658\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0215264\r\n",
      "exploration/env_infos/reward_energy Min                 -0.579413\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0112904\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.327038\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.477011\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.755059\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00506901\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0120278\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0286795\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0138935\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0114126\r\n",
      "exploration/env_infos/end_effector_loc Std               0.196484\r\n",
      "exploration/env_infos/end_effector_loc Max               0.477011\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.755059\r\n",
      "evaluation/num steps total                           24000\r\n",
      "evaluation/num paths total                            1200\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0668052\r\n",
      "evaluation/Rewards Std                                   0.0905556\r\n",
      "evaluation/Rewards Max                                   0.117957\r\n",
      "evaluation/Rewards Min                                  -0.587621\r\n",
      "evaluation/Returns Mean                                 -1.3361\r\n",
      "evaluation/Returns Std                                   1.32925\r\n",
      "evaluation/Returns Max                                   0.637623\r\n",
      "evaluation/Returns Min                                  -5.46529\r\n",
      "evaluation/Actions Mean                                 -0.0045642\r\n",
      "evaluation/Actions Std                                   0.070436\r\n",
      "evaluation/Actions Max                                   0.427441\r\n",
      "evaluation/Actions Min                                  -0.789231\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.3361\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.103558\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.205055\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.734825\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.50247e-44\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00467618\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00749599\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0327052\r\n",
      "evaluation/env_infos/initial/reward_dist Min             6.04762e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.106973\r\n",
      "evaluation/env_infos/reward_dist Std                     0.18091\r\n",
      "evaluation/env_infos/reward_dist Max                     0.988565\r\n",
      "evaluation/env_infos/reward_dist Min                     4.50247e-44\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0779269\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0656574\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00832536\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.418494\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.189587\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.168144\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0310469\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.799563\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0689684\r\n",
      "evaluation/env_infos/reward_energy Std                   0.072163\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00133464\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.799563\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0959188\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.299924\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.757402\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.683907\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00286836\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00848777\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0163386\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0394615\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.05306\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.18292\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.757402\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.683907\r\n",
      "time/data storing (s)                                    0.00310695\r\n",
      "time/evaluation sampling (s)                             0.977444\r\n",
      "time/exploration sampling (s)                            0.115789\r\n",
      "time/logging (s)                                         0.0196122\r\n",
      "time/saving (s)                                          0.0297334\r\n",
      "time/training (s)                                       45.7869\r\n",
      "time/epoch (s)                                          46.9326\r\n",
      "time/total (s)                                        1126.42\r\n",
      "Epoch                                                   23\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:44:30.773688 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 24 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00103472\r\n",
      "trainer/QF2 Loss                                         0.00262344\r\n",
      "trainer/Policy Loss                                      3.38863\r\n",
      "trainer/Q1 Predictions Mean                             -1.50708\r\n",
      "trainer/Q1 Predictions Std                               0.903178\r\n",
      "trainer/Q1 Predictions Max                               0.028404\r\n",
      "trainer/Q1 Predictions Min                              -3.87102\r\n",
      "trainer/Q2 Predictions Mean                             -1.49204\r\n",
      "trainer/Q2 Predictions Std                               0.897398\r\n",
      "trainer/Q2 Predictions Max                               0.0144268\r\n",
      "trainer/Q2 Predictions Min                              -3.91756\r\n",
      "trainer/Q Targets Mean                                  -1.49779\r\n",
      "trainer/Q Targets Std                                    0.90045\r\n",
      "trainer/Q Targets Max                                    0.0203456\r\n",
      "trainer/Q Targets Min                                   -3.89184\r\n",
      "trainer/Log Pis Mean                                     1.89091\r\n",
      "trainer/Log Pis Std                                      1.35864\r\n",
      "trainer/Log Pis Max                                      4.39738\r\n",
      "trainer/Log Pis Min                                     -3.28733\r\n",
      "trainer/Policy mu Mean                                  -0.0201926\r\n",
      "trainer/Policy mu Std                                    0.303422\r\n",
      "trainer/Policy mu Max                                    2.2092\r\n",
      "trainer/Policy mu Min                                   -2.16765\r\n",
      "trainer/Policy log std Mean                             -2.2858\r\n",
      "trainer/Policy log std Std                               0.519817\r\n",
      "trainer/Policy log std Max                              -0.418502\r\n",
      "trainer/Policy log std Min                              -3.35979\r\n",
      "trainer/Alpha                                            0.0203439\r\n",
      "trainer/Alpha Loss                                      -0.424652\r\n",
      "exploration/num steps total                           3500\r\n",
      "exploration/num paths total                            175\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.119444\r\n",
      "exploration/Rewards Std                                  0.0577276\r\n",
      "exploration/Rewards Max                                 -0.0380516\r\n",
      "exploration/Rewards Min                                 -0.390374\r\n",
      "exploration/Returns Mean                                -2.38888\r\n",
      "exploration/Returns Std                                  0.314438\r\n",
      "exploration/Returns Max                                 -1.99009\r\n",
      "exploration/Returns Min                                 -2.7715\r\n",
      "exploration/Actions Mean                                -0.00777254\r\n",
      "exploration/Actions Std                                  0.080656\r\n",
      "exploration/Actions Max                                  0.257109\r\n",
      "exploration/Actions Min                                 -0.31836\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.38888\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.000831564\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00160144\r\n",
      "exploration/env_infos/final/reward_dist Max              0.00403375\r\n",
      "exploration/env_infos/final/reward_dist Min              2.83523e-14\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00235145\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00446793\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0112845\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.70187e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0256169\r\n",
      "exploration/env_infos/reward_dist Std                    0.0713609\r\n",
      "exploration/env_infos/reward_dist Max                    0.46613\r\n",
      "exploration/env_infos/reward_dist Min                    2.83523e-14\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.162422\r\n",
      "exploration/env_infos/final/reward_energy Std            0.100143\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0511202\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.345145\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.151178\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0734582\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0657675\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.257935\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0980831\r\n",
      "exploration/env_infos/reward_energy Std                  0.0592564\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0136538\r\n",
      "exploration/env_infos/reward_energy Min                 -0.345145\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0624545\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.254441\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.316923\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.556561\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00187483\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00563901\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0128554\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00828111\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0306966\r\n",
      "exploration/env_infos/end_effector_loc Std               0.14491\r\n",
      "exploration/env_infos/end_effector_loc Max               0.316923\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.556561\r\n",
      "evaluation/num steps total                           25000\r\n",
      "evaluation/num paths total                            1250\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0664353\r\n",
      "evaluation/Rewards Std                                   0.0728691\r\n",
      "evaluation/Rewards Max                                   0.12772\r\n",
      "evaluation/Rewards Min                                  -0.597638\r\n",
      "evaluation/Returns Mean                                 -1.32871\r\n",
      "evaluation/Returns Std                                   1.04792\r\n",
      "evaluation/Returns Max                                   0.472059\r\n",
      "evaluation/Returns Min                                  -3.803\r\n",
      "evaluation/Actions Mean                                 -0.00219815\r\n",
      "evaluation/Actions Std                                   0.0759196\r\n",
      "evaluation/Actions Max                                   0.933951\r\n",
      "evaluation/Actions Min                                  -0.919217\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.32871\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0767016\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.178398\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.83875\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.18975e-37\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00639741\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0192363\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.132789\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0043e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.103883\r\n",
      "evaluation/env_infos/reward_dist Std                     0.197216\r\n",
      "evaluation/env_infos/reward_dist Max                     0.973228\r\n",
      "evaluation/env_infos/reward_dist Min                     1.18975e-37\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.068687\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0368885\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0120729\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.148536\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.215859\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.263531\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.028544\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.31043\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0665136\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0843396\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000437994\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.31043\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0437687\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.305752\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.757639\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.739353\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00162892\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0119332\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0466975\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0459608\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0248649\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.189145\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.757639\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.739353\r\n",
      "time/data storing (s)                                    0.00295545\r\n",
      "time/evaluation sampling (s)                             1.05557\r\n",
      "time/exploration sampling (s)                            0.121922\r\n",
      "time/logging (s)                                         0.0196189\r\n",
      "time/saving (s)                                          0.0281453\r\n",
      "time/training (s)                                       46.2969\r\n",
      "time/epoch (s)                                          47.5251\r\n",
      "time/total (s)                                        1174.28\r\n",
      "Epoch                                                   24\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:45:18.462529 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 25 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00192642\n",
      "trainer/QF2 Loss                                         0.0037633\n",
      "trainer/Policy Loss                                      3.61657\n",
      "trainer/Q1 Predictions Mean                             -1.44162\n",
      "trainer/Q1 Predictions Std                               0.874737\n",
      "trainer/Q1 Predictions Max                              -0.000672296\n",
      "trainer/Q1 Predictions Min                              -4.15411\n",
      "trainer/Q2 Predictions Mean                             -1.4362\n",
      "trainer/Q2 Predictions Std                               0.886233\n",
      "trainer/Q2 Predictions Max                               0.0374026\n",
      "trainer/Q2 Predictions Min                              -4.28273\n",
      "trainer/Q Targets Mean                                  -1.43645\n",
      "trainer/Q Targets Std                                    0.880393\n",
      "trainer/Q Targets Max                                    0.0350086\n",
      "trainer/Q Targets Min                                   -4.17356\n",
      "trainer/Log Pis Mean                                     2.19224\n",
      "trainer/Log Pis Std                                      1.34198\n",
      "trainer/Log Pis Max                                      4.39552\n",
      "trainer/Log Pis Min                                     -4.02113\n",
      "trainer/Policy mu Mean                                  -0.00145799\n",
      "trainer/Policy mu Std                                    0.292317\n",
      "trainer/Policy mu Max                                    2.3997\n",
      "trainer/Policy mu Min                                   -2.14093\n",
      "trainer/Policy log std Mean                             -2.41379\n",
      "trainer/Policy log std Std                               0.548914\n",
      "trainer/Policy log std Max                              -0.257925\n",
      "trainer/Policy log std Min                              -3.27876\n",
      "trainer/Alpha                                            0.020647\n",
      "trainer/Alpha Loss                                       0.745945\n",
      "exploration/num steps total                           3600\n",
      "exploration/num paths total                            180\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.188863\n",
      "exploration/Rewards Std                                  0.122258\n",
      "exploration/Rewards Max                                 -0.0628688\n",
      "exploration/Rewards Min                                 -0.742205\n",
      "exploration/Returns Mean                                -3.77726\n",
      "exploration/Returns Std                                  1.49607\n",
      "exploration/Returns Max                                 -2.45397\n",
      "exploration/Returns Min                                 -6.47996\n",
      "exploration/Actions Mean                                -0.0105934\n",
      "exploration/Actions Std                                  0.166051\n",
      "exploration/Actions Max                                  0.831515\n",
      "exploration/Actions Min                                 -0.707685\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.77726\n",
      "exploration/env_infos/final/reward_dist Mean             0.0199669\n",
      "exploration/env_infos/final/reward_dist Std              0.0399338\n",
      "exploration/env_infos/final/reward_dist Max              0.0998346\n",
      "exploration/env_infos/final/reward_dist Min              1.02853e-83\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00363422\n",
      "exploration/env_infos/initial/reward_dist Std            0.00333577\n",
      "exploration/env_infos/initial/reward_dist Max            0.00870617\n",
      "exploration/env_infos/initial/reward_dist Min            2.07171e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.029488\n",
      "exploration/env_infos/reward_dist Std                    0.0977123\n",
      "exploration/env_infos/reward_dist Max                    0.748551\n",
      "exploration/env_infos/reward_dist Min                    1.02853e-83\n",
      "exploration/env_infos/final/reward_energy Mean          -0.172607\n",
      "exploration/env_infos/final/reward_energy Std            0.0955641\n",
      "exploration/env_infos/final/reward_energy Max           -0.0616236\n",
      "exploration/env_infos/final/reward_energy Min           -0.321439\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.393064\n",
      "exploration/env_infos/initial/reward_energy Std          0.382627\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0980722\n",
      "exploration/env_infos/initial/reward_energy Min         -1.0919\n",
      "exploration/env_infos/reward_energy Mean                -0.175356\n",
      "exploration/env_infos/reward_energy Std                  0.156909\n",
      "exploration/env_infos/reward_energy Max                 -0.0251731\n",
      "exploration/env_infos/reward_energy Min                 -1.0919\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0834723\n",
      "exploration/env_infos/final/end_effector_loc Std         0.526149\n",
      "exploration/env_infos/final/end_effector_loc Max         0.634092\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.919464\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000959299\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0193703\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0415758\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0353842\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0260919\n",
      "exploration/env_infos/end_effector_loc Std               0.301315\n",
      "exploration/env_infos/end_effector_loc Max               0.634092\n",
      "exploration/env_infos/end_effector_loc Min              -0.919464\n",
      "evaluation/num steps total                           26000\n",
      "evaluation/num paths total                            1300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0819103\n",
      "evaluation/Rewards Std                                   0.115202\n",
      "evaluation/Rewards Max                                   0.141297\n",
      "evaluation/Rewards Min                                  -0.749245\n",
      "evaluation/Returns Mean                                 -1.63821\n",
      "evaluation/Returns Std                                   1.90608\n",
      "evaluation/Returns Max                                   1.0731\n",
      "evaluation/Returns Min                                  -9.08768\n",
      "evaluation/Actions Mean                                  0.00163399\n",
      "evaluation/Actions Std                                   0.0945268\n",
      "evaluation/Actions Max                                   0.697414\n",
      "evaluation/Actions Min                                  -0.914458\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.63821\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0589946\n",
      "evaluation/env_infos/final/reward_dist Std               0.152116\n",
      "evaluation/env_infos/final/reward_dist Max               0.89552\n",
      "evaluation/env_infos/final/reward_dist Min               8.33754e-73\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0054505\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00889244\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0362764\n",
      "evaluation/env_infos/initial/reward_dist Min             1.58227e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.110301\n",
      "evaluation/env_infos/reward_dist Std                     0.210319\n",
      "evaluation/env_infos/reward_dist Max                     0.997319\n",
      "evaluation/env_infos/reward_dist Min                     8.33754e-73\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0667749\n",
      "evaluation/env_infos/final/reward_energy Std             0.0548726\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00602081\n",
      "evaluation/env_infos/final/reward_energy Min            -0.2698\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.24335\n",
      "evaluation/env_infos/initial/reward_energy Std           0.257038\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0147075\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.971556\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0795428\n",
      "evaluation/env_infos/reward_energy Std                   0.107466\n",
      "evaluation/env_infos/reward_energy Max                  -0.000613486\n",
      "evaluation/env_infos/reward_energy Min                  -1.10715\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0362375\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.301432\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.839747\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.844749\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00369727\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0119557\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0302735\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0457229\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0377041\n",
      "evaluation/env_infos/end_effector_loc Std                0.198189\n",
      "evaluation/env_infos/end_effector_loc Max                0.839747\n",
      "evaluation/env_infos/end_effector_loc Min               -0.844749\n",
      "time/data storing (s)                                    0.00344935\n",
      "time/evaluation sampling (s)                             0.939588\n",
      "time/exploration sampling (s)                            0.12251\n",
      "time/logging (s)                                         0.0211877\n",
      "time/saving (s)                                          0.0284435\n",
      "time/training (s)                                       46.1892\n",
      "time/epoch (s)                                          47.3044\n",
      "time/total (s)                                        1221.97\n",
      "Epoch                                                   25\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:46:06.819076 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 26 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00228066\r\n",
      "trainer/QF2 Loss                                         0.00218909\r\n",
      "trainer/Policy Loss                                      3.46209\r\n",
      "trainer/Q1 Predictions Mean                             -1.43709\r\n",
      "trainer/Q1 Predictions Std                               0.846088\r\n",
      "trainer/Q1 Predictions Max                               0.115269\r\n",
      "trainer/Q1 Predictions Min                              -3.89597\r\n",
      "trainer/Q2 Predictions Mean                             -1.40883\r\n",
      "trainer/Q2 Predictions Std                               0.837883\r\n",
      "trainer/Q2 Predictions Max                               0.0753636\r\n",
      "trainer/Q2 Predictions Min                              -3.87255\r\n",
      "trainer/Q Targets Mean                                  -1.42218\r\n",
      "trainer/Q Targets Std                                    0.848074\r\n",
      "trainer/Q Targets Max                                    0.126095\r\n",
      "trainer/Q Targets Min                                   -3.9578\r\n",
      "trainer/Log Pis Mean                                     2.0403\r\n",
      "trainer/Log Pis Std                                      1.21157\r\n",
      "trainer/Log Pis Max                                      4.0894\r\n",
      "trainer/Log Pis Min                                     -3.38142\r\n",
      "trainer/Policy mu Mean                                   0.00570102\r\n",
      "trainer/Policy mu Std                                    0.235485\r\n",
      "trainer/Policy mu Max                                    1.83575\r\n",
      "trainer/Policy mu Min                                   -1.07412\r\n",
      "trainer/Policy log std Mean                             -2.37641\r\n",
      "trainer/Policy log std Std                               0.473602\r\n",
      "trainer/Policy log std Max                              -0.612779\r\n",
      "trainer/Policy log std Min                              -3.316\r\n",
      "trainer/Alpha                                            0.0202056\r\n",
      "trainer/Alpha Loss                                       0.157148\r\n",
      "exploration/num steps total                           3700\r\n",
      "exploration/num paths total                            185\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0719371\r\n",
      "exploration/Rewards Std                                  0.0726162\r\n",
      "exploration/Rewards Max                                  0.0450167\r\n",
      "exploration/Rewards Min                                 -0.318041\r\n",
      "exploration/Returns Mean                                -1.43874\r\n",
      "exploration/Returns Std                                  0.652951\r\n",
      "exploration/Returns Max                                 -0.519682\r\n",
      "exploration/Returns Min                                 -2.48418\r\n",
      "exploration/Actions Mean                                -0.00549966\r\n",
      "exploration/Actions Std                                  0.135288\r\n",
      "exploration/Actions Max                                  0.439915\r\n",
      "exploration/Actions Min                                 -0.443709\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.43874\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00355946\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00412964\r\n",
      "exploration/env_infos/final/reward_dist Max              0.00906741\r\n",
      "exploration/env_infos/final/reward_dist Min              1.01275e-10\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0227187\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0265253\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0724936\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.30809e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.132084\r\n",
      "exploration/env_infos/reward_dist Std                    0.239816\r\n",
      "exploration/env_infos/reward_dist Max                    0.952683\r\n",
      "exploration/env_infos/reward_dist Min                    1.01275e-10\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0974783\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0707078\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.016771\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.19294\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.29403\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.148949\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.095127\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.473352\r\n",
      "exploration/env_infos/reward_energy Mean                -0.153649\r\n",
      "exploration/env_infos/reward_energy Std                  0.114272\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00377446\r\n",
      "exploration/env_infos/reward_energy Min                 -0.473352\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.147408\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.261906\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.339589\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.526127\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00759073\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00884194\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00780837\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0221855\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0985716\r\n",
      "exploration/env_infos/end_effector_loc Std               0.186107\r\n",
      "exploration/env_infos/end_effector_loc Max               0.339589\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.526127\r\n",
      "evaluation/num steps total                           27000\r\n",
      "evaluation/num paths total                            1350\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0824593\r\n",
      "evaluation/Rewards Std                                   0.0807482\r\n",
      "evaluation/Rewards Max                                   0.109391\r\n",
      "evaluation/Rewards Min                                  -0.496691\r\n",
      "evaluation/Returns Mean                                 -1.64919\r\n",
      "evaluation/Returns Std                                   1.25101\r\n",
      "evaluation/Returns Max                                   0.80004\r\n",
      "evaluation/Returns Min                                  -6.2604\r\n",
      "evaluation/Actions Mean                                  0.00734949\r\n",
      "evaluation/Actions Std                                   0.0895855\r\n",
      "evaluation/Actions Max                                   0.868865\r\n",
      "evaluation/Actions Min                                  -0.552673\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.64919\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.050003\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.113415\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.452838\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.13232e-41\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00736621\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0180794\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0981235\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.07941e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.110812\r\n",
      "evaluation/env_infos/reward_dist Std                     0.21178\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993859\r\n",
      "evaluation/env_infos/reward_dist Min                     5.13232e-41\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.060528\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0485656\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00707899\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.210945\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.251763\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.221763\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0139842\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.877523\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0870708\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0926166\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00169746\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.877523\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0302717\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.294874\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.582197\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.831018\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00337334\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0113721\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0434432\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0276336\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0446409\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.196958\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.582197\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.831018\r\n",
      "time/data storing (s)                                    0.00289835\r\n",
      "time/evaluation sampling (s)                             1.13078\r\n",
      "time/exploration sampling (s)                            0.124635\r\n",
      "time/logging (s)                                         0.0192274\r\n",
      "time/saving (s)                                          0.0276353\r\n",
      "time/training (s)                                       46.6736\r\n",
      "time/epoch (s)                                          47.9788\r\n",
      "time/total (s)                                        1270.32\r\n",
      "Epoch                                                   26\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:46:54.671616 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 27 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00117056\n",
      "trainer/QF2 Loss                                         0.000933309\n",
      "trainer/Policy Loss                                      3.38432\n",
      "trainer/Q1 Predictions Mean                             -1.43432\n",
      "trainer/Q1 Predictions Std                               0.837177\n",
      "trainer/Q1 Predictions Max                               0.272819\n",
      "trainer/Q1 Predictions Min                              -3.93213\n",
      "trainer/Q2 Predictions Mean                             -1.43306\n",
      "trainer/Q2 Predictions Std                               0.83038\n",
      "trainer/Q2 Predictions Max                               0.233711\n",
      "trainer/Q2 Predictions Min                              -3.85239\n",
      "trainer/Q Targets Mean                                  -1.43132\n",
      "trainer/Q Targets Std                                    0.831348\n",
      "trainer/Q Targets Max                                    0.243764\n",
      "trainer/Q Targets Min                                   -3.87282\n",
      "trainer/Log Pis Mean                                     1.96409\n",
      "trainer/Log Pis Std                                      1.2736\n",
      "trainer/Log Pis Max                                      4.53841\n",
      "trainer/Log Pis Min                                     -2.97317\n",
      "trainer/Policy mu Mean                                   0.0194747\n",
      "trainer/Policy mu Std                                    0.376847\n",
      "trainer/Policy mu Max                                    2.04941\n",
      "trainer/Policy mu Min                                   -2.04462\n",
      "trainer/Policy log std Mean                             -2.2632\n",
      "trainer/Policy log std Std                               0.586772\n",
      "trainer/Policy log std Max                              -0.363287\n",
      "trainer/Policy log std Min                              -3.12931\n",
      "trainer/Alpha                                            0.0206545\n",
      "trainer/Alpha Loss                                      -0.139351\n",
      "exploration/num steps total                           3800\n",
      "exploration/num paths total                            190\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0936505\n",
      "exploration/Rewards Std                                  0.0833835\n",
      "exploration/Rewards Max                                  0.119909\n",
      "exploration/Rewards Min                                 -0.242174\n",
      "exploration/Returns Mean                                -1.87301\n",
      "exploration/Returns Std                                  1.18356\n",
      "exploration/Returns Max                                  0.381063\n",
      "exploration/Returns Min                                 -3.0046\n",
      "exploration/Actions Mean                                -9.28162e-05\n",
      "exploration/Actions Std                                  0.147226\n",
      "exploration/Actions Max                                  0.731528\n",
      "exploration/Actions Min                                 -0.424503\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.87301\n",
      "exploration/env_infos/final/reward_dist Mean             0.14122\n",
      "exploration/env_infos/final/reward_dist Std              0.197063\n",
      "exploration/env_infos/final/reward_dist Max              0.502831\n",
      "exploration/env_infos/final/reward_dist Min              2.50857e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00665475\n",
      "exploration/env_infos/initial/reward_dist Std            0.0129119\n",
      "exploration/env_infos/initial/reward_dist Max            0.0324748\n",
      "exploration/env_infos/initial/reward_dist Min            6.036e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.170704\n",
      "exploration/env_infos/reward_dist Std                    0.280902\n",
      "exploration/env_infos/reward_dist Max                    0.894967\n",
      "exploration/env_infos/reward_dist Min                    2.50857e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.117\n",
      "exploration/env_infos/final/reward_energy Std            0.02686\n",
      "exploration/env_infos/final/reward_energy Max           -0.0773788\n",
      "exploration/env_infos/final/reward_energy Min           -0.14586\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.316058\n",
      "exploration/env_infos/initial/reward_energy Std          0.316338\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0763855\n",
      "exploration/env_infos/initial/reward_energy Min         -0.93236\n",
      "exploration/env_infos/reward_energy Mean                -0.155975\n",
      "exploration/env_infos/reward_energy Std                  0.137924\n",
      "exploration/env_infos/reward_energy Max                 -0.0136486\n",
      "exploration/env_infos/reward_energy Min                 -0.93236\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0689644\n",
      "exploration/env_infos/final/end_effector_loc Std         0.284833\n",
      "exploration/env_infos/final/end_effector_loc Max         0.753108\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.2885\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00292423\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0155371\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0365764\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0142319\n",
      "exploration/env_infos/end_effector_loc Mean              0.0355849\n",
      "exploration/env_infos/end_effector_loc Std               0.193668\n",
      "exploration/env_infos/end_effector_loc Max               0.753108\n",
      "exploration/env_infos/end_effector_loc Min              -0.2885\n",
      "evaluation/num steps total                           28000\n",
      "evaluation/num paths total                            1400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0778595\n",
      "evaluation/Rewards Std                                   0.121249\n",
      "evaluation/Rewards Max                                   0.175705\n",
      "evaluation/Rewards Min                                  -0.856595\n",
      "evaluation/Returns Mean                                 -1.55719\n",
      "evaluation/Returns Std                                   2.1308\n",
      "evaluation/Returns Max                                   1.38935\n",
      "evaluation/Returns Min                                 -13.1545\n",
      "evaluation/Actions Mean                                  0.00542355\n",
      "evaluation/Actions Std                                   0.0728291\n",
      "evaluation/Actions Max                                   0.543115\n",
      "evaluation/Actions Min                                  -0.776873\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.55719\n",
      "evaluation/env_infos/final/reward_dist Mean              0.121078\n",
      "evaluation/env_infos/final/reward_dist Std               0.227312\n",
      "evaluation/env_infos/final/reward_dist Max               0.84511\n",
      "evaluation/env_infos/final/reward_dist Min               3.69766e-110\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00300244\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00723685\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0472442\n",
      "evaluation/env_infos/initial/reward_dist Min             1.49442e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.144645\n",
      "evaluation/env_infos/reward_dist Std                     0.250774\n",
      "evaluation/env_infos/reward_dist Max                     0.996554\n",
      "evaluation/env_infos/reward_dist Min                     3.69766e-110\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0578203\n",
      "evaluation/env_infos/final/reward_energy Std             0.0604497\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00587563\n",
      "evaluation/env_infos/final/reward_energy Min            -0.296658\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.178263\n",
      "evaluation/env_infos/initial/reward_energy Std           0.199052\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.011272\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.945953\n",
      "evaluation/env_infos/reward_energy Mean                 -0.061335\n",
      "evaluation/env_infos/reward_energy Std                   0.0830965\n",
      "evaluation/env_infos/reward_energy Max                  -0.000613207\n",
      "evaluation/env_infos/reward_energy Min                  -0.945953\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.046963\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.288993\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.694169\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000632911\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00942597\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0206334\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0388437\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00851982\n",
      "evaluation/env_infos/end_effector_loc Std                0.187776\n",
      "evaluation/env_infos/end_effector_loc Max                0.694169\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00293677\n",
      "time/evaluation sampling (s)                             0.954434\n",
      "time/exploration sampling (s)                            0.124643\n",
      "time/logging (s)                                         0.0186633\n",
      "time/saving (s)                                          0.0273865\n",
      "time/training (s)                                       46.3614\n",
      "time/epoch (s)                                          47.4894\n",
      "time/total (s)                                        1318.17\n",
      "Epoch                                                   27\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:47:42.051056 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 28 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000836844\r\n",
      "trainer/QF2 Loss                                         0.00103203\r\n",
      "trainer/Policy Loss                                      3.42811\r\n",
      "trainer/Q1 Predictions Mean                             -1.37537\r\n",
      "trainer/Q1 Predictions Std                               0.864069\r\n",
      "trainer/Q1 Predictions Max                               0.472538\r\n",
      "trainer/Q1 Predictions Min                              -3.8801\r\n",
      "trainer/Q2 Predictions Mean                             -1.39298\r\n",
      "trainer/Q2 Predictions Std                               0.871996\r\n",
      "trainer/Q2 Predictions Max                               0.40889\r\n",
      "trainer/Q2 Predictions Min                              -4.02032\r\n",
      "trainer/Q Targets Mean                                  -1.37427\r\n",
      "trainer/Q Targets Std                                    0.868458\r\n",
      "trainer/Q Targets Max                                    0.4348\r\n",
      "trainer/Q Targets Min                                   -4.00653\r\n",
      "trainer/Log Pis Mean                                     2.05911\r\n",
      "trainer/Log Pis Std                                      1.15668\r\n",
      "trainer/Log Pis Max                                      6.28543\r\n",
      "trainer/Log Pis Min                                     -3.77694\r\n",
      "trainer/Policy mu Mean                                   0.0183732\r\n",
      "trainer/Policy mu Std                                    0.367676\r\n",
      "trainer/Policy mu Max                                    2.24762\r\n",
      "trainer/Policy mu Min                                   -2.01615\r\n",
      "trainer/Policy log std Mean                             -2.3182\r\n",
      "trainer/Policy log std Std                               0.514683\r\n",
      "trainer/Policy log std Max                              -0.280837\r\n",
      "trainer/Policy log std Min                              -3.23126\r\n",
      "trainer/Alpha                                            0.0202401\r\n",
      "trainer/Alpha Loss                                       0.230551\r\n",
      "exploration/num steps total                           3900\r\n",
      "exploration/num paths total                            195\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.055702\r\n",
      "exploration/Rewards Std                                  0.0625389\r\n",
      "exploration/Rewards Max                                  0.0752089\r\n",
      "exploration/Rewards Min                                 -0.228344\r\n",
      "exploration/Returns Mean                                -1.11404\r\n",
      "exploration/Returns Std                                  0.917538\r\n",
      "exploration/Returns Max                                  0.421251\r\n",
      "exploration/Returns Min                                 -2.14373\r\n",
      "exploration/Actions Mean                                 0.00608474\r\n",
      "exploration/Actions Std                                  0.100982\r\n",
      "exploration/Actions Max                                  0.430569\r\n",
      "exploration/Actions Min                                 -0.226309\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.11404\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.139011\r\n",
      "exploration/env_infos/final/reward_dist Std              0.221973\r\n",
      "exploration/env_infos/final/reward_dist Max              0.578239\r\n",
      "exploration/env_infos/final/reward_dist Min              3.05102e-08\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0119336\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0130206\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0307964\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000392136\r\n",
      "exploration/env_infos/reward_dist Mean                   0.275087\r\n",
      "exploration/env_infos/reward_dist Std                    0.285184\r\n",
      "exploration/env_infos/reward_dist Max                    0.930571\r\n",
      "exploration/env_infos/reward_dist Min                    3.05102e-08\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.11755\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0451618\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.065436\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.196319\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.11201\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0449788\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0557613\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.18078\r\n",
      "exploration/env_infos/reward_energy Mean                -0.117153\r\n",
      "exploration/env_infos/reward_energy Std                  0.0821204\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0105278\r\n",
      "exploration/env_infos/reward_energy Min                 -0.430907\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0929161\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.176735\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.336813\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.226047\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00296963\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00306481\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00833534\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00322619\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0446034\r\n",
      "exploration/env_infos/end_effector_loc Std               0.104675\r\n",
      "exploration/env_infos/end_effector_loc Max               0.336813\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.226047\r\n",
      "evaluation/num steps total                           29000\r\n",
      "evaluation/num paths total                            1450\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0775506\r\n",
      "evaluation/Rewards Std                                   0.0889816\r\n",
      "evaluation/Rewards Max                                   0.150317\r\n",
      "evaluation/Rewards Min                                  -0.563364\r\n",
      "evaluation/Returns Mean                                 -1.55101\r\n",
      "evaluation/Returns Std                                   1.45118\r\n",
      "evaluation/Returns Max                                   1.20005\r\n",
      "evaluation/Returns Min                                  -8.71153\r\n",
      "evaluation/Actions Mean                                  0.00568594\r\n",
      "evaluation/Actions Std                                   0.0851227\r\n",
      "evaluation/Actions Max                                   0.510682\r\n",
      "evaluation/Actions Min                                  -0.961774\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.55101\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0536298\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.151092\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.688131\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.36428e-86\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00708609\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117662\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0420442\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.01616e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0916732\r\n",
      "evaluation/env_infos/reward_dist Std                     0.188689\r\n",
      "evaluation/env_infos/reward_dist Max                     0.984362\r\n",
      "evaluation/env_infos/reward_dist Min                     1.71307e-88\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0823674\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0842128\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00826024\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.322326\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.216973\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.215967\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0215917\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.27582\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0783424\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0917545\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00152775\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.27582\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0173664\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.290968\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.543374\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00183255\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106673\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0255341\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0480887\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00579446\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.198391\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.543374\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00315859\r\n",
      "time/evaluation sampling (s)                             0.955819\r\n",
      "time/exploration sampling (s)                            0.121984\r\n",
      "time/logging (s)                                         0.0192455\r\n",
      "time/saving (s)                                          0.0298329\r\n",
      "time/training (s)                                       45.8728\r\n",
      "time/epoch (s)                                          47.0028\r\n",
      "time/total (s)                                        1365.55\r\n",
      "Epoch                                                   28\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:48:29.850608 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 29 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00143346\n",
      "trainer/QF2 Loss                                         0.00145426\n",
      "trainer/Policy Loss                                      3.40982\n",
      "trainer/Q1 Predictions Mean                             -1.34385\n",
      "trainer/Q1 Predictions Std                               0.794182\n",
      "trainer/Q1 Predictions Max                               0.338798\n",
      "trainer/Q1 Predictions Min                              -3.89966\n",
      "trainer/Q2 Predictions Mean                             -1.32362\n",
      "trainer/Q2 Predictions Std                               0.784791\n",
      "trainer/Q2 Predictions Max                               0.381921\n",
      "trainer/Q2 Predictions Min                              -3.87196\n",
      "trainer/Q Targets Mean                                  -1.33658\n",
      "trainer/Q Targets Std                                    0.784752\n",
      "trainer/Q Targets Max                                    0.396806\n",
      "trainer/Q Targets Min                                   -3.93741\n",
      "trainer/Log Pis Mean                                     2.08061\n",
      "trainer/Log Pis Std                                      1.28496\n",
      "trainer/Log Pis Max                                      4.50485\n",
      "trainer/Log Pis Min                                     -3.01126\n",
      "trainer/Policy mu Mean                                   0.00242612\n",
      "trainer/Policy mu Std                                    0.235283\n",
      "trainer/Policy mu Max                                    1.7354\n",
      "trainer/Policy mu Min                                   -1.14995\n",
      "trainer/Policy log std Mean                             -2.39065\n",
      "trainer/Policy log std Std                               0.500581\n",
      "trainer/Policy log std Max                              -0.535807\n",
      "trainer/Policy log std Min                              -3.18573\n",
      "trainer/Alpha                                            0.0207749\n",
      "trainer/Alpha Loss                                       0.312314\n",
      "exploration/num steps total                           4000\n",
      "exploration/num paths total                            200\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.101811\n",
      "exploration/Rewards Std                                  0.0752361\n",
      "exploration/Rewards Max                                  0.0854098\n",
      "exploration/Rewards Min                                 -0.320172\n",
      "exploration/Returns Mean                                -2.03623\n",
      "exploration/Returns Std                                  1.00636\n",
      "exploration/Returns Max                                 -0.826922\n",
      "exploration/Returns Min                                 -3.51336\n",
      "exploration/Actions Mean                                -0.00267165\n",
      "exploration/Actions Std                                  0.125441\n",
      "exploration/Actions Max                                  0.512903\n",
      "exploration/Actions Min                                 -0.451126\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.03623\n",
      "exploration/env_infos/final/reward_dist Mean             0.185554\n",
      "exploration/env_infos/final/reward_dist Std              0.364861\n",
      "exploration/env_infos/final/reward_dist Max              0.915219\n",
      "exploration/env_infos/final/reward_dist Min              1.05527e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103528\n",
      "exploration/env_infos/initial/reward_dist Std            0.0125724\n",
      "exploration/env_infos/initial/reward_dist Max            0.0339781\n",
      "exploration/env_infos/initial/reward_dist Min            0.000198093\n",
      "exploration/env_infos/reward_dist Mean                   0.134622\n",
      "exploration/env_infos/reward_dist Std                    0.249953\n",
      "exploration/env_infos/reward_dist Max                    0.96125\n",
      "exploration/env_infos/reward_dist Min                    1.05527e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.112694\n",
      "exploration/env_infos/final/reward_energy Std            0.0892674\n",
      "exploration/env_infos/final/reward_energy Max           -0.0205583\n",
      "exploration/env_infos/final/reward_energy Min           -0.270084\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.295558\n",
      "exploration/env_infos/initial/reward_energy Std          0.193242\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0610989\n",
      "exploration/env_infos/initial/reward_energy Min         -0.634772\n",
      "exploration/env_infos/reward_energy Mean                -0.142418\n",
      "exploration/env_infos/reward_energy Std                  0.10584\n",
      "exploration/env_infos/reward_energy Max                 -0.00904316\n",
      "exploration/env_infos/reward_energy Min                 -0.634772\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0252763\n",
      "exploration/env_infos/final/end_effector_loc Std         0.299038\n",
      "exploration/env_infos/final/end_effector_loc Max         0.4248\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.673905\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00717461\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0102174\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0256451\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00838616\n",
      "exploration/env_infos/end_effector_loc Mean              0.0192251\n",
      "exploration/env_infos/end_effector_loc Std               0.195176\n",
      "exploration/env_infos/end_effector_loc Max               0.4248\n",
      "exploration/env_infos/end_effector_loc Min              -0.673905\n",
      "evaluation/num steps total                           30000\n",
      "evaluation/num paths total                            1500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0476888\n",
      "evaluation/Rewards Std                                   0.076602\n",
      "evaluation/Rewards Max                                   0.137631\n",
      "evaluation/Rewards Min                                  -0.487326\n",
      "evaluation/Returns Mean                                 -0.953775\n",
      "evaluation/Returns Std                                   1.10766\n",
      "evaluation/Returns Max                                   1.03744\n",
      "evaluation/Returns Min                                  -2.87491\n",
      "evaluation/Actions Mean                                 -0.00248753\n",
      "evaluation/Actions Std                                   0.0801856\n",
      "evaluation/Actions Max                                   0.911033\n",
      "evaluation/Actions Min                                  -0.625356\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.953775\n",
      "evaluation/env_infos/final/reward_dist Mean              0.137634\n",
      "evaluation/env_infos/final/reward_dist Std               0.251972\n",
      "evaluation/env_infos/final/reward_dist Max               0.890035\n",
      "evaluation/env_infos/final/reward_dist Min               2.05399e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00587226\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00960469\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0422461\n",
      "evaluation/env_infos/initial/reward_dist Min             2.35855e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.150229\n",
      "evaluation/env_infos/reward_dist Std                     0.243486\n",
      "evaluation/env_infos/reward_dist Max                     0.99847\n",
      "evaluation/env_infos/reward_dist Min                     2.05399e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0898408\n",
      "evaluation/env_infos/final/reward_energy Std             0.0686247\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00632332\n",
      "evaluation/env_infos/final/reward_energy Min            -0.297953\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.205826\n",
      "evaluation/env_infos/initial/reward_energy Std           0.2116\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0105307\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.921956\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0734961\n",
      "evaluation/env_infos/reward_energy Std                   0.0864301\n",
      "evaluation/env_infos/reward_energy Max                  -0.00111476\n",
      "evaluation/env_infos/reward_energy Min                  -0.921956\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0226645\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.240071\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.466689\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.576811\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000952578\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0103931\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0455516\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0312678\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0100615\n",
      "evaluation/env_infos/end_effector_loc Std                0.157375\n",
      "evaluation/env_infos/end_effector_loc Max                0.49087\n",
      "evaluation/env_infos/end_effector_loc Min               -0.576811\n",
      "time/data storing (s)                                    0.00300334\n",
      "time/evaluation sampling (s)                             0.931769\n",
      "time/exploration sampling (s)                            0.120044\n",
      "time/logging (s)                                         0.0187717\n",
      "time/saving (s)                                          0.0274834\n",
      "time/training (s)                                       46.3177\n",
      "time/epoch (s)                                          47.4188\n",
      "time/total (s)                                        1413.35\n",
      "Epoch                                                   29\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:49:17.360204 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 30 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000868392\n",
      "trainer/QF2 Loss                                         0.00092665\n",
      "trainer/Policy Loss                                      3.17587\n",
      "trainer/Q1 Predictions Mean                             -1.23718\n",
      "trainer/Q1 Predictions Std                               0.813709\n",
      "trainer/Q1 Predictions Max                               0.556193\n",
      "trainer/Q1 Predictions Min                              -3.25137\n",
      "trainer/Q2 Predictions Mean                             -1.2578\n",
      "trainer/Q2 Predictions Std                               0.813467\n",
      "trainer/Q2 Predictions Max                               0.578312\n",
      "trainer/Q2 Predictions Min                              -3.3113\n",
      "trainer/Q Targets Mean                                  -1.25083\n",
      "trainer/Q Targets Std                                    0.816737\n",
      "trainer/Q Targets Max                                    0.578554\n",
      "trainer/Q Targets Min                                   -3.28503\n",
      "trainer/Log Pis Mean                                     1.92624\n",
      "trainer/Log Pis Std                                      1.33906\n",
      "trainer/Log Pis Max                                      4.33227\n",
      "trainer/Log Pis Min                                     -3.99861\n",
      "trainer/Policy mu Mean                                   0.0291556\n",
      "trainer/Policy mu Std                                    0.302697\n",
      "trainer/Policy mu Max                                    1.92191\n",
      "trainer/Policy mu Min                                   -1.52025\n",
      "trainer/Policy log std Mean                             -2.34375\n",
      "trainer/Policy log std Std                               0.552229\n",
      "trainer/Policy log std Max                              -0.321568\n",
      "trainer/Policy log std Min                              -3.19551\n",
      "trainer/Alpha                                            0.0219214\n",
      "trainer/Alpha Loss                                      -0.281782\n",
      "exploration/num steps total                           4100\n",
      "exploration/num paths total                            205\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.124742\n",
      "exploration/Rewards Std                                  0.101751\n",
      "exploration/Rewards Max                                  0.0233141\n",
      "exploration/Rewards Min                                 -0.566925\n",
      "exploration/Returns Mean                                -2.49483\n",
      "exploration/Returns Std                                  1.10729\n",
      "exploration/Returns Max                                 -0.990037\n",
      "exploration/Returns Min                                 -4.37202\n",
      "exploration/Actions Mean                                -0.0192899\n",
      "exploration/Actions Std                                  0.131652\n",
      "exploration/Actions Max                                  0.557396\n",
      "exploration/Actions Min                                 -0.515605\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.49483\n",
      "exploration/env_infos/final/reward_dist Mean             0.000115024\n",
      "exploration/env_infos/final/reward_dist Std              0.000162676\n",
      "exploration/env_infos/final/reward_dist Max              0.000416184\n",
      "exploration/env_infos/final/reward_dist Min              3.0369e-69\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00445585\n",
      "exploration/env_infos/initial/reward_dist Std            0.00507897\n",
      "exploration/env_infos/initial/reward_dist Max            0.0132387\n",
      "exploration/env_infos/initial/reward_dist Min            1.48586e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0473141\n",
      "exploration/env_infos/reward_dist Std                    0.132383\n",
      "exploration/env_infos/reward_dist Max                    0.820441\n",
      "exploration/env_infos/reward_dist Min                    3.0369e-69\n",
      "exploration/env_infos/final/reward_energy Mean          -0.176782\n",
      "exploration/env_infos/final/reward_energy Std            0.101689\n",
      "exploration/env_infos/final/reward_energy Max           -0.0817857\n",
      "exploration/env_infos/final/reward_energy Min           -0.368716\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.11499\n",
      "exploration/env_infos/initial/reward_energy Std          0.0772959\n",
      "exploration/env_infos/initial/reward_energy Max         -0.039025\n",
      "exploration/env_infos/initial/reward_energy Min         -0.209132\n",
      "exploration/env_infos/reward_energy Mean                -0.140006\n",
      "exploration/env_infos/reward_energy Std                  0.125726\n",
      "exploration/env_infos/reward_energy Max                 -0.0039966\n",
      "exploration/env_infos/reward_energy Min                 -0.565691\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.188437\n",
      "exploration/env_infos/final/end_effector_loc Std         0.341598\n",
      "exploration/env_infos/final/end_effector_loc Max         0.329339\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.802257\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00087698\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0048195\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00969946\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00420323\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0700567\n",
      "exploration/env_infos/end_effector_loc Std               0.173904\n",
      "exploration/env_infos/end_effector_loc Max               0.329339\n",
      "exploration/env_infos/end_effector_loc Min              -0.802257\n",
      "evaluation/num steps total                           31000\n",
      "evaluation/num paths total                            1550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0729044\n",
      "evaluation/Rewards Std                                   0.0687716\n",
      "evaluation/Rewards Max                                   0.133651\n",
      "evaluation/Rewards Min                                  -0.363407\n",
      "evaluation/Returns Mean                                 -1.45809\n",
      "evaluation/Returns Std                                   0.958826\n",
      "evaluation/Returns Max                                   0.420791\n",
      "evaluation/Returns Min                                  -3.30132\n",
      "evaluation/Actions Mean                                  0.000209947\n",
      "evaluation/Actions Std                                   0.101427\n",
      "evaluation/Actions Max                                   0.954438\n",
      "evaluation/Actions Min                                  -0.886743\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.45809\n",
      "evaluation/env_infos/final/reward_dist Mean              0.172209\n",
      "evaluation/env_infos/final/reward_dist Std               0.294937\n",
      "evaluation/env_infos/final/reward_dist Max               0.900849\n",
      "evaluation/env_infos/final/reward_dist Min               5.27157e-26\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00578265\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0102414\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0462467\n",
      "evaluation/env_infos/initial/reward_dist Min             1.52378e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.121241\n",
      "evaluation/env_infos/reward_dist Std                     0.22821\n",
      "evaluation/env_infos/reward_dist Max                     0.992421\n",
      "evaluation/env_infos/reward_dist Min                     5.27157e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0603641\n",
      "evaluation/env_infos/final/reward_energy Std             0.0601814\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00366965\n",
      "evaluation/env_infos/final/reward_energy Min            -0.284695\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.23473\n",
      "evaluation/env_infos/initial/reward_energy Std           0.289546\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00779645\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.30279\n",
      "evaluation/env_infos/reward_energy Mean                 -0.084739\n",
      "evaluation/env_infos/reward_energy Std                   0.115734\n",
      "evaluation/env_infos/reward_energy Max                  -0.00101156\n",
      "evaluation/env_infos/reward_energy Min                  -1.30279\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0283729\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.249677\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.542368\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.762873\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000518219\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0131681\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0477219\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0443371\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0177462\n",
      "evaluation/env_infos/end_effector_loc Std                0.16411\n",
      "evaluation/env_infos/end_effector_loc Max                0.548663\n",
      "evaluation/env_infos/end_effector_loc Min               -0.762873\n",
      "time/data storing (s)                                    0.00299595\n",
      "time/evaluation sampling (s)                             0.96168\n",
      "time/exploration sampling (s)                            0.127034\n",
      "time/logging (s)                                         0.0200868\n",
      "time/saving (s)                                          0.0273687\n",
      "time/training (s)                                       45.9843\n",
      "time/epoch (s)                                          47.1235\n",
      "time/total (s)                                        1460.86\n",
      "Epoch                                                   30\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:50:05.177174 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 31 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00105319\n",
      "trainer/QF2 Loss                                         0.00170622\n",
      "trainer/Policy Loss                                      3.33576\n",
      "trainer/Q1 Predictions Mean                             -1.30821\n",
      "trainer/Q1 Predictions Std                               0.841368\n",
      "trainer/Q1 Predictions Max                               0.773772\n",
      "trainer/Q1 Predictions Min                              -3.40724\n",
      "trainer/Q2 Predictions Mean                             -1.31752\n",
      "trainer/Q2 Predictions Std                               0.840216\n",
      "trainer/Q2 Predictions Max                               0.692052\n",
      "trainer/Q2 Predictions Min                              -3.38911\n",
      "trainer/Q Targets Mean                                  -1.30253\n",
      "trainer/Q Targets Std                                    0.842284\n",
      "trainer/Q Targets Max                                    0.78719\n",
      "trainer/Q Targets Min                                   -3.42878\n",
      "trainer/Log Pis Mean                                     2.02568\n",
      "trainer/Log Pis Std                                      1.28525\n",
      "trainer/Log Pis Max                                      4.27801\n",
      "trainer/Log Pis Min                                     -4.08691\n",
      "trainer/Policy mu Mean                                   0.0266079\n",
      "trainer/Policy mu Std                                    0.284967\n",
      "trainer/Policy mu Max                                    2.2716\n",
      "trainer/Policy mu Min                                   -1.65812\n",
      "trainer/Policy log std Mean                             -2.30338\n",
      "trainer/Policy log std Std                               0.558107\n",
      "trainer/Policy log std Max                              -0.192994\n",
      "trainer/Policy log std Min                              -3.22668\n",
      "trainer/Alpha                                            0.0208096\n",
      "trainer/Alpha Loss                                       0.0994424\n",
      "exploration/num steps total                           4200\n",
      "exploration/num paths total                            210\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.136397\n",
      "exploration/Rewards Std                                  0.0449336\n",
      "exploration/Rewards Max                                 -0.00982367\n",
      "exploration/Rewards Min                                 -0.268115\n",
      "exploration/Returns Mean                                -2.72794\n",
      "exploration/Returns Std                                  0.598944\n",
      "exploration/Returns Max                                 -1.74675\n",
      "exploration/Returns Min                                 -3.46367\n",
      "exploration/Actions Mean                                 0.00441041\n",
      "exploration/Actions Std                                  0.0754965\n",
      "exploration/Actions Max                                  0.279963\n",
      "exploration/Actions Min                                 -0.205511\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.72794\n",
      "exploration/env_infos/final/reward_dist Mean             0.023512\n",
      "exploration/env_infos/final/reward_dist Std              0.0355496\n",
      "exploration/env_infos/final/reward_dist Max              0.0927815\n",
      "exploration/env_infos/final/reward_dist Min              4.00104e-13\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0042544\n",
      "exploration/env_infos/initial/reward_dist Std            0.00673618\n",
      "exploration/env_infos/initial/reward_dist Max            0.0176893\n",
      "exploration/env_infos/initial/reward_dist Min            1.28221e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0522036\n",
      "exploration/env_infos/reward_dist Std                    0.153622\n",
      "exploration/env_infos/reward_dist Max                    0.862431\n",
      "exploration/env_infos/reward_dist Min                    4.00104e-13\n",
      "exploration/env_infos/final/reward_energy Mean          -0.139108\n",
      "exploration/env_infos/final/reward_energy Std            0.0435108\n",
      "exploration/env_infos/final/reward_energy Max           -0.0916708\n",
      "exploration/env_infos/final/reward_energy Min           -0.206007\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.146633\n",
      "exploration/env_infos/initial/reward_energy Std          0.0673603\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0331241\n",
      "exploration/env_infos/initial/reward_energy Min         -0.233346\n",
      "exploration/env_infos/reward_energy Mean                -0.0953342\n",
      "exploration/env_infos/reward_energy Std                  0.0484742\n",
      "exploration/env_infos/reward_energy Max                 -0.0240482\n",
      "exploration/env_infos/reward_energy Min                 -0.293867\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0853826\n",
      "exploration/env_infos/final/end_effector_loc Std         0.222733\n",
      "exploration/env_infos/final/end_effector_loc Max         0.457031\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.169338\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00191643\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00537361\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0104349\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00677597\n",
      "exploration/env_infos/end_effector_loc Mean              0.040722\n",
      "exploration/env_infos/end_effector_loc Std               0.135335\n",
      "exploration/env_infos/end_effector_loc Max               0.457031\n",
      "exploration/env_infos/end_effector_loc Min              -0.169338\n",
      "evaluation/num steps total                           32000\n",
      "evaluation/num paths total                            1600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0670496\n",
      "evaluation/Rewards Std                                   0.0762236\n",
      "evaluation/Rewards Max                                   0.138701\n",
      "evaluation/Rewards Min                                  -0.502041\n",
      "evaluation/Returns Mean                                 -1.34099\n",
      "evaluation/Returns Std                                   1.00427\n",
      "evaluation/Returns Max                                   0.819274\n",
      "evaluation/Returns Min                                  -3.85441\n",
      "evaluation/Actions Mean                                  0.00661723\n",
      "evaluation/Actions Std                                   0.0779997\n",
      "evaluation/Actions Max                                   0.724893\n",
      "evaluation/Actions Min                                  -0.605992\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.34099\n",
      "evaluation/env_infos/final/reward_dist Mean              0.10896\n",
      "evaluation/env_infos/final/reward_dist Std               0.199922\n",
      "evaluation/env_infos/final/reward_dist Max               0.896509\n",
      "evaluation/env_infos/final/reward_dist Min               1.64797e-26\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00685253\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0112947\n",
      "evaluation/env_infos/initial/reward_dist Max             0.043635\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97466e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.152013\n",
      "evaluation/env_infos/reward_dist Std                     0.246042\n",
      "evaluation/env_infos/reward_dist Max                     0.995809\n",
      "evaluation/env_infos/reward_dist Min                     1.64797e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0478993\n",
      "evaluation/env_infos/final/reward_energy Std             0.0527738\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00340879\n",
      "evaluation/env_infos/final/reward_energy Min            -0.231816\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.192832\n",
      "evaluation/env_infos/initial/reward_energy Std           0.189688\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0174097\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.81228\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0698787\n",
      "evaluation/env_infos/reward_energy Std                   0.085863\n",
      "evaluation/env_infos/reward_energy Max                  -0.00327803\n",
      "evaluation/env_infos/reward_energy Min                  -0.81228\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0637486\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.28143\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.812117\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.475605\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000404155\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00955479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0362446\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0302996\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0207534\n",
      "evaluation/env_infos/end_effector_loc Std                0.177362\n",
      "evaluation/env_infos/end_effector_loc Max                0.812117\n",
      "evaluation/env_infos/end_effector_loc Min               -0.475605\n",
      "time/data storing (s)                                    0.00294456\n",
      "time/evaluation sampling (s)                             0.979122\n",
      "time/exploration sampling (s)                            0.124152\n",
      "time/logging (s)                                         0.0210123\n",
      "time/saving (s)                                          0.0300203\n",
      "time/training (s)                                       46.242\n",
      "time/epoch (s)                                          47.3992\n",
      "time/total (s)                                        1508.67\n",
      "Epoch                                                   31\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:50:53.506924 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 32 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00075019\n",
      "trainer/QF2 Loss                                         0.000711884\n",
      "trainer/Policy Loss                                      3.25488\n",
      "trainer/Q1 Predictions Mean                             -1.25544\n",
      "trainer/Q1 Predictions Std                               0.785948\n",
      "trainer/Q1 Predictions Max                               0.821711\n",
      "trainer/Q1 Predictions Min                              -3.24505\n",
      "trainer/Q2 Predictions Mean                             -1.25161\n",
      "trainer/Q2 Predictions Std                               0.783929\n",
      "trainer/Q2 Predictions Max                               0.821986\n",
      "trainer/Q2 Predictions Min                              -3.24602\n",
      "trainer/Q Targets Mean                                  -1.2596\n",
      "trainer/Q Targets Std                                    0.792916\n",
      "trainer/Q Targets Max                                    0.840977\n",
      "trainer/Q Targets Min                                   -3.27868\n",
      "trainer/Log Pis Mean                                     2.01259\n",
      "trainer/Log Pis Std                                      1.27611\n",
      "trainer/Log Pis Max                                      4.23461\n",
      "trainer/Log Pis Min                                     -3.20308\n",
      "trainer/Policy mu Mean                                   0.0494255\n",
      "trainer/Policy mu Std                                    0.262388\n",
      "trainer/Policy mu Max                                    2.25552\n",
      "trainer/Policy mu Min                                   -0.902589\n",
      "trainer/Policy log std Mean                             -2.34858\n",
      "trainer/Policy log std Std                               0.543264\n",
      "trainer/Policy log std Max                              -0.265509\n",
      "trainer/Policy log std Min                              -3.14608\n",
      "trainer/Alpha                                            0.0229554\n",
      "trainer/Alpha Loss                                       0.0475195\n",
      "exploration/num steps total                           4300\n",
      "exploration/num paths total                            215\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.103459\n",
      "exploration/Rewards Std                                  0.0947882\n",
      "exploration/Rewards Max                                  0.081175\n",
      "exploration/Rewards Min                                 -0.336021\n",
      "exploration/Returns Mean                                -2.06919\n",
      "exploration/Returns Std                                  1.3674\n",
      "exploration/Returns Max                                  0.265208\n",
      "exploration/Returns Min                                 -3.75538\n",
      "exploration/Actions Mean                                 0.00442631\n",
      "exploration/Actions Std                                  0.203609\n",
      "exploration/Actions Max                                  0.910404\n",
      "exploration/Actions Min                                 -0.879292\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.06919\n",
      "exploration/env_infos/final/reward_dist Mean             0.0819507\n",
      "exploration/env_infos/final/reward_dist Std              0.103409\n",
      "exploration/env_infos/final/reward_dist Max              0.244303\n",
      "exploration/env_infos/final/reward_dist Min              1.84696e-20\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0172631\n",
      "exploration/env_infos/initial/reward_dist Std            0.0113907\n",
      "exploration/env_infos/initial/reward_dist Max            0.0363455\n",
      "exploration/env_infos/initial/reward_dist Min            0.00545669\n",
      "exploration/env_infos/reward_dist Mean                   0.0924413\n",
      "exploration/env_infos/reward_dist Std                    0.17842\n",
      "exploration/env_infos/reward_dist Max                    0.720027\n",
      "exploration/env_infos/reward_dist Min                    1.84696e-20\n",
      "exploration/env_infos/final/reward_energy Mean          -0.102541\n",
      "exploration/env_infos/final/reward_energy Std            0.0596053\n",
      "exploration/env_infos/final/reward_energy Max           -0.0280562\n",
      "exploration/env_infos/final/reward_energy Min           -0.210534\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.510908\n",
      "exploration/env_infos/initial/reward_energy Std          0.487991\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0457979\n",
      "exploration/env_infos/initial/reward_energy Min         -1.2657\n",
      "exploration/env_infos/reward_energy Mean                -0.207598\n",
      "exploration/env_infos/reward_energy Std                  0.199638\n",
      "exploration/env_infos/reward_energy Max                 -0.0112263\n",
      "exploration/env_infos/reward_energy Min                 -1.2657\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.185495\n",
      "exploration/env_infos/final/end_effector_loc Std         0.212326\n",
      "exploration/env_infos/final/end_effector_loc Max         0.72325\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.0334009\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00754701\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0238117\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0455202\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0439646\n",
      "exploration/env_infos/end_effector_loc Mean              0.102439\n",
      "exploration/env_infos/end_effector_loc Std               0.204913\n",
      "exploration/env_infos/end_effector_loc Max               0.72325\n",
      "exploration/env_infos/end_effector_loc Min              -0.424709\n",
      "evaluation/num steps total                           33000\n",
      "evaluation/num paths total                            1650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0597704\n",
      "evaluation/Rewards Std                                   0.0790739\n",
      "evaluation/Rewards Max                                   0.122847\n",
      "evaluation/Rewards Min                                  -0.456664\n",
      "evaluation/Returns Mean                                 -1.19541\n",
      "evaluation/Returns Std                                   1.17868\n",
      "evaluation/Returns Max                                   1.16564\n",
      "evaluation/Returns Min                                  -3.30228\n",
      "evaluation/Actions Mean                                  0.00946841\n",
      "evaluation/Actions Std                                   0.0817082\n",
      "evaluation/Actions Max                                   0.98935\n",
      "evaluation/Actions Min                                  -0.758869\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.19541\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0795869\n",
      "evaluation/env_infos/final/reward_dist Std               0.195023\n",
      "evaluation/env_infos/final/reward_dist Max               0.952876\n",
      "evaluation/env_infos/final/reward_dist Min               6.67184e-31\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00708472\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0109615\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0487913\n",
      "evaluation/env_infos/initial/reward_dist Min             1.2844e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.137477\n",
      "evaluation/env_infos/reward_dist Std                     0.228536\n",
      "evaluation/env_infos/reward_dist Max                     0.990842\n",
      "evaluation/env_infos/reward_dist Min                     6.67184e-31\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0360925\n",
      "evaluation/env_infos/final/reward_energy Std             0.0434452\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00639578\n",
      "evaluation/env_infos/final/reward_energy Min            -0.262577\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.204623\n",
      "evaluation/env_infos/initial/reward_energy Std           0.221104\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0057304\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.24687\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0726766\n",
      "evaluation/env_infos/reward_energy Std                   0.0908289\n",
      "evaluation/env_infos/reward_energy Max                  -0.00103516\n",
      "evaluation/env_infos/reward_energy Min                  -1.24687\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0655513\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.264282\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.805094\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.649783\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000438643\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106421\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0494675\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0379435\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0121129\n",
      "evaluation/env_infos/end_effector_loc Std                0.176808\n",
      "evaluation/env_infos/end_effector_loc Max                0.805094\n",
      "evaluation/env_infos/end_effector_loc Min               -0.649783\n",
      "time/data storing (s)                                    0.00304569\n",
      "time/evaluation sampling (s)                             1.03075\n",
      "time/exploration sampling (s)                            0.121423\n",
      "time/logging (s)                                         0.0205147\n",
      "time/saving (s)                                          0.0296664\n",
      "time/training (s)                                       46.6471\n",
      "time/epoch (s)                                          47.8525\n",
      "time/total (s)                                        1557\n",
      "Epoch                                                   32\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:51:41.688999 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 33 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00109556\n",
      "trainer/QF2 Loss                                         0.00108879\n",
      "trainer/Policy Loss                                      3.17819\n",
      "trainer/Q1 Predictions Mean                             -1.28395\n",
      "trainer/Q1 Predictions Std                               0.817455\n",
      "trainer/Q1 Predictions Max                               0.853905\n",
      "trainer/Q1 Predictions Min                              -3.31159\n",
      "trainer/Q2 Predictions Mean                             -1.29418\n",
      "trainer/Q2 Predictions Std                               0.816651\n",
      "trainer/Q2 Predictions Max                               0.871622\n",
      "trainer/Q2 Predictions Min                              -3.38168\n",
      "trainer/Q Targets Mean                                  -1.28498\n",
      "trainer/Q Targets Std                                    0.821735\n",
      "trainer/Q Targets Max                                    0.852357\n",
      "trainer/Q Targets Min                                   -3.3464\n",
      "trainer/Log Pis Mean                                     1.88772\n",
      "trainer/Log Pis Std                                      1.21953\n",
      "trainer/Log Pis Max                                      4.80274\n",
      "trainer/Log Pis Min                                     -2.60258\n",
      "trainer/Policy mu Mean                                   0.0277613\n",
      "trainer/Policy mu Std                                    0.277339\n",
      "trainer/Policy mu Max                                    2.29856\n",
      "trainer/Policy mu Min                                   -1.79327\n",
      "trainer/Policy log std Mean                             -2.29707\n",
      "trainer/Policy log std Std                               0.531072\n",
      "trainer/Policy log std Max                              -0.413334\n",
      "trainer/Policy log std Min                              -3.1734\n",
      "trainer/Alpha                                            0.021398\n",
      "trainer/Alpha Loss                                      -0.4316\n",
      "exploration/num steps total                           4400\n",
      "exploration/num paths total                            220\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.127169\n",
      "exploration/Rewards Std                                  0.068734\n",
      "exploration/Rewards Max                                 -0.0110828\n",
      "exploration/Rewards Min                                 -0.305158\n",
      "exploration/Returns Mean                                -2.54338\n",
      "exploration/Returns Std                                  0.540036\n",
      "exploration/Returns Max                                 -1.68848\n",
      "exploration/Returns Min                                 -3.21045\n",
      "exploration/Actions Mean                                 0.00333491\n",
      "exploration/Actions Std                                  0.1516\n",
      "exploration/Actions Max                                  0.584744\n",
      "exploration/Actions Min                                 -0.53535\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.54338\n",
      "exploration/env_infos/final/reward_dist Mean             0.00256698\n",
      "exploration/env_infos/final/reward_dist Std              0.00489217\n",
      "exploration/env_infos/final/reward_dist Max              0.0123439\n",
      "exploration/env_infos/final/reward_dist Min              5.6276e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0066095\n",
      "exploration/env_infos/initial/reward_dist Std            0.00741077\n",
      "exploration/env_infos/initial/reward_dist Max            0.0163727\n",
      "exploration/env_infos/initial/reward_dist Min            0.000131801\n",
      "exploration/env_infos/reward_dist Mean                   0.0315535\n",
      "exploration/env_infos/reward_dist Std                    0.0797907\n",
      "exploration/env_infos/reward_dist Max                    0.393827\n",
      "exploration/env_infos/reward_dist Min                    5.6276e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.161087\n",
      "exploration/env_infos/final/reward_energy Std            0.0405295\n",
      "exploration/env_infos/final/reward_energy Max           -0.101114\n",
      "exploration/env_infos/final/reward_energy Min           -0.221528\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.243577\n",
      "exploration/env_infos/initial/reward_energy Std          0.130683\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0813547\n",
      "exploration/env_infos/initial/reward_energy Min         -0.375911\n",
      "exploration/env_infos/reward_energy Mean                -0.163187\n",
      "exploration/env_infos/reward_energy Std                  0.139131\n",
      "exploration/env_infos/reward_energy Max                 -0.0153714\n",
      "exploration/env_infos/reward_energy Min                 -0.686673\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.127949\n",
      "exploration/env_infos/final/end_effector_loc Std         0.260978\n",
      "exploration/env_infos/final/end_effector_loc Max         0.269642\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.562699\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00716842\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00664254\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0018379\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0187743\n",
      "exploration/env_infos/end_effector_loc Mean             -0.111732\n",
      "exploration/env_infos/end_effector_loc Std               0.172079\n",
      "exploration/env_infos/end_effector_loc Max               0.269642\n",
      "exploration/env_infos/end_effector_loc Min              -0.562699\n",
      "evaluation/num steps total                           34000\n",
      "evaluation/num paths total                            1700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0538329\n",
      "evaluation/Rewards Std                                   0.0730123\n",
      "evaluation/Rewards Max                                   0.140252\n",
      "evaluation/Rewards Min                                  -0.360975\n",
      "evaluation/Returns Mean                                 -1.07666\n",
      "evaluation/Returns Std                                   1.16411\n",
      "evaluation/Returns Max                                   1.23055\n",
      "evaluation/Returns Min                                  -3.85707\n",
      "evaluation/Actions Mean                                  0.00152176\n",
      "evaluation/Actions Std                                   0.0728253\n",
      "evaluation/Actions Max                                   0.945597\n",
      "evaluation/Actions Min                                  -0.339191\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.07666\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0862982\n",
      "evaluation/env_infos/final/reward_dist Std               0.187931\n",
      "evaluation/env_infos/final/reward_dist Max               0.879575\n",
      "evaluation/env_infos/final/reward_dist Min               4.95935e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00678808\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0149283\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0788802\n",
      "evaluation/env_infos/initial/reward_dist Min             1.58291e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.140438\n",
      "evaluation/env_infos/reward_dist Std                     0.224561\n",
      "evaluation/env_infos/reward_dist Max                     0.991858\n",
      "evaluation/env_infos/reward_dist Min                     4.95935e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0387068\n",
      "evaluation/env_infos/final/reward_energy Std             0.0371852\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00278421\n",
      "evaluation/env_infos/final/reward_energy Min            -0.177427\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.206786\n",
      "evaluation/env_infos/initial/reward_energy Std           0.230985\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0180013\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.967905\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0600531\n",
      "evaluation/env_infos/reward_energy Std                   0.0836978\n",
      "evaluation/env_infos/reward_energy Max                  -9.70881e-05\n",
      "evaluation/env_infos/reward_energy Min                  -0.967905\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.046359\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.254531\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.609016\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.544094\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00307715\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0105202\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0472798\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0169596\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0300307\n",
      "evaluation/env_infos/end_effector_loc Std                0.163301\n",
      "evaluation/env_infos/end_effector_loc Max                0.609016\n",
      "evaluation/env_infos/end_effector_loc Min               -0.544094\n",
      "time/data storing (s)                                    0.00297949\n",
      "time/evaluation sampling (s)                             1.09667\n",
      "time/exploration sampling (s)                            0.12444\n",
      "time/logging (s)                                         0.0194861\n",
      "time/saving (s)                                          0.0277493\n",
      "time/training (s)                                       46.4329\n",
      "time/epoch (s)                                          47.7042\n",
      "time/total (s)                                        1605.18\n",
      "Epoch                                                   33\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:52:29.977929 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 34 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00123744\n",
      "trainer/QF2 Loss                                         0.00117208\n",
      "trainer/Policy Loss                                      3.32772\n",
      "trainer/Q1 Predictions Mean                             -1.28273\n",
      "trainer/Q1 Predictions Std                               0.84898\n",
      "trainer/Q1 Predictions Max                               0.672572\n",
      "trainer/Q1 Predictions Min                              -3.11149\n",
      "trainer/Q2 Predictions Mean                             -1.275\n",
      "trainer/Q2 Predictions Std                               0.841071\n",
      "trainer/Q2 Predictions Max                               0.661838\n",
      "trainer/Q2 Predictions Min                              -3.11998\n",
      "trainer/Q Targets Mean                                  -1.27041\n",
      "trainer/Q Targets Std                                    0.845921\n",
      "trainer/Q Targets Max                                    0.656677\n",
      "trainer/Q Targets Min                                   -3.36522\n",
      "trainer/Log Pis Mean                                     2.04894\n",
      "trainer/Log Pis Std                                      1.10214\n",
      "trainer/Log Pis Max                                      4.00603\n",
      "trainer/Log Pis Min                                     -2.7506\n",
      "trainer/Policy mu Mean                                   0.0252811\n",
      "trainer/Policy mu Std                                    0.292933\n",
      "trainer/Policy mu Max                                    2.1663\n",
      "trainer/Policy mu Min                                   -1.55671\n",
      "trainer/Policy log std Mean                             -2.33273\n",
      "trainer/Policy log std Std                               0.54674\n",
      "trainer/Policy log std Max                               0.0783681\n",
      "trainer/Policy log std Min                              -3.25117\n",
      "trainer/Alpha                                            0.0206205\n",
      "trainer/Alpha Loss                                       0.189974\n",
      "exploration/num steps total                           4500\n",
      "exploration/num paths total                            225\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.10358\n",
      "exploration/Rewards Std                                  0.0530624\n",
      "exploration/Rewards Max                                 -0.010319\n",
      "exploration/Rewards Min                                 -0.260495\n",
      "exploration/Returns Mean                                -2.0716\n",
      "exploration/Returns Std                                  0.605178\n",
      "exploration/Returns Max                                 -1.2873\n",
      "exploration/Returns Min                                 -2.7749\n",
      "exploration/Actions Mean                                 0.00846258\n",
      "exploration/Actions Std                                  0.136673\n",
      "exploration/Actions Max                                  0.659234\n",
      "exploration/Actions Min                                 -0.568087\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.0716\n",
      "exploration/env_infos/final/reward_dist Mean             0.015277\n",
      "exploration/env_infos/final/reward_dist Std              0.0221676\n",
      "exploration/env_infos/final/reward_dist Max              0.0570412\n",
      "exploration/env_infos/final/reward_dist Min              2.64592e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00646447\n",
      "exploration/env_infos/initial/reward_dist Std            0.0128014\n",
      "exploration/env_infos/initial/reward_dist Max            0.0320671\n",
      "exploration/env_infos/initial/reward_dist Min            1.75148e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.110006\n",
      "exploration/env_infos/reward_dist Std                    0.197375\n",
      "exploration/env_infos/reward_dist Max                    0.914457\n",
      "exploration/env_infos/reward_dist Min                    2.64592e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.13615\n",
      "exploration/env_infos/final/reward_energy Std            0.140604\n",
      "exploration/env_infos/final/reward_energy Max           -0.00226691\n",
      "exploration/env_infos/final/reward_energy Min           -0.382436\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.147605\n",
      "exploration/env_infos/initial/reward_energy Std          0.0844157\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0599493\n",
      "exploration/env_infos/initial/reward_energy Min         -0.280021\n",
      "exploration/env_infos/reward_energy Mean                -0.145985\n",
      "exploration/env_infos/reward_energy Std                  0.127242\n",
      "exploration/env_infos/reward_energy Max                 -0.00226691\n",
      "exploration/env_infos/reward_energy Min                 -0.666892\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.136851\n",
      "exploration/env_infos/final/end_effector_loc Std         0.227358\n",
      "exploration/env_infos/final/end_effector_loc Max         0.489115\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.27221\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000436647\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00599591\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0118256\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0100258\n",
      "exploration/env_infos/end_effector_loc Mean              0.0616875\n",
      "exploration/env_infos/end_effector_loc Std               0.162682\n",
      "exploration/env_infos/end_effector_loc Max               0.489115\n",
      "exploration/env_infos/end_effector_loc Min              -0.324256\n",
      "evaluation/num steps total                           35000\n",
      "evaluation/num paths total                            1750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0720636\n",
      "evaluation/Rewards Std                                   0.0964161\n",
      "evaluation/Rewards Max                                   0.12641\n",
      "evaluation/Rewards Min                                  -0.842333\n",
      "evaluation/Returns Mean                                 -1.44127\n",
      "evaluation/Returns Std                                   1.38452\n",
      "evaluation/Returns Max                                   1.29367\n",
      "evaluation/Returns Min                                  -6.45703\n",
      "evaluation/Actions Mean                                  0.00649545\n",
      "evaluation/Actions Std                                   0.0819222\n",
      "evaluation/Actions Max                                   0.986867\n",
      "evaluation/Actions Min                                  -0.536424\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.44127\n",
      "evaluation/env_infos/final/reward_dist Mean              0.138868\n",
      "evaluation/env_infos/final/reward_dist Std               0.222794\n",
      "evaluation/env_infos/final/reward_dist Max               0.889928\n",
      "evaluation/env_infos/final/reward_dist Min               1.6727e-116\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00768201\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117812\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0425665\n",
      "evaluation/env_infos/initial/reward_dist Min             9.07773e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.155045\n",
      "evaluation/env_infos/reward_dist Std                     0.245595\n",
      "evaluation/env_infos/reward_dist Max                     0.976948\n",
      "evaluation/env_infos/reward_dist Min                     1.6727e-116\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0393703\n",
      "evaluation/env_infos/final/reward_energy Std             0.0433413\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00190631\n",
      "evaluation/env_infos/final/reward_energy Min            -0.25295\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.192133\n",
      "evaluation/env_infos/initial/reward_energy Std           0.259236\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00540779\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.09411\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0612043\n",
      "evaluation/env_infos/reward_energy Std                   0.0987973\n",
      "evaluation/env_infos/reward_energy Max                  -0.00190631\n",
      "evaluation/env_infos/reward_energy Min                  -1.09411\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.113839\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.264476\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.688718\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00374914\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107746\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0493433\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0268212\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0567187\n",
      "evaluation/env_infos/end_effector_loc Std                0.165499\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.688718\n",
      "time/data storing (s)                                    0.00294383\n",
      "time/evaluation sampling (s)                             0.985696\n",
      "time/exploration sampling (s)                            0.121794\n",
      "time/logging (s)                                         0.0194978\n",
      "time/saving (s)                                          0.0272527\n",
      "time/training (s)                                       46.68\n",
      "time/epoch (s)                                          47.8371\n",
      "time/total (s)                                        1653.47\n",
      "Epoch                                                   34\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:53:18.341202 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 35 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000918973\r\n",
      "trainer/QF2 Loss                                         0.000874934\r\n",
      "trainer/Policy Loss                                      3.22342\r\n",
      "trainer/Q1 Predictions Mean                             -1.30244\r\n",
      "trainer/Q1 Predictions Std                               0.844705\r\n",
      "trainer/Q1 Predictions Max                               0.869148\r\n",
      "trainer/Q1 Predictions Min                              -3.63314\r\n",
      "trainer/Q2 Predictions Mean                             -1.2915\r\n",
      "trainer/Q2 Predictions Std                               0.846021\r\n",
      "trainer/Q2 Predictions Max                               0.877048\r\n",
      "trainer/Q2 Predictions Min                              -3.64759\r\n",
      "trainer/Q Targets Mean                                  -1.29583\r\n",
      "trainer/Q Targets Std                                    0.846769\r\n",
      "trainer/Q Targets Max                                    0.850652\r\n",
      "trainer/Q Targets Min                                   -3.65207\r\n",
      "trainer/Log Pis Mean                                     1.936\r\n",
      "trainer/Log Pis Std                                      1.23043\r\n",
      "trainer/Log Pis Max                                      4.27421\r\n",
      "trainer/Log Pis Min                                     -2.82327\r\n",
      "trainer/Policy mu Mean                                   0.0236413\r\n",
      "trainer/Policy mu Std                                    0.314129\r\n",
      "trainer/Policy mu Max                                    2.25861\r\n",
      "trainer/Policy mu Min                                   -1.72985\r\n",
      "trainer/Policy log std Mean                             -2.27345\r\n",
      "trainer/Policy log std Std                               0.557868\r\n",
      "trainer/Policy log std Max                              -0.3534\r\n",
      "trainer/Policy log std Min                              -3.12462\r\n",
      "trainer/Alpha                                            0.0210955\r\n",
      "trainer/Alpha Loss                                      -0.246942\r\n",
      "exploration/num steps total                           4600\r\n",
      "exploration/num paths total                            230\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0795748\r\n",
      "exploration/Rewards Std                                  0.0944403\r\n",
      "exploration/Rewards Max                                  0.112282\r\n",
      "exploration/Rewards Min                                 -0.341413\r\n",
      "exploration/Returns Mean                                -1.5915\r\n",
      "exploration/Returns Std                                  1.63505\r\n",
      "exploration/Returns Max                                  1.13\r\n",
      "exploration/Returns Min                                 -3.5346\r\n",
      "exploration/Actions Mean                                -0.00763083\r\n",
      "exploration/Actions Std                                  0.13031\r\n",
      "exploration/Actions Max                                  0.43694\r\n",
      "exploration/Actions Min                                 -0.492049\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.5915\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00227609\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00438705\r\n",
      "exploration/env_infos/final/reward_dist Max              0.011047\r\n",
      "exploration/env_infos/final/reward_dist Min              6.17515e-12\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00723898\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0141428\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0355232\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.29277e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.149889\r\n",
      "exploration/env_infos/reward_dist Std                    0.230354\r\n",
      "exploration/env_infos/reward_dist Max                    0.808675\r\n",
      "exploration/env_infos/reward_dist Min                    6.17515e-12\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.215276\r\n",
      "exploration/env_infos/final/reward_energy Std            0.159544\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0520282\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.492142\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.270284\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.145091\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.12556\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.540685\r\n",
      "exploration/env_infos/reward_energy Mean                -0.153594\r\n",
      "exploration/env_infos/reward_energy Std                  0.102404\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0113371\r\n",
      "exploration/env_infos/reward_energy Min                 -0.540685\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0141\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.224407\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.305665\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.411739\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00540735\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0094017\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.021847\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0063274\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00832377\r\n",
      "exploration/env_infos/end_effector_loc Std               0.139918\r\n",
      "exploration/env_infos/end_effector_loc Max               0.305665\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.411739\r\n",
      "evaluation/num steps total                           36000\r\n",
      "evaluation/num paths total                            1800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0726216\r\n",
      "evaluation/Rewards Std                                   0.0805609\r\n",
      "evaluation/Rewards Max                                   0.143964\r\n",
      "evaluation/Rewards Min                                  -0.567815\r\n",
      "evaluation/Returns Mean                                 -1.45243\r\n",
      "evaluation/Returns Std                                   1.29939\r\n",
      "evaluation/Returns Max                                   1.10687\r\n",
      "evaluation/Returns Min                                  -4.63886\r\n",
      "evaluation/Actions Mean                                 -0.000746167\r\n",
      "evaluation/Actions Std                                   0.0728619\r\n",
      "evaluation/Actions Max                                   0.743739\r\n",
      "evaluation/Actions Min                                  -0.597957\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.45243\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.125576\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.212716\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.86688\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.93355e-37\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00533571\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116613\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0751584\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.45765e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.134894\r\n",
      "evaluation/env_infos/reward_dist Std                     0.23056\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997463\r\n",
      "evaluation/env_infos/reward_dist Min                     2.93355e-37\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0482072\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0466425\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000284555\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.253728\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.1745\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.204381\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0187585\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.834149\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0644544\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0804018\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000284555\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.834149\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0342238\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.26522\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.442488\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.641251\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000869935\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00946152\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.037187\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0298979\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0184406\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.17705\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.442488\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.689336\r\n",
      "time/data storing (s)                                    0.00308145\r\n",
      "time/evaluation sampling (s)                             0.957582\r\n",
      "time/exploration sampling (s)                            0.126669\r\n",
      "time/logging (s)                                         0.0199518\r\n",
      "time/saving (s)                                          0.0334987\r\n",
      "time/training (s)                                       46.7208\r\n",
      "time/epoch (s)                                          47.8616\r\n",
      "time/total (s)                                        1701.83\r\n",
      "Epoch                                                   35\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:54:06.893160 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 36 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000605631\n",
      "trainer/QF2 Loss                                         0.000626211\n",
      "trainer/Policy Loss                                      3.27922\n",
      "trainer/Q1 Predictions Mean                             -1.2923\n",
      "trainer/Q1 Predictions Std                               0.770117\n",
      "trainer/Q1 Predictions Max                               0.316088\n",
      "trainer/Q1 Predictions Min                              -3.11537\n",
      "trainer/Q2 Predictions Mean                             -1.28661\n",
      "trainer/Q2 Predictions Std                               0.76713\n",
      "trainer/Q2 Predictions Max                               0.320936\n",
      "trainer/Q2 Predictions Min                              -3.06836\n",
      "trainer/Q Targets Mean                                  -1.29106\n",
      "trainer/Q Targets Std                                    0.770174\n",
      "trainer/Q Targets Max                                    0.33578\n",
      "trainer/Q Targets Min                                   -3.12505\n",
      "trainer/Log Pis Mean                                     1.99836\n",
      "trainer/Log Pis Std                                      1.30368\n",
      "trainer/Log Pis Max                                      4.56045\n",
      "trainer/Log Pis Min                                     -2.66712\n",
      "trainer/Policy mu Mean                                   0.0320942\n",
      "trainer/Policy mu Std                                    0.265762\n",
      "trainer/Policy mu Max                                    2.19052\n",
      "trainer/Policy mu Min                                   -1.17318\n",
      "trainer/Policy log std Mean                             -2.34607\n",
      "trainer/Policy log std Std                               0.539033\n",
      "trainer/Policy log std Max                              -0.520894\n",
      "trainer/Policy log std Min                              -3.2558\n",
      "trainer/Alpha                                            0.020808\n",
      "trainer/Alpha Loss                                      -0.0063619\n",
      "exploration/num steps total                           4700\n",
      "exploration/num paths total                            235\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.122402\n",
      "exploration/Rewards Std                                  0.0694997\n",
      "exploration/Rewards Max                                  0.0336042\n",
      "exploration/Rewards Min                                 -0.351463\n",
      "exploration/Returns Mean                                -2.44804\n",
      "exploration/Returns Std                                  0.916239\n",
      "exploration/Returns Max                                 -0.971969\n",
      "exploration/Returns Min                                 -3.5443\n",
      "exploration/Actions Mean                                 0.00713647\n",
      "exploration/Actions Std                                  0.0897322\n",
      "exploration/Actions Max                                  0.462318\n",
      "exploration/Actions Min                                 -0.26242\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.44804\n",
      "exploration/env_infos/final/reward_dist Mean             0.0353323\n",
      "exploration/env_infos/final/reward_dist Std              0.0706645\n",
      "exploration/env_infos/final/reward_dist Max              0.176661\n",
      "exploration/env_infos/final/reward_dist Min              2.54528e-33\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0045831\n",
      "exploration/env_infos/initial/reward_dist Std            0.0081152\n",
      "exploration/env_infos/initial/reward_dist Max            0.020783\n",
      "exploration/env_infos/initial/reward_dist Min            1.58025e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0461514\n",
      "exploration/env_infos/reward_dist Std                    0.153784\n",
      "exploration/env_infos/reward_dist Max                    0.838086\n",
      "exploration/env_infos/reward_dist Min                    2.54528e-33\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0893728\n",
      "exploration/env_infos/final/reward_energy Std            0.0278585\n",
      "exploration/env_infos/final/reward_energy Max           -0.0431968\n",
      "exploration/env_infos/final/reward_energy Min           -0.123315\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.15062\n",
      "exploration/env_infos/initial/reward_energy Std          0.158345\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0282364\n",
      "exploration/env_infos/initial/reward_energy Min         -0.462479\n",
      "exploration/env_infos/reward_energy Mean                -0.0997835\n",
      "exploration/env_infos/reward_energy Std                  0.0790497\n",
      "exploration/env_infos/reward_energy Max                 -0.00578606\n",
      "exploration/env_infos/reward_energy Min                 -0.462479\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0663997\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275907\n",
      "exploration/env_infos/final/end_effector_loc Max         0.493978\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.392582\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00188392\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00749334\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0231159\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00475101\n",
      "exploration/env_infos/end_effector_loc Mean              0.0265999\n",
      "exploration/env_infos/end_effector_loc Std               0.160198\n",
      "exploration/env_infos/end_effector_loc Max               0.493978\n",
      "exploration/env_infos/end_effector_loc Min              -0.392582\n",
      "evaluation/num steps total                           37000\n",
      "evaluation/num paths total                            1850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0675885\n",
      "evaluation/Rewards Std                                   0.0827899\n",
      "evaluation/Rewards Max                                   0.160845\n",
      "evaluation/Rewards Min                                  -0.480762\n",
      "evaluation/Returns Mean                                 -1.35177\n",
      "evaluation/Returns Std                                   1.32298\n",
      "evaluation/Returns Max                                   1.52933\n",
      "evaluation/Returns Min                                  -4.65521\n",
      "evaluation/Actions Mean                                  0.00619177\n",
      "evaluation/Actions Std                                   0.0895627\n",
      "evaluation/Actions Max                                   0.933886\n",
      "evaluation/Actions Min                                  -0.648239\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.35177\n",
      "evaluation/env_infos/final/reward_dist Mean              0.130794\n",
      "evaluation/env_infos/final/reward_dist Std               0.245266\n",
      "evaluation/env_infos/final/reward_dist Max               0.967942\n",
      "evaluation/env_infos/final/reward_dist Min               3.13799e-53\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00716443\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00966465\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0367492\n",
      "evaluation/env_infos/initial/reward_dist Min             1.39948e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.134682\n",
      "evaluation/env_infos/reward_dist Std                     0.22906\n",
      "evaluation/env_infos/reward_dist Max                     0.997701\n",
      "evaluation/env_infos/reward_dist Min                     3.13799e-53\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0406197\n",
      "evaluation/env_infos/final/reward_energy Std             0.035818\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00428035\n",
      "evaluation/env_infos/final/reward_energy Min            -0.205486\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.246199\n",
      "evaluation/env_infos/initial/reward_energy Std           0.261537\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0156449\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.97082\n",
      "evaluation/env_infos/reward_energy Mean                 -0.075024\n",
      "evaluation/env_infos/reward_energy Std                   0.102426\n",
      "evaluation/env_infos/reward_energy Max                  -0.00118013\n",
      "evaluation/env_infos/reward_energy Min                  -0.97082\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.105188\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.247231\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.941617\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.448258\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00412429\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0120108\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0466943\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0324119\n",
      "evaluation/env_infos/end_effector_loc Mean               0.055323\n",
      "evaluation/env_infos/end_effector_loc Std                0.176673\n",
      "evaluation/env_infos/end_effector_loc Max                0.941617\n",
      "evaluation/env_infos/end_effector_loc Min               -0.562808\n",
      "time/data storing (s)                                    0.00297228\n",
      "time/evaluation sampling (s)                             1.07534\n",
      "time/exploration sampling (s)                            0.132309\n",
      "time/logging (s)                                         0.0191431\n",
      "time/saving (s)                                          0.0281384\n",
      "time/training (s)                                       46.797\n",
      "time/epoch (s)                                          48.0549\n",
      "time/total (s)                                        1750.38\n",
      "Epoch                                                   36\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:54:55.585387 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 37 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000478914\n",
      "trainer/QF2 Loss                                         0.00061002\n",
      "trainer/Policy Loss                                      3.20777\n",
      "trainer/Q1 Predictions Mean                             -1.25784\n",
      "trainer/Q1 Predictions Std                               0.780462\n",
      "trainer/Q1 Predictions Max                               0.10543\n",
      "trainer/Q1 Predictions Min                              -3.36135\n",
      "trainer/Q2 Predictions Mean                             -1.26037\n",
      "trainer/Q2 Predictions Std                               0.784712\n",
      "trainer/Q2 Predictions Max                               0.0904833\n",
      "trainer/Q2 Predictions Min                              -3.37937\n",
      "trainer/Q Targets Mean                                  -1.25452\n",
      "trainer/Q Targets Std                                    0.78277\n",
      "trainer/Q Targets Max                                    0.140852\n",
      "trainer/Q Targets Min                                   -3.3592\n",
      "trainer/Log Pis Mean                                     1.95308\n",
      "trainer/Log Pis Std                                      1.39233\n",
      "trainer/Log Pis Max                                      4.49226\n",
      "trainer/Log Pis Min                                     -4.11478\n",
      "trainer/Policy mu Mean                                   0.0185315\n",
      "trainer/Policy mu Std                                    0.29171\n",
      "trainer/Policy mu Max                                    2.04366\n",
      "trainer/Policy mu Min                                   -1.71033\n",
      "trainer/Policy log std Mean                             -2.35222\n",
      "trainer/Policy log std Std                               0.553983\n",
      "trainer/Policy log std Max                              -0.417703\n",
      "trainer/Policy log std Min                              -3.30163\n",
      "trainer/Alpha                                            0.0210571\n",
      "trainer/Alpha Loss                                      -0.181161\n",
      "exploration/num steps total                           4800\n",
      "exploration/num paths total                            240\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.114128\n",
      "exploration/Rewards Std                                  0.073588\n",
      "exploration/Rewards Max                                  0.024674\n",
      "exploration/Rewards Min                                 -0.338285\n",
      "exploration/Returns Mean                                -2.28257\n",
      "exploration/Returns Std                                  1.04006\n",
      "exploration/Returns Max                                 -0.948907\n",
      "exploration/Returns Min                                 -4.10179\n",
      "exploration/Actions Mean                                 0.005233\n",
      "exploration/Actions Std                                  0.0944893\n",
      "exploration/Actions Max                                  0.429682\n",
      "exploration/Actions Min                                 -0.38747\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.28257\n",
      "exploration/env_infos/final/reward_dist Mean             0.213596\n",
      "exploration/env_infos/final/reward_dist Std              0.373236\n",
      "exploration/env_infos/final/reward_dist Max              0.956741\n",
      "exploration/env_infos/final/reward_dist Min              3.59703e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00584195\n",
      "exploration/env_infos/initial/reward_dist Std            0.011241\n",
      "exploration/env_infos/initial/reward_dist Max            0.0283149\n",
      "exploration/env_infos/initial/reward_dist Min            1.49293e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.171957\n",
      "exploration/env_infos/reward_dist Std                    0.277847\n",
      "exploration/env_infos/reward_dist Max                    0.99032\n",
      "exploration/env_infos/reward_dist Min                    1.49293e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.112556\n",
      "exploration/env_infos/final/reward_energy Std            0.0197922\n",
      "exploration/env_infos/final/reward_energy Max           -0.0876452\n",
      "exploration/env_infos/final/reward_energy Min           -0.143331\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.17816\n",
      "exploration/env_infos/initial/reward_energy Std          0.138614\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0575201\n",
      "exploration/env_infos/initial/reward_energy Min         -0.440084\n",
      "exploration/env_infos/reward_energy Mean                -0.113082\n",
      "exploration/env_infos/reward_energy Std                  0.0715795\n",
      "exploration/env_infos/reward_energy Max                 -0.0170477\n",
      "exploration/env_infos/reward_energy Min                 -0.440084\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.100949\n",
      "exploration/env_infos/final/end_effector_loc Std         0.208494\n",
      "exploration/env_infos/final/end_effector_loc Max         0.348982\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.255912\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0045759\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00653871\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0214841\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00248805\n",
      "exploration/env_infos/end_effector_loc Mean              0.0566068\n",
      "exploration/env_infos/end_effector_loc Std               0.1448\n",
      "exploration/env_infos/end_effector_loc Max               0.348982\n",
      "exploration/env_infos/end_effector_loc Min              -0.255912\n",
      "evaluation/num steps total                           38000\n",
      "evaluation/num paths total                            1900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.072401\n",
      "evaluation/Rewards Std                                   0.0788888\n",
      "evaluation/Rewards Max                                   0.145412\n",
      "evaluation/Rewards Min                                  -0.451531\n",
      "evaluation/Returns Mean                                 -1.44802\n",
      "evaluation/Returns Std                                   1.25622\n",
      "evaluation/Returns Max                                   1.63671\n",
      "evaluation/Returns Min                                  -4.53131\n",
      "evaluation/Actions Mean                                  0.00359544\n",
      "evaluation/Actions Std                                   0.0750294\n",
      "evaluation/Actions Max                                   0.889578\n",
      "evaluation/Actions Min                                  -0.556226\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.44802\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0830005\n",
      "evaluation/env_infos/final/reward_dist Std               0.186767\n",
      "evaluation/env_infos/final/reward_dist Max               0.916391\n",
      "evaluation/env_infos/final/reward_dist Min               2.58885e-29\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00542448\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0089922\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0453284\n",
      "evaluation/env_infos/initial/reward_dist Min             1.62572e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.14984\n",
      "evaluation/env_infos/reward_dist Std                     0.254803\n",
      "evaluation/env_infos/reward_dist Max                     0.995023\n",
      "evaluation/env_infos/reward_dist Min                     2.58885e-29\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0627122\n",
      "evaluation/env_infos/final/reward_energy Std             0.0710985\n",
      "evaluation/env_infos/final/reward_energy Max            -0.003445\n",
      "evaluation/env_infos/final/reward_energy Min            -0.378386\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.202046\n",
      "evaluation/env_infos/initial/reward_energy Std           0.23204\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.000982548\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.04916\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0649262\n",
      "evaluation/env_infos/reward_energy Std                   0.084079\n",
      "evaluation/env_infos/reward_energy Max                  -0.000982548\n",
      "evaluation/env_infos/reward_energy Min                  -1.04916\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0927944\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.231278\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.592756\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.416722\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00228665\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010635\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0444789\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0278113\n",
      "evaluation/env_infos/end_effector_loc Mean               0.045263\n",
      "evaluation/env_infos/end_effector_loc Std                0.152889\n",
      "evaluation/env_infos/end_effector_loc Max                0.592756\n",
      "evaluation/env_infos/end_effector_loc Min               -0.416722\n",
      "time/data storing (s)                                    0.0046635\n",
      "time/evaluation sampling (s)                             0.945931\n",
      "time/exploration sampling (s)                            0.122238\n",
      "time/logging (s)                                         0.0189699\n",
      "time/saving (s)                                          0.0287787\n",
      "time/training (s)                                       47.0396\n",
      "time/epoch (s)                                          48.1602\n",
      "time/total (s)                                        1799.07\n",
      "Epoch                                                   37\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:55:44.965920 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 38 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000709893\n",
      "trainer/QF2 Loss                                         0.000669694\n",
      "trainer/Policy Loss                                      3.16473\n",
      "trainer/Q1 Predictions Mean                             -1.24494\n",
      "trainer/Q1 Predictions Std                               0.806771\n",
      "trainer/Q1 Predictions Max                               0.823547\n",
      "trainer/Q1 Predictions Min                              -3.09551\n",
      "trainer/Q2 Predictions Mean                             -1.24495\n",
      "trainer/Q2 Predictions Std                               0.808075\n",
      "trainer/Q2 Predictions Max                               0.825101\n",
      "trainer/Q2 Predictions Min                              -3.06\n",
      "trainer/Q Targets Mean                                  -1.2452\n",
      "trainer/Q Targets Std                                    0.810376\n",
      "trainer/Q Targets Max                                    0.823385\n",
      "trainer/Q Targets Min                                   -3.14826\n",
      "trainer/Log Pis Mean                                     1.91563\n",
      "trainer/Log Pis Std                                      1.34852\n",
      "trainer/Log Pis Max                                      4.1649\n",
      "trainer/Log Pis Min                                     -4.68074\n",
      "trainer/Policy mu Mean                                   0.0130495\n",
      "trainer/Policy mu Std                                    0.230862\n",
      "trainer/Policy mu Max                                    1.7075\n",
      "trainer/Policy mu Min                                   -1.62628\n",
      "trainer/Policy log std Mean                             -2.36426\n",
      "trainer/Policy log std Std                               0.534829\n",
      "trainer/Policy log std Max                              -0.507135\n",
      "trainer/Policy log std Min                              -3.12252\n",
      "trainer/Alpha                                            0.0216825\n",
      "trainer/Alpha Loss                                      -0.323071\n",
      "exploration/num steps total                           4900\n",
      "exploration/num paths total                            245\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.119867\n",
      "exploration/Rewards Std                                  0.0591079\n",
      "exploration/Rewards Max                                  0.00324871\n",
      "exploration/Rewards Min                                 -0.337458\n",
      "exploration/Returns Mean                                -2.39735\n",
      "exploration/Returns Std                                  0.67777\n",
      "exploration/Returns Max                                 -1.43607\n",
      "exploration/Returns Min                                 -3.05605\n",
      "exploration/Actions Mean                                -0.00741399\n",
      "exploration/Actions Std                                  0.145643\n",
      "exploration/Actions Max                                  0.538114\n",
      "exploration/Actions Min                                 -0.873221\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.39735\n",
      "exploration/env_infos/final/reward_dist Mean             0.0037306\n",
      "exploration/env_infos/final/reward_dist Std              0.00720544\n",
      "exploration/env_infos/final/reward_dist Max              0.0181367\n",
      "exploration/env_infos/final/reward_dist Min              2.23936e-23\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00439563\n",
      "exploration/env_infos/initial/reward_dist Std            0.00501227\n",
      "exploration/env_infos/initial/reward_dist Max            0.0115229\n",
      "exploration/env_infos/initial/reward_dist Min            2.31067e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0215425\n",
      "exploration/env_infos/reward_dist Std                    0.045021\n",
      "exploration/env_infos/reward_dist Max                    0.219322\n",
      "exploration/env_infos/reward_dist Min                    2.23936e-23\n",
      "exploration/env_infos/final/reward_energy Mean          -0.118173\n",
      "exploration/env_infos/final/reward_energy Std            0.0338859\n",
      "exploration/env_infos/final/reward_energy Max           -0.0563318\n",
      "exploration/env_infos/final/reward_energy Min           -0.159307\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.418842\n",
      "exploration/env_infos/initial/reward_energy Std          0.30533\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0627019\n",
      "exploration/env_infos/initial/reward_energy Min         -0.912623\n",
      "exploration/env_infos/reward_energy Mean                -0.151197\n",
      "exploration/env_infos/reward_energy Std                  0.140261\n",
      "exploration/env_infos/reward_energy Max                 -0.0100802\n",
      "exploration/env_infos/reward_energy Min                 -0.912623\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0723041\n",
      "exploration/env_infos/final/end_effector_loc Std         0.301271\n",
      "exploration/env_infos/final/end_effector_loc Max         0.572492\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.542393\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00326211\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0180327\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0211642\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0436611\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0334776\n",
      "exploration/env_infos/end_effector_loc Std               0.208733\n",
      "exploration/env_infos/end_effector_loc Max               0.572492\n",
      "exploration/env_infos/end_effector_loc Min              -0.542393\n",
      "evaluation/num steps total                           39000\n",
      "evaluation/num paths total                            1950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0677237\n",
      "evaluation/Rewards Std                                   0.0785379\n",
      "evaluation/Rewards Max                                   0.15477\n",
      "evaluation/Rewards Min                                  -0.485431\n",
      "evaluation/Returns Mean                                 -1.35447\n",
      "evaluation/Returns Std                                   1.24179\n",
      "evaluation/Returns Max                                   1.4804\n",
      "evaluation/Returns Min                                  -3.63833\n",
      "evaluation/Actions Mean                                  0.00500738\n",
      "evaluation/Actions Std                                   0.0877938\n",
      "evaluation/Actions Max                                   0.788563\n",
      "evaluation/Actions Min                                  -0.710088\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.35447\n",
      "evaluation/env_infos/final/reward_dist Mean              0.115904\n",
      "evaluation/env_infos/final/reward_dist Std               0.197942\n",
      "evaluation/env_infos/final/reward_dist Max               0.849919\n",
      "evaluation/env_infos/final/reward_dist Min               2.40591e-15\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00524921\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0122458\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0767013\n",
      "evaluation/env_infos/initial/reward_dist Min             1.37391e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.154826\n",
      "evaluation/env_infos/reward_dist Std                     0.241374\n",
      "evaluation/env_infos/reward_dist Max                     0.996201\n",
      "evaluation/env_infos/reward_dist Min                     2.40591e-15\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0340604\n",
      "evaluation/env_infos/final/reward_energy Std             0.0304063\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00191928\n",
      "evaluation/env_infos/final/reward_energy Min            -0.177506\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.300135\n",
      "evaluation/env_infos/initial/reward_energy Std           0.258532\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00992488\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.955547\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0760907\n",
      "evaluation/env_infos/reward_energy Std                   0.0983659\n",
      "evaluation/env_infos/reward_energy Max                  -0.00136889\n",
      "evaluation/env_infos/reward_energy Min                  -0.955547\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0665309\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.238011\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.631617\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.506816\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000112451\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0140049\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0394282\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0355044\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0230084\n",
      "evaluation/env_infos/end_effector_loc Std                0.168105\n",
      "evaluation/env_infos/end_effector_loc Max                0.631617\n",
      "evaluation/env_infos/end_effector_loc Min               -0.506816\n",
      "time/data storing (s)                                    0.00303799\n",
      "time/evaluation sampling (s)                             0.977547\n",
      "time/exploration sampling (s)                            0.128789\n",
      "time/logging (s)                                         0.0208401\n",
      "time/saving (s)                                          0.0263786\n",
      "time/training (s)                                       47.7263\n",
      "time/epoch (s)                                          48.8828\n",
      "time/total (s)                                        1848.45\n",
      "Epoch                                                   38\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:56:34.198910 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 39 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000599681\n",
      "trainer/QF2 Loss                                         0.000993493\n",
      "trainer/Policy Loss                                      3.20033\n",
      "trainer/Q1 Predictions Mean                             -1.17695\n",
      "trainer/Q1 Predictions Std                               0.742562\n",
      "trainer/Q1 Predictions Max                               0.465623\n",
      "trainer/Q1 Predictions Min                              -3.06184\n",
      "trainer/Q2 Predictions Mean                             -1.17315\n",
      "trainer/Q2 Predictions Std                               0.741192\n",
      "trainer/Q2 Predictions Max                               0.458362\n",
      "trainer/Q2 Predictions Min                              -3.05362\n",
      "trainer/Q Targets Mean                                  -1.18372\n",
      "trainer/Q Targets Std                                    0.739572\n",
      "trainer/Q Targets Max                                    0.453457\n",
      "trainer/Q Targets Min                                   -3.05665\n",
      "trainer/Log Pis Mean                                     2.02124\n",
      "trainer/Log Pis Std                                      1.40537\n",
      "trainer/Log Pis Max                                      4.15086\n",
      "trainer/Log Pis Min                                     -3.56272\n",
      "trainer/Policy mu Mean                                  -0.0114388\n",
      "trainer/Policy mu Std                                    0.193637\n",
      "trainer/Policy mu Max                                    1.02487\n",
      "trainer/Policy mu Min                                   -1.74578\n",
      "trainer/Policy log std Mean                             -2.40552\n",
      "trainer/Policy log std Std                               0.533291\n",
      "trainer/Policy log std Max                              -0.620102\n",
      "trainer/Policy log std Min                              -3.16634\n",
      "trainer/Alpha                                            0.0212451\n",
      "trainer/Alpha Loss                                       0.0818544\n",
      "exploration/num steps total                           5000\n",
      "exploration/num paths total                            250\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0878613\n",
      "exploration/Rewards Std                                  0.0479503\n",
      "exploration/Rewards Max                                  0.000648297\n",
      "exploration/Rewards Min                                 -0.231784\n",
      "exploration/Returns Mean                                -1.75723\n",
      "exploration/Returns Std                                  0.620113\n",
      "exploration/Returns Max                                 -0.96165\n",
      "exploration/Returns Min                                 -2.62138\n",
      "exploration/Actions Mean                                -0.000760485\n",
      "exploration/Actions Std                                  0.076859\n",
      "exploration/Actions Max                                  0.243629\n",
      "exploration/Actions Min                                 -0.286061\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.75723\n",
      "exploration/env_infos/final/reward_dist Mean             0.144432\n",
      "exploration/env_infos/final/reward_dist Std              0.144781\n",
      "exploration/env_infos/final/reward_dist Max              0.377852\n",
      "exploration/env_infos/final/reward_dist Min              1.58067e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00690956\n",
      "exploration/env_infos/initial/reward_dist Std            0.0049987\n",
      "exploration/env_infos/initial/reward_dist Max            0.0140139\n",
      "exploration/env_infos/initial/reward_dist Min            6.5385e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0743844\n",
      "exploration/env_infos/reward_dist Std                    0.114904\n",
      "exploration/env_infos/reward_dist Max                    0.448983\n",
      "exploration/env_infos/reward_dist Min                    1.58067e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0575939\n",
      "exploration/env_infos/final/reward_energy Std            0.0319958\n",
      "exploration/env_infos/final/reward_energy Max           -0.0294491\n",
      "exploration/env_infos/final/reward_energy Min           -0.107363\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0802943\n",
      "exploration/env_infos/initial/reward_energy Std          0.0554765\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0246659\n",
      "exploration/env_infos/initial/reward_energy Min         -0.148315\n",
      "exploration/env_infos/reward_energy Mean                -0.0932643\n",
      "exploration/env_infos/reward_energy Std                  0.055835\n",
      "exploration/env_infos/reward_energy Max                 -0.0177855\n",
      "exploration/env_infos/reward_energy Min                 -0.289577\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00213125\n",
      "exploration/env_infos/final/end_effector_loc Std         0.172134\n",
      "exploration/env_infos/final/end_effector_loc Max         0.198807\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.341642\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000158937\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00344685\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00496141\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00630682\n",
      "exploration/env_infos/end_effector_loc Mean              0.00260502\n",
      "exploration/env_infos/end_effector_loc Std               0.109488\n",
      "exploration/env_infos/end_effector_loc Max               0.240129\n",
      "exploration/env_infos/end_effector_loc Min              -0.341642\n",
      "evaluation/num steps total                           40000\n",
      "evaluation/num paths total                            2000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0620722\n",
      "evaluation/Rewards Std                                   0.0652007\n",
      "evaluation/Rewards Max                                   0.113681\n",
      "evaluation/Rewards Min                                  -0.295942\n",
      "evaluation/Returns Mean                                 -1.24144\n",
      "evaluation/Returns Std                                   1.04463\n",
      "evaluation/Returns Max                                   1.14738\n",
      "evaluation/Returns Min                                  -3.26202\n",
      "evaluation/Actions Mean                                 -0.000104147\n",
      "evaluation/Actions Std                                   0.0592643\n",
      "evaluation/Actions Max                                   0.581588\n",
      "evaluation/Actions Min                                  -0.670397\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.24144\n",
      "evaluation/env_infos/final/reward_dist Mean              0.217677\n",
      "evaluation/env_infos/final/reward_dist Std               0.308197\n",
      "evaluation/env_infos/final/reward_dist Max               0.980079\n",
      "evaluation/env_infos/final/reward_dist Min               1.10536e-12\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00620091\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0153886\n",
      "evaluation/env_infos/initial/reward_dist Max             0.080859\n",
      "evaluation/env_infos/initial/reward_dist Min             1.59024e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.189278\n",
      "evaluation/env_infos/reward_dist Std                     0.277066\n",
      "evaluation/env_infos/reward_dist Max                     0.995948\n",
      "evaluation/env_infos/reward_dist Min                     1.10536e-12\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0319272\n",
      "evaluation/env_infos/final/reward_energy Std             0.0403296\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00124271\n",
      "evaluation/env_infos/final/reward_energy Min            -0.207715\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.147048\n",
      "evaluation/env_infos/initial/reward_energy Std           0.152158\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0134551\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.685786\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0508773\n",
      "evaluation/env_infos/reward_energy Std                   0.0666036\n",
      "evaluation/env_infos/reward_energy Max                  -0.000832061\n",
      "evaluation/env_infos/reward_energy Min                  -0.685786\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00149691\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.212658\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.388273\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.535446\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000127981\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00748014\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0290794\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0321602\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00113944\n",
      "evaluation/env_infos/end_effector_loc Std                0.139535\n",
      "evaluation/env_infos/end_effector_loc Max                0.388273\n",
      "evaluation/env_infos/end_effector_loc Min               -0.535446\n",
      "time/data storing (s)                                    0.00303274\n",
      "time/evaluation sampling (s)                             1.04249\n",
      "time/exploration sampling (s)                            0.128576\n",
      "time/logging (s)                                         0.0198837\n",
      "time/saving (s)                                          0.0273886\n",
      "time/training (s)                                       47.5236\n",
      "time/epoch (s)                                          48.745\n",
      "time/total (s)                                        1897.68\n",
      "Epoch                                                   39\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:57:23.996077 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 40 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000472922\r\n",
      "trainer/QF2 Loss                                         0.000627347\r\n",
      "trainer/Policy Loss                                      3.57672\r\n",
      "trainer/Q1 Predictions Mean                             -1.24836\r\n",
      "trainer/Q1 Predictions Std                               0.849226\r\n",
      "trainer/Q1 Predictions Max                               0.812357\r\n",
      "trainer/Q1 Predictions Min                              -3.49059\r\n",
      "trainer/Q2 Predictions Mean                             -1.25727\r\n",
      "trainer/Q2 Predictions Std                               0.854329\r\n",
      "trainer/Q2 Predictions Max                               0.795855\r\n",
      "trainer/Q2 Predictions Min                              -3.52104\r\n",
      "trainer/Q Targets Mean                                  -1.25259\r\n",
      "trainer/Q Targets Std                                    0.852995\r\n",
      "trainer/Q Targets Max                                    0.815927\r\n",
      "trainer/Q Targets Min                                   -3.53714\r\n",
      "trainer/Log Pis Mean                                     2.33122\r\n",
      "trainer/Log Pis Std                                      1.14259\r\n",
      "trainer/Log Pis Max                                      4.52423\r\n",
      "trainer/Log Pis Min                                     -1.42159\r\n",
      "trainer/Policy mu Mean                                   0.00227673\r\n",
      "trainer/Policy mu Std                                    0.181763\r\n",
      "trainer/Policy mu Max                                    0.959961\r\n",
      "trainer/Policy mu Min                                   -1.69594\r\n",
      "trainer/Policy log std Mean                             -2.48799\r\n",
      "trainer/Policy log std Std                               0.468314\r\n",
      "trainer/Policy log std Max                              -0.298582\r\n",
      "trainer/Policy log std Min                              -3.27929\r\n",
      "trainer/Alpha                                            0.0219262\r\n",
      "trainer/Alpha Loss                                       1.26528\r\n",
      "exploration/num steps total                           5100\r\n",
      "exploration/num paths total                            255\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.128128\r\n",
      "exploration/Rewards Std                                  0.0565535\r\n",
      "exploration/Rewards Max                                  0.0141675\r\n",
      "exploration/Rewards Min                                 -0.276301\r\n",
      "exploration/Returns Mean                                -2.56256\r\n",
      "exploration/Returns Std                                  0.769104\r\n",
      "exploration/Returns Max                                 -1.10142\r\n",
      "exploration/Returns Min                                 -3.21845\r\n",
      "exploration/Actions Mean                                 0.00499555\r\n",
      "exploration/Actions Std                                  0.174426\r\n",
      "exploration/Actions Max                                  0.543405\r\n",
      "exploration/Actions Min                                 -0.684274\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.56256\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.166963\r\n",
      "exploration/env_infos/final/reward_dist Std              0.307587\r\n",
      "exploration/env_infos/final/reward_dist Max              0.780948\r\n",
      "exploration/env_infos/final/reward_dist Min              1.14756e-11\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00702498\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0139059\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0348364\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.38913e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.104056\r\n",
      "exploration/env_infos/reward_dist Std                    0.198211\r\n",
      "exploration/env_infos/reward_dist Max                    0.84455\r\n",
      "exploration/env_infos/reward_dist Min                    1.14756e-11\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0685416\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0528541\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.00923155\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.166851\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.266674\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.252018\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0588264\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.684386\r\n",
      "exploration/env_infos/reward_energy Mean                -0.18475\r\n",
      "exploration/env_infos/reward_energy Std                  0.163605\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00923155\r\n",
      "exploration/env_infos/reward_energy Min                 -0.72312\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0322486\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.262746\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.37748\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.454455\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00541435\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0117885\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00395661\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0342137\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0513177\r\n",
      "exploration/env_infos/end_effector_loc Std               0.135811\r\n",
      "exploration/env_infos/end_effector_loc Max               0.37748\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.454455\r\n",
      "evaluation/num steps total                           41000\r\n",
      "evaluation/num paths total                            2050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0448905\r\n",
      "evaluation/Rewards Std                                   0.0774582\r\n",
      "evaluation/Rewards Max                                   0.142465\r\n",
      "evaluation/Rewards Min                                  -0.273529\r\n",
      "evaluation/Returns Mean                                 -0.89781\r\n",
      "evaluation/Returns Std                                   1.22358\r\n",
      "evaluation/Returns Max                                   1.57651\r\n",
      "evaluation/Returns Min                                  -3.41745\r\n",
      "evaluation/Actions Mean                                  0.00543332\r\n",
      "evaluation/Actions Std                                   0.0591869\r\n",
      "evaluation/Actions Max                                   0.397638\r\n",
      "evaluation/Actions Min                                  -0.451406\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.89781\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.199916\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.286096\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.950568\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.71141e-43\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00522038\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00776596\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0300724\r\n",
      "evaluation/env_infos/initial/reward_dist Min             9.61035e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.19379\r\n",
      "evaluation/env_infos/reward_dist Std                     0.275412\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999862\r\n",
      "evaluation/env_infos/reward_dist Min                     9.71141e-43\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0355357\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.026707\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00295833\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.106319\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.155964\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.127927\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0117579\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.480844\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.056391\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0623319\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000854299\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.484473\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0627012\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.24452\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.852845\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.4289\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000719107\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00709545\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0198819\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0225703\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0231385\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.154933\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.852845\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.4289\r\n",
      "time/data storing (s)                                    0.00296992\r\n",
      "time/evaluation sampling (s)                             0.980987\r\n",
      "time/exploration sampling (s)                            0.133396\r\n",
      "time/logging (s)                                         0.0191136\r\n",
      "time/saving (s)                                          0.0262884\r\n",
      "time/training (s)                                       48.1154\r\n",
      "time/epoch (s)                                          49.2781\r\n",
      "time/total (s)                                        1947.48\r\n",
      "Epoch                                                   40\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:58:14.034606 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 41 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000747012\r\n",
      "trainer/QF2 Loss                                         0.000716691\r\n",
      "trainer/Policy Loss                                      3.22332\r\n",
      "trainer/Q1 Predictions Mean                             -1.24725\r\n",
      "trainer/Q1 Predictions Std                               0.819687\r\n",
      "trainer/Q1 Predictions Max                               0.372572\r\n",
      "trainer/Q1 Predictions Min                              -3.02345\r\n",
      "trainer/Q2 Predictions Mean                             -1.24745\r\n",
      "trainer/Q2 Predictions Std                               0.817364\r\n",
      "trainer/Q2 Predictions Max                               0.35331\r\n",
      "trainer/Q2 Predictions Min                              -3.00914\r\n",
      "trainer/Q Targets Mean                                  -1.24107\r\n",
      "trainer/Q Targets Std                                    0.815637\r\n",
      "trainer/Q Targets Max                                    0.345243\r\n",
      "trainer/Q Targets Min                                   -2.98973\r\n",
      "trainer/Log Pis Mean                                     1.9767\r\n",
      "trainer/Log Pis Std                                      1.37722\r\n",
      "trainer/Log Pis Max                                      4.21065\r\n",
      "trainer/Log Pis Min                                     -3.74701\r\n",
      "trainer/Policy mu Mean                                  -0.014389\r\n",
      "trainer/Policy mu Std                                    0.226946\r\n",
      "trainer/Policy mu Max                                    0.952786\r\n",
      "trainer/Policy mu Min                                   -1.92989\r\n",
      "trainer/Policy log std Mean                             -2.3508\r\n",
      "trainer/Policy log std Std                               0.525453\r\n",
      "trainer/Policy log std Max                              -0.118352\r\n",
      "trainer/Policy log std Min                              -3.08316\r\n",
      "trainer/Alpha                                            0.0222943\r\n",
      "trainer/Alpha Loss                                      -0.088621\r\n",
      "exploration/num steps total                           5200\r\n",
      "exploration/num paths total                            260\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.108586\r\n",
      "exploration/Rewards Std                                  0.089061\r\n",
      "exploration/Rewards Max                                  0.0780773\r\n",
      "exploration/Rewards Min                                 -0.363096\r\n",
      "exploration/Returns Mean                                -2.17172\r\n",
      "exploration/Returns Std                                  0.476525\r\n",
      "exploration/Returns Max                                 -1.513\r\n",
      "exploration/Returns Min                                 -2.99671\r\n",
      "exploration/Actions Mean                                -0.000347208\r\n",
      "exploration/Actions Std                                  0.126289\r\n",
      "exploration/Actions Max                                  0.595977\r\n",
      "exploration/Actions Min                                 -0.368301\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.17172\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0515242\r\n",
      "exploration/env_infos/final/reward_dist Std              0.096562\r\n",
      "exploration/env_infos/final/reward_dist Max              0.244467\r\n",
      "exploration/env_infos/final/reward_dist Min              1.37169e-21\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00943608\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00567965\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0159991\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000409653\r\n",
      "exploration/env_infos/reward_dist Mean                   0.161945\r\n",
      "exploration/env_infos/reward_dist Std                    0.253269\r\n",
      "exploration/env_infos/reward_dist Max                    0.988925\r\n",
      "exploration/env_infos/reward_dist Min                    1.37169e-21\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0975759\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0689862\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0261655\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.22667\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.186507\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.112063\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0618262\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.368315\r\n",
      "exploration/env_infos/reward_energy Mean                -0.141516\r\n",
      "exploration/env_infos/reward_energy Std                  0.108955\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00648332\r\n",
      "exploration/env_infos/reward_energy Min                 -0.597092\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0242527\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.294609\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.661142\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.379402\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000176617\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00769073\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0130303\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.018415\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00394223\r\n",
      "exploration/env_infos/end_effector_loc Std               0.200957\r\n",
      "exploration/env_infos/end_effector_loc Max               0.670875\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.379402\r\n",
      "evaluation/num steps total                           42000\r\n",
      "evaluation/num paths total                            2100\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0378611\r\n",
      "evaluation/Rewards Std                                   0.0735909\r\n",
      "evaluation/Rewards Max                                   0.168612\r\n",
      "evaluation/Rewards Min                                  -0.359448\r\n",
      "evaluation/Returns Mean                                 -0.757221\r\n",
      "evaluation/Returns Std                                   1.16719\r\n",
      "evaluation/Returns Max                                   2.23396\r\n",
      "evaluation/Returns Min                                  -3.95632\r\n",
      "evaluation/Actions Mean                                  0.00121715\r\n",
      "evaluation/Actions Std                                   0.0748966\r\n",
      "evaluation/Actions Max                                   0.696414\r\n",
      "evaluation/Actions Min                                  -0.63322\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.757221\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.153036\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.245556\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.984794\r\n",
      "evaluation/env_infos/final/reward_dist Min               7.12841e-24\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00544539\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0114839\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0635339\r\n",
      "evaluation/env_infos/initial/reward_dist Min             4.06273e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.162361\r\n",
      "evaluation/env_infos/reward_dist Std                     0.244943\r\n",
      "evaluation/env_infos/reward_dist Max                     0.986074\r\n",
      "evaluation/env_infos/reward_dist Min                     7.12841e-24\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0454797\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0661946\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00126028\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.334627\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.22456\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.217492\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0186805\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.748676\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0646599\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0839111\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00113334\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.748676\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0621651\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.220084\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.633095\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.505645\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0027727\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106993\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0348207\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.031661\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0388569\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.148087\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.633095\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.505645\r\n",
      "time/data storing (s)                                    0.00293002\r\n",
      "time/evaluation sampling (s)                             1.16047\r\n",
      "time/exploration sampling (s)                            0.124084\r\n",
      "time/logging (s)                                         0.0198531\r\n",
      "time/saving (s)                                          0.0281677\r\n",
      "time/training (s)                                       48.1775\r\n",
      "time/epoch (s)                                          49.513\r\n",
      "time/total (s)                                        1997.52\r\n",
      "Epoch                                                   41\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:59:02.992978 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 42 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000850917\r\n",
      "trainer/QF2 Loss                                         0.000875528\r\n",
      "trainer/Policy Loss                                      3.30849\r\n",
      "trainer/Q1 Predictions Mean                             -1.26606\r\n",
      "trainer/Q1 Predictions Std                               0.840976\r\n",
      "trainer/Q1 Predictions Max                               0.398435\r\n",
      "trainer/Q1 Predictions Min                              -3.44398\r\n",
      "trainer/Q2 Predictions Mean                             -1.27072\r\n",
      "trainer/Q2 Predictions Std                               0.83733\r\n",
      "trainer/Q2 Predictions Max                               0.383272\r\n",
      "trainer/Q2 Predictions Min                              -3.41526\r\n",
      "trainer/Q Targets Mean                                  -1.26838\r\n",
      "trainer/Q Targets Std                                    0.842464\r\n",
      "trainer/Q Targets Max                                    0.432687\r\n",
      "trainer/Q Targets Min                                   -3.4265\r\n",
      "trainer/Log Pis Mean                                     2.04169\r\n",
      "trainer/Log Pis Std                                      1.29696\r\n",
      "trainer/Log Pis Max                                      4.18876\r\n",
      "trainer/Log Pis Min                                     -3.54574\r\n",
      "trainer/Policy mu Mean                                  -0.0471084\r\n",
      "trainer/Policy mu Std                                    0.2494\r\n",
      "trainer/Policy mu Max                                    0.849315\r\n",
      "trainer/Policy mu Min                                   -2.06625\r\n",
      "trainer/Policy log std Mean                             -2.34516\r\n",
      "trainer/Policy log std Std                               0.555005\r\n",
      "trainer/Policy log std Max                              -0.40944\r\n",
      "trainer/Policy log std Min                              -3.24337\r\n",
      "trainer/Alpha                                            0.0223974\r\n",
      "trainer/Alpha Loss                                       0.158394\r\n",
      "exploration/num steps total                           5300\r\n",
      "exploration/num paths total                            265\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0431316\r\n",
      "exploration/Rewards Std                                  0.0976292\r\n",
      "exploration/Rewards Max                                  0.156133\r\n",
      "exploration/Rewards Min                                 -0.231852\r\n",
      "exploration/Returns Mean                                -0.862632\r\n",
      "exploration/Returns Std                                  1.60861\r\n",
      "exploration/Returns Max                                  1.11907\r\n",
      "exploration/Returns Min                                 -2.83472\r\n",
      "exploration/Actions Mean                                 0.00843724\r\n",
      "exploration/Actions Std                                  0.146911\r\n",
      "exploration/Actions Max                                  0.72211\r\n",
      "exploration/Actions Min                                 -0.54174\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.862632\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.20898\r\n",
      "exploration/env_infos/final/reward_dist Std              0.371134\r\n",
      "exploration/env_infos/final/reward_dist Max              0.950795\r\n",
      "exploration/env_infos/final/reward_dist Min              0.0104473\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0069302\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.010399\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0274205\r\n",
      "exploration/env_infos/initial/reward_dist Min            8.05678e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.255152\r\n",
      "exploration/env_infos/reward_dist Std                    0.292324\r\n",
      "exploration/env_infos/reward_dist Max                    0.994649\r\n",
      "exploration/env_infos/reward_dist Min                    3.32693e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.150789\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0299189\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0921139\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.17491\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.217276\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.20601\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0943882\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.628005\r\n",
      "exploration/env_infos/reward_energy Mean                -0.166247\r\n",
      "exploration/env_infos/reward_energy Std                  0.125181\r\n",
      "exploration/env_infos/reward_energy Max                 -0.013504\r\n",
      "exploration/env_infos/reward_energy Min                 -0.726907\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0832773\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.20932\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.367984\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.253348\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00335396\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0100405\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0264398\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00558222\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0477682\r\n",
      "exploration/env_infos/end_effector_loc Std               0.142517\r\n",
      "exploration/env_infos/end_effector_loc Max               0.367984\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.279784\r\n",
      "evaluation/num steps total                           43000\r\n",
      "evaluation/num paths total                            2150\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0523175\r\n",
      "evaluation/Rewards Std                                   0.0680046\r\n",
      "evaluation/Rewards Max                                   0.113837\r\n",
      "evaluation/Rewards Min                                  -0.316292\r\n",
      "evaluation/Returns Mean                                 -1.04635\r\n",
      "evaluation/Returns Std                                   1.03263\r\n",
      "evaluation/Returns Max                                   1.00249\r\n",
      "evaluation/Returns Min                                  -3.51896\r\n",
      "evaluation/Actions Mean                                 -0.00314619\r\n",
      "evaluation/Actions Std                                   0.0586854\r\n",
      "evaluation/Actions Max                                   0.371142\r\n",
      "evaluation/Actions Min                                  -0.53612\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.04635\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.119154\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.202417\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.834441\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.55731e-22\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00609618\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0104669\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0528491\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.54296e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.138369\r\n",
      "evaluation/env_infos/reward_dist Std                     0.221431\r\n",
      "evaluation/env_infos/reward_dist Max                     0.995074\r\n",
      "evaluation/env_infos/reward_dist Min                     9.55731e-22\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0400891\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0471676\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00107949\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.247729\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.153027\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.132133\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0169855\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.560632\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0537658\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0633797\r\n",
      "evaluation/env_infos/reward_energy Max                  -8.70494e-05\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.593021\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0523264\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.251819\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.66553\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.578466\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00152484\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00698358\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0185571\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.026806\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0283597\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.155945\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.66553\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.578466\r\n",
      "time/data storing (s)                                    0.00296025\r\n",
      "time/evaluation sampling (s)                             0.959177\r\n",
      "time/exploration sampling (s)                            0.121693\r\n",
      "time/logging (s)                                         0.0201969\r\n",
      "time/saving (s)                                          0.0277381\r\n",
      "time/training (s)                                       47.2361\r\n",
      "time/epoch (s)                                          48.3679\r\n",
      "time/total (s)                                        2046.47\r\n",
      "Epoch                                                   42\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:59:53.639985 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 43 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000701614\n",
      "trainer/QF2 Loss                                         0.000675195\n",
      "trainer/Policy Loss                                      3.07476\n",
      "trainer/Q1 Predictions Mean                             -1.2415\n",
      "trainer/Q1 Predictions Std                               0.786815\n",
      "trainer/Q1 Predictions Max                               0.240751\n",
      "trainer/Q1 Predictions Min                              -3.42513\n",
      "trainer/Q2 Predictions Mean                             -1.24719\n",
      "trainer/Q2 Predictions Std                               0.788789\n",
      "trainer/Q2 Predictions Max                               0.24001\n",
      "trainer/Q2 Predictions Min                              -3.4348\n",
      "trainer/Q Targets Mean                                  -1.24297\n",
      "trainer/Q Targets Std                                    0.793317\n",
      "trainer/Q Targets Max                                    0.26578\n",
      "trainer/Q Targets Min                                   -3.43635\n",
      "trainer/Log Pis Mean                                     1.83313\n",
      "trainer/Log Pis Std                                      1.28414\n",
      "trainer/Log Pis Max                                      4.44941\n",
      "trainer/Log Pis Min                                     -2.81072\n",
      "trainer/Policy mu Mean                                  -0.0552155\n",
      "trainer/Policy mu Std                                    0.328977\n",
      "trainer/Policy mu Max                                    0.936544\n",
      "trainer/Policy mu Min                                   -2.62048\n",
      "trainer/Policy log std Mean                             -2.24213\n",
      "trainer/Policy log std Std                               0.615682\n",
      "trainer/Policy log std Max                              -0.169917\n",
      "trainer/Policy log std Min                              -3.19286\n",
      "trainer/Alpha                                            0.0219053\n",
      "trainer/Alpha Loss                                      -0.63744\n",
      "exploration/num steps total                           5400\n",
      "exploration/num paths total                            270\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0858878\n",
      "exploration/Rewards Std                                  0.0689896\n",
      "exploration/Rewards Max                                  0.0639313\n",
      "exploration/Rewards Min                                 -0.269376\n",
      "exploration/Returns Mean                                -1.71776\n",
      "exploration/Returns Std                                  0.999047\n",
      "exploration/Returns Max                                 -0.0894993\n",
      "exploration/Returns Min                                 -2.7722\n",
      "exploration/Actions Mean                                -0.00211149\n",
      "exploration/Actions Std                                  0.107425\n",
      "exploration/Actions Max                                  0.296136\n",
      "exploration/Actions Min                                 -0.472578\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.71776\n",
      "exploration/env_infos/final/reward_dist Mean             0.00769153\n",
      "exploration/env_infos/final/reward_dist Std              0.00788375\n",
      "exploration/env_infos/final/reward_dist Max              0.0180249\n",
      "exploration/env_infos/final/reward_dist Min              2.68527e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00136561\n",
      "exploration/env_infos/initial/reward_dist Std            0.00154054\n",
      "exploration/env_infos/initial/reward_dist Max            0.00411628\n",
      "exploration/env_infos/initial/reward_dist Min            1.95066e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.132176\n",
      "exploration/env_infos/reward_dist Std                    0.217536\n",
      "exploration/env_infos/reward_dist Max                    0.877957\n",
      "exploration/env_infos/reward_dist Min                    2.68527e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.116339\n",
      "exploration/env_infos/final/reward_energy Std            0.0507543\n",
      "exploration/env_infos/final/reward_energy Max           -0.0734598\n",
      "exploration/env_infos/final/reward_energy Min           -0.215515\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.193666\n",
      "exploration/env_infos/initial/reward_energy Std          0.179065\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0332592\n",
      "exploration/env_infos/initial/reward_energy Min         -0.526532\n",
      "exploration/env_infos/reward_energy Mean                -0.122136\n",
      "exploration/env_infos/reward_energy Std                  0.0903985\n",
      "exploration/env_infos/reward_energy Max                 -0.0179513\n",
      "exploration/env_infos/reward_energy Min                 -0.526532\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0371892\n",
      "exploration/env_infos/final/end_effector_loc Std         0.211536\n",
      "exploration/env_infos/final/end_effector_loc Max         0.347999\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.325093\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000255723\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0093219\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0116088\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0236289\n",
      "exploration/env_infos/end_effector_loc Mean              0.0223937\n",
      "exploration/env_infos/end_effector_loc Std               0.127985\n",
      "exploration/env_infos/end_effector_loc Max               0.347999\n",
      "exploration/env_infos/end_effector_loc Min              -0.325093\n",
      "evaluation/num steps total                           44000\n",
      "evaluation/num paths total                            2200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0586977\n",
      "evaluation/Rewards Std                                   0.0775934\n",
      "evaluation/Rewards Max                                   0.104497\n",
      "evaluation/Rewards Min                                  -0.463564\n",
      "evaluation/Returns Mean                                 -1.17395\n",
      "evaluation/Returns Std                                   1.26158\n",
      "evaluation/Returns Max                                   1.04113\n",
      "evaluation/Returns Min                                  -5.61074\n",
      "evaluation/Actions Mean                                 -0.0041032\n",
      "evaluation/Actions Std                                   0.0771713\n",
      "evaluation/Actions Max                                   0.477392\n",
      "evaluation/Actions Min                                  -0.648114\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.17395\n",
      "evaluation/env_infos/final/reward_dist Mean              0.126497\n",
      "evaluation/env_infos/final/reward_dist Std               0.256468\n",
      "evaluation/env_infos/final/reward_dist Max               0.959871\n",
      "evaluation/env_infos/final/reward_dist Min               5.46059e-40\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00519711\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126464\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0833445\n",
      "evaluation/env_infos/initial/reward_dist Min             1.02164e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.136444\n",
      "evaluation/env_infos/reward_dist Std                     0.230421\n",
      "evaluation/env_infos/reward_dist Max                     0.99041\n",
      "evaluation/env_infos/reward_dist Min                     5.46059e-40\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0452094\n",
      "evaluation/env_infos/final/reward_energy Std             0.0382867\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00628401\n",
      "evaluation/env_infos/final/reward_energy Min            -0.216881\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.175961\n",
      "evaluation/env_infos/initial/reward_energy Std           0.17863\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00673251\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.683875\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0692886\n",
      "evaluation/env_infos/reward_energy Std                   0.0845196\n",
      "evaluation/env_infos/reward_energy Max                  -0.0022403\n",
      "evaluation/env_infos/reward_energy Min                  -0.697775\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0322904\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.234744\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.691879\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.470574\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00112831\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00879294\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0238696\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0324057\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0144875\n",
      "evaluation/env_infos/end_effector_loc Std                0.156009\n",
      "evaluation/env_infos/end_effector_loc Max                0.691879\n",
      "evaluation/env_infos/end_effector_loc Min               -0.470574\n",
      "time/data storing (s)                                    0.00295611\n",
      "time/evaluation sampling (s)                             0.956482\n",
      "time/exploration sampling (s)                            0.119601\n",
      "time/logging (s)                                         0.0200073\n",
      "time/saving (s)                                          0.0274302\n",
      "time/training (s)                                       48.9737\n",
      "time/epoch (s)                                          50.1002\n",
      "time/total (s)                                        2097.12\n",
      "Epoch                                                   43\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:00:44.292976 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 44 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000748182\n",
      "trainer/QF2 Loss                                         0.00102367\n",
      "trainer/Policy Loss                                      3.47922\n",
      "trainer/Q1 Predictions Mean                             -1.23141\n",
      "trainer/Q1 Predictions Std                               0.798353\n",
      "trainer/Q1 Predictions Max                               0.464253\n",
      "trainer/Q1 Predictions Min                              -3.03907\n",
      "trainer/Q2 Predictions Mean                             -1.23692\n",
      "trainer/Q2 Predictions Std                               0.79921\n",
      "trainer/Q2 Predictions Max                               0.424\n",
      "trainer/Q2 Predictions Min                              -3.01259\n",
      "trainer/Q Targets Mean                                  -1.21919\n",
      "trainer/Q Targets Std                                    0.795592\n",
      "trainer/Q Targets Max                                    0.476011\n",
      "trainer/Q Targets Min                                   -3.00801\n",
      "trainer/Log Pis Mean                                     2.25467\n",
      "trainer/Log Pis Std                                      1.17681\n",
      "trainer/Log Pis Max                                      4.18947\n",
      "trainer/Log Pis Min                                     -2.71596\n",
      "trainer/Policy mu Mean                                  -0.00766301\n",
      "trainer/Policy mu Std                                    0.261707\n",
      "trainer/Policy mu Max                                    1.55361\n",
      "trainer/Policy mu Min                                   -1.77585\n",
      "trainer/Policy log std Mean                             -2.38917\n",
      "trainer/Policy log std Std                               0.561883\n",
      "trainer/Policy log std Max                               0.134769\n",
      "trainer/Policy log std Min                              -3.23257\n",
      "trainer/Alpha                                            0.0236469\n",
      "trainer/Alpha Loss                                       0.953495\n",
      "exploration/num steps total                           5500\n",
      "exploration/num paths total                            275\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0479709\n",
      "exploration/Rewards Std                                  0.082753\n",
      "exploration/Rewards Max                                  0.103674\n",
      "exploration/Rewards Min                                 -0.279683\n",
      "exploration/Returns Mean                                -0.959418\n",
      "exploration/Returns Std                                  1.42687\n",
      "exploration/Returns Max                                  0.840224\n",
      "exploration/Returns Min                                 -3.09562\n",
      "exploration/Actions Mean                                -0.00842861\n",
      "exploration/Actions Std                                  0.171472\n",
      "exploration/Actions Max                                  0.534538\n",
      "exploration/Actions Min                                 -0.52086\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.959418\n",
      "exploration/env_infos/final/reward_dist Mean             0.432434\n",
      "exploration/env_infos/final/reward_dist Std              0.39814\n",
      "exploration/env_infos/final/reward_dist Max              0.998297\n",
      "exploration/env_infos/final/reward_dist Min              1.11105e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00589768\n",
      "exploration/env_infos/initial/reward_dist Std            0.00722452\n",
      "exploration/env_infos/initial/reward_dist Max            0.0196201\n",
      "exploration/env_infos/initial/reward_dist Min            1.67762e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.337003\n",
      "exploration/env_infos/reward_dist Std                    0.355849\n",
      "exploration/env_infos/reward_dist Max                    0.999681\n",
      "exploration/env_infos/reward_dist Min                    1.11105e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.125655\n",
      "exploration/env_infos/final/reward_energy Std            0.0993442\n",
      "exploration/env_infos/final/reward_energy Max           -0.0186167\n",
      "exploration/env_infos/final/reward_energy Min           -0.300706\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.312951\n",
      "exploration/env_infos/initial/reward_energy Std          0.281435\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0352025\n",
      "exploration/env_infos/initial/reward_energy Min         -0.724098\n",
      "exploration/env_infos/reward_energy Mean                -0.196518\n",
      "exploration/env_infos/reward_energy Std                  0.142577\n",
      "exploration/env_infos/reward_energy Max                 -0.00647169\n",
      "exploration/env_infos/reward_energy Min                 -0.724098\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0622881\n",
      "exploration/env_infos/final/end_effector_loc Std         0.168431\n",
      "exploration/env_infos/final/end_effector_loc Max         0.194777\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.371081\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00201908\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0147429\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0267269\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.026043\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0321917\n",
      "exploration/env_infos/end_effector_loc Std               0.142831\n",
      "exploration/env_infos/end_effector_loc Max               0.230049\n",
      "exploration/env_infos/end_effector_loc Min              -0.371081\n",
      "evaluation/num steps total                           45000\n",
      "evaluation/num paths total                            2250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0468926\n",
      "evaluation/Rewards Std                                   0.0632369\n",
      "evaluation/Rewards Max                                   0.12741\n",
      "evaluation/Rewards Min                                  -0.271024\n",
      "evaluation/Returns Mean                                 -0.937851\n",
      "evaluation/Returns Std                                   0.942558\n",
      "evaluation/Returns Max                                   1.45192\n",
      "evaluation/Returns Min                                  -3.01985\n",
      "evaluation/Actions Mean                                  0.0032215\n",
      "evaluation/Actions Std                                   0.0638241\n",
      "evaluation/Actions Max                                   0.461854\n",
      "evaluation/Actions Min                                  -0.476166\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.937851\n",
      "evaluation/env_infos/final/reward_dist Mean              0.106158\n",
      "evaluation/env_infos/final/reward_dist Std               0.215342\n",
      "evaluation/env_infos/final/reward_dist Max               0.848232\n",
      "evaluation/env_infos/final/reward_dist Min               1.91472e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00487184\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0103034\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0579828\n",
      "evaluation/env_infos/initial/reward_dist Min             1.80714e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.188391\n",
      "evaluation/env_infos/reward_dist Std                     0.276533\n",
      "evaluation/env_infos/reward_dist Max                     0.997011\n",
      "evaluation/env_infos/reward_dist Min                     1.91472e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0453677\n",
      "evaluation/env_infos/final/reward_energy Std             0.0395575\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00274374\n",
      "evaluation/env_infos/final/reward_energy Min            -0.153075\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.173995\n",
      "evaluation/env_infos/initial/reward_energy Std           0.161721\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0110972\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.608174\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0619193\n",
      "evaluation/env_infos/reward_energy Std                   0.0658315\n",
      "evaluation/env_infos/reward_energy Max                  -0.000763547\n",
      "evaluation/env_infos/reward_energy Min                  -0.608174\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0537029\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.246393\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.601495\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.580937\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0006205\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00837553\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0230927\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0238083\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0233474\n",
      "evaluation/env_infos/end_effector_loc Std                0.152319\n",
      "evaluation/env_infos/end_effector_loc Max                0.601495\n",
      "evaluation/env_infos/end_effector_loc Min               -0.580937\n",
      "time/data storing (s)                                    0.00305638\n",
      "time/evaluation sampling (s)                             1.02163\n",
      "time/exploration sampling (s)                            0.139308\n",
      "time/logging (s)                                         0.0197278\n",
      "time/saving (s)                                          0.0281175\n",
      "time/training (s)                                       48.8621\n",
      "time/epoch (s)                                          50.0739\n",
      "time/total (s)                                        2147.77\n",
      "Epoch                                                   44\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:01:35.251941 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 45 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000711372\r\n",
      "trainer/QF2 Loss                                         0.000584052\r\n",
      "trainer/Policy Loss                                      3.23355\r\n",
      "trainer/Q1 Predictions Mean                             -1.17799\r\n",
      "trainer/Q1 Predictions Std                               0.765086\r\n",
      "trainer/Q1 Predictions Max                               0.494181\r\n",
      "trainer/Q1 Predictions Min                              -3.04878\r\n",
      "trainer/Q2 Predictions Mean                             -1.19311\r\n",
      "trainer/Q2 Predictions Std                               0.764668\r\n",
      "trainer/Q2 Predictions Max                               0.47158\r\n",
      "trainer/Q2 Predictions Min                              -3.02392\r\n",
      "trainer/Q Targets Mean                                  -1.18521\r\n",
      "trainer/Q Targets Std                                    0.758649\r\n",
      "trainer/Q Targets Max                                    0.421255\r\n",
      "trainer/Q Targets Min                                   -3.01927\r\n",
      "trainer/Log Pis Mean                                     2.05547\r\n",
      "trainer/Log Pis Std                                      1.47473\r\n",
      "trainer/Log Pis Max                                      4.33901\r\n",
      "trainer/Log Pis Min                                     -4.75551\r\n",
      "trainer/Policy mu Mean                                  -0.0376243\r\n",
      "trainer/Policy mu Std                                    0.269961\r\n",
      "trainer/Policy mu Max                                    1.65856\r\n",
      "trainer/Policy mu Min                                   -1.66792\r\n",
      "trainer/Policy log std Mean                             -2.3677\r\n",
      "trainer/Policy log std Std                               0.573836\r\n",
      "trainer/Policy log std Max                               0.106461\r\n",
      "trainer/Policy log std Min                              -3.24439\r\n",
      "trainer/Alpha                                            0.0219387\r\n",
      "trainer/Alpha Loss                                       0.211859\r\n",
      "exploration/num steps total                           5600\r\n",
      "exploration/num paths total                            280\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.047284\r\n",
      "exploration/Rewards Std                                  0.0528119\r\n",
      "exploration/Rewards Max                                  0.0788098\r\n",
      "exploration/Rewards Min                                 -0.17006\r\n",
      "exploration/Returns Mean                                -0.945679\r\n",
      "exploration/Returns Std                                  0.612891\r\n",
      "exploration/Returns Max                                 -0.139005\r\n",
      "exploration/Returns Min                                 -1.76834\r\n",
      "exploration/Actions Mean                                -0.0117357\r\n",
      "exploration/Actions Std                                  0.0845483\r\n",
      "exploration/Actions Max                                  0.291401\r\n",
      "exploration/Actions Min                                 -0.330207\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.945679\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.225307\r\n",
      "exploration/env_infos/final/reward_dist Std              0.179444\r\n",
      "exploration/env_infos/final/reward_dist Max              0.514492\r\n",
      "exploration/env_infos/final/reward_dist Min              3.88946e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00614593\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00614189\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0166657\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.1473e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.244683\r\n",
      "exploration/env_infos/reward_dist Std                    0.293866\r\n",
      "exploration/env_infos/reward_dist Max                    0.994921\r\n",
      "exploration/env_infos/reward_dist Min                    3.88946e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.137586\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0724317\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0577703\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.25774\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0724467\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0456279\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.026533\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.157387\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0997514\r\n",
      "exploration/env_infos/reward_energy Std                  0.0679847\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00510332\r\n",
      "exploration/env_infos/reward_energy Min                 -0.385931\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0400647\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.169492\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.254897\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.303288\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000930076\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00288063\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00531375\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0058044\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0086292\r\n",
      "exploration/env_infos/end_effector_loc Std               0.0976801\r\n",
      "exploration/env_infos/end_effector_loc Max               0.254897\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.303288\r\n",
      "evaluation/num steps total                           46000\r\n",
      "evaluation/num paths total                            2300\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0530775\r\n",
      "evaluation/Rewards Std                                   0.0932497\r\n",
      "evaluation/Rewards Max                                   0.14873\r\n",
      "evaluation/Rewards Min                                  -0.465293\r\n",
      "evaluation/Returns Mean                                 -1.06155\r\n",
      "evaluation/Returns Std                                   1.58134\r\n",
      "evaluation/Returns Max                                   2.01854\r\n",
      "evaluation/Returns Min                                  -6.25032\r\n",
      "evaluation/Actions Mean                                  0.000519342\r\n",
      "evaluation/Actions Std                                   0.0869449\r\n",
      "evaluation/Actions Max                                   0.633883\r\n",
      "evaluation/Actions Min                                  -0.684968\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.06155\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.144997\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.228853\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.954783\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.91716e-48\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0081041\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0135097\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0651729\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.87412e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.191669\r\n",
      "evaluation/env_infos/reward_dist Std                     0.271851\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998671\r\n",
      "evaluation/env_infos/reward_dist Min                     1.91716e-48\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0656291\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0767479\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00926416\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.293782\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.242403\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.229313\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0165183\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.876969\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0787594\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0944263\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00126959\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.876969\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0629101\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.22908\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.723135\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.384817\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00107889\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.011748\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0316941\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0342484\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0262031\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.158691\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.723135\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.384817\r\n",
      "time/data storing (s)                                    0.00295078\r\n",
      "time/evaluation sampling (s)                             1.08838\r\n",
      "time/exploration sampling (s)                            0.132017\r\n",
      "time/logging (s)                                         0.0192029\r\n",
      "time/saving (s)                                          0.0273022\r\n",
      "time/training (s)                                       49.0983\r\n",
      "time/epoch (s)                                          50.3681\r\n",
      "time/total (s)                                        2198.73\r\n",
      "Epoch                                                   45\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:02:25.710948 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 46 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000731135\n",
      "trainer/QF2 Loss                                         0.0011455\n",
      "trainer/Policy Loss                                      3.24412\n",
      "trainer/Q1 Predictions Mean                             -1.08897\n",
      "trainer/Q1 Predictions Std                               0.826447\n",
      "trainer/Q1 Predictions Max                               0.776581\n",
      "trainer/Q1 Predictions Min                              -3.24344\n",
      "trainer/Q2 Predictions Mean                             -1.09035\n",
      "trainer/Q2 Predictions Std                               0.831832\n",
      "trainer/Q2 Predictions Max                               0.792003\n",
      "trainer/Q2 Predictions Min                              -3.24359\n",
      "trainer/Q Targets Mean                                  -1.09619\n",
      "trainer/Q Targets Std                                    0.828363\n",
      "trainer/Q Targets Max                                    0.810937\n",
      "trainer/Q Targets Min                                   -3.25975\n",
      "trainer/Log Pis Mean                                     2.16483\n",
      "trainer/Log Pis Std                                      1.27434\n",
      "trainer/Log Pis Max                                      4.41629\n",
      "trainer/Log Pis Min                                     -3.43697\n",
      "trainer/Policy mu Mean                                  -0.0261343\n",
      "trainer/Policy mu Std                                    0.350219\n",
      "trainer/Policy mu Max                                    1.53506\n",
      "trainer/Policy mu Min                                   -2.28192\n",
      "trainer/Policy log std Mean                             -2.33425\n",
      "trainer/Policy log std Std                               0.56743\n",
      "trainer/Policy log std Max                              -0.372923\n",
      "trainer/Policy log std Min                              -3.24036\n",
      "trainer/Alpha                                            0.0234274\n",
      "trainer/Alpha Loss                                       0.618813\n",
      "exploration/num steps total                           5700\n",
      "exploration/num paths total                            285\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0307633\n",
      "exploration/Rewards Std                                  0.10012\n",
      "exploration/Rewards Max                                  0.166475\n",
      "exploration/Rewards Min                                 -0.214093\n",
      "exploration/Returns Mean                                -0.615266\n",
      "exploration/Returns Std                                  1.82093\n",
      "exploration/Returns Max                                  2.323\n",
      "exploration/Returns Min                                 -2.53005\n",
      "exploration/Actions Mean                                 0.00610728\n",
      "exploration/Actions Std                                  0.15783\n",
      "exploration/Actions Max                                  0.520756\n",
      "exploration/Actions Min                                 -0.520308\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.615266\n",
      "exploration/env_infos/final/reward_dist Mean             0.0705296\n",
      "exploration/env_infos/final/reward_dist Std              0.0666859\n",
      "exploration/env_infos/final/reward_dist Max              0.173321\n",
      "exploration/env_infos/final/reward_dist Min              1.54049e-23\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0147467\n",
      "exploration/env_infos/initial/reward_dist Std            0.017984\n",
      "exploration/env_infos/initial/reward_dist Max            0.0503523\n",
      "exploration/env_infos/initial/reward_dist Min            0.00350369\n",
      "exploration/env_infos/reward_dist Mean                   0.222839\n",
      "exploration/env_infos/reward_dist Std                    0.31309\n",
      "exploration/env_infos/reward_dist Max                    0.997014\n",
      "exploration/env_infos/reward_dist Min                    1.54049e-23\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135396\n",
      "exploration/env_infos/final/reward_energy Std            0.0543594\n",
      "exploration/env_infos/final/reward_energy Max           -0.0738233\n",
      "exploration/env_infos/final/reward_energy Min           -0.219418\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.291178\n",
      "exploration/env_infos/initial/reward_energy Std          0.241808\n",
      "exploration/env_infos/initial/reward_energy Max         -0.00166394\n",
      "exploration/env_infos/initial/reward_energy Min         -0.613194\n",
      "exploration/env_infos/reward_energy Mean                -0.171586\n",
      "exploration/env_infos/reward_energy Std                  0.143016\n",
      "exploration/env_infos/reward_energy Max                 -0.00166394\n",
      "exploration/env_infos/reward_energy Min                 -0.617841\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0167907\n",
      "exploration/env_infos/final/end_effector_loc Std         0.277538\n",
      "exploration/env_infos/final/end_effector_loc Max         0.382034\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.648924\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00770275\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0109425\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00301881\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0260154\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0363368\n",
      "exploration/env_infos/end_effector_loc Std               0.176114\n",
      "exploration/env_infos/end_effector_loc Max               0.382034\n",
      "exploration/env_infos/end_effector_loc Min              -0.648924\n",
      "evaluation/num steps total                           47000\n",
      "evaluation/num paths total                            2350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0418353\n",
      "evaluation/Rewards Std                                   0.0687402\n",
      "evaluation/Rewards Max                                   0.165579\n",
      "evaluation/Rewards Min                                  -0.355658\n",
      "evaluation/Returns Mean                                 -0.836705\n",
      "evaluation/Returns Std                                   1.09906\n",
      "evaluation/Returns Max                                   1.28144\n",
      "evaluation/Returns Min                                  -3.28158\n",
      "evaluation/Actions Mean                                 -0.00293082\n",
      "evaluation/Actions Std                                   0.0597479\n",
      "evaluation/Actions Max                                   0.527181\n",
      "evaluation/Actions Min                                  -0.497226\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.836705\n",
      "evaluation/env_infos/final/reward_dist Mean              0.174195\n",
      "evaluation/env_infos/final/reward_dist Std               0.260631\n",
      "evaluation/env_infos/final/reward_dist Max               0.973332\n",
      "evaluation/env_infos/final/reward_dist Min               8.92665e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00675324\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0129852\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0599654\n",
      "evaluation/env_infos/initial/reward_dist Min             2.00439e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.181861\n",
      "evaluation/env_infos/reward_dist Std                     0.268184\n",
      "evaluation/env_infos/reward_dist Max                     0.998878\n",
      "evaluation/env_infos/reward_dist Min                     8.92665e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0349873\n",
      "evaluation/env_infos/final/reward_energy Std             0.0438961\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000704192\n",
      "evaluation/env_infos/final/reward_energy Min            -0.218094\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.142178\n",
      "evaluation/env_infos/initial/reward_energy Std           0.15502\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0184667\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.628238\n",
      "evaluation/env_infos/reward_energy Mean                 -0.053339\n",
      "evaluation/env_infos/reward_energy Std                   0.0656638\n",
      "evaluation/env_infos/reward_energy Max                  -0.000704192\n",
      "evaluation/env_infos/reward_energy Min                  -0.628238\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.011154\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.208737\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.386924\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.438762\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       2.45855e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00743684\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0263591\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0248613\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0010924\n",
      "evaluation/env_infos/end_effector_loc Std                0.135737\n",
      "evaluation/env_infos/end_effector_loc Max                0.386924\n",
      "evaluation/env_infos/end_effector_loc Min               -0.438762\n",
      "time/data storing (s)                                    0.00290145\n",
      "time/evaluation sampling (s)                             0.987762\n",
      "time/exploration sampling (s)                            0.140735\n",
      "time/logging (s)                                         0.0195866\n",
      "time/saving (s)                                          0.0415051\n",
      "time/training (s)                                       48.6614\n",
      "time/epoch (s)                                          49.8539\n",
      "time/total (s)                                        2249.19\n",
      "Epoch                                                   46\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:03:16.325036 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 47 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000774705\n",
      "trainer/QF2 Loss                                         0.000982771\n",
      "trainer/Policy Loss                                      3.06444\n",
      "trainer/Q1 Predictions Mean                             -1.14256\n",
      "trainer/Q1 Predictions Std                               0.847949\n",
      "trainer/Q1 Predictions Max                               0.584781\n",
      "trainer/Q1 Predictions Min                              -3.37716\n",
      "trainer/Q2 Predictions Mean                             -1.13176\n",
      "trainer/Q2 Predictions Std                               0.846155\n",
      "trainer/Q2 Predictions Max                               0.68082\n",
      "trainer/Q2 Predictions Min                              -3.35201\n",
      "trainer/Q Targets Mean                                  -1.14642\n",
      "trainer/Q Targets Std                                    0.850951\n",
      "trainer/Q Targets Max                                    0.621236\n",
      "trainer/Q Targets Min                                   -3.39536\n",
      "trainer/Log Pis Mean                                     1.92306\n",
      "trainer/Log Pis Std                                      1.39018\n",
      "trainer/Log Pis Max                                      4.06419\n",
      "trainer/Log Pis Min                                     -3.18251\n",
      "trainer/Policy mu Mean                                   0.00438545\n",
      "trainer/Policy mu Std                                    0.219616\n",
      "trainer/Policy mu Max                                    1.36695\n",
      "trainer/Policy mu Min                                   -1.24017\n",
      "trainer/Policy log std Mean                             -2.35851\n",
      "trainer/Policy log std Std                               0.537844\n",
      "trainer/Policy log std Max                              -0.630345\n",
      "trainer/Policy log std Min                              -3.17348\n",
      "trainer/Alpha                                            0.0222845\n",
      "trainer/Alpha Loss                                      -0.292622\n",
      "exploration/num steps total                           5800\n",
      "exploration/num paths total                            290\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.108283\n",
      "exploration/Rewards Std                                  0.0750876\n",
      "exploration/Rewards Max                                  0.0599312\n",
      "exploration/Rewards Min                                 -0.394537\n",
      "exploration/Returns Mean                                -2.16566\n",
      "exploration/Returns Std                                  0.461911\n",
      "exploration/Returns Max                                 -1.6923\n",
      "exploration/Returns Min                                 -3.05098\n",
      "exploration/Actions Mean                                 0.0026477\n",
      "exploration/Actions Std                                  0.127673\n",
      "exploration/Actions Max                                  0.56105\n",
      "exploration/Actions Min                                 -0.758632\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.16566\n",
      "exploration/env_infos/final/reward_dist Mean             0.0130267\n",
      "exploration/env_infos/final/reward_dist Std              0.0208692\n",
      "exploration/env_infos/final/reward_dist Max              0.0538719\n",
      "exploration/env_infos/final/reward_dist Min              9.44704e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00165926\n",
      "exploration/env_infos/initial/reward_dist Std            0.00202333\n",
      "exploration/env_infos/initial/reward_dist Max            0.00434629\n",
      "exploration/env_infos/initial/reward_dist Min            2.85964e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0874414\n",
      "exploration/env_infos/reward_dist Std                    0.175431\n",
      "exploration/env_infos/reward_dist Max                    0.870125\n",
      "exploration/env_infos/reward_dist Min                    9.44704e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0990031\n",
      "exploration/env_infos/final/reward_energy Std            0.0625659\n",
      "exploration/env_infos/final/reward_energy Max           -0.0131941\n",
      "exploration/env_infos/final/reward_energy Min           -0.161805\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.148668\n",
      "exploration/env_infos/initial/reward_energy Std          0.179622\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0276055\n",
      "exploration/env_infos/initial/reward_energy Min         -0.501463\n",
      "exploration/env_infos/reward_energy Mean                -0.1275\n",
      "exploration/env_infos/reward_energy Std                  0.127901\n",
      "exploration/env_infos/reward_energy Max                 -0.0109609\n",
      "exploration/env_infos/reward_energy Min                 -0.760287\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0609577\n",
      "exploration/env_infos/final/end_effector_loc Std         0.277129\n",
      "exploration/env_infos/final/end_effector_loc Max         0.338086\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.492948\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00317513\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00760766\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0249635\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00234285\n",
      "exploration/env_infos/end_effector_loc Mean              0.0363605\n",
      "exploration/env_infos/end_effector_loc Std               0.180014\n",
      "exploration/env_infos/end_effector_loc Max               0.552751\n",
      "exploration/env_infos/end_effector_loc Min              -0.492948\n",
      "evaluation/num steps total                           48000\n",
      "evaluation/num paths total                            2400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0569282\n",
      "evaluation/Rewards Std                                   0.0905301\n",
      "evaluation/Rewards Max                                   0.159303\n",
      "evaluation/Rewards Min                                  -0.64312\n",
      "evaluation/Returns Mean                                 -1.13856\n",
      "evaluation/Returns Std                                   1.47482\n",
      "evaluation/Returns Max                                   2.39439\n",
      "evaluation/Returns Min                                  -8.0949\n",
      "evaluation/Actions Mean                                 -0.00245383\n",
      "evaluation/Actions Std                                   0.0776349\n",
      "evaluation/Actions Max                                   0.794481\n",
      "evaluation/Actions Min                                  -0.580887\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.13856\n",
      "evaluation/env_infos/final/reward_dist Mean              0.185527\n",
      "evaluation/env_infos/final/reward_dist Std               0.276476\n",
      "evaluation/env_infos/final/reward_dist Max               0.905666\n",
      "evaluation/env_infos/final/reward_dist Min               6.06237e-55\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00615371\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00906853\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0366437\n",
      "evaluation/env_infos/initial/reward_dist Min             3.13296e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.163577\n",
      "evaluation/env_infos/reward_dist Std                     0.245385\n",
      "evaluation/env_infos/reward_dist Max                     0.988988\n",
      "evaluation/env_infos/reward_dist Min                     6.05746e-55\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0570947\n",
      "evaluation/env_infos/final/reward_energy Std             0.0839103\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00445416\n",
      "evaluation/env_infos/final/reward_energy Min            -0.423823\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.180761\n",
      "evaluation/env_infos/initial/reward_energy Std           0.194831\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0191576\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.794753\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0656209\n",
      "evaluation/env_infos/reward_energy Std                   0.0880926\n",
      "evaluation/env_infos/reward_energy Max                  -0.00219267\n",
      "evaluation/env_infos/reward_energy Min                  -0.794753\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0159003\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.272365\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.812746\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000841064\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00935866\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0397241\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0277926\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0111766\n",
      "evaluation/env_infos/end_effector_loc Std                0.173308\n",
      "evaluation/env_infos/end_effector_loc Max                0.826669\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00285794\n",
      "time/evaluation sampling (s)                             1.005\n",
      "time/exploration sampling (s)                            0.123496\n",
      "time/logging (s)                                         0.0203152\n",
      "time/saving (s)                                          0.0286376\n",
      "time/training (s)                                       48.7519\n",
      "time/epoch (s)                                          49.9323\n",
      "time/total (s)                                        2299.8\n",
      "Epoch                                                   47\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:04:07.084234 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 48 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000812128\n",
      "trainer/QF2 Loss                                         0.000894113\n",
      "trainer/Policy Loss                                      2.96098\n",
      "trainer/Q1 Predictions Mean                             -1.05747\n",
      "trainer/Q1 Predictions Std                               0.87824\n",
      "trainer/Q1 Predictions Max                               0.949891\n",
      "trainer/Q1 Predictions Min                              -3.4402\n",
      "trainer/Q2 Predictions Mean                             -1.04787\n",
      "trainer/Q2 Predictions Std                               0.875799\n",
      "trainer/Q2 Predictions Max                               0.918833\n",
      "trainer/Q2 Predictions Min                              -3.43295\n",
      "trainer/Q Targets Mean                                  -1.04485\n",
      "trainer/Q Targets Std                                    0.876854\n",
      "trainer/Q Targets Max                                    0.919985\n",
      "trainer/Q Targets Min                                   -3.38668\n",
      "trainer/Log Pis Mean                                     1.90101\n",
      "trainer/Log Pis Std                                      1.32671\n",
      "trainer/Log Pis Max                                      4.28463\n",
      "trainer/Log Pis Min                                     -2.79723\n",
      "trainer/Policy mu Mean                                  -0.0416288\n",
      "trainer/Policy mu Std                                    0.237069\n",
      "trainer/Policy mu Max                                    0.841938\n",
      "trainer/Policy mu Min                                   -1.63776\n",
      "trainer/Policy log std Mean                             -2.32188\n",
      "trainer/Policy log std Std                               0.548151\n",
      "trainer/Policy log std Max                              -0.451461\n",
      "trainer/Policy log std Min                              -3.21882\n",
      "trainer/Alpha                                            0.0235794\n",
      "trainer/Alpha Loss                                      -0.37087\n",
      "exploration/num steps total                           5900\n",
      "exploration/num paths total                            295\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.146647\n",
      "exploration/Rewards Std                                  0.0479953\n",
      "exploration/Rewards Max                                 -0.037941\n",
      "exploration/Rewards Min                                 -0.310911\n",
      "exploration/Returns Mean                                -2.93294\n",
      "exploration/Returns Std                                  0.546128\n",
      "exploration/Returns Max                                 -2.38807\n",
      "exploration/Returns Min                                 -3.88707\n",
      "exploration/Actions Mean                                -0.0157726\n",
      "exploration/Actions Std                                  0.0801929\n",
      "exploration/Actions Max                                  0.272226\n",
      "exploration/Actions Min                                 -0.271025\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.93294\n",
      "exploration/env_infos/final/reward_dist Mean             0.183623\n",
      "exploration/env_infos/final/reward_dist Std              0.264698\n",
      "exploration/env_infos/final/reward_dist Max              0.679839\n",
      "exploration/env_infos/final/reward_dist Min              2.64084e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00628251\n",
      "exploration/env_infos/initial/reward_dist Std            0.00897646\n",
      "exploration/env_infos/initial/reward_dist Max            0.0230351\n",
      "exploration/env_infos/initial/reward_dist Min            6.445e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0394018\n",
      "exploration/env_infos/reward_dist Std                    0.106806\n",
      "exploration/env_infos/reward_dist Max                    0.679839\n",
      "exploration/env_infos/reward_dist Min                    2.64084e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0649841\n",
      "exploration/env_infos/final/reward_energy Std            0.0248938\n",
      "exploration/env_infos/final/reward_energy Max           -0.0240804\n",
      "exploration/env_infos/final/reward_energy Min           -0.0927007\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.139215\n",
      "exploration/env_infos/initial/reward_energy Std          0.0586564\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0783967\n",
      "exploration/env_infos/initial/reward_energy Min         -0.243009\n",
      "exploration/env_infos/reward_energy Mean                -0.0983774\n",
      "exploration/env_infos/reward_energy Std                  0.0606733\n",
      "exploration/env_infos/reward_energy Max                 -0.00709111\n",
      "exploration/env_infos/reward_energy Min                 -0.293829\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.17989\n",
      "exploration/env_infos/final/end_effector_loc Std         0.228178\n",
      "exploration/env_infos/final/end_effector_loc Max         0.222055\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.595208\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000749055\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00528826\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0095394\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00729682\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0631703\n",
      "exploration/env_infos/end_effector_loc Std               0.135452\n",
      "exploration/env_infos/end_effector_loc Max               0.222055\n",
      "exploration/env_infos/end_effector_loc Min              -0.595208\n",
      "evaluation/num steps total                           49000\n",
      "evaluation/num paths total                            2450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0541914\n",
      "evaluation/Rewards Std                                   0.0821416\n",
      "evaluation/Rewards Max                                   0.173135\n",
      "evaluation/Rewards Min                                  -0.626226\n",
      "evaluation/Returns Mean                                 -1.08383\n",
      "evaluation/Returns Std                                   1.27306\n",
      "evaluation/Returns Max                                   2.13822\n",
      "evaluation/Returns Min                                  -4.21051\n",
      "evaluation/Actions Mean                                 -0.0037657\n",
      "evaluation/Actions Std                                   0.0695748\n",
      "evaluation/Actions Max                                   0.48012\n",
      "evaluation/Actions Min                                  -0.724343\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.08383\n",
      "evaluation/env_infos/final/reward_dist Mean              0.113957\n",
      "evaluation/env_infos/final/reward_dist Std               0.19304\n",
      "evaluation/env_infos/final/reward_dist Max               0.878264\n",
      "evaluation/env_infos/final/reward_dist Min               1.35699e-49\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00516275\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00930889\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0381481\n",
      "evaluation/env_infos/initial/reward_dist Min             1.6002e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.145049\n",
      "evaluation/env_infos/reward_dist Std                     0.234339\n",
      "evaluation/env_infos/reward_dist Max                     0.998005\n",
      "evaluation/env_infos/reward_dist Min                     1.35699e-49\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0613291\n",
      "evaluation/env_infos/final/reward_energy Std             0.127723\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00420815\n",
      "evaluation/env_infos/final/reward_energy Min            -0.728221\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.149562\n",
      "evaluation/env_infos/initial/reward_energy Std           0.157299\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00662381\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.696372\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0552113\n",
      "evaluation/env_infos/reward_energy Std                   0.0816174\n",
      "evaluation/env_infos/reward_energy Max                  -0.000392736\n",
      "evaluation/env_infos/reward_energy Min                  -0.728221\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0230599\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.26\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.580109\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00100097\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0076084\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.024006\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0321826\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0128841\n",
      "evaluation/env_infos/end_effector_loc Std                0.155643\n",
      "evaluation/env_infos/end_effector_loc Max                0.580109\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00290828\n",
      "time/evaluation sampling (s)                             0.99231\n",
      "time/exploration sampling (s)                            0.124966\n",
      "time/logging (s)                                         0.0194402\n",
      "time/saving (s)                                          0.0275424\n",
      "time/training (s)                                       48.9671\n",
      "time/epoch (s)                                          50.1342\n",
      "time/total (s)                                        2350.56\n",
      "Epoch                                                   48\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:04:57.930368 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 49 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000790089\r\n",
      "trainer/QF2 Loss                                         0.000909833\r\n",
      "trainer/Policy Loss                                      3.14872\r\n",
      "trainer/Q1 Predictions Mean                             -1.17362\r\n",
      "trainer/Q1 Predictions Std                               0.819447\r\n",
      "trainer/Q1 Predictions Max                               1.10229\r\n",
      "trainer/Q1 Predictions Min                              -3.27301\r\n",
      "trainer/Q2 Predictions Mean                             -1.17793\r\n",
      "trainer/Q2 Predictions Std                               0.819963\r\n",
      "trainer/Q2 Predictions Max                               1.07898\r\n",
      "trainer/Q2 Predictions Min                              -3.29812\r\n",
      "trainer/Q Targets Mean                                  -1.16026\r\n",
      "trainer/Q Targets Std                                    0.817276\r\n",
      "trainer/Q Targets Max                                    1.12376\r\n",
      "trainer/Q Targets Min                                   -3.26364\r\n",
      "trainer/Log Pis Mean                                     1.97034\r\n",
      "trainer/Log Pis Std                                      1.25284\r\n",
      "trainer/Log Pis Max                                      4.09925\r\n",
      "trainer/Log Pis Min                                     -2.16304\r\n",
      "trainer/Policy mu Mean                                  -0.0109839\r\n",
      "trainer/Policy mu Std                                    0.251774\r\n",
      "trainer/Policy mu Max                                    1.38071\r\n",
      "trainer/Policy mu Min                                   -1.64826\r\n",
      "trainer/Policy log std Mean                             -2.35616\r\n",
      "trainer/Policy log std Std                               0.557364\r\n",
      "trainer/Policy log std Max                              -0.373805\r\n",
      "trainer/Policy log std Min                              -3.25251\r\n",
      "trainer/Alpha                                            0.0230331\r\n",
      "trainer/Alpha Loss                                      -0.111848\r\n",
      "exploration/num steps total                           6000\r\n",
      "exploration/num paths total                            300\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0918758\r\n",
      "exploration/Rewards Std                                  0.079307\r\n",
      "exploration/Rewards Max                                  0.0731935\r\n",
      "exploration/Rewards Min                                 -0.253194\r\n",
      "exploration/Returns Mean                                -1.83752\r\n",
      "exploration/Returns Std                                  1.22137\r\n",
      "exploration/Returns Max                                 -0.210576\r\n",
      "exploration/Returns Min                                 -3.81122\r\n",
      "exploration/Actions Mean                                 0.0150513\r\n",
      "exploration/Actions Std                                  0.142931\r\n",
      "exploration/Actions Max                                  0.51355\r\n",
      "exploration/Actions Min                                 -0.476604\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.83752\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.227148\r\n",
      "exploration/env_infos/final/reward_dist Std              0.283692\r\n",
      "exploration/env_infos/final/reward_dist Max              0.695794\r\n",
      "exploration/env_infos/final/reward_dist Min              0.000102888\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00511143\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.009444\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0239766\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.89836e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.250638\r\n",
      "exploration/env_infos/reward_dist Std                    0.303744\r\n",
      "exploration/env_infos/reward_dist Max                    0.985887\r\n",
      "exploration/env_infos/reward_dist Min                    5.89836e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.142058\r\n",
      "exploration/env_infos/final/reward_energy Std            0.04869\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0927796\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.226253\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.210934\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.140638\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0809642\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.479591\r\n",
      "exploration/env_infos/reward_energy Mean                -0.16182\r\n",
      "exploration/env_infos/reward_energy Std                  0.122988\r\n",
      "exploration/env_infos/reward_energy Max                 -0.015301\r\n",
      "exploration/env_infos/reward_energy Min                 -0.585933\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0688178\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.204546\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.437379\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.273206\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00334609\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00831529\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00758251\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0238302\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00272038\r\n",
      "exploration/env_infos/end_effector_loc Std               0.14418\r\n",
      "exploration/env_infos/end_effector_loc Max               0.437379\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.273206\r\n",
      "evaluation/num steps total                           50000\r\n",
      "evaluation/num paths total                            2500\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0645907\r\n",
      "evaluation/Rewards Std                                   0.0694995\r\n",
      "evaluation/Rewards Max                                   0.121869\r\n",
      "evaluation/Rewards Min                                  -0.51852\r\n",
      "evaluation/Returns Mean                                 -1.29181\r\n",
      "evaluation/Returns Std                                   1.04072\r\n",
      "evaluation/Returns Max                                   1.48893\r\n",
      "evaluation/Returns Min                                  -3.67947\r\n",
      "evaluation/Actions Mean                                  0.00312979\r\n",
      "evaluation/Actions Std                                   0.0800667\r\n",
      "evaluation/Actions Max                                   0.919365\r\n",
      "evaluation/Actions Min                                  -0.600676\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.29181\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.133643\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.258193\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.944719\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.63041e-41\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0056937\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00796159\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0305873\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.87776e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.141818\r\n",
      "evaluation/env_infos/reward_dist Std                     0.238998\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999341\r\n",
      "evaluation/env_infos/reward_dist Min                     2.63041e-41\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0553054\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0678317\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00378141\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.367493\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.207392\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.226891\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0119363\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.961091\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0723403\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0872229\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00240827\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.961091\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0672111\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.237864\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.512369\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.876454\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00135258\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107835\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0459682\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0300338\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0340352\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.153224\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.512369\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.876454\r\n",
      "time/data storing (s)                                    0.00304457\r\n",
      "time/evaluation sampling (s)                             1.08001\r\n",
      "time/exploration sampling (s)                            0.122393\r\n",
      "time/logging (s)                                         0.0191793\r\n",
      "time/saving (s)                                          0.0293789\r\n",
      "time/training (s)                                       48.9631\r\n",
      "time/epoch (s)                                          50.2171\r\n",
      "time/total (s)                                        2401.4\r\n",
      "Epoch                                                   49\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:05:51.283340 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 50 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000646107\r\n",
      "trainer/QF2 Loss                                         0.000763435\r\n",
      "trainer/Policy Loss                                      3.08049\r\n",
      "trainer/Q1 Predictions Mean                             -1.12613\r\n",
      "trainer/Q1 Predictions Std                               0.847626\r\n",
      "trainer/Q1 Predictions Max                               1.23249\r\n",
      "trainer/Q1 Predictions Min                              -3.1243\r\n",
      "trainer/Q2 Predictions Mean                             -1.13239\r\n",
      "trainer/Q2 Predictions Std                               0.84789\r\n",
      "trainer/Q2 Predictions Max                               1.23346\r\n",
      "trainer/Q2 Predictions Min                              -3.14379\r\n",
      "trainer/Q Targets Mean                                  -1.12996\r\n",
      "trainer/Q Targets Std                                    0.846965\r\n",
      "trainer/Q Targets Max                                    1.27236\r\n",
      "trainer/Q Targets Min                                   -3.1479\r\n",
      "trainer/Log Pis Mean                                     1.94655\r\n",
      "trainer/Log Pis Std                                      1.29308\r\n",
      "trainer/Log Pis Max                                      4.42344\r\n",
      "trainer/Log Pis Min                                     -3.75637\r\n",
      "trainer/Policy mu Mean                                  -0.0159303\r\n",
      "trainer/Policy mu Std                                    0.25567\r\n",
      "trainer/Policy mu Max                                    1.33346\r\n",
      "trainer/Policy mu Min                                   -1.84889\r\n",
      "trainer/Policy log std Mean                             -2.33046\r\n",
      "trainer/Policy log std Std                               0.551454\r\n",
      "trainer/Policy log std Max                              -0.00246775\r\n",
      "trainer/Policy log std Min                              -3.22621\r\n",
      "trainer/Alpha                                            0.0226247\r\n",
      "trainer/Alpha Loss                                      -0.20253\r\n",
      "exploration/num steps total                           6100\r\n",
      "exploration/num paths total                            305\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0694542\r\n",
      "exploration/Rewards Std                                  0.0700169\r\n",
      "exploration/Rewards Max                                  0.107779\r\n",
      "exploration/Rewards Min                                 -0.236101\r\n",
      "exploration/Returns Mean                                -1.38908\r\n",
      "exploration/Returns Std                                  0.996937\r\n",
      "exploration/Returns Max                                  0.188735\r\n",
      "exploration/Returns Min                                 -2.8631\r\n",
      "exploration/Actions Mean                                -0.00530229\r\n",
      "exploration/Actions Std                                  0.0883433\r\n",
      "exploration/Actions Max                                  0.351393\r\n",
      "exploration/Actions Min                                 -0.403368\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.38908\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0459113\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0604781\r\n",
      "exploration/env_infos/final/reward_dist Max              0.162781\r\n",
      "exploration/env_infos/final/reward_dist Min              1.28882e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00874509\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0094665\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.026402\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.81177e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.246123\r\n",
      "exploration/env_infos/reward_dist Std                    0.298077\r\n",
      "exploration/env_infos/reward_dist Max                    0.991014\r\n",
      "exploration/env_infos/reward_dist Min                    1.28882e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.160259\r\n",
      "exploration/env_infos/final/reward_energy Std            0.126515\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0814626\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.410211\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.124858\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.128788\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0269304\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.37607\r\n",
      "exploration/env_infos/reward_energy Mean                -0.102595\r\n",
      "exploration/env_infos/reward_energy Std                  0.0716914\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00549877\r\n",
      "exploration/env_infos/reward_energy Min                 -0.410211\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.112286\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.203921\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.255814\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.372387\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000279998\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00633573\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0175697\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00669907\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0508431\r\n",
      "exploration/env_infos/end_effector_loc Std               0.12762\r\n",
      "exploration/env_infos/end_effector_loc Max               0.255814\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.372387\r\n",
      "evaluation/num steps total                           51000\r\n",
      "evaluation/num paths total                            2550\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0394188\r\n",
      "evaluation/Rewards Std                                   0.068004\r\n",
      "evaluation/Rewards Max                                   0.159538\r\n",
      "evaluation/Rewards Min                                  -0.378376\r\n",
      "evaluation/Returns Mean                                 -0.788376\r\n",
      "evaluation/Returns Std                                   1.08727\r\n",
      "evaluation/Returns Max                                   1.71132\r\n",
      "evaluation/Returns Min                                  -2.56107\r\n",
      "evaluation/Actions Mean                                  8.65382e-05\r\n",
      "evaluation/Actions Std                                   0.0566716\r\n",
      "evaluation/Actions Max                                   0.715245\r\n",
      "evaluation/Actions Min                                  -0.668209\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.788376\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.226187\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.286432\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.915394\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.85922e-13\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00605323\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00964549\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0418206\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.85426e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.171169\r\n",
      "evaluation/env_infos/reward_dist Std                     0.271545\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993102\r\n",
      "evaluation/env_infos/reward_dist Min                     5.85922e-13\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0470937\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0466738\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00944293\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.197267\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.153707\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.18391\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.014149\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.755233\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0459281\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0656808\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00090866\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.755233\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00523209\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.209677\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.638184\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.527048\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000843556\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00843206\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0357622\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0334105\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00479586\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.131875\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.638184\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.527048\r\n",
      "time/data storing (s)                                    0.00407339\r\n",
      "time/evaluation sampling (s)                             1.04371\r\n",
      "time/exploration sampling (s)                            0.140701\r\n",
      "time/logging (s)                                         0.0196148\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/saving (s)                                          0.0553031\n",
      "time/training (s)                                       51.4698\n",
      "time/epoch (s)                                          52.7332\n",
      "time/total (s)                                        2454.75\n",
      "Epoch                                                   50\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 11:06:41.676805 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 51 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00193865\n",
      "trainer/QF2 Loss                                         0.00061397\n",
      "trainer/Policy Loss                                      3.22226\n",
      "trainer/Q1 Predictions Mean                             -1.13893\n",
      "trainer/Q1 Predictions Std                               0.884325\n",
      "trainer/Q1 Predictions Max                               1.38836\n",
      "trainer/Q1 Predictions Min                              -3.1857\n",
      "trainer/Q2 Predictions Mean                             -1.14884\n",
      "trainer/Q2 Predictions Std                               0.887894\n",
      "trainer/Q2 Predictions Max                               1.36732\n",
      "trainer/Q2 Predictions Min                              -3.21057\n",
      "trainer/Q Targets Mean                                  -1.14358\n",
      "trainer/Q Targets Std                                    0.88556\n",
      "trainer/Q Targets Max                                    1.41657\n",
      "trainer/Q Targets Min                                   -3.18895\n",
      "trainer/Log Pis Mean                                     2.07997\n",
      "trainer/Log Pis Std                                      1.36571\n",
      "trainer/Log Pis Max                                      4.57685\n",
      "trainer/Log Pis Min                                     -4.41859\n",
      "trainer/Policy mu Mean                                  -0.00798388\n",
      "trainer/Policy mu Std                                    0.212919\n",
      "trainer/Policy mu Max                                    1.4918\n",
      "trainer/Policy mu Min                                   -1.49872\n",
      "trainer/Policy log std Mean                             -2.3728\n",
      "trainer/Policy log std Std                               0.555353\n",
      "trainer/Policy log std Max                              -0.622116\n",
      "trainer/Policy log std Min                              -3.26376\n",
      "trainer/Alpha                                            0.0228128\n",
      "trainer/Alpha Loss                                       0.302467\n",
      "exploration/num steps total                           6200\n",
      "exploration/num paths total                            310\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.100228\n",
      "exploration/Rewards Std                                  0.0910357\n",
      "exploration/Rewards Max                                  0.0732557\n",
      "exploration/Rewards Min                                 -0.317285\n",
      "exploration/Returns Mean                                -2.00456\n",
      "exploration/Returns Std                                  1.59692\n",
      "exploration/Returns Max                                  0.409792\n",
      "exploration/Returns Min                                 -4.09928\n",
      "exploration/Actions Mean                                -0.0099438\n",
      "exploration/Actions Std                                  0.182871\n",
      "exploration/Actions Max                                  0.431977\n",
      "exploration/Actions Min                                 -0.664595\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.00456\n",
      "exploration/env_infos/final/reward_dist Mean             0.178916\n",
      "exploration/env_infos/final/reward_dist Std              0.357577\n",
      "exploration/env_infos/final/reward_dist Max              0.894069\n",
      "exploration/env_infos/final/reward_dist Min              2.02335e-31\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0160775\n",
      "exploration/env_infos/initial/reward_dist Std            0.0227473\n",
      "exploration/env_infos/initial/reward_dist Max            0.0606214\n",
      "exploration/env_infos/initial/reward_dist Min            3.5386e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.113754\n",
      "exploration/env_infos/reward_dist Std                    0.213596\n",
      "exploration/env_infos/reward_dist Max                    0.914231\n",
      "exploration/env_infos/reward_dist Min                    2.02335e-31\n",
      "exploration/env_infos/final/reward_energy Mean          -0.301809\n",
      "exploration/env_infos/final/reward_energy Std            0.196542\n",
      "exploration/env_infos/final/reward_energy Max           -0.151391\n",
      "exploration/env_infos/final/reward_energy Min           -0.683236\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.367712\n",
      "exploration/env_infos/initial/reward_energy Std          0.26079\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0804808\n",
      "exploration/env_infos/initial/reward_energy Min         -0.755714\n",
      "exploration/env_infos/reward_energy Mean                -0.214166\n",
      "exploration/env_infos/reward_energy Std                  0.145652\n",
      "exploration/env_infos/reward_energy Max                 -0.018483\n",
      "exploration/env_infos/reward_energy Min                 -0.755714\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00417504\n",
      "exploration/env_infos/final/end_effector_loc Std         0.271127\n",
      "exploration/env_infos/final/end_effector_loc Max         0.357213\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.490395\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00850626\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0134786\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00852682\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0300337\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00311436\n",
      "exploration/env_infos/end_effector_loc Std               0.146163\n",
      "exploration/env_infos/end_effector_loc Max               0.357213\n",
      "exploration/env_infos/end_effector_loc Min              -0.490395\n",
      "evaluation/num steps total                           52000\n",
      "evaluation/num paths total                            2600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0423782\n",
      "evaluation/Rewards Std                                   0.0763285\n",
      "evaluation/Rewards Max                                   0.16563\n",
      "evaluation/Rewards Min                                  -0.310916\n",
      "evaluation/Returns Mean                                 -0.847564\n",
      "evaluation/Returns Std                                   1.1552\n",
      "evaluation/Returns Max                                   2.12833\n",
      "evaluation/Returns Min                                  -3.09494\n",
      "evaluation/Actions Mean                                  0.000836034\n",
      "evaluation/Actions Std                                   0.0746768\n",
      "evaluation/Actions Max                                   0.493386\n",
      "evaluation/Actions Min                                  -0.66004\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.847564\n",
      "evaluation/env_infos/final/reward_dist Mean              0.148473\n",
      "evaluation/env_infos/final/reward_dist Std               0.266156\n",
      "evaluation/env_infos/final/reward_dist Max               0.906336\n",
      "evaluation/env_infos/final/reward_dist Min               9.88054e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00543535\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0105028\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0478917\n",
      "evaluation/env_infos/initial/reward_dist Min             1.27419e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.165136\n",
      "evaluation/env_infos/reward_dist Std                     0.252478\n",
      "evaluation/env_infos/reward_dist Max                     0.998643\n",
      "evaluation/env_infos/reward_dist Min                     9.88054e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0714105\n",
      "evaluation/env_infos/final/reward_energy Std             0.0690687\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00940822\n",
      "evaluation/env_infos/final/reward_energy Min            -0.370618\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.209358\n",
      "evaluation/env_infos/initial/reward_energy Std           0.18823\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00724747\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.661129\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0647582\n",
      "evaluation/env_infos/reward_energy Std                   0.0834327\n",
      "evaluation/env_infos/reward_energy Max                  -0.001005\n",
      "evaluation/env_infos/reward_energy Min                  -0.661129\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.023684\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.221197\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.427228\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.704078\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000745616\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00992576\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0212594\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.033002\n",
      "evaluation/env_infos/end_effector_loc Mean               0.010156\n",
      "evaluation/env_infos/end_effector_loc Std                0.142869\n",
      "evaluation/env_infos/end_effector_loc Max                0.427228\n",
      "evaluation/env_infos/end_effector_loc Min               -0.704078\n",
      "time/data storing (s)                                    0.00310105\n",
      "time/evaluation sampling (s)                             1.09496\n",
      "time/exploration sampling (s)                            0.124947\n",
      "time/logging (s)                                         0.0188341\n",
      "time/saving (s)                                          0.0270247\n",
      "time/training (s)                                       48.4462\n",
      "time/epoch (s)                                          49.7151\n",
      "time/total (s)                                        2505.14\n",
      "Epoch                                                   51\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:07:32.943008 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 52 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000564138\n",
      "trainer/QF2 Loss                                         0.000882073\n",
      "trainer/Policy Loss                                      3.13608\n",
      "trainer/Q1 Predictions Mean                             -1.06428\n",
      "trainer/Q1 Predictions Std                               0.811253\n",
      "trainer/Q1 Predictions Max                               0.961758\n",
      "trainer/Q1 Predictions Min                              -3.43856\n",
      "trainer/Q2 Predictions Mean                             -1.06456\n",
      "trainer/Q2 Predictions Std                               0.810862\n",
      "trainer/Q2 Predictions Max                               0.965712\n",
      "trainer/Q2 Predictions Min                              -3.43368\n",
      "trainer/Q Targets Mean                                  -1.06752\n",
      "trainer/Q Targets Std                                    0.814868\n",
      "trainer/Q Targets Max                                    0.953399\n",
      "trainer/Q Targets Min                                   -3.43996\n",
      "trainer/Log Pis Mean                                     2.07794\n",
      "trainer/Log Pis Std                                      1.32794\n",
      "trainer/Log Pis Max                                      4.4231\n",
      "trainer/Log Pis Min                                     -5.59204\n",
      "trainer/Policy mu Mean                                  -0.0514461\n",
      "trainer/Policy mu Std                                    0.259162\n",
      "trainer/Policy mu Max                                    1.008\n",
      "trainer/Policy mu Min                                   -1.90385\n",
      "trainer/Policy log std Mean                             -2.34018\n",
      "trainer/Policy log std Std                               0.548303\n",
      "trainer/Policy log std Max                              -0.401168\n",
      "trainer/Policy log std Min                              -3.25115\n",
      "trainer/Alpha                                            0.0214122\n",
      "trainer/Alpha Loss                                       0.299631\n",
      "exploration/num steps total                           6300\n",
      "exploration/num paths total                            315\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117094\n",
      "exploration/Rewards Std                                  0.0536121\n",
      "exploration/Rewards Max                                 -0.00310531\n",
      "exploration/Rewards Min                                 -0.294406\n",
      "exploration/Returns Mean                                -2.34188\n",
      "exploration/Returns Std                                  0.548216\n",
      "exploration/Returns Max                                 -1.35404\n",
      "exploration/Returns Min                                 -3.02284\n",
      "exploration/Actions Mean                                 0.00205229\n",
      "exploration/Actions Std                                  0.0755326\n",
      "exploration/Actions Max                                  0.277145\n",
      "exploration/Actions Min                                 -0.228927\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.34188\n",
      "exploration/env_infos/final/reward_dist Mean             0.0734465\n",
      "exploration/env_infos/final/reward_dist Std              0.127777\n",
      "exploration/env_infos/final/reward_dist Max              0.328035\n",
      "exploration/env_infos/final/reward_dist Min              5.05137e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000439209\n",
      "exploration/env_infos/initial/reward_dist Std            0.000292858\n",
      "exploration/env_infos/initial/reward_dist Max            0.000739874\n",
      "exploration/env_infos/initial/reward_dist Min            5.38416e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.05603\n",
      "exploration/env_infos/reward_dist Std                    0.174253\n",
      "exploration/env_infos/reward_dist Max                    0.915356\n",
      "exploration/env_infos/reward_dist Min                    1.09447e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.14332\n",
      "exploration/env_infos/final/reward_energy Std            0.0622155\n",
      "exploration/env_infos/final/reward_energy Max           -0.0716452\n",
      "exploration/env_infos/final/reward_energy Min           -0.233507\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0806226\n",
      "exploration/env_infos/initial/reward_energy Std          0.0288107\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0509701\n",
      "exploration/env_infos/initial/reward_energy Min         -0.131429\n",
      "exploration/env_infos/reward_energy Mean                -0.0907904\n",
      "exploration/env_infos/reward_energy Std                  0.0563549\n",
      "exploration/env_infos/reward_energy Max                 -0.00697259\n",
      "exploration/env_infos/reward_energy Min                 -0.307387\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.022086\n",
      "exploration/env_infos/final/end_effector_loc Std         0.167831\n",
      "exploration/env_infos/final/end_effector_loc Max         0.267382\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.372321\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00135697\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00270577\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00404251\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00469231\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0226019\n",
      "exploration/env_infos/end_effector_loc Std               0.0910463\n",
      "exploration/env_infos/end_effector_loc Max               0.267382\n",
      "exploration/env_infos/end_effector_loc Min              -0.372321\n",
      "evaluation/num steps total                           53000\n",
      "evaluation/num paths total                            2650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0570043\n",
      "evaluation/Rewards Std                                   0.0678026\n",
      "evaluation/Rewards Max                                   0.144139\n",
      "evaluation/Rewards Min                                  -0.337982\n",
      "evaluation/Returns Mean                                 -1.14009\n",
      "evaluation/Returns Std                                   1.0589\n",
      "evaluation/Returns Max                                   1.15055\n",
      "evaluation/Returns Min                                  -3.65873\n",
      "evaluation/Actions Mean                                 -0.00157285\n",
      "evaluation/Actions Std                                   0.0630739\n",
      "evaluation/Actions Max                                   0.674026\n",
      "evaluation/Actions Min                                  -0.586379\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.14009\n",
      "evaluation/env_infos/final/reward_dist Mean              0.183789\n",
      "evaluation/env_infos/final/reward_dist Std               0.28059\n",
      "evaluation/env_infos/final/reward_dist Max               0.980919\n",
      "evaluation/env_infos/final/reward_dist Min               2.05697e-26\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00562016\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117248\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0707175\n",
      "evaluation/env_infos/initial/reward_dist Min             3.60645e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.155375\n",
      "evaluation/env_infos/reward_dist Std                     0.236562\n",
      "evaluation/env_infos/reward_dist Max                     0.996748\n",
      "evaluation/env_infos/reward_dist Min                     2.05697e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0751938\n",
      "evaluation/env_infos/final/reward_energy Std             0.0723688\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00520269\n",
      "evaluation/env_infos/final/reward_energy Min            -0.293956\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.1643\n",
      "evaluation/env_infos/initial/reward_energy Std           0.184008\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0142004\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.790587\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0525415\n",
      "evaluation/env_infos/reward_energy Std                   0.0721177\n",
      "evaluation/env_infos/reward_energy Max                  -0.000646712\n",
      "evaluation/env_infos/reward_energy Min                  -0.790587\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0372193\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.215862\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.317112\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.645088\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000574897\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00870265\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0337013\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0293189\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0190383\n",
      "evaluation/env_infos/end_effector_loc Std                0.136677\n",
      "evaluation/env_infos/end_effector_loc Max                0.317112\n",
      "evaluation/env_infos/end_effector_loc Min               -0.645088\n",
      "time/data storing (s)                                    0.00316824\n",
      "time/evaluation sampling (s)                             0.993103\n",
      "time/exploration sampling (s)                            0.127051\n",
      "time/logging (s)                                         0.0185869\n",
      "time/saving (s)                                          0.0264119\n",
      "time/training (s)                                       49.4501\n",
      "time/epoch (s)                                          50.6184\n",
      "time/total (s)                                        2556.41\n",
      "Epoch                                                   52\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:08:23.608887 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 53 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000572624\n",
      "trainer/QF2 Loss                                         0.000973084\n",
      "trainer/Policy Loss                                      2.99688\n",
      "trainer/Q1 Predictions Mean                             -1.15497\n",
      "trainer/Q1 Predictions Std                               0.849146\n",
      "trainer/Q1 Predictions Max                               0.737176\n",
      "trainer/Q1 Predictions Min                              -3.27979\n",
      "trainer/Q2 Predictions Mean                             -1.1408\n",
      "trainer/Q2 Predictions Std                               0.844036\n",
      "trainer/Q2 Predictions Max                               0.728662\n",
      "trainer/Q2 Predictions Min                              -3.27321\n",
      "trainer/Q Targets Mean                                  -1.15099\n",
      "trainer/Q Targets Std                                    0.852005\n",
      "trainer/Q Targets Max                                    0.70616\n",
      "trainer/Q Targets Min                                   -3.2992\n",
      "trainer/Log Pis Mean                                     1.84293\n",
      "trainer/Log Pis Std                                      1.39176\n",
      "trainer/Log Pis Max                                      4.22056\n",
      "trainer/Log Pis Min                                     -2.56069\n",
      "trainer/Policy mu Mean                                  -0.0383871\n",
      "trainer/Policy mu Std                                    0.241176\n",
      "trainer/Policy mu Max                                    1.00329\n",
      "trainer/Policy mu Min                                   -2.02289\n",
      "trainer/Policy log std Mean                             -2.32471\n",
      "trainer/Policy log std Std                               0.527772\n",
      "trainer/Policy log std Max                              -0.315075\n",
      "trainer/Policy log std Min                              -3.20793\n",
      "trainer/Alpha                                            0.0227258\n",
      "trainer/Alpha Loss                                      -0.59412\n",
      "exploration/num steps total                           6400\n",
      "exploration/num paths total                            320\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0904083\n",
      "exploration/Rewards Std                                  0.0934899\n",
      "exploration/Rewards Max                                  0.0521065\n",
      "exploration/Rewards Min                                 -0.464915\n",
      "exploration/Returns Mean                                -1.80817\n",
      "exploration/Returns Std                                  1.38176\n",
      "exploration/Returns Max                                 -0.662538\n",
      "exploration/Returns Min                                 -3.53301\n",
      "exploration/Actions Mean                                 0.0212716\n",
      "exploration/Actions Std                                  0.164074\n",
      "exploration/Actions Max                                  0.715719\n",
      "exploration/Actions Min                                 -0.518244\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.80817\n",
      "exploration/env_infos/final/reward_dist Mean             0.0416745\n",
      "exploration/env_infos/final/reward_dist Std              0.0833418\n",
      "exploration/env_infos/final/reward_dist Max              0.208358\n",
      "exploration/env_infos/final/reward_dist Min              1.15508e-93\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00106381\n",
      "exploration/env_infos/initial/reward_dist Std            0.00125184\n",
      "exploration/env_infos/initial/reward_dist Max            0.00350076\n",
      "exploration/env_infos/initial/reward_dist Min            1.16555e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0454611\n",
      "exploration/env_infos/reward_dist Std                    0.0882724\n",
      "exploration/env_infos/reward_dist Max                    0.454585\n",
      "exploration/env_infos/reward_dist Min                    1.15508e-93\n",
      "exploration/env_infos/final/reward_energy Mean          -0.184593\n",
      "exploration/env_infos/final/reward_energy Std            0.0831683\n",
      "exploration/env_infos/final/reward_energy Max           -0.117049\n",
      "exploration/env_infos/final/reward_energy Min           -0.33386\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.404757\n",
      "exploration/env_infos/initial/reward_energy Std          0.223312\n",
      "exploration/env_infos/initial/reward_energy Max         -0.102838\n",
      "exploration/env_infos/initial/reward_energy Min         -0.685325\n",
      "exploration/env_infos/reward_energy Mean                -0.193288\n",
      "exploration/env_infos/reward_energy Std                  0.131853\n",
      "exploration/env_infos/reward_energy Max                 -0.0238137\n",
      "exploration/env_infos/reward_energy Min                 -0.723145\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.18155\n",
      "exploration/env_infos/final/end_effector_loc Std         0.389973\n",
      "exploration/env_infos/final/end_effector_loc Max         0.900878\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.294195\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00160495\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0162648\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0319206\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0259122\n",
      "exploration/env_infos/end_effector_loc Mean              0.0613797\n",
      "exploration/env_infos/end_effector_loc Std               0.236086\n",
      "exploration/env_infos/end_effector_loc Max               0.900878\n",
      "exploration/env_infos/end_effector_loc Min              -0.294195\n",
      "evaluation/num steps total                           54000\n",
      "evaluation/num paths total                            2700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0615927\n",
      "evaluation/Rewards Std                                   0.0875146\n",
      "evaluation/Rewards Max                                   0.129299\n",
      "evaluation/Rewards Min                                  -0.47225\n",
      "evaluation/Returns Mean                                 -1.23185\n",
      "evaluation/Returns Std                                   1.39009\n",
      "evaluation/Returns Max                                   1.51972\n",
      "evaluation/Returns Min                                  -5.00488\n",
      "evaluation/Actions Mean                                 -0.0032988\n",
      "evaluation/Actions Std                                   0.0786492\n",
      "evaluation/Actions Max                                   0.776927\n",
      "evaluation/Actions Min                                  -0.852732\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.23185\n",
      "evaluation/env_infos/final/reward_dist Mean              0.177501\n",
      "evaluation/env_infos/final/reward_dist Std               0.263068\n",
      "evaluation/env_infos/final/reward_dist Max               0.970842\n",
      "evaluation/env_infos/final/reward_dist Min               3.14615e-39\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00688088\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0099455\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0422336\n",
      "evaluation/env_infos/initial/reward_dist Min             1.33985e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.176659\n",
      "evaluation/env_infos/reward_dist Std                     0.259607\n",
      "evaluation/env_infos/reward_dist Max                     0.997383\n",
      "evaluation/env_infos/reward_dist Min                     3.14615e-39\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0719284\n",
      "evaluation/env_infos/final/reward_energy Std             0.0703338\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00300241\n",
      "evaluation/env_infos/final/reward_energy Min            -0.293106\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.248596\n",
      "evaluation/env_infos/initial/reward_energy Std           0.246405\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.012014\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.956006\n",
      "evaluation/env_infos/reward_energy Mean                 -0.067307\n",
      "evaluation/env_infos/reward_energy Std                   0.0886732\n",
      "evaluation/env_infos/reward_energy Max                  -0.00120531\n",
      "evaluation/env_infos/reward_energy Min                  -0.956006\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00501051\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.239866\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.532985\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.614261\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00100322\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123344\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0388463\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0426366\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00247718\n",
      "evaluation/env_infos/end_effector_loc Std                0.160254\n",
      "evaluation/env_infos/end_effector_loc Max                0.532985\n",
      "evaluation/env_infos/end_effector_loc Min               -0.614261\n",
      "time/data storing (s)                                    0.00288061\n",
      "time/evaluation sampling (s)                             0.96954\n",
      "time/exploration sampling (s)                            0.134335\n",
      "time/logging (s)                                         0.0214879\n",
      "time/saving (s)                                          0.0276615\n",
      "time/training (s)                                       48.8595\n",
      "time/epoch (s)                                          50.0154\n",
      "time/total (s)                                        2607.07\n",
      "Epoch                                                   53\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:09:13.806202 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 54 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000602111\n",
      "trainer/QF2 Loss                                         0.000698058\n",
      "trainer/Policy Loss                                      3.1455\n",
      "trainer/Q1 Predictions Mean                             -1.06032\n",
      "trainer/Q1 Predictions Std                               0.813\n",
      "trainer/Q1 Predictions Max                               1.46217\n",
      "trainer/Q1 Predictions Min                              -3.10419\n",
      "trainer/Q2 Predictions Mean                             -1.05144\n",
      "trainer/Q2 Predictions Std                               0.812666\n",
      "trainer/Q2 Predictions Max                               1.49839\n",
      "trainer/Q2 Predictions Min                              -3.06528\n",
      "trainer/Q Targets Mean                                  -1.05529\n",
      "trainer/Q Targets Std                                    0.812686\n",
      "trainer/Q Targets Max                                    1.42107\n",
      "trainer/Q Targets Min                                   -3.08488\n",
      "trainer/Log Pis Mean                                     2.07855\n",
      "trainer/Log Pis Std                                      1.36986\n",
      "trainer/Log Pis Max                                      4.36102\n",
      "trainer/Log Pis Min                                     -4.29018\n",
      "trainer/Policy mu Mean                                  -0.0166128\n",
      "trainer/Policy mu Std                                    0.166369\n",
      "trainer/Policy mu Max                                    0.695085\n",
      "trainer/Policy mu Min                                   -2.10868\n",
      "trainer/Policy log std Mean                             -2.4178\n",
      "trainer/Policy log std Std                               0.501994\n",
      "trainer/Policy log std Max                              -0.518801\n",
      "trainer/Policy log std Min                              -3.25803\n",
      "trainer/Alpha                                            0.0224601\n",
      "trainer/Alpha Loss                                       0.298174\n",
      "exploration/num steps total                           6500\n",
      "exploration/num paths total                            325\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0896921\n",
      "exploration/Rewards Std                                  0.071465\n",
      "exploration/Rewards Max                                  0.0365829\n",
      "exploration/Rewards Min                                 -0.371033\n",
      "exploration/Returns Mean                                -1.79384\n",
      "exploration/Returns Std                                  0.485314\n",
      "exploration/Returns Max                                 -1.16159\n",
      "exploration/Returns Min                                 -2.61891\n",
      "exploration/Actions Mean                                 0.00977882\n",
      "exploration/Actions Std                                  0.108313\n",
      "exploration/Actions Max                                  0.431954\n",
      "exploration/Actions Min                                 -0.345894\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.79384\n",
      "exploration/env_infos/final/reward_dist Mean             0.150734\n",
      "exploration/env_infos/final/reward_dist Std              0.242625\n",
      "exploration/env_infos/final/reward_dist Max              0.628069\n",
      "exploration/env_infos/final/reward_dist Min              2.50777e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00534776\n",
      "exploration/env_infos/initial/reward_dist Std            0.00594692\n",
      "exploration/env_infos/initial/reward_dist Max            0.0150082\n",
      "exploration/env_infos/initial/reward_dist Min            1.95935e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.11308\n",
      "exploration/env_infos/reward_dist Std                    0.154352\n",
      "exploration/env_infos/reward_dist Max                    0.628069\n",
      "exploration/env_infos/reward_dist Min                    2.50777e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.255023\n",
      "exploration/env_infos/final/reward_energy Std            0.143467\n",
      "exploration/env_infos/final/reward_energy Max           -0.118\n",
      "exploration/env_infos/final/reward_energy Min           -0.517729\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.178196\n",
      "exploration/env_infos/initial/reward_energy Std          0.0945371\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0520423\n",
      "exploration/env_infos/initial/reward_energy Min         -0.265767\n",
      "exploration/env_infos/reward_energy Mean                -0.129746\n",
      "exploration/env_infos/reward_energy Std                  0.0825865\n",
      "exploration/env_infos/reward_energy Max                 -0.0190906\n",
      "exploration/env_infos/reward_energy Min                 -0.517729\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0316354\n",
      "exploration/env_infos/final/end_effector_loc Std         0.215032\n",
      "exploration/env_infos/final/end_effector_loc Max         0.36066\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.307917\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000861554\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00707965\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0132239\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0116155\n",
      "exploration/env_infos/end_effector_loc Mean              0.0059525\n",
      "exploration/env_infos/end_effector_loc Std               0.135553\n",
      "exploration/env_infos/end_effector_loc Max               0.36066\n",
      "exploration/env_infos/end_effector_loc Min              -0.311552\n",
      "evaluation/num steps total                           55000\n",
      "evaluation/num paths total                            2750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.052766\n",
      "evaluation/Rewards Std                                   0.0717795\n",
      "evaluation/Rewards Max                                   0.128068\n",
      "evaluation/Rewards Min                                  -0.521683\n",
      "evaluation/Returns Mean                                 -1.05532\n",
      "evaluation/Returns Std                                   1.13114\n",
      "evaluation/Returns Max                                   1.14936\n",
      "evaluation/Returns Min                                  -5.28174\n",
      "evaluation/Actions Mean                                  0.00251551\n",
      "evaluation/Actions Std                                   0.065148\n",
      "evaluation/Actions Max                                   0.549805\n",
      "evaluation/Actions Min                                  -0.553513\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.05532\n",
      "evaluation/env_infos/final/reward_dist Mean              0.21628\n",
      "evaluation/env_infos/final/reward_dist Std               0.292008\n",
      "evaluation/env_infos/final/reward_dist Max               0.952999\n",
      "evaluation/env_infos/final/reward_dist Min               1.47572e-93\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00708446\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116563\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0486307\n",
      "evaluation/env_infos/initial/reward_dist Min             8.98009e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.199824\n",
      "evaluation/env_infos/reward_dist Std                     0.292146\n",
      "evaluation/env_infos/reward_dist Max                     0.998379\n",
      "evaluation/env_infos/reward_dist Min                     1.47572e-93\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0685116\n",
      "evaluation/env_infos/final/reward_energy Std             0.0876042\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00785327\n",
      "evaluation/env_infos/final/reward_energy Min            -0.587198\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.169714\n",
      "evaluation/env_infos/initial/reward_energy Std           0.16625\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00259799\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.749208\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0569569\n",
      "evaluation/env_infos/reward_energy Std                   0.0725058\n",
      "evaluation/env_infos/reward_energy Max                  -0.00109553\n",
      "evaluation/env_infos/reward_energy Min                  -0.749208\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0494042\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.250392\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.573149\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00150277\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00826401\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0274903\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.02698\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0244095\n",
      "evaluation/env_infos/end_effector_loc Std                0.155295\n",
      "evaluation/env_infos/end_effector_loc Max                0.573149\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00289018\n",
      "time/evaluation sampling (s)                             0.985175\n",
      "time/exploration sampling (s)                            0.119853\n",
      "time/logging (s)                                         0.0196587\n",
      "time/saving (s)                                          0.0273358\n",
      "time/training (s)                                       48.4307\n",
      "time/epoch (s)                                          49.5856\n",
      "time/total (s)                                        2657.27\n",
      "Epoch                                                   54\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:10:04.066068 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 55 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000593128\n",
      "trainer/QF2 Loss                                         0.000621741\n",
      "trainer/Policy Loss                                      3.03282\n",
      "trainer/Q1 Predictions Mean                             -1.07043\n",
      "trainer/Q1 Predictions Std                               0.834907\n",
      "trainer/Q1 Predictions Max                               0.695357\n",
      "trainer/Q1 Predictions Min                              -3.25502\n",
      "trainer/Q2 Predictions Mean                             -1.0609\n",
      "trainer/Q2 Predictions Std                               0.82877\n",
      "trainer/Q2 Predictions Max                               0.70105\n",
      "trainer/Q2 Predictions Min                              -3.2107\n",
      "trainer/Q Targets Mean                                  -1.06613\n",
      "trainer/Q Targets Std                                    0.836254\n",
      "trainer/Q Targets Max                                    0.716177\n",
      "trainer/Q Targets Min                                   -3.24291\n",
      "trainer/Log Pis Mean                                     1.96523\n",
      "trainer/Log Pis Std                                      1.2988\n",
      "trainer/Log Pis Max                                      4.62568\n",
      "trainer/Log Pis Min                                     -2.57531\n",
      "trainer/Policy mu Mean                                  -0.027083\n",
      "trainer/Policy mu Std                                    0.265751\n",
      "trainer/Policy mu Max                                    0.895987\n",
      "trainer/Policy mu Min                                   -1.82981\n",
      "trainer/Policy log std Mean                             -2.29266\n",
      "trainer/Policy log std Std                               0.590715\n",
      "trainer/Policy log std Max                              -0.195252\n",
      "trainer/Policy log std Min                              -3.23085\n",
      "trainer/Alpha                                            0.0230214\n",
      "trainer/Alpha Loss                                      -0.131146\n",
      "exploration/num steps total                           6600\n",
      "exploration/num paths total                            330\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0764029\n",
      "exploration/Rewards Std                                  0.104132\n",
      "exploration/Rewards Max                                  0.16182\n",
      "exploration/Rewards Min                                 -0.238052\n",
      "exploration/Returns Mean                                -1.52806\n",
      "exploration/Returns Std                                  1.93502\n",
      "exploration/Returns Max                                  1.98621\n",
      "exploration/Returns Min                                 -3.47602\n",
      "exploration/Actions Mean                                 0.00125412\n",
      "exploration/Actions Std                                  0.134406\n",
      "exploration/Actions Max                                  0.473603\n",
      "exploration/Actions Min                                 -0.880081\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.52806\n",
      "exploration/env_infos/final/reward_dist Mean             0.186837\n",
      "exploration/env_infos/final/reward_dist Std              0.324201\n",
      "exploration/env_infos/final/reward_dist Max              0.831045\n",
      "exploration/env_infos/final/reward_dist Min              4.90153e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0149268\n",
      "exploration/env_infos/initial/reward_dist Std            0.0238764\n",
      "exploration/env_infos/initial/reward_dist Max            0.0623438\n",
      "exploration/env_infos/initial/reward_dist Min            5.19226e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0850211\n",
      "exploration/env_infos/reward_dist Std                    0.17913\n",
      "exploration/env_infos/reward_dist Max                    0.838509\n",
      "exploration/env_infos/reward_dist Min                    4.90153e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.180544\n",
      "exploration/env_infos/final/reward_energy Std            0.0990073\n",
      "exploration/env_infos/final/reward_energy Max           -0.0667163\n",
      "exploration/env_infos/final/reward_energy Min           -0.349078\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.269136\n",
      "exploration/env_infos/initial/reward_energy Std          0.330088\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0220289\n",
      "exploration/env_infos/initial/reward_energy Min         -0.892314\n",
      "exploration/env_infos/reward_energy Mean                -0.131652\n",
      "exploration/env_infos/reward_energy Std                  0.137116\n",
      "exploration/env_infos/reward_energy Max                 -0.00727495\n",
      "exploration/env_infos/reward_energy Min                 -0.892314\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.033585\n",
      "exploration/env_infos/final/end_effector_loc Std         0.262782\n",
      "exploration/env_infos/final/end_effector_loc Max         0.639494\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.301857\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00335862\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0146785\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0160162\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.044004\n",
      "exploration/env_infos/end_effector_loc Mean              0.0107712\n",
      "exploration/env_infos/end_effector_loc Std               0.168243\n",
      "exploration/env_infos/end_effector_loc Max               0.639494\n",
      "exploration/env_infos/end_effector_loc Min              -0.303993\n",
      "evaluation/num steps total                           56000\n",
      "evaluation/num paths total                            2800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0480364\n",
      "evaluation/Rewards Std                                   0.0750632\n",
      "evaluation/Rewards Max                                   0.126706\n",
      "evaluation/Rewards Min                                  -0.456857\n",
      "evaluation/Returns Mean                                 -0.960727\n",
      "evaluation/Returns Std                                   1.12656\n",
      "evaluation/Returns Max                                   1.77404\n",
      "evaluation/Returns Min                                  -4.81758\n",
      "evaluation/Actions Mean                                  0.000799285\n",
      "evaluation/Actions Std                                   0.0720022\n",
      "evaluation/Actions Max                                   0.393275\n",
      "evaluation/Actions Min                                  -0.844084\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.960727\n",
      "evaluation/env_infos/final/reward_dist Mean              0.130547\n",
      "evaluation/env_infos/final/reward_dist Std               0.233542\n",
      "evaluation/env_infos/final/reward_dist Max               0.878043\n",
      "evaluation/env_infos/final/reward_dist Min               6.89677e-78\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00950609\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0171082\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0907809\n",
      "evaluation/env_infos/initial/reward_dist Min             9.45511e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.188513\n",
      "evaluation/env_infos/reward_dist Std                     0.259368\n",
      "evaluation/env_infos/reward_dist Max                     0.997478\n",
      "evaluation/env_infos/reward_dist Min                     6.89677e-78\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0778148\n",
      "evaluation/env_infos/final/reward_energy Std             0.0925562\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00349504\n",
      "evaluation/env_infos/final/reward_energy Min            -0.548578\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.194743\n",
      "evaluation/env_infos/initial/reward_energy Std           0.192678\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0289425\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.9069\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0620299\n",
      "evaluation/env_infos/reward_energy Std                   0.0807601\n",
      "evaluation/env_infos/reward_energy Max                  -0.00182103\n",
      "evaluation/env_infos/reward_energy Min                  -0.9069\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0408104\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27453\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.471268\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000318141\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00968045\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0196637\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0422042\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0181344\n",
      "evaluation/env_infos/end_effector_loc Std                0.172707\n",
      "evaluation/env_infos/end_effector_loc Max                0.471268\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00299232\n",
      "time/evaluation sampling (s)                             1.02214\n",
      "time/exploration sampling (s)                            0.14017\n",
      "time/logging (s)                                         0.0197715\n",
      "time/saving (s)                                          0.03487\n",
      "time/training (s)                                       48.328\n",
      "time/epoch (s)                                          49.548\n",
      "time/total (s)                                        2707.53\n",
      "Epoch                                                   55\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:10:55.944776 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 56 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00103821\r\n",
      "trainer/QF2 Loss                                         0.000697852\r\n",
      "trainer/Policy Loss                                      3.18031\r\n",
      "trainer/Q1 Predictions Mean                             -1.11693\r\n",
      "trainer/Q1 Predictions Std                               0.864548\r\n",
      "trainer/Q1 Predictions Max                               1.37104\r\n",
      "trainer/Q1 Predictions Min                              -2.99365\r\n",
      "trainer/Q2 Predictions Mean                             -1.13266\r\n",
      "trainer/Q2 Predictions Std                               0.872073\r\n",
      "trainer/Q2 Predictions Max                               1.38206\r\n",
      "trainer/Q2 Predictions Min                              -3.01791\r\n",
      "trainer/Q Targets Mean                                  -1.12416\r\n",
      "trainer/Q Targets Std                                    0.864477\r\n",
      "trainer/Q Targets Max                                    1.36514\r\n",
      "trainer/Q Targets Min                                   -3.00417\r\n",
      "trainer/Log Pis Mean                                     2.05176\r\n",
      "trainer/Log Pis Std                                      1.30808\r\n",
      "trainer/Log Pis Max                                      4.28097\r\n",
      "trainer/Log Pis Min                                     -2.14099\r\n",
      "trainer/Policy mu Mean                                  -0.0145926\r\n",
      "trainer/Policy mu Std                                    0.176873\r\n",
      "trainer/Policy mu Max                                    0.677015\r\n",
      "trainer/Policy mu Min                                   -1.3311\r\n",
      "trainer/Policy log std Mean                             -2.36058\r\n",
      "trainer/Policy log std Std                               0.559371\r\n",
      "trainer/Policy log std Max                              -0.578373\r\n",
      "trainer/Policy log std Min                              -3.36633\r\n",
      "trainer/Alpha                                            0.0234853\r\n",
      "trainer/Alpha Loss                                       0.19414\r\n",
      "exploration/num steps total                           6700\r\n",
      "exploration/num paths total                            335\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0919842\r\n",
      "exploration/Rewards Std                                  0.11143\r\n",
      "exploration/Rewards Max                                  0.111324\r\n",
      "exploration/Rewards Min                                 -0.540389\r\n",
      "exploration/Returns Mean                                -1.83968\r\n",
      "exploration/Returns Std                                  1.36444\r\n",
      "exploration/Returns Max                                  0.0752275\r\n",
      "exploration/Returns Min                                 -3.78938\r\n",
      "exploration/Actions Mean                                 0.00801843\r\n",
      "exploration/Actions Std                                  0.105194\r\n",
      "exploration/Actions Max                                  0.532639\r\n",
      "exploration/Actions Min                                 -0.437578\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.83968\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0544772\r\n",
      "exploration/env_infos/final/reward_dist Std              0.108954\r\n",
      "exploration/env_infos/final/reward_dist Max              0.272386\r\n",
      "exploration/env_infos/final/reward_dist Min              4.81574e-22\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00540059\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00649916\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0179335\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.76774e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.175592\r\n",
      "exploration/env_infos/reward_dist Std                    0.287512\r\n",
      "exploration/env_infos/reward_dist Max                    0.982445\r\n",
      "exploration/env_infos/reward_dist Min                    4.81574e-22\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.209329\r\n",
      "exploration/env_infos/final/reward_energy Std            0.193591\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.05436\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.591051\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.185301\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.142165\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0188189\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.437981\r\n",
      "exploration/env_infos/reward_energy Mean                -0.122424\r\n",
      "exploration/env_infos/reward_energy Std                  0.0852795\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00298507\r\n",
      "exploration/env_infos/reward_energy Min                 -0.591051\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00189352\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.374657\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.495658\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.759981\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      2.25967e-05\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00825732\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00994174\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0218789\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0140311\r\n",
      "exploration/env_infos/end_effector_loc Std               0.212849\r\n",
      "exploration/env_infos/end_effector_loc Max               0.495658\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.759981\r\n",
      "evaluation/num steps total                           57000\r\n",
      "evaluation/num paths total                            2850\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0460892\r\n",
      "evaluation/Rewards Std                                   0.107942\r\n",
      "evaluation/Rewards Max                                   0.144132\r\n",
      "evaluation/Rewards Min                                  -0.896632\r\n",
      "evaluation/Returns Mean                                 -0.921784\r\n",
      "evaluation/Returns Std                                   1.7149\r\n",
      "evaluation/Returns Max                                   1.88585\r\n",
      "evaluation/Returns Min                                  -9.6688\r\n",
      "evaluation/Actions Mean                                 -0.000575136\r\n",
      "evaluation/Actions Std                                   0.0851187\r\n",
      "evaluation/Actions Max                                   0.434438\r\n",
      "evaluation/Actions Min                                  -0.774292\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.921784\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.154316\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.264287\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.991114\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.78169e-77\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00856504\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0243941\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.160821\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.79399e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.228195\r\n",
      "evaluation/env_infos/reward_dist Std                     0.296986\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997694\r\n",
      "evaluation/env_infos/reward_dist Min                     3.78169e-77\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0711521\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.105453\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00555379\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.596977\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.26154\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.224334\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0222993\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.860797\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0705596\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0975315\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178729\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.860797\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0369735\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.286926\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.669286\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00131289\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121115\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0217219\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0387146\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.015287\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.18299\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.669286\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00300688\r\n",
      "time/evaluation sampling (s)                             1.07989\r\n",
      "time/exploration sampling (s)                            0.1281\r\n",
      "time/logging (s)                                         0.0193335\r\n",
      "time/saving (s)                                          0.0281824\r\n",
      "time/training (s)                                       49.8395\r\n",
      "time/epoch (s)                                          51.098\r\n",
      "time/total (s)                                        2759.41\r\n",
      "Epoch                                                   56\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:11:47.670761 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 57 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000736703\n",
      "trainer/QF2 Loss                                         0.00139431\n",
      "trainer/Policy Loss                                      3.07645\n",
      "trainer/Q1 Predictions Mean                             -1.11824\n",
      "trainer/Q1 Predictions Std                               0.848807\n",
      "trainer/Q1 Predictions Max                               1.3714\n",
      "trainer/Q1 Predictions Min                              -3.08349\n",
      "trainer/Q2 Predictions Mean                             -1.13871\n",
      "trainer/Q2 Predictions Std                               0.854693\n",
      "trainer/Q2 Predictions Max                               1.36565\n",
      "trainer/Q2 Predictions Min                              -3.14645\n",
      "trainer/Q Targets Mean                                  -1.11986\n",
      "trainer/Q Targets Std                                    0.850055\n",
      "trainer/Q Targets Max                                    1.36535\n",
      "trainer/Q Targets Min                                   -3.10122\n",
      "trainer/Log Pis Mean                                     1.93985\n",
      "trainer/Log Pis Std                                      1.39609\n",
      "trainer/Log Pis Max                                      4.15604\n",
      "trainer/Log Pis Min                                     -2.32912\n",
      "trainer/Policy mu Mean                                  -0.0272378\n",
      "trainer/Policy mu Std                                    0.218933\n",
      "trainer/Policy mu Max                                    0.883041\n",
      "trainer/Policy mu Min                                   -1.63654\n",
      "trainer/Policy log std Mean                             -2.361\n",
      "trainer/Policy log std Std                               0.612402\n",
      "trainer/Policy log std Max                               0.165138\n",
      "trainer/Policy log std Min                              -3.25185\n",
      "trainer/Alpha                                            0.0238173\n",
      "trainer/Alpha Loss                                      -0.224707\n",
      "exploration/num steps total                           6800\n",
      "exploration/num paths total                            340\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.107745\n",
      "exploration/Rewards Std                                  0.0765248\n",
      "exploration/Rewards Max                                  0.0742506\n",
      "exploration/Rewards Min                                 -0.340967\n",
      "exploration/Returns Mean                                -2.15491\n",
      "exploration/Returns Std                                  1.07939\n",
      "exploration/Returns Max                                 -0.139814\n",
      "exploration/Returns Min                                 -3.1687\n",
      "exploration/Actions Mean                                 0.0111467\n",
      "exploration/Actions Std                                  0.132408\n",
      "exploration/Actions Max                                  0.521979\n",
      "exploration/Actions Min                                 -0.439911\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.15491\n",
      "exploration/env_infos/final/reward_dist Mean             0.141021\n",
      "exploration/env_infos/final/reward_dist Std              0.206453\n",
      "exploration/env_infos/final/reward_dist Max              0.531406\n",
      "exploration/env_infos/final/reward_dist Min              8.0462e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000176101\n",
      "exploration/env_infos/initial/reward_dist Std            0.000147691\n",
      "exploration/env_infos/initial/reward_dist Max            0.000420959\n",
      "exploration/env_infos/initial/reward_dist Min            1.01247e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.19069\n",
      "exploration/env_infos/reward_dist Std                    0.291601\n",
      "exploration/env_infos/reward_dist Max                    0.967412\n",
      "exploration/env_infos/reward_dist Min                    8.0462e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.115605\n",
      "exploration/env_infos/final/reward_energy Std            0.0603252\n",
      "exploration/env_infos/final/reward_energy Max           -0.0468414\n",
      "exploration/env_infos/final/reward_energy Min           -0.223197\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.325379\n",
      "exploration/env_infos/initial/reward_energy Std          0.167988\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0845183\n",
      "exploration/env_infos/initial/reward_energy Min         -0.57063\n",
      "exploration/env_infos/reward_energy Mean                -0.153218\n",
      "exploration/env_infos/reward_energy Std                  0.108795\n",
      "exploration/env_infos/reward_energy Max                 -0.0131685\n",
      "exploration/env_infos/reward_energy Min                 -0.57063\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0460593\n",
      "exploration/env_infos/final/end_effector_loc Std         0.321278\n",
      "exploration/env_infos/final/end_effector_loc Max         0.572841\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.348545\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00305663\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0125806\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0260989\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0219955\n",
      "exploration/env_infos/end_effector_loc Mean              0.01469\n",
      "exploration/env_infos/end_effector_loc Std               0.211635\n",
      "exploration/env_infos/end_effector_loc Max               0.572841\n",
      "exploration/env_infos/end_effector_loc Min              -0.393952\n",
      "evaluation/num steps total                           58000\n",
      "evaluation/num paths total                            2900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0628498\n",
      "evaluation/Rewards Std                                   0.0752698\n",
      "evaluation/Rewards Max                                   0.147402\n",
      "evaluation/Rewards Min                                  -0.558639\n",
      "evaluation/Returns Mean                                 -1.257\n",
      "evaluation/Returns Std                                   1.06187\n",
      "evaluation/Returns Max                                   0.936845\n",
      "evaluation/Returns Min                                  -3.73891\n",
      "evaluation/Actions Mean                                 -0.00691471\n",
      "evaluation/Actions Std                                   0.0696975\n",
      "evaluation/Actions Max                                   0.430153\n",
      "evaluation/Actions Min                                  -0.696\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.257\n",
      "evaluation/env_infos/final/reward_dist Mean              0.130742\n",
      "evaluation/env_infos/final/reward_dist Std               0.247027\n",
      "evaluation/env_infos/final/reward_dist Max               0.95122\n",
      "evaluation/env_infos/final/reward_dist Min               3.98917e-53\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00562622\n",
      "evaluation/env_infos/initial/reward_dist Std             0.01203\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0605262\n",
      "evaluation/env_infos/initial/reward_dist Min             1.13677e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.157453\n",
      "evaluation/env_infos/reward_dist Std                     0.260578\n",
      "evaluation/env_infos/reward_dist Max                     0.995197\n",
      "evaluation/env_infos/reward_dist Min                     3.98917e-53\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0693599\n",
      "evaluation/env_infos/final/reward_energy Std             0.0884549\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00376967\n",
      "evaluation/env_infos/final/reward_energy Min            -0.410207\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.186663\n",
      "evaluation/env_infos/initial/reward_energy Std           0.182228\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0117582\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.797529\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0584573\n",
      "evaluation/env_infos/reward_energy Std                   0.0799616\n",
      "evaluation/env_infos/reward_energy Max                  -0.000445657\n",
      "evaluation/env_infos/reward_energy Min                  -0.797529\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0719455\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.286102\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.561632\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.954515\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00153214\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00909479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0215076\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0348\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0310406\n",
      "evaluation/env_infos/end_effector_loc Std                0.166966\n",
      "evaluation/env_infos/end_effector_loc Max                0.561632\n",
      "evaluation/env_infos/end_effector_loc Min               -0.954515\n",
      "time/data storing (s)                                    0.00295063\n",
      "time/evaluation sampling (s)                             0.986685\n",
      "time/exploration sampling (s)                            0.122752\n",
      "time/logging (s)                                         0.0227919\n",
      "time/saving (s)                                          0.0292932\n",
      "time/training (s)                                       49.7764\n",
      "time/epoch (s)                                          50.9409\n",
      "time/total (s)                                        2811.13\n",
      "Epoch                                                   57\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:12:39.177089 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 58 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000976209\n",
      "trainer/QF2 Loss                                         0.000556483\n",
      "trainer/Policy Loss                                      3.10385\n",
      "trainer/Q1 Predictions Mean                             -1.13657\n",
      "trainer/Q1 Predictions Std                               0.913435\n",
      "trainer/Q1 Predictions Max                               1.4514\n",
      "trainer/Q1 Predictions Min                              -3.15312\n",
      "trainer/Q2 Predictions Mean                             -1.13386\n",
      "trainer/Q2 Predictions Std                               0.906887\n",
      "trainer/Q2 Predictions Max                               1.41451\n",
      "trainer/Q2 Predictions Min                              -3.12819\n",
      "trainer/Q Targets Mean                                  -1.13305\n",
      "trainer/Q Targets Std                                    0.906633\n",
      "trainer/Q Targets Max                                    1.41203\n",
      "trainer/Q Targets Min                                   -3.12796\n",
      "trainer/Log Pis Mean                                     1.96587\n",
      "trainer/Log Pis Std                                      1.36929\n",
      "trainer/Log Pis Max                                      4.27113\n",
      "trainer/Log Pis Min                                     -2.901\n",
      "trainer/Policy mu Mean                                  -0.0107412\n",
      "trainer/Policy mu Std                                    0.289848\n",
      "trainer/Policy mu Max                                    2.03518\n",
      "trainer/Policy mu Min                                   -2.12561\n",
      "trainer/Policy log std Mean                             -2.34451\n",
      "trainer/Policy log std Std                               0.555469\n",
      "trainer/Policy log std Max                              -0.483498\n",
      "trainer/Policy log std Min                              -3.17089\n",
      "trainer/Alpha                                            0.0237251\n",
      "trainer/Alpha Loss                                      -0.127666\n",
      "exploration/num steps total                           6900\n",
      "exploration/num paths total                            345\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0637124\n",
      "exploration/Rewards Std                                  0.0641904\n",
      "exploration/Rewards Max                                  0.0882144\n",
      "exploration/Rewards Min                                 -0.232782\n",
      "exploration/Returns Mean                                -1.27425\n",
      "exploration/Returns Std                                  0.839123\n",
      "exploration/Returns Max                                 -0.545506\n",
      "exploration/Returns Min                                 -2.78509\n",
      "exploration/Actions Mean                                -0.00674216\n",
      "exploration/Actions Std                                  0.10708\n",
      "exploration/Actions Max                                  0.399047\n",
      "exploration/Actions Min                                 -0.369892\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.27425\n",
      "exploration/env_infos/final/reward_dist Mean             0.225606\n",
      "exploration/env_infos/final/reward_dist Std              0.201701\n",
      "exploration/env_infos/final/reward_dist Max              0.478563\n",
      "exploration/env_infos/final/reward_dist Min              1.75888e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0102332\n",
      "exploration/env_infos/initial/reward_dist Std            0.0187589\n",
      "exploration/env_infos/initial/reward_dist Max            0.04772\n",
      "exploration/env_infos/initial/reward_dist Min            1.12261e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.218896\n",
      "exploration/env_infos/reward_dist Std                    0.298107\n",
      "exploration/env_infos/reward_dist Max                    0.969618\n",
      "exploration/env_infos/reward_dist Min                    1.75888e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.140288\n",
      "exploration/env_infos/final/reward_energy Std            0.081693\n",
      "exploration/env_infos/final/reward_energy Max           -0.0446855\n",
      "exploration/env_infos/final/reward_energy Min           -0.268648\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.174828\n",
      "exploration/env_infos/initial/reward_energy Std          0.160067\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0265545\n",
      "exploration/env_infos/initial/reward_energy Min         -0.451147\n",
      "exploration/env_infos/reward_energy Mean                -0.120361\n",
      "exploration/env_infos/reward_energy Std                  0.092392\n",
      "exploration/env_infos/reward_energy Max                 -0.00737304\n",
      "exploration/env_infos/reward_energy Min                 -0.451147\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0113732\n",
      "exploration/env_infos/final/end_effector_loc Std         0.219228\n",
      "exploration/env_infos/final/end_effector_loc Max         0.415162\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.319477\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000944217\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00832714\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0199523\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0105232\n",
      "exploration/env_infos/end_effector_loc Mean              0.0129345\n",
      "exploration/env_infos/end_effector_loc Std               0.138301\n",
      "exploration/env_infos/end_effector_loc Max               0.415162\n",
      "exploration/env_infos/end_effector_loc Min              -0.319477\n",
      "evaluation/num steps total                           59000\n",
      "evaluation/num paths total                            2950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0580023\n",
      "evaluation/Rewards Std                                   0.0984882\n",
      "evaluation/Rewards Max                                   0.149515\n",
      "evaluation/Rewards Min                                  -0.625158\n",
      "evaluation/Returns Mean                                 -1.16005\n",
      "evaluation/Returns Std                                   1.535\n",
      "evaluation/Returns Max                                   1.24575\n",
      "evaluation/Returns Min                                  -6.05891\n",
      "evaluation/Actions Mean                                 -0.00258568\n",
      "evaluation/Actions Std                                   0.0869172\n",
      "evaluation/Actions Max                                   0.848333\n",
      "evaluation/Actions Min                                  -0.75162\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.16005\n",
      "evaluation/env_infos/final/reward_dist Mean              0.172223\n",
      "evaluation/env_infos/final/reward_dist Std               0.278525\n",
      "evaluation/env_infos/final/reward_dist Max               0.993757\n",
      "evaluation/env_infos/final/reward_dist Min               4.2049e-42\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00447183\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00827516\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0426785\n",
      "evaluation/env_infos/initial/reward_dist Min             4.92073e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.182244\n",
      "evaluation/env_infos/reward_dist Std                     0.270307\n",
      "evaluation/env_infos/reward_dist Max                     0.998749\n",
      "evaluation/env_infos/reward_dist Min                     4.2049e-42\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0623619\n",
      "evaluation/env_infos/final/reward_energy Std             0.0708515\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00242769\n",
      "evaluation/env_infos/final/reward_energy Min            -0.30588\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.265714\n",
      "evaluation/env_infos/initial/reward_energy Std           0.246532\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0175726\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.0769\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0700888\n",
      "evaluation/env_infos/reward_energy Std                   0.101045\n",
      "evaluation/env_infos/reward_energy Max                  -0.000338659\n",
      "evaluation/env_infos/reward_energy Min                  -1.0769\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00128081\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.283692\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.754932\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.731241\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00163343\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127106\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0424167\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.037581\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0126277\n",
      "evaluation/env_infos/end_effector_loc Std                0.188323\n",
      "evaluation/env_infos/end_effector_loc Max                0.754932\n",
      "evaluation/env_infos/end_effector_loc Min               -0.731241\n",
      "time/data storing (s)                                    0.00298244\n",
      "time/evaluation sampling (s)                             1.19395\n",
      "time/exploration sampling (s)                            0.142179\n",
      "time/logging (s)                                         0.0202151\n",
      "time/saving (s)                                          0.0322962\n",
      "time/training (s)                                       49.2897\n",
      "time/epoch (s)                                          50.6813\n",
      "time/total (s)                                        2862.63\n",
      "Epoch                                                   58\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:13:30.595475 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 59 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000782608\r\n",
      "trainer/QF2 Loss                                         0.000914096\r\n",
      "trainer/Policy Loss                                      3.0445\r\n",
      "trainer/Q1 Predictions Mean                             -1.0383\r\n",
      "trainer/Q1 Predictions Std                               0.875598\r\n",
      "trainer/Q1 Predictions Max                               1.14027\r\n",
      "trainer/Q1 Predictions Min                              -3.20058\r\n",
      "trainer/Q2 Predictions Mean                             -1.03925\r\n",
      "trainer/Q2 Predictions Std                               0.867133\r\n",
      "trainer/Q2 Predictions Max                               1.09196\r\n",
      "trainer/Q2 Predictions Min                              -3.14994\r\n",
      "trainer/Q Targets Mean                                  -1.03407\r\n",
      "trainer/Q Targets Std                                    0.868148\r\n",
      "trainer/Q Targets Max                                    1.12238\r\n",
      "trainer/Q Targets Min                                   -3.15873\r\n",
      "trainer/Log Pis Mean                                     2.00652\r\n",
      "trainer/Log Pis Std                                      1.33477\r\n",
      "trainer/Log Pis Max                                      4.30143\r\n",
      "trainer/Log Pis Min                                     -3.96297\r\n",
      "trainer/Policy mu Mean                                   0.00192278\r\n",
      "trainer/Policy mu Std                                    0.244312\r\n",
      "trainer/Policy mu Max                                    1.40005\r\n",
      "trainer/Policy mu Min                                   -1.86538\r\n",
      "trainer/Policy log std Mean                             -2.3422\r\n",
      "trainer/Policy log std Std                               0.574239\r\n",
      "trainer/Policy log std Max                              -0.494797\r\n",
      "trainer/Policy log std Min                              -3.3438\r\n",
      "trainer/Alpha                                            0.0246097\r\n",
      "trainer/Alpha Loss                                       0.0241708\r\n",
      "exploration/num steps total                           7000\r\n",
      "exploration/num paths total                            350\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.106712\r\n",
      "exploration/Rewards Std                                  0.063485\r\n",
      "exploration/Rewards Max                                  0.0208196\r\n",
      "exploration/Rewards Min                                 -0.299185\r\n",
      "exploration/Returns Mean                                -2.13424\r\n",
      "exploration/Returns Std                                  0.724292\r\n",
      "exploration/Returns Max                                 -0.834191\r\n",
      "exploration/Returns Min                                 -2.89898\r\n",
      "exploration/Actions Mean                                -0.0113687\r\n",
      "exploration/Actions Std                                  0.0964343\r\n",
      "exploration/Actions Max                                  0.491671\r\n",
      "exploration/Actions Min                                 -0.342736\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.13424\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0055943\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0111753\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0279448\r\n",
      "exploration/env_infos/final/reward_dist Min              4.81377e-18\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0115378\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0144357\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0387283\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.96002e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.178113\r\n",
      "exploration/env_infos/reward_dist Std                    0.261905\r\n",
      "exploration/env_infos/reward_dist Max                    0.923988\r\n",
      "exploration/env_infos/reward_dist Min                    4.81377e-18\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.200212\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0742213\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0821549\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.285096\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.166925\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.109903\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0626359\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.344821\r\n",
      "exploration/env_infos/reward_energy Mean                -0.112005\r\n",
      "exploration/env_infos/reward_energy Std                  0.0794508\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00264672\r\n",
      "exploration/env_infos/reward_energy Min                 -0.496845\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0859518\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.330942\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.648032\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.40822\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00121525\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00696071\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0113109\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.012883\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0270654\r\n",
      "exploration/env_infos/end_effector_loc Std               0.199471\r\n",
      "exploration/env_infos/end_effector_loc Max               0.648032\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.40822\r\n",
      "evaluation/num steps total                           60000\r\n",
      "evaluation/num paths total                            3000\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0538115\r\n",
      "evaluation/Rewards Std                                   0.0700771\r\n",
      "evaluation/Rewards Max                                   0.152198\r\n",
      "evaluation/Rewards Min                                  -0.327423\r\n",
      "evaluation/Returns Mean                                 -1.07623\r\n",
      "evaluation/Returns Std                                   1.0822\r\n",
      "evaluation/Returns Max                                   1.15248\r\n",
      "evaluation/Returns Min                                  -3.57458\r\n",
      "evaluation/Actions Mean                                  0.00159971\r\n",
      "evaluation/Actions Std                                   0.0588618\r\n",
      "evaluation/Actions Max                                   0.579086\r\n",
      "evaluation/Actions Min                                  -0.658098\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.07623\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.163761\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.240206\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.943602\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.57219e-28\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00632625\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0118294\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0648479\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.21272e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.176089\r\n",
      "evaluation/env_infos/reward_dist Std                     0.237339\r\n",
      "evaluation/env_infos/reward_dist Max                     0.991186\r\n",
      "evaluation/env_infos/reward_dist Min                     1.57219e-28\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0439887\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0433368\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00175422\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.231913\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.165517\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.168008\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0144981\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.725906\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.05179\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0652099\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000944197\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.725906\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0350867\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.228897\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.767799\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.50932\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00153801\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00819529\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0289543\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0329049\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0202614\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.146125\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.767799\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.50932\r\n",
      "time/data storing (s)                                    0.00286541\r\n",
      "time/evaluation sampling (s)                             0.946543\r\n",
      "time/exploration sampling (s)                            0.125678\r\n",
      "time/logging (s)                                         0.0190117\r\n",
      "time/saving (s)                                          0.027809\r\n",
      "time/training (s)                                       49.544\r\n",
      "time/epoch (s)                                          50.666\r\n",
      "time/total (s)                                        2914.05\r\n",
      "Epoch                                                   59\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:14:22.186877 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 60 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000758772\r\n",
      "trainer/QF2 Loss                                         0.00123063\r\n",
      "trainer/Policy Loss                                      3.00413\r\n",
      "trainer/Q1 Predictions Mean                             -1.0597\r\n",
      "trainer/Q1 Predictions Std                               0.871687\r\n",
      "trainer/Q1 Predictions Max                               0.854861\r\n",
      "trainer/Q1 Predictions Min                              -3.29896\r\n",
      "trainer/Q2 Predictions Mean                             -1.0733\r\n",
      "trainer/Q2 Predictions Std                               0.871997\r\n",
      "trainer/Q2 Predictions Max                               0.820439\r\n",
      "trainer/Q2 Predictions Min                              -3.35344\r\n",
      "trainer/Q Targets Mean                                  -1.05068\r\n",
      "trainer/Q Targets Std                                    0.869957\r\n",
      "trainer/Q Targets Max                                    0.835995\r\n",
      "trainer/Q Targets Min                                   -3.41339\r\n",
      "trainer/Log Pis Mean                                     1.9389\r\n",
      "trainer/Log Pis Std                                      1.3448\r\n",
      "trainer/Log Pis Max                                      4.24585\r\n",
      "trainer/Log Pis Min                                     -2.92971\r\n",
      "trainer/Policy mu Mean                                  -0.00811573\r\n",
      "trainer/Policy mu Std                                    0.266706\r\n",
      "trainer/Policy mu Max                                    1.74583\r\n",
      "trainer/Policy mu Min                                   -1.85107\r\n",
      "trainer/Policy log std Mean                             -2.27666\r\n",
      "trainer/Policy log std Std                               0.559586\r\n",
      "trainer/Policy log std Max                              -0.270967\r\n",
      "trainer/Policy log std Min                              -3.30598\r\n",
      "trainer/Alpha                                            0.0239073\r\n",
      "trainer/Alpha Loss                                      -0.22809\r\n",
      "exploration/num steps total                           7100\r\n",
      "exploration/num paths total                            355\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0826143\r\n",
      "exploration/Rewards Std                                  0.0992189\r\n",
      "exploration/Rewards Max                                  0.112004\r\n",
      "exploration/Rewards Min                                 -0.444243\r\n",
      "exploration/Returns Mean                                -1.65229\r\n",
      "exploration/Returns Std                                  1.37535\r\n",
      "exploration/Returns Max                                  0.639751\r\n",
      "exploration/Returns Min                                 -3.38329\r\n",
      "exploration/Actions Mean                                 0.0265292\r\n",
      "exploration/Actions Std                                  0.20738\r\n",
      "exploration/Actions Max                                  0.839389\r\n",
      "exploration/Actions Min                                 -0.615247\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.65229\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.11065\r\n",
      "exploration/env_infos/final/reward_dist Std              0.141825\r\n",
      "exploration/env_infos/final/reward_dist Max              0.342756\r\n",
      "exploration/env_infos/final/reward_dist Min              3.12357e-39\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00251599\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00384338\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00994946\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.30252e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.142978\r\n",
      "exploration/env_infos/reward_dist Std                    0.25517\r\n",
      "exploration/env_infos/reward_dist Max                    0.944986\r\n",
      "exploration/env_infos/reward_dist Min                    3.12357e-39\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.344343\r\n",
      "exploration/env_infos/final/reward_energy Std            0.180763\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0719853\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.570626\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.363396\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.266043\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.111392\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.860793\r\n",
      "exploration/env_infos/reward_energy Mean                -0.233865\r\n",
      "exploration/env_infos/reward_energy Std                  0.180909\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0200967\r\n",
      "exploration/env_infos/reward_energy Min                 -0.860793\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.204942\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.341885\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.746935\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.158708\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00604595\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0147306\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0419695\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0198725\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.101134\r\n",
      "exploration/env_infos/end_effector_loc Std               0.215568\r\n",
      "exploration/env_infos/end_effector_loc Max               0.746935\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.239929\r\n",
      "evaluation/num steps total                           61000\r\n",
      "evaluation/num paths total                            3050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.065043\r\n",
      "evaluation/Rewards Std                                   0.0998449\r\n",
      "evaluation/Rewards Max                                   0.145244\r\n",
      "evaluation/Rewards Min                                  -0.958442\r\n",
      "evaluation/Returns Mean                                 -1.30086\r\n",
      "evaluation/Returns Std                                   1.5564\r\n",
      "evaluation/Returns Max                                   1.77085\r\n",
      "evaluation/Returns Min                                  -7.72202\r\n",
      "evaluation/Actions Mean                                 -0.00232126\r\n",
      "evaluation/Actions Std                                   0.0773594\r\n",
      "evaluation/Actions Max                                   0.392522\r\n",
      "evaluation/Actions Min                                  -0.836239\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.30086\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.110964\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.184167\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.801857\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.65837e-80\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00695834\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0153477\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0891525\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.01567e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.142707\r\n",
      "evaluation/env_infos/reward_dist Std                     0.228521\r\n",
      "evaluation/env_infos/reward_dist Max                     0.991639\r\n",
      "evaluation/env_infos/reward_dist Min                     2.65837e-80\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0532923\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0682248\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00497583\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.405238\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.218493\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.239794\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00649765\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.929943\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0625255\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0898349\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000964328\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.929943\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0824807\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.296863\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.513556\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00470444\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0104603\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0196261\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.041812\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0583683\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.188871\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.513556\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00302425\r\n",
      "time/evaluation sampling (s)                             1.04908\r\n",
      "time/exploration sampling (s)                            0.121913\r\n",
      "time/logging (s)                                         0.0188005\r\n",
      "time/saving (s)                                          0.0266367\r\n",
      "time/training (s)                                       49.5222\r\n",
      "time/epoch (s)                                          50.7417\r\n",
      "time/total (s)                                        2965.64\r\n",
      "Epoch                                                   60\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:15:14.076004 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 61 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0012214\n",
      "trainer/QF2 Loss                                         0.000619664\n",
      "trainer/Policy Loss                                      3.28278\n",
      "trainer/Q1 Predictions Mean                             -1.16082\n",
      "trainer/Q1 Predictions Std                               0.840847\n",
      "trainer/Q1 Predictions Max                               1.28081\n",
      "trainer/Q1 Predictions Min                              -3.25172\n",
      "trainer/Q2 Predictions Mean                             -1.16785\n",
      "trainer/Q2 Predictions Std                               0.84773\n",
      "trainer/Q2 Predictions Max                               1.24534\n",
      "trainer/Q2 Predictions Min                              -3.2589\n",
      "trainer/Q Targets Mean                                  -1.16183\n",
      "trainer/Q Targets Std                                    0.843013\n",
      "trainer/Q Targets Max                                    1.23618\n",
      "trainer/Q Targets Min                                   -3.24207\n",
      "trainer/Log Pis Mean                                     2.11582\n",
      "trainer/Log Pis Std                                      1.28537\n",
      "trainer/Log Pis Max                                      4.27045\n",
      "trainer/Log Pis Min                                     -2.6927\n",
      "trainer/Policy mu Mean                                  -0.0272072\n",
      "trainer/Policy mu Std                                    0.291138\n",
      "trainer/Policy mu Max                                    1.64686\n",
      "trainer/Policy mu Min                                   -2.60137\n",
      "trainer/Policy log std Mean                             -2.3803\n",
      "trainer/Policy log std Std                               0.554214\n",
      "trainer/Policy log std Max                               0.476095\n",
      "trainer/Policy log std Min                              -3.29479\n",
      "trainer/Alpha                                            0.023034\n",
      "trainer/Alpha Loss                                       0.436729\n",
      "exploration/num steps total                           7200\n",
      "exploration/num paths total                            360\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0865088\n",
      "exploration/Rewards Std                                  0.0776299\n",
      "exploration/Rewards Max                                  0.0716411\n",
      "exploration/Rewards Min                                 -0.337966\n",
      "exploration/Returns Mean                                -1.73018\n",
      "exploration/Returns Std                                  0.946684\n",
      "exploration/Returns Max                                 -0.550522\n",
      "exploration/Returns Min                                 -3.23919\n",
      "exploration/Actions Mean                                -6.5368e-05\n",
      "exploration/Actions Std                                  0.167597\n",
      "exploration/Actions Max                                  0.645976\n",
      "exploration/Actions Min                                 -0.746401\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.73018\n",
      "exploration/env_infos/final/reward_dist Mean             0.1487\n",
      "exploration/env_infos/final/reward_dist Std              0.292279\n",
      "exploration/env_infos/final/reward_dist Max              0.733225\n",
      "exploration/env_infos/final/reward_dist Min              5.06713e-13\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0100371\n",
      "exploration/env_infos/initial/reward_dist Std            0.0146866\n",
      "exploration/env_infos/initial/reward_dist Max            0.0381545\n",
      "exploration/env_infos/initial/reward_dist Min            8.16505e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.170701\n",
      "exploration/env_infos/reward_dist Std                    0.276374\n",
      "exploration/env_infos/reward_dist Max                    0.945495\n",
      "exploration/env_infos/reward_dist Min                    2.81892e-13\n",
      "exploration/env_infos/final/reward_energy Mean          -0.224791\n",
      "exploration/env_infos/final/reward_energy Std            0.115314\n",
      "exploration/env_infos/final/reward_energy Max           -0.0611035\n",
      "exploration/env_infos/final/reward_energy Min           -0.384905\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.366105\n",
      "exploration/env_infos/initial/reward_energy Std          0.286352\n",
      "exploration/env_infos/initial/reward_energy Max         -0.124378\n",
      "exploration/env_infos/initial/reward_energy Min         -0.923587\n",
      "exploration/env_infos/reward_energy Mean                -0.186817\n",
      "exploration/env_infos/reward_energy Std                  0.145867\n",
      "exploration/env_infos/reward_energy Max                 -0.0280366\n",
      "exploration/env_infos/reward_energy Min                 -0.923587\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0293502\n",
      "exploration/env_infos/final/end_effector_loc Std         0.235665\n",
      "exploration/env_infos/final/end_effector_loc Max         0.306177\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.50276\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00500199\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0156531\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0109284\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0373201\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0286744\n",
      "exploration/env_infos/end_effector_loc Std               0.166731\n",
      "exploration/env_infos/end_effector_loc Max               0.306177\n",
      "exploration/env_infos/end_effector_loc Min              -0.505157\n",
      "evaluation/num steps total                           62000\n",
      "evaluation/num paths total                            3100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0511719\n",
      "evaluation/Rewards Std                                   0.0959191\n",
      "evaluation/Rewards Max                                   0.174871\n",
      "evaluation/Rewards Min                                  -0.790145\n",
      "evaluation/Returns Mean                                 -1.02344\n",
      "evaluation/Returns Std                                   1.43992\n",
      "evaluation/Returns Max                                   2.24638\n",
      "evaluation/Returns Min                                  -7.16074\n",
      "evaluation/Actions Mean                                 -0.00581291\n",
      "evaluation/Actions Std                                   0.0783518\n",
      "evaluation/Actions Max                                   0.528124\n",
      "evaluation/Actions Min                                  -0.880301\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.02344\n",
      "evaluation/env_infos/final/reward_dist Mean              0.178361\n",
      "evaluation/env_infos/final/reward_dist Std               0.269663\n",
      "evaluation/env_infos/final/reward_dist Max               0.910572\n",
      "evaluation/env_infos/final/reward_dist Min               3.23083e-63\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00778542\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0121605\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0510564\n",
      "evaluation/env_infos/initial/reward_dist Min             1.71364e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.207906\n",
      "evaluation/env_infos/reward_dist Std                     0.279442\n",
      "evaluation/env_infos/reward_dist Max                     0.992906\n",
      "evaluation/env_infos/reward_dist Min                     3.23083e-63\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0563884\n",
      "evaluation/env_infos/final/reward_energy Std             0.0577781\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00594085\n",
      "evaluation/env_infos/final/reward_energy Min            -0.339755\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.236974\n",
      "evaluation/env_infos/initial/reward_energy Std           0.22122\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00544037\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.914412\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0706185\n",
      "evaluation/env_infos/reward_energy Std                   0.0857824\n",
      "evaluation/env_infos/reward_energy Max                  -0.00037715\n",
      "evaluation/env_infos/reward_energy Min                  -0.914412\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0697611\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.250344\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.373101\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00319469\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0110074\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0264062\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0440151\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0375194\n",
      "evaluation/env_infos/end_effector_loc Std                0.170166\n",
      "evaluation/env_infos/end_effector_loc Max                0.373101\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00311045\n",
      "time/evaluation sampling (s)                             0.973542\n",
      "time/exploration sampling (s)                            0.133126\n",
      "time/logging (s)                                         0.019252\n",
      "time/saving (s)                                          0.0274365\n",
      "time/training (s)                                       49.9696\n",
      "time/epoch (s)                                          51.1261\n",
      "time/total (s)                                        3017.52\n",
      "Epoch                                                   61\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:16:08.738929 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 62 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000634692\n",
      "trainer/QF2 Loss                                         0.000627221\n",
      "trainer/Policy Loss                                      2.91604\n",
      "trainer/Q1 Predictions Mean                             -0.997989\n",
      "trainer/Q1 Predictions Std                               0.871205\n",
      "trainer/Q1 Predictions Max                               1.2956\n",
      "trainer/Q1 Predictions Min                              -3.42981\n",
      "trainer/Q2 Predictions Mean                             -0.998782\n",
      "trainer/Q2 Predictions Std                               0.872183\n",
      "trainer/Q2 Predictions Max                               1.29475\n",
      "trainer/Q2 Predictions Min                              -3.37904\n",
      "trainer/Q Targets Mean                                  -0.996887\n",
      "trainer/Q Targets Std                                    0.871294\n",
      "trainer/Q Targets Max                                    1.25639\n",
      "trainer/Q Targets Min                                   -3.41908\n",
      "trainer/Log Pis Mean                                     1.91374\n",
      "trainer/Log Pis Std                                      1.28089\n",
      "trainer/Log Pis Max                                      5.30689\n",
      "trainer/Log Pis Min                                     -2.26128\n",
      "trainer/Policy mu Mean                                  -0.00171755\n",
      "trainer/Policy mu Std                                    0.300823\n",
      "trainer/Policy mu Max                                    2.14798\n",
      "trainer/Policy mu Min                                   -2.33292\n",
      "trainer/Policy log std Mean                             -2.32926\n",
      "trainer/Policy log std Std                               0.536209\n",
      "trainer/Policy log std Max                              -0.296443\n",
      "trainer/Policy log std Min                              -3.27395\n",
      "trainer/Alpha                                            0.0224037\n",
      "trainer/Alpha Loss                                      -0.327583\n",
      "exploration/num steps total                           7300\n",
      "exploration/num paths total                            365\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.103345\n",
      "exploration/Rewards Std                                  0.126713\n",
      "exploration/Rewards Max                                  0.116165\n",
      "exploration/Rewards Min                                 -0.393746\n",
      "exploration/Returns Mean                                -2.06691\n",
      "exploration/Returns Std                                  2.15482\n",
      "exploration/Returns Max                                  0.494875\n",
      "exploration/Returns Min                                 -5.61846\n",
      "exploration/Actions Mean                                -0.0226414\n",
      "exploration/Actions Std                                  0.154638\n",
      "exploration/Actions Max                                  0.511255\n",
      "exploration/Actions Min                                 -0.784482\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.06691\n",
      "exploration/env_infos/final/reward_dist Mean             0.0740026\n",
      "exploration/env_infos/final/reward_dist Std              0.139397\n",
      "exploration/env_infos/final/reward_dist Max              0.352465\n",
      "exploration/env_infos/final/reward_dist Min              2.8596e-56\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00767819\n",
      "exploration/env_infos/initial/reward_dist Std            0.0113012\n",
      "exploration/env_infos/initial/reward_dist Max            0.0300937\n",
      "exploration/env_infos/initial/reward_dist Min            5.81273e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.204585\n",
      "exploration/env_infos/reward_dist Std                    0.326299\n",
      "exploration/env_infos/reward_dist Max                    0.995997\n",
      "exploration/env_infos/reward_dist Min                    1.41095e-57\n",
      "exploration/env_infos/final/reward_energy Mean          -0.236766\n",
      "exploration/env_infos/final/reward_energy Std            0.0976469\n",
      "exploration/env_infos/final/reward_energy Max           -0.129846\n",
      "exploration/env_infos/final/reward_energy Min           -0.386049\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.361647\n",
      "exploration/env_infos/initial/reward_energy Std          0.286251\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0770294\n",
      "exploration/env_infos/initial/reward_energy Min         -0.787053\n",
      "exploration/env_infos/reward_energy Mean                -0.174346\n",
      "exploration/env_infos/reward_energy Std                  0.135848\n",
      "exploration/env_infos/reward_energy Max                 -0.00539338\n",
      "exploration/env_infos/reward_energy Min                 -0.787053\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.283214\n",
      "exploration/env_infos/final/end_effector_loc Std         0.415396\n",
      "exploration/env_infos/final/end_effector_loc Max         0.202268\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00581344\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0152353\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00858226\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0392241\n",
      "exploration/env_infos/end_effector_loc Mean             -0.16644\n",
      "exploration/env_infos/end_effector_loc Std               0.324875\n",
      "exploration/env_infos/end_effector_loc Max               0.202268\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           63000\n",
      "evaluation/num paths total                            3150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0621365\n",
      "evaluation/Rewards Std                                   0.0926426\n",
      "evaluation/Rewards Max                                   0.161391\n",
      "evaluation/Rewards Min                                  -0.826141\n",
      "evaluation/Returns Mean                                 -1.24273\n",
      "evaluation/Returns Std                                   1.37406\n",
      "evaluation/Returns Max                                   1.89437\n",
      "evaluation/Returns Min                                  -7.17047\n",
      "evaluation/Actions Mean                                 -0.00401785\n",
      "evaluation/Actions Std                                   0.065011\n",
      "evaluation/Actions Max                                   0.409663\n",
      "evaluation/Actions Min                                  -0.661217\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.24273\n",
      "evaluation/env_infos/final/reward_dist Mean              0.210435\n",
      "evaluation/env_infos/final/reward_dist Std               0.284031\n",
      "evaluation/env_infos/final/reward_dist Max               0.970319\n",
      "evaluation/env_infos/final/reward_dist Min               2.15102e-70\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00690663\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111925\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0350946\n",
      "evaluation/env_infos/initial/reward_dist Min             1.24206e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.188523\n",
      "evaluation/env_infos/reward_dist Std                     0.266471\n",
      "evaluation/env_infos/reward_dist Max                     0.996731\n",
      "evaluation/env_infos/reward_dist Min                     2.15102e-70\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0532387\n",
      "evaluation/env_infos/final/reward_energy Std             0.0676217\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00431878\n",
      "evaluation/env_infos/final/reward_energy Min            -0.428684\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.182515\n",
      "evaluation/env_infos/initial/reward_energy Std           0.172348\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0174584\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.751374\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0571062\n",
      "evaluation/env_infos/reward_energy Std                   0.0722774\n",
      "evaluation/env_infos/reward_energy Max                  -0.00110665\n",
      "evaluation/env_infos/reward_energy Min                  -0.751374\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0597645\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.259055\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.638273\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00161082\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00872781\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0204832\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0330609\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0315314\n",
      "evaluation/env_infos/end_effector_loc Std                0.167403\n",
      "evaluation/env_infos/end_effector_loc Max                0.638273\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00299993\n",
      "time/evaluation sampling (s)                             1.06472\n",
      "time/exploration sampling (s)                            0.152037\n",
      "time/logging (s)                                         0.0277487\n",
      "time/saving (s)                                          0.0280732\n",
      "time/training (s)                                       52.5623\n",
      "time/epoch (s)                                          53.8379\n",
      "time/total (s)                                        3072.19\n",
      "Epoch                                                   62\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:17:04.384801 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 63 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00106572\n",
      "trainer/QF2 Loss                                         0.000923544\n",
      "trainer/Policy Loss                                      2.98495\n",
      "trainer/Q1 Predictions Mean                             -0.98084\n",
      "trainer/Q1 Predictions Std                               0.863898\n",
      "trainer/Q1 Predictions Max                               1.26089\n",
      "trainer/Q1 Predictions Min                              -3.51502\n",
      "trainer/Q2 Predictions Mean                             -0.986053\n",
      "trainer/Q2 Predictions Std                               0.864993\n",
      "trainer/Q2 Predictions Max                               1.25423\n",
      "trainer/Q2 Predictions Min                              -3.55425\n",
      "trainer/Q Targets Mean                                  -0.982552\n",
      "trainer/Q Targets Std                                    0.862493\n",
      "trainer/Q Targets Max                                    1.26187\n",
      "trainer/Q Targets Min                                   -3.50164\n",
      "trainer/Log Pis Mean                                     1.99904\n",
      "trainer/Log Pis Std                                      1.48848\n",
      "trainer/Log Pis Max                                      4.51535\n",
      "trainer/Log Pis Min                                     -4.5159\n",
      "trainer/Policy mu Mean                                  -0.0435677\n",
      "trainer/Policy mu Std                                    0.300672\n",
      "trainer/Policy mu Max                                    1.33234\n",
      "trainer/Policy mu Min                                   -1.96731\n",
      "trainer/Policy log std Mean                             -2.35595\n",
      "trainer/Policy log std Std                               0.644975\n",
      "trainer/Policy log std Max                              -0.0728238\n",
      "trainer/Policy log std Min                              -3.29243\n",
      "trainer/Alpha                                            0.0219921\n",
      "trainer/Alpha Loss                                      -0.00367916\n",
      "exploration/num steps total                           7400\n",
      "exploration/num paths total                            370\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0892055\n",
      "exploration/Rewards Std                                  0.0449565\n",
      "exploration/Rewards Max                                  0.00168235\n",
      "exploration/Rewards Min                                 -0.211877\n",
      "exploration/Returns Mean                                -1.78411\n",
      "exploration/Returns Std                                  0.243676\n",
      "exploration/Returns Max                                 -1.55699\n",
      "exploration/Returns Min                                 -2.14896\n",
      "exploration/Actions Mean                                 0.00524486\n",
      "exploration/Actions Std                                  0.0660066\n",
      "exploration/Actions Max                                  0.175894\n",
      "exploration/Actions Min                                 -0.244261\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.78411\n",
      "exploration/env_infos/final/reward_dist Mean             0.117463\n",
      "exploration/env_infos/final/reward_dist Std              0.177927\n",
      "exploration/env_infos/final/reward_dist Max              0.470724\n",
      "exploration/env_infos/final/reward_dist Min              0.00337346\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00246474\n",
      "exploration/env_infos/initial/reward_dist Std            0.00344925\n",
      "exploration/env_infos/initial/reward_dist Max            0.00929892\n",
      "exploration/env_infos/initial/reward_dist Min            0.000224256\n",
      "exploration/env_infos/reward_dist Mean                   0.174359\n",
      "exploration/env_infos/reward_dist Std                    0.280385\n",
      "exploration/env_infos/reward_dist Max                    0.967187\n",
      "exploration/env_infos/reward_dist Min                    0.000224256\n",
      "exploration/env_infos/final/reward_energy Mean          -0.119796\n",
      "exploration/env_infos/final/reward_energy Std            0.0752\n",
      "exploration/env_infos/final/reward_energy Max           -0.0389277\n",
      "exploration/env_infos/final/reward_energy Min           -0.251704\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.120366\n",
      "exploration/env_infos/initial/reward_energy Std          0.0347554\n",
      "exploration/env_infos/initial/reward_energy Max         -0.086151\n",
      "exploration/env_infos/initial/reward_energy Min         -0.172405\n",
      "exploration/env_infos/reward_energy Mean                -0.0816451\n",
      "exploration/env_infos/reward_energy Std                  0.0458567\n",
      "exploration/env_infos/reward_energy Max                 -0.00361415\n",
      "exploration/env_infos/reward_energy Min                 -0.251704\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0960533\n",
      "exploration/env_infos/final/end_effector_loc Std         0.17954\n",
      "exploration/env_infos/final/end_effector_loc Max         0.310679\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.17368\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00115417\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00427643\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00791804\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00523494\n",
      "exploration/env_infos/end_effector_loc Mean              0.0432469\n",
      "exploration/env_infos/end_effector_loc Std               0.100648\n",
      "exploration/env_infos/end_effector_loc Max               0.310679\n",
      "exploration/env_infos/end_effector_loc Min              -0.17368\n",
      "evaluation/num steps total                           64000\n",
      "evaluation/num paths total                            3200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0729532\n",
      "evaluation/Rewards Std                                   0.119224\n",
      "evaluation/Rewards Max                                   0.132267\n",
      "evaluation/Rewards Min                                  -0.998626\n",
      "evaluation/Returns Mean                                 -1.45906\n",
      "evaluation/Returns Std                                   1.94882\n",
      "evaluation/Returns Max                                   1.05038\n",
      "evaluation/Returns Min                                 -12.8342\n",
      "evaluation/Actions Mean                                 -0.00645751\n",
      "evaluation/Actions Std                                   0.0909429\n",
      "evaluation/Actions Max                                   0.731594\n",
      "evaluation/Actions Min                                  -0.938262\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.45906\n",
      "evaluation/env_infos/final/reward_dist Mean              0.053989\n",
      "evaluation/env_infos/final/reward_dist Std               0.108936\n",
      "evaluation/env_infos/final/reward_dist Max               0.59619\n",
      "evaluation/env_infos/final/reward_dist Min               2.61209e-82\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00641776\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116615\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0504439\n",
      "evaluation/env_infos/initial/reward_dist Min             1.43281e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.128383\n",
      "evaluation/env_infos/reward_dist Std                     0.224211\n",
      "evaluation/env_infos/reward_dist Max                     0.993204\n",
      "evaluation/env_infos/reward_dist Min                     2.99727e-111\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0520669\n",
      "evaluation/env_infos/final/reward_energy Std             0.0679275\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00124606\n",
      "evaluation/env_infos/final/reward_energy Min            -0.290667\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.245273\n",
      "evaluation/env_infos/initial/reward_energy Std           0.260467\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00425869\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.956161\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0671221\n",
      "evaluation/env_infos/reward_energy Std                   0.110087\n",
      "evaluation/env_infos/reward_energy Max                  -0.000193892\n",
      "evaluation/env_infos/reward_energy Min                  -0.956161\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0874433\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.307375\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.597116\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00122363\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0125899\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0365797\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0469131\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0404215\n",
      "evaluation/env_infos/end_effector_loc Std                0.205834\n",
      "evaluation/env_infos/end_effector_loc Max                0.597116\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00310708\n",
      "time/evaluation sampling (s)                             1.0403\n",
      "time/exploration sampling (s)                            0.127455\n",
      "time/logging (s)                                         0.0203977\n",
      "time/saving (s)                                          0.0346161\n",
      "time/training (s)                                       53.4961\n",
      "time/epoch (s)                                          54.7219\n",
      "time/total (s)                                        3127.83\n",
      "Epoch                                                   63\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:18:00.175788 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 64 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00146345\r\n",
      "trainer/QF2 Loss                                         0.000959836\r\n",
      "trainer/Policy Loss                                      2.95088\r\n",
      "trainer/Q1 Predictions Mean                             -0.985467\r\n",
      "trainer/Q1 Predictions Std                               0.837433\r\n",
      "trainer/Q1 Predictions Max                               1.41956\r\n",
      "trainer/Q1 Predictions Min                              -2.83056\r\n",
      "trainer/Q2 Predictions Mean                             -0.980667\r\n",
      "trainer/Q2 Predictions Std                               0.840654\r\n",
      "trainer/Q2 Predictions Max                               1.40363\r\n",
      "trainer/Q2 Predictions Min                              -2.85136\r\n",
      "trainer/Q Targets Mean                                  -0.983918\r\n",
      "trainer/Q Targets Std                                    0.83444\r\n",
      "trainer/Q Targets Max                                    1.39927\r\n",
      "trainer/Q Targets Min                                   -2.84695\r\n",
      "trainer/Log Pis Mean                                     1.97365\r\n",
      "trainer/Log Pis Std                                      1.38198\r\n",
      "trainer/Log Pis Max                                      4.30566\r\n",
      "trainer/Log Pis Min                                     -3.60185\r\n",
      "trainer/Policy mu Mean                                  -0.0430279\r\n",
      "trainer/Policy mu Std                                    0.312673\r\n",
      "trainer/Policy mu Max                                    1.56305\r\n",
      "trainer/Policy mu Min                                   -1.5937\r\n",
      "trainer/Policy log std Mean                             -2.29679\r\n",
      "trainer/Policy log std Std                               0.629537\r\n",
      "trainer/Policy log std Max                              -0.351987\r\n",
      "trainer/Policy log std Min                              -3.29245\r\n",
      "trainer/Alpha                                            0.0225247\r\n",
      "trainer/Alpha Loss                                      -0.0999372\r\n",
      "exploration/num steps total                           7500\r\n",
      "exploration/num paths total                            375\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0918094\r\n",
      "exploration/Rewards Std                                  0.0751304\r\n",
      "exploration/Rewards Max                                  0.0656775\r\n",
      "exploration/Rewards Min                                 -0.305532\r\n",
      "exploration/Returns Mean                                -1.83619\r\n",
      "exploration/Returns Std                                  1.16984\r\n",
      "exploration/Returns Max                                 -0.964449\r\n",
      "exploration/Returns Min                                 -3.98421\r\n",
      "exploration/Actions Mean                                -0.00280096\r\n",
      "exploration/Actions Std                                  0.156518\r\n",
      "exploration/Actions Max                                  0.701314\r\n",
      "exploration/Actions Min                                 -0.713328\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.83619\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.356948\r\n",
      "exploration/env_infos/final/reward_dist Std              0.381137\r\n",
      "exploration/env_infos/final/reward_dist Max              0.904839\r\n",
      "exploration/env_infos/final/reward_dist Min              3.28296e-08\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00833009\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0118972\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0306929\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.95232e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.238959\r\n",
      "exploration/env_infos/reward_dist Std                    0.324704\r\n",
      "exploration/env_infos/reward_dist Max                    0.986571\r\n",
      "exploration/env_infos/reward_dist Min                    3.28296e-08\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.110257\r\n",
      "exploration/env_infos/final/reward_energy Std            0.134651\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0056416\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.375983\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.181091\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.161119\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0251869\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.48828\r\n",
      "exploration/env_infos/reward_energy Mean                -0.158781\r\n",
      "exploration/env_infos/reward_energy Std                  0.154273\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0056416\r\n",
      "exploration/env_infos/reward_energy Min                 -0.831194\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0140176\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.24606\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.342668\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.358879\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00124133\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00847943\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0234854\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00666941\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0118163\r\n",
      "exploration/env_infos/end_effector_loc Std               0.163333\r\n",
      "exploration/env_infos/end_effector_loc Max               0.342668\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.358879\r\n",
      "evaluation/num steps total                           65000\r\n",
      "evaluation/num paths total                            3250\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0592212\r\n",
      "evaluation/Rewards Std                                   0.0819531\r\n",
      "evaluation/Rewards Max                                   0.144569\r\n",
      "evaluation/Rewards Min                                  -0.575305\r\n",
      "evaluation/Returns Mean                                 -1.18442\r\n",
      "evaluation/Returns Std                                   1.29447\r\n",
      "evaluation/Returns Max                                   1.40656\r\n",
      "evaluation/Returns Min                                  -5.3822\r\n",
      "evaluation/Actions Mean                                 -0.00310106\r\n",
      "evaluation/Actions Std                                   0.0782224\r\n",
      "evaluation/Actions Max                                   0.757156\r\n",
      "evaluation/Actions Min                                  -0.893266\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.18442\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.103723\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.215073\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.933461\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.08878e-32\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00727164\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00949375\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0374116\r\n",
      "evaluation/env_infos/initial/reward_dist Min             9.22651e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.163312\r\n",
      "evaluation/env_infos/reward_dist Std                     0.255998\r\n",
      "evaluation/env_infos/reward_dist Max                     0.990845\r\n",
      "evaluation/env_infos/reward_dist Min                     4.08878e-32\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0328447\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0488877\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00564629\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.326877\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.220762\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255839\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0124398\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.939393\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0606826\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0925978\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00117461\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.939393\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0927931\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.250779\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.417388\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.695747\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00256128\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116695\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0378578\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0446633\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.048938\n",
      "evaluation/env_infos/end_effector_loc Std                0.170599\n",
      "evaluation/env_infos/end_effector_loc Max                0.417388\n",
      "evaluation/env_infos/end_effector_loc Min               -0.695747\n",
      "time/data storing (s)                                    0.00313944\n",
      "time/evaluation sampling (s)                             1.32119\n",
      "time/exploration sampling (s)                            0.131314\n",
      "time/logging (s)                                         0.0194275\n",
      "time/saving (s)                                          0.0288478\n",
      "time/training (s)                                       53.336\n",
      "time/epoch (s)                                          54.8399\n",
      "time/total (s)                                        3183.62\n",
      "Epoch                                                   64\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 11:18:54.743043 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 65 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00111018\n",
      "trainer/QF2 Loss                                         0.0011456\n",
      "trainer/Policy Loss                                      2.99685\n",
      "trainer/Q1 Predictions Mean                             -1.11838\n",
      "trainer/Q1 Predictions Std                               0.872252\n",
      "trainer/Q1 Predictions Max                               1.03295\n",
      "trainer/Q1 Predictions Min                              -3.53844\n",
      "trainer/Q2 Predictions Mean                             -1.12378\n",
      "trainer/Q2 Predictions Std                               0.873145\n",
      "trainer/Q2 Predictions Max                               1.05597\n",
      "trainer/Q2 Predictions Min                              -3.54662\n",
      "trainer/Q Targets Mean                                  -1.12275\n",
      "trainer/Q Targets Std                                    0.870789\n",
      "trainer/Q Targets Max                                    0.995587\n",
      "trainer/Q Targets Min                                   -3.54026\n",
      "trainer/Log Pis Mean                                     1.87769\n",
      "trainer/Log Pis Std                                      1.41446\n",
      "trainer/Log Pis Max                                      4.43374\n",
      "trainer/Log Pis Min                                     -5.73508\n",
      "trainer/Policy mu Mean                                  -0.035743\n",
      "trainer/Policy mu Std                                    0.290049\n",
      "trainer/Policy mu Max                                    1.43821\n",
      "trainer/Policy mu Min                                   -1.74043\n",
      "trainer/Policy log std Mean                             -2.29613\n",
      "trainer/Policy log std Std                               0.624719\n",
      "trainer/Policy log std Max                              -0.317344\n",
      "trainer/Policy log std Min                              -3.29857\n",
      "trainer/Alpha                                            0.0226675\n",
      "trainer/Alpha Loss                                      -0.463112\n",
      "exploration/num steps total                           7600\n",
      "exploration/num paths total                            380\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0904829\n",
      "exploration/Rewards Std                                  0.0723519\n",
      "exploration/Rewards Max                                  0.0842216\n",
      "exploration/Rewards Min                                 -0.319509\n",
      "exploration/Returns Mean                                -1.80966\n",
      "exploration/Returns Std                                  0.637551\n",
      "exploration/Returns Max                                 -0.868496\n",
      "exploration/Returns Min                                 -2.53577\n",
      "exploration/Actions Mean                                 0.00380706\n",
      "exploration/Actions Std                                  0.162694\n",
      "exploration/Actions Max                                  0.435481\n",
      "exploration/Actions Min                                 -0.533255\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.80966\n",
      "exploration/env_infos/final/reward_dist Mean             0.0867236\n",
      "exploration/env_infos/final/reward_dist Std              0.101871\n",
      "exploration/env_infos/final/reward_dist Max              0.275649\n",
      "exploration/env_infos/final/reward_dist Min              1.191e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00586635\n",
      "exploration/env_infos/initial/reward_dist Std            0.00878111\n",
      "exploration/env_infos/initial/reward_dist Max            0.0227394\n",
      "exploration/env_infos/initial/reward_dist Min            2.90312e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.112475\n",
      "exploration/env_infos/reward_dist Std                    0.199378\n",
      "exploration/env_infos/reward_dist Max                    0.948732\n",
      "exploration/env_infos/reward_dist Min                    1.191e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.224514\n",
      "exploration/env_infos/final/reward_energy Std            0.126094\n",
      "exploration/env_infos/final/reward_energy Max           -0.0981367\n",
      "exploration/env_infos/final/reward_energy Min           -0.449233\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.274589\n",
      "exploration/env_infos/initial/reward_energy Std          0.0983146\n",
      "exploration/env_infos/initial/reward_energy Max         -0.121018\n",
      "exploration/env_infos/initial/reward_energy Min         -0.419779\n",
      "exploration/env_infos/reward_energy Mean                -0.192393\n",
      "exploration/env_infos/reward_energy Std                  0.126303\n",
      "exploration/env_infos/reward_energy Max                 -0.0189515\n",
      "exploration/env_infos/reward_energy Min                 -0.604316\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0716393\n",
      "exploration/env_infos/final/end_effector_loc Std         0.253295\n",
      "exploration/env_infos/final/end_effector_loc Max         0.554907\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.252142\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00347871\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00970721\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0157297\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0146996\n",
      "exploration/env_infos/end_effector_loc Mean              0.0356649\n",
      "exploration/env_infos/end_effector_loc Std               0.178066\n",
      "exploration/env_infos/end_effector_loc Max               0.554907\n",
      "exploration/env_infos/end_effector_loc Min              -0.276975\n",
      "evaluation/num steps total                           66000\n",
      "evaluation/num paths total                            3300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0672046\n",
      "evaluation/Rewards Std                                   0.0765211\n",
      "evaluation/Rewards Max                                   0.113239\n",
      "evaluation/Rewards Min                                  -0.437832\n",
      "evaluation/Returns Mean                                 -1.34409\n",
      "evaluation/Returns Std                                   1.22498\n",
      "evaluation/Returns Max                                   0.806703\n",
      "evaluation/Returns Min                                  -4.07027\n",
      "evaluation/Actions Mean                                 -0.00254544\n",
      "evaluation/Actions Std                                   0.0704033\n",
      "evaluation/Actions Max                                   0.386287\n",
      "evaluation/Actions Min                                  -0.822902\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.34409\n",
      "evaluation/env_infos/final/reward_dist Mean              0.108959\n",
      "evaluation/env_infos/final/reward_dist Std               0.226537\n",
      "evaluation/env_infos/final/reward_dist Max               0.983472\n",
      "evaluation/env_infos/final/reward_dist Min               2.0964e-42\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00516189\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00870364\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0499665\n",
      "evaluation/env_infos/initial/reward_dist Min             1.21681e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.144947\n",
      "evaluation/env_infos/reward_dist Std                     0.231538\n",
      "evaluation/env_infos/reward_dist Max                     0.989499\n",
      "evaluation/env_infos/reward_dist Min                     2.0964e-42\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0327534\n",
      "evaluation/env_infos/final/reward_energy Std             0.0287464\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00019799\n",
      "evaluation/env_infos/final/reward_energy Min            -0.17201\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.207138\n",
      "evaluation/env_infos/initial/reward_energy Std           0.237337\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0112435\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.904715\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0540245\n",
      "evaluation/env_infos/reward_energy Std                   0.0837111\n",
      "evaluation/env_infos/reward_energy Max                  -0.00019799\n",
      "evaluation/env_infos/reward_energy Min                  -0.904715\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0588115\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27292\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.548047\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.994406\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00244908\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0108649\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0193143\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0411451\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0307573\n",
      "evaluation/env_infos/end_effector_loc Std                0.174223\n",
      "evaluation/env_infos/end_effector_loc Max                0.548047\n",
      "evaluation/env_infos/end_effector_loc Min               -0.994406\n",
      "time/data storing (s)                                    0.00308966\n",
      "time/evaluation sampling (s)                             1.05659\n",
      "time/exploration sampling (s)                            0.119199\n",
      "time/logging (s)                                         0.0224172\n",
      "time/saving (s)                                          0.0277751\n",
      "time/training (s)                                       52.4682\n",
      "time/epoch (s)                                          53.6973\n",
      "time/total (s)                                        3238.18\n",
      "Epoch                                                   65\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:19:49.578002 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 66 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000759035\n",
      "trainer/QF2 Loss                                         0.000647573\n",
      "trainer/Policy Loss                                      3.02721\n",
      "trainer/Q1 Predictions Mean                             -1.01486\n",
      "trainer/Q1 Predictions Std                               0.809172\n",
      "trainer/Q1 Predictions Max                               1.45411\n",
      "trainer/Q1 Predictions Min                              -3.25609\n",
      "trainer/Q2 Predictions Mean                             -1.02544\n",
      "trainer/Q2 Predictions Std                               0.817793\n",
      "trainer/Q2 Predictions Max                               1.46488\n",
      "trainer/Q2 Predictions Min                              -3.26749\n",
      "trainer/Q Targets Mean                                  -1.02067\n",
      "trainer/Q Targets Std                                    0.816052\n",
      "trainer/Q Targets Max                                    1.46706\n",
      "trainer/Q Targets Min                                   -3.28621\n",
      "trainer/Log Pis Mean                                     2.01245\n",
      "trainer/Log Pis Std                                      1.41913\n",
      "trainer/Log Pis Max                                      4.26551\n",
      "trainer/Log Pis Min                                     -6.26971\n",
      "trainer/Policy mu Mean                                  -0.0241908\n",
      "trainer/Policy mu Std                                    0.356843\n",
      "trainer/Policy mu Max                                    2.2086\n",
      "trainer/Policy mu Min                                   -1.60176\n",
      "trainer/Policy log std Mean                             -2.28841\n",
      "trainer/Policy log std Std                               0.63886\n",
      "trainer/Policy log std Max                              -0.259482\n",
      "trainer/Policy log std Min                              -3.2367\n",
      "trainer/Alpha                                            0.0225177\n",
      "trainer/Alpha Loss                                       0.0472552\n",
      "exploration/num steps total                           7700\n",
      "exploration/num paths total                            385\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.108111\n",
      "exploration/Rewards Std                                  0.059479\n",
      "exploration/Rewards Max                                  0.00093035\n",
      "exploration/Rewards Min                                 -0.218068\n",
      "exploration/Returns Mean                                -2.16223\n",
      "exploration/Returns Std                                  0.948068\n",
      "exploration/Returns Max                                 -1.00734\n",
      "exploration/Returns Min                                 -3.39744\n",
      "exploration/Actions Mean                                -0.00118048\n",
      "exploration/Actions Std                                  0.0629602\n",
      "exploration/Actions Max                                  0.147206\n",
      "exploration/Actions Min                                 -0.239547\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.16223\n",
      "exploration/env_infos/final/reward_dist Mean             0.106805\n",
      "exploration/env_infos/final/reward_dist Std              0.117899\n",
      "exploration/env_infos/final/reward_dist Max              0.264615\n",
      "exploration/env_infos/final/reward_dist Min              0.00100071\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00371643\n",
      "exploration/env_infos/initial/reward_dist Std            0.00484532\n",
      "exploration/env_infos/initial/reward_dist Max            0.0120798\n",
      "exploration/env_infos/initial/reward_dist Min            4.24627e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0624837\n",
      "exploration/env_infos/reward_dist Std                    0.110854\n",
      "exploration/env_infos/reward_dist Max                    0.52481\n",
      "exploration/env_infos/reward_dist Min                    4.24627e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0790392\n",
      "exploration/env_infos/final/reward_energy Std            0.0658179\n",
      "exploration/env_infos/final/reward_energy Max           -0.0215403\n",
      "exploration/env_infos/final/reward_energy Min           -0.203739\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.114406\n",
      "exploration/env_infos/initial/reward_energy Std          0.0379014\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0653145\n",
      "exploration/env_infos/initial/reward_energy Min         -0.165814\n",
      "exploration/env_infos/reward_energy Mean                -0.0771334\n",
      "exploration/env_infos/reward_energy Std                  0.0445106\n",
      "exploration/env_infos/reward_energy Max                 -0.00861514\n",
      "exploration/env_infos/reward_energy Min                 -0.241468\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.011071\n",
      "exploration/env_infos/final/end_effector_loc Std         0.154221\n",
      "exploration/env_infos/final/end_effector_loc Max         0.360802\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.181624\n",
      "exploration/env_infos/initial/end_effector_loc Mean      5.5099e-05\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00426069\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0067763\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00660314\n",
      "exploration/env_infos/end_effector_loc Mean              0.00610171\n",
      "exploration/env_infos/end_effector_loc Std               0.0868714\n",
      "exploration/env_infos/end_effector_loc Max               0.360802\n",
      "exploration/env_infos/end_effector_loc Min              -0.181624\n",
      "evaluation/num steps total                           67000\n",
      "evaluation/num paths total                            3350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0560858\n",
      "evaluation/Rewards Std                                   0.0728026\n",
      "evaluation/Rewards Max                                   0.144882\n",
      "evaluation/Rewards Min                                  -0.451024\n",
      "evaluation/Returns Mean                                 -1.12172\n",
      "evaluation/Returns Std                                   1.1978\n",
      "evaluation/Returns Max                                   2.02571\n",
      "evaluation/Returns Min                                  -3.89042\n",
      "evaluation/Actions Mean                                 -0.00236113\n",
      "evaluation/Actions Std                                   0.0746259\n",
      "evaluation/Actions Max                                   0.731236\n",
      "evaluation/Actions Min                                  -0.933932\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.12172\n",
      "evaluation/env_infos/final/reward_dist Mean              0.114248\n",
      "evaluation/env_infos/final/reward_dist Std               0.223237\n",
      "evaluation/env_infos/final/reward_dist Max               0.955604\n",
      "evaluation/env_infos/final/reward_dist Min               7.40154e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00761311\n",
      "evaluation/env_infos/initial/reward_dist Std             0.018282\n",
      "evaluation/env_infos/initial/reward_dist Max             0.110233\n",
      "evaluation/env_infos/initial/reward_dist Min             1.12122e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.14064\n",
      "evaluation/env_infos/reward_dist Std                     0.247623\n",
      "evaluation/env_infos/reward_dist Max                     0.999347\n",
      "evaluation/env_infos/reward_dist Min                     7.40154e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0400683\n",
      "evaluation/env_infos/final/reward_energy Std             0.0387507\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0020063\n",
      "evaluation/env_infos/final/reward_energy Min            -0.184611\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.215374\n",
      "evaluation/env_infos/initial/reward_energy Std           0.272116\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00197372\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.965591\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0541837\n",
      "evaluation/env_infos/reward_energy Std                   0.0906274\n",
      "evaluation/env_infos/reward_energy Max                  -0.000797285\n",
      "evaluation/env_infos/reward_energy Min                  -0.965591\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0739885\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.209977\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.470421\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.456137\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0033332\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0118081\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0365618\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0466966\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0447561\n",
      "evaluation/env_infos/end_effector_loc Std                0.141382\n",
      "evaluation/env_infos/end_effector_loc Max                0.470421\n",
      "evaluation/env_infos/end_effector_loc Min               -0.539406\n",
      "time/data storing (s)                                    0.00291019\n",
      "time/evaluation sampling (s)                             1.0474\n",
      "time/exploration sampling (s)                            0.132867\n",
      "time/logging (s)                                         0.0283029\n",
      "time/saving (s)                                          0.0345324\n",
      "time/training (s)                                       52.618\n",
      "time/epoch (s)                                          53.864\n",
      "time/total (s)                                        3293.02\n",
      "Epoch                                                   66\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:20:42.839717 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 67 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00138736\n",
      "trainer/QF2 Loss                                         0.00131741\n",
      "trainer/Policy Loss                                      3.22321\n",
      "trainer/Q1 Predictions Mean                             -1.17844\n",
      "trainer/Q1 Predictions Std                               0.822925\n",
      "trainer/Q1 Predictions Max                               1.12253\n",
      "trainer/Q1 Predictions Min                              -3.28685\n",
      "trainer/Q2 Predictions Mean                             -1.18412\n",
      "trainer/Q2 Predictions Std                               0.831319\n",
      "trainer/Q2 Predictions Max                               1.17507\n",
      "trainer/Q2 Predictions Min                              -3.31397\n",
      "trainer/Q Targets Mean                                  -1.19035\n",
      "trainer/Q Targets Std                                    0.833043\n",
      "trainer/Q Targets Max                                    1.09842\n",
      "trainer/Q Targets Min                                   -3.29937\n",
      "trainer/Log Pis Mean                                     2.04886\n",
      "trainer/Log Pis Std                                      1.26423\n",
      "trainer/Log Pis Max                                      4.15641\n",
      "trainer/Log Pis Min                                     -2.24546\n",
      "trainer/Policy mu Mean                                  -0.0110192\n",
      "trainer/Policy mu Std                                    0.381566\n",
      "trainer/Policy mu Max                                    2.09145\n",
      "trainer/Policy mu Min                                   -1.73722\n",
      "trainer/Policy log std Mean                             -2.24308\n",
      "trainer/Policy log std Std                               0.667282\n",
      "trainer/Policy log std Max                               0.0805831\n",
      "trainer/Policy log std Min                              -3.12712\n",
      "trainer/Alpha                                            0.0236443\n",
      "trainer/Alpha Loss                                       0.183072\n",
      "exploration/num steps total                           7800\n",
      "exploration/num paths total                            390\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.119585\n",
      "exploration/Rewards Std                                  0.0843299\n",
      "exploration/Rewards Max                                  0.073158\n",
      "exploration/Rewards Min                                 -0.332855\n",
      "exploration/Returns Mean                                -2.3917\n",
      "exploration/Returns Std                                  1.29422\n",
      "exploration/Returns Max                                 -0.457542\n",
      "exploration/Returns Min                                 -4.27129\n",
      "exploration/Actions Mean                                 0.00250776\n",
      "exploration/Actions Std                                  0.0782392\n",
      "exploration/Actions Max                                  0.243326\n",
      "exploration/Actions Min                                 -0.385816\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.3917\n",
      "exploration/env_infos/final/reward_dist Mean             0.166595\n",
      "exploration/env_infos/final/reward_dist Std              0.186827\n",
      "exploration/env_infos/final/reward_dist Max              0.435503\n",
      "exploration/env_infos/final/reward_dist Min              0.000106283\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00575197\n",
      "exploration/env_infos/initial/reward_dist Std            0.010757\n",
      "exploration/env_infos/initial/reward_dist Max            0.0272548\n",
      "exploration/env_infos/initial/reward_dist Min            3.07816e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0942637\n",
      "exploration/env_infos/reward_dist Std                    0.141065\n",
      "exploration/env_infos/reward_dist Max                    0.500407\n",
      "exploration/env_infos/reward_dist Min                    3.07816e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.065404\n",
      "exploration/env_infos/final/reward_energy Std            0.021657\n",
      "exploration/env_infos/final/reward_energy Max           -0.0396475\n",
      "exploration/env_infos/final/reward_energy Min           -0.0934896\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.122478\n",
      "exploration/env_infos/initial/reward_energy Std          0.145565\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0261379\n",
      "exploration/env_infos/initial/reward_energy Min         -0.407206\n",
      "exploration/env_infos/reward_energy Mean                -0.0903008\n",
      "exploration/env_infos/reward_energy Std                  0.0640397\n",
      "exploration/env_infos/reward_energy Max                 -0.00138165\n",
      "exploration/env_infos/reward_energy Min                 -0.407206\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0245658\n",
      "exploration/env_infos/final/end_effector_loc Std         0.167298\n",
      "exploration/env_infos/final/end_effector_loc Max         0.147302\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.355609\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00251515\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00623792\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00343912\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0192908\n",
      "exploration/env_infos/end_effector_loc Mean             -0.021229\n",
      "exploration/env_infos/end_effector_loc Std               0.111621\n",
      "exploration/env_infos/end_effector_loc Max               0.154897\n",
      "exploration/env_infos/end_effector_loc Min              -0.401651\n",
      "evaluation/num steps total                           68000\n",
      "evaluation/num paths total                            3400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0463511\n",
      "evaluation/Rewards Std                                   0.0761563\n",
      "evaluation/Rewards Max                                   0.15573\n",
      "evaluation/Rewards Min                                  -0.740937\n",
      "evaluation/Returns Mean                                 -0.927022\n",
      "evaluation/Returns Std                                   1.0236\n",
      "evaluation/Returns Max                                   1.52996\n",
      "evaluation/Returns Min                                  -4.07462\n",
      "evaluation/Actions Mean                                  0.00486511\n",
      "evaluation/Actions Std                                   0.0776408\n",
      "evaluation/Actions Max                                   0.492104\n",
      "evaluation/Actions Min                                  -0.905732\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.927022\n",
      "evaluation/env_infos/final/reward_dist Mean              0.163287\n",
      "evaluation/env_infos/final/reward_dist Std               0.241353\n",
      "evaluation/env_infos/final/reward_dist Max               0.947334\n",
      "evaluation/env_infos/final/reward_dist Min               8.00598e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00490888\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00936141\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0458326\n",
      "evaluation/env_infos/initial/reward_dist Min             1.12382e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.183521\n",
      "evaluation/env_infos/reward_dist Std                     0.254716\n",
      "evaluation/env_infos/reward_dist Max                     0.98955\n",
      "evaluation/env_infos/reward_dist Min                     8.00598e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0652995\n",
      "evaluation/env_infos/final/reward_energy Std             0.102098\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00237647\n",
      "evaluation/env_infos/final/reward_energy Min            -0.497693\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.221858\n",
      "evaluation/env_infos/initial/reward_energy Std           0.249592\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0124987\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.988887\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0640896\n",
      "evaluation/env_infos/reward_energy Std                   0.0894207\n",
      "evaluation/env_infos/reward_energy Max                  -0.0010393\n",
      "evaluation/env_infos/reward_energy Min                  -0.988887\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0153496\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.248185\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.545767\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.563308\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0021244\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116139\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0229086\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0452866\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00409039\n",
      "evaluation/env_infos/end_effector_loc Std                0.163927\n",
      "evaluation/env_infos/end_effector_loc Max                0.545767\n",
      "evaluation/env_infos/end_effector_loc Min               -0.579242\n",
      "time/data storing (s)                                    0.00298185\n",
      "time/evaluation sampling (s)                             1.04222\n",
      "time/exploration sampling (s)                            0.127583\n",
      "time/logging (s)                                         0.0192208\n",
      "time/saving (s)                                          0.0292508\n",
      "time/training (s)                                       51.1149\n",
      "time/epoch (s)                                          52.3361\n",
      "time/total (s)                                        3346.27\n",
      "Epoch                                                   67\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:21:34.494471 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 68 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00172722\n",
      "trainer/QF2 Loss                                         0.000777282\n",
      "trainer/Policy Loss                                      3.22243\n",
      "trainer/Q1 Predictions Mean                             -1.13803\n",
      "trainer/Q1 Predictions Std                               0.760872\n",
      "trainer/Q1 Predictions Max                               0.676283\n",
      "trainer/Q1 Predictions Min                              -3.28053\n",
      "trainer/Q2 Predictions Mean                             -1.12633\n",
      "trainer/Q2 Predictions Std                               0.755157\n",
      "trainer/Q2 Predictions Max                               0.671782\n",
      "trainer/Q2 Predictions Min                              -3.29702\n",
      "trainer/Q Targets Mean                                  -1.12269\n",
      "trainer/Q Targets Std                                    0.755176\n",
      "trainer/Q Targets Max                                    0.654324\n",
      "trainer/Q Targets Min                                   -3.28438\n",
      "trainer/Log Pis Mean                                     2.09192\n",
      "trainer/Log Pis Std                                      1.32213\n",
      "trainer/Log Pis Max                                      4.543\n",
      "trainer/Log Pis Min                                     -2.33965\n",
      "trainer/Policy mu Mean                                   0.00024379\n",
      "trainer/Policy mu Std                                    0.384885\n",
      "trainer/Policy mu Max                                    1.97334\n",
      "trainer/Policy mu Min                                   -1.74154\n",
      "trainer/Policy log std Mean                             -2.30036\n",
      "trainer/Policy log std Std                               0.655229\n",
      "trainer/Policy log std Max                               0.104348\n",
      "trainer/Policy log std Min                              -3.20241\n",
      "trainer/Alpha                                            0.0234401\n",
      "trainer/Alpha Loss                                       0.345104\n",
      "exploration/num steps total                           7900\n",
      "exploration/num paths total                            395\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.115336\n",
      "exploration/Rewards Std                                  0.147503\n",
      "exploration/Rewards Max                                  0.06014\n",
      "exploration/Rewards Min                                 -0.632688\n",
      "exploration/Returns Mean                                -2.30672\n",
      "exploration/Returns Std                                  2.5205\n",
      "exploration/Returns Max                                  0.292477\n",
      "exploration/Returns Min                                 -6.73799\n",
      "exploration/Actions Mean                                -0.00740298\n",
      "exploration/Actions Std                                  0.19192\n",
      "exploration/Actions Max                                  0.598612\n",
      "exploration/Actions Min                                 -0.712987\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.30672\n",
      "exploration/env_infos/final/reward_dist Mean             0.273079\n",
      "exploration/env_infos/final/reward_dist Std              0.364135\n",
      "exploration/env_infos/final/reward_dist Max              0.968844\n",
      "exploration/env_infos/final/reward_dist Min              1.30113e-31\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00736255\n",
      "exploration/env_infos/initial/reward_dist Std            0.00654993\n",
      "exploration/env_infos/initial/reward_dist Max            0.0176867\n",
      "exploration/env_infos/initial/reward_dist Min            0.000226222\n",
      "exploration/env_infos/reward_dist Mean                   0.227021\n",
      "exploration/env_infos/reward_dist Std                    0.324032\n",
      "exploration/env_infos/reward_dist Max                    0.999999\n",
      "exploration/env_infos/reward_dist Min                    1.30113e-31\n",
      "exploration/env_infos/final/reward_energy Mean          -0.328671\n",
      "exploration/env_infos/final/reward_energy Std            0.19416\n",
      "exploration/env_infos/final/reward_energy Max           -0.115863\n",
      "exploration/env_infos/final/reward_energy Min           -0.681776\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.324851\n",
      "exploration/env_infos/initial/reward_energy Std          0.0874401\n",
      "exploration/env_infos/initial/reward_energy Max         -0.200166\n",
      "exploration/env_infos/initial/reward_energy Min         -0.413335\n",
      "exploration/env_infos/reward_energy Mean                -0.228935\n",
      "exploration/env_infos/reward_energy Std                  0.146167\n",
      "exploration/env_infos/reward_energy Max                 -0.0211623\n",
      "exploration/env_infos/reward_energy Min                 -0.807032\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.135059\n",
      "exploration/env_infos/final/end_effector_loc Std         0.264153\n",
      "exploration/env_infos/final/end_effector_loc Max         0.269729\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.537556\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00227542\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0116743\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0183508\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0180107\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0749623\n",
      "exploration/env_infos/end_effector_loc Std               0.18569\n",
      "exploration/env_infos/end_effector_loc Max               0.276773\n",
      "exploration/env_infos/end_effector_loc Min              -0.537556\n",
      "evaluation/num steps total                           69000\n",
      "evaluation/num paths total                            3450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0629682\n",
      "evaluation/Rewards Std                                   0.0732746\n",
      "evaluation/Rewards Max                                   0.0909085\n",
      "evaluation/Rewards Min                                  -0.422571\n",
      "evaluation/Returns Mean                                 -1.25936\n",
      "evaluation/Returns Std                                   1.03871\n",
      "evaluation/Returns Max                                   0.727792\n",
      "evaluation/Returns Min                                  -3.40581\n",
      "evaluation/Actions Mean                                 -0.000187787\n",
      "evaluation/Actions Std                                   0.0813938\n",
      "evaluation/Actions Max                                   0.730927\n",
      "evaluation/Actions Min                                  -0.903367\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.25936\n",
      "evaluation/env_infos/final/reward_dist Mean              0.135996\n",
      "evaluation/env_infos/final/reward_dist Std               0.224241\n",
      "evaluation/env_infos/final/reward_dist Max               0.896155\n",
      "evaluation/env_infos/final/reward_dist Min               2.10406e-23\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705512\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0171036\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0899132\n",
      "evaluation/env_infos/initial/reward_dist Min             1.4271e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.165155\n",
      "evaluation/env_infos/reward_dist Std                     0.247343\n",
      "evaluation/env_infos/reward_dist Max                     0.996618\n",
      "evaluation/env_infos/reward_dist Min                     2.10406e-23\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0458434\n",
      "evaluation/env_infos/final/reward_energy Std             0.0398149\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000655285\n",
      "evaluation/env_infos/final/reward_energy Min            -0.170266\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.246643\n",
      "evaluation/env_infos/initial/reward_energy Std           0.234207\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0161244\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.958\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0680376\n",
      "evaluation/env_infos/reward_energy Std                   0.0928486\n",
      "evaluation/env_infos/reward_energy Max                  -0.00027507\n",
      "evaluation/env_infos/reward_energy Min                  -0.958\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0159061\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.261089\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.564158\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.630649\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -9.83276e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0120249\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0365463\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0451683\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00828856\n",
      "evaluation/env_infos/end_effector_loc Std                0.167835\n",
      "evaluation/env_infos/end_effector_loc Max                0.564158\n",
      "evaluation/env_infos/end_effector_loc Min               -0.630649\n",
      "time/data storing (s)                                    0.00303915\n",
      "time/evaluation sampling (s)                             1.03508\n",
      "time/exploration sampling (s)                            0.142272\n",
      "time/logging (s)                                         0.0193741\n",
      "time/saving (s)                                          0.0283905\n",
      "time/training (s)                                       49.5159\n",
      "time/epoch (s)                                          50.7441\n",
      "time/total (s)                                        3397.93\n",
      "Epoch                                                   68\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:22:25.228978 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 69 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000933154\n",
      "trainer/QF2 Loss                                         0.000682523\n",
      "trainer/Policy Loss                                      3.07447\n",
      "trainer/Q1 Predictions Mean                             -1.11253\n",
      "trainer/Q1 Predictions Std                               0.843683\n",
      "trainer/Q1 Predictions Max                               0.838345\n",
      "trainer/Q1 Predictions Min                              -3.43732\n",
      "trainer/Q2 Predictions Mean                             -1.11363\n",
      "trainer/Q2 Predictions Std                               0.842185\n",
      "trainer/Q2 Predictions Max                               0.849407\n",
      "trainer/Q2 Predictions Min                              -3.43083\n",
      "trainer/Q Targets Mean                                  -1.11499\n",
      "trainer/Q Targets Std                                    0.844636\n",
      "trainer/Q Targets Max                                    0.845606\n",
      "trainer/Q Targets Min                                   -3.41585\n",
      "trainer/Log Pis Mean                                     1.97151\n",
      "trainer/Log Pis Std                                      1.47977\n",
      "trainer/Log Pis Max                                      4.3675\n",
      "trainer/Log Pis Min                                     -7.78597\n",
      "trainer/Policy mu Mean                                   0.0141081\n",
      "trainer/Policy mu Std                                    0.308223\n",
      "trainer/Policy mu Max                                    1.42802\n",
      "trainer/Policy mu Min                                   -1.58876\n",
      "trainer/Policy log std Mean                             -2.30164\n",
      "trainer/Policy log std Std                               0.672328\n",
      "trainer/Policy log std Max                               0.53859\n",
      "trainer/Policy log std Min                              -3.23721\n",
      "trainer/Alpha                                            0.0224914\n",
      "trainer/Alpha Loss                                      -0.108064\n",
      "exploration/num steps total                           8000\n",
      "exploration/num paths total                            400\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.10127\n",
      "exploration/Rewards Std                                  0.0577385\n",
      "exploration/Rewards Max                                  0.012892\n",
      "exploration/Rewards Min                                 -0.285635\n",
      "exploration/Returns Mean                                -2.02541\n",
      "exploration/Returns Std                                  0.250597\n",
      "exploration/Returns Max                                 -1.75535\n",
      "exploration/Returns Min                                 -2.42366\n",
      "exploration/Actions Mean                                 0.00349451\n",
      "exploration/Actions Std                                  0.0994182\n",
      "exploration/Actions Max                                  0.332261\n",
      "exploration/Actions Min                                 -0.360477\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.02541\n",
      "exploration/env_infos/final/reward_dist Mean             0.0537053\n",
      "exploration/env_infos/final/reward_dist Std              0.106457\n",
      "exploration/env_infos/final/reward_dist Max              0.266616\n",
      "exploration/env_infos/final/reward_dist Min              2.96158e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000434665\n",
      "exploration/env_infos/initial/reward_dist Std            0.000494509\n",
      "exploration/env_infos/initial/reward_dist Max            0.0013831\n",
      "exploration/env_infos/initial/reward_dist Min            4.68113e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.110503\n",
      "exploration/env_infos/reward_dist Std                    0.212478\n",
      "exploration/env_infos/reward_dist Max                    0.979295\n",
      "exploration/env_infos/reward_dist Min                    2.96158e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.121605\n",
      "exploration/env_infos/final/reward_energy Std            0.0442026\n",
      "exploration/env_infos/final/reward_energy Max           -0.0591545\n",
      "exploration/env_infos/final/reward_energy Min           -0.182283\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.19603\n",
      "exploration/env_infos/initial/reward_energy Std          0.0890731\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0961772\n",
      "exploration/env_infos/initial/reward_energy Min         -0.348777\n",
      "exploration/env_infos/reward_energy Mean                -0.114617\n",
      "exploration/env_infos/reward_energy Std                  0.08158\n",
      "exploration/env_infos/reward_energy Max                 -0.00967994\n",
      "exploration/env_infos/reward_energy Min                 -0.408036\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.172271\n",
      "exploration/env_infos/final/end_effector_loc Std         0.184701\n",
      "exploration/env_infos/final/end_effector_loc Max         0.467517\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.163024\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00358183\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00671733\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.013139\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0114665\n",
      "exploration/env_infos/end_effector_loc Mean              0.105474\n",
      "exploration/env_infos/end_effector_loc Std               0.121177\n",
      "exploration/env_infos/end_effector_loc Max               0.467517\n",
      "exploration/env_infos/end_effector_loc Min              -0.163024\n",
      "evaluation/num steps total                           70000\n",
      "evaluation/num paths total                            3500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0532707\n",
      "evaluation/Rewards Std                                   0.0712409\n",
      "evaluation/Rewards Max                                   0.111361\n",
      "evaluation/Rewards Min                                  -0.629201\n",
      "evaluation/Returns Mean                                 -1.06541\n",
      "evaluation/Returns Std                                   1.02132\n",
      "evaluation/Returns Max                                   0.983864\n",
      "evaluation/Returns Min                                  -3.70963\n",
      "evaluation/Actions Mean                                  0.00187879\n",
      "evaluation/Actions Std                                   0.0728665\n",
      "evaluation/Actions Max                                   0.688405\n",
      "evaluation/Actions Min                                  -0.745729\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.06541\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0685799\n",
      "evaluation/env_infos/final/reward_dist Std               0.134646\n",
      "evaluation/env_infos/final/reward_dist Max               0.545118\n",
      "evaluation/env_infos/final/reward_dist Min               2.97538e-33\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00566398\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00932576\n",
      "evaluation/env_infos/initial/reward_dist Max             0.038538\n",
      "evaluation/env_infos/initial/reward_dist Min             9.12586e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.165822\n",
      "evaluation/env_infos/reward_dist Std                     0.245469\n",
      "evaluation/env_infos/reward_dist Max                     0.999444\n",
      "evaluation/env_infos/reward_dist Min                     2.97538e-33\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0367164\n",
      "evaluation/env_infos/final/reward_energy Std             0.0464634\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00170104\n",
      "evaluation/env_infos/final/reward_energy Min            -0.273491\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.231512\n",
      "evaluation/env_infos/initial/reward_energy Std           0.230245\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0159139\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.868661\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0609396\n",
      "evaluation/env_infos/reward_energy Std                   0.0831413\n",
      "evaluation/env_infos/reward_energy Max                  -0.00146658\n",
      "evaluation/env_infos/reward_energy Min                  -0.868661\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00256867\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.264259\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.492451\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.618989\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00120672\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0114807\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0344202\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0372864\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00540467\n",
      "evaluation/env_infos/end_effector_loc Std                0.16763\n",
      "evaluation/env_infos/end_effector_loc Max                0.492451\n",
      "evaluation/env_infos/end_effector_loc Min               -0.618989\n",
      "time/data storing (s)                                    0.00302765\n",
      "time/evaluation sampling (s)                             0.985354\n",
      "time/exploration sampling (s)                            0.126233\n",
      "time/logging (s)                                         0.0192381\n",
      "time/saving (s)                                          0.0270467\n",
      "time/training (s)                                       48.6291\n",
      "time/epoch (s)                                          49.79\n",
      "time/total (s)                                        3448.66\n",
      "Epoch                                                   69\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:23:15.934962 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 70 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00067761\n",
      "trainer/QF2 Loss                                         0.00082893\n",
      "trainer/Policy Loss                                      3.28411\n",
      "trainer/Q1 Predictions Mean                             -1.26548\n",
      "trainer/Q1 Predictions Std                               0.881988\n",
      "trainer/Q1 Predictions Max                               0.863735\n",
      "trainer/Q1 Predictions Min                              -3.31856\n",
      "trainer/Q2 Predictions Mean                             -1.26307\n",
      "trainer/Q2 Predictions Std                               0.881034\n",
      "trainer/Q2 Predictions Max                               0.858544\n",
      "trainer/Q2 Predictions Min                              -3.33858\n",
      "trainer/Q Targets Mean                                  -1.25928\n",
      "trainer/Q Targets Std                                    0.881933\n",
      "trainer/Q Targets Max                                    0.847508\n",
      "trainer/Q Targets Min                                   -3.3003\n",
      "trainer/Log Pis Mean                                     2.02841\n",
      "trainer/Log Pis Std                                      1.47298\n",
      "trainer/Log Pis Max                                      4.61068\n",
      "trainer/Log Pis Min                                     -4.23207\n",
      "trainer/Policy mu Mean                                   0.00976759\n",
      "trainer/Policy mu Std                                    0.308773\n",
      "trainer/Policy mu Max                                    1.36709\n",
      "trainer/Policy mu Min                                   -1.80422\n",
      "trainer/Policy log std Mean                             -2.34348\n",
      "trainer/Policy log std Std                               0.623164\n",
      "trainer/Policy log std Max                              -0.155634\n",
      "trainer/Policy log std Min                              -3.25313\n",
      "trainer/Alpha                                            0.0226011\n",
      "trainer/Alpha Loss                                       0.107683\n",
      "exploration/num steps total                           8100\n",
      "exploration/num paths total                            405\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.107869\n",
      "exploration/Rewards Std                                  0.0516748\n",
      "exploration/Rewards Max                                  0.0359467\n",
      "exploration/Rewards Min                                 -0.302525\n",
      "exploration/Returns Mean                                -2.15739\n",
      "exploration/Returns Std                                  0.385669\n",
      "exploration/Returns Max                                 -1.70303\n",
      "exploration/Returns Min                                 -2.62353\n",
      "exploration/Actions Mean                                 0.00210449\n",
      "exploration/Actions Std                                  0.136825\n",
      "exploration/Actions Max                                  0.501863\n",
      "exploration/Actions Min                                 -0.935936\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.15739\n",
      "exploration/env_infos/final/reward_dist Mean             0.000709981\n",
      "exploration/env_infos/final/reward_dist Std              0.00141995\n",
      "exploration/env_infos/final/reward_dist Max              0.00354988\n",
      "exploration/env_infos/final/reward_dist Min              4.1737e-26\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0219133\n",
      "exploration/env_infos/initial/reward_dist Std            0.0271101\n",
      "exploration/env_infos/initial/reward_dist Max            0.0690864\n",
      "exploration/env_infos/initial/reward_dist Min            1.308e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0990189\n",
      "exploration/env_infos/reward_dist Std                    0.233166\n",
      "exploration/env_infos/reward_dist Max                    0.983897\n",
      "exploration/env_infos/reward_dist Min                    4.1737e-26\n",
      "exploration/env_infos/final/reward_energy Mean          -0.148502\n",
      "exploration/env_infos/final/reward_energy Std            0.0489454\n",
      "exploration/env_infos/final/reward_energy Max           -0.0998077\n",
      "exploration/env_infos/final/reward_energy Min           -0.232283\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.329278\n",
      "exploration/env_infos/initial/reward_energy Std          0.371611\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0427611\n",
      "exploration/env_infos/initial/reward_energy Min         -0.985159\n",
      "exploration/env_infos/reward_energy Mean                -0.144381\n",
      "exploration/env_infos/reward_energy Std                  0.12886\n",
      "exploration/env_infos/reward_energy Max                 -0.013158\n",
      "exploration/env_infos/reward_energy Min                 -0.985159\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0634185\n",
      "exploration/env_infos/final/end_effector_loc Std         0.266352\n",
      "exploration/env_infos/final/end_effector_loc Max         0.546713\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.245968\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00354208\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0171931\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0250931\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0467968\n",
      "exploration/env_infos/end_effector_loc Mean              0.0236789\n",
      "exploration/env_infos/end_effector_loc Std               0.193137\n",
      "exploration/env_infos/end_effector_loc Max               0.551667\n",
      "exploration/env_infos/end_effector_loc Min              -0.417148\n",
      "evaluation/num steps total                           71000\n",
      "evaluation/num paths total                            3550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.048161\n",
      "evaluation/Rewards Std                                   0.076957\n",
      "evaluation/Rewards Max                                   0.16054\n",
      "evaluation/Rewards Min                                  -0.373781\n",
      "evaluation/Returns Mean                                 -0.96322\n",
      "evaluation/Returns Std                                   1.16668\n",
      "evaluation/Returns Max                                   1.15382\n",
      "evaluation/Returns Min                                  -3.08357\n",
      "evaluation/Actions Mean                                  0.0029595\n",
      "evaluation/Actions Std                                   0.0792346\n",
      "evaluation/Actions Max                                   0.494885\n",
      "evaluation/Actions Min                                  -0.986349\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.96322\n",
      "evaluation/env_infos/final/reward_dist Mean              0.29648\n",
      "evaluation/env_infos/final/reward_dist Std               0.327964\n",
      "evaluation/env_infos/final/reward_dist Max               0.999525\n",
      "evaluation/env_infos/final/reward_dist Min               2.12241e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0108816\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0197919\n",
      "evaluation/env_infos/initial/reward_dist Max             0.105143\n",
      "evaluation/env_infos/initial/reward_dist Min             1.77161e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.235278\n",
      "evaluation/env_infos/reward_dist Std                     0.296738\n",
      "evaluation/env_infos/reward_dist Max                     0.999525\n",
      "evaluation/env_infos/reward_dist Min                     2.12241e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0539763\n",
      "evaluation/env_infos/final/reward_energy Std             0.084284\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0037007\n",
      "evaluation/env_infos/final/reward_energy Min            -0.518926\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.237599\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255794\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0132959\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.21304\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0644339\n",
      "evaluation/env_infos/reward_energy Std                   0.0917715\n",
      "evaluation/env_infos/reward_energy Max                  -0.000902422\n",
      "evaluation/env_infos/reward_energy Min                  -1.21304\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00756622\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.253989\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.682794\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.488399\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00165384\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122319\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0247442\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0493174\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00263564\n",
      "evaluation/env_infos/end_effector_loc Std                0.167649\n",
      "evaluation/env_infos/end_effector_loc Max                0.682794\n",
      "evaluation/env_infos/end_effector_loc Min               -0.488399\n",
      "time/data storing (s)                                    0.00302955\n",
      "time/evaluation sampling (s)                             0.992106\n",
      "time/exploration sampling (s)                            0.12606\n",
      "time/logging (s)                                         0.0234389\n",
      "time/saving (s)                                          0.0293689\n",
      "time/training (s)                                       48.6712\n",
      "time/epoch (s)                                          49.8452\n",
      "time/total (s)                                        3499.37\n",
      "Epoch                                                   70\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:24:06.020678 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 71 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000933849\r\n",
      "trainer/QF2 Loss                                         0.000966241\r\n",
      "trainer/Policy Loss                                      2.91713\r\n",
      "trainer/Q1 Predictions Mean                             -1.07943\r\n",
      "trainer/Q1 Predictions Std                               0.866178\r\n",
      "trainer/Q1 Predictions Max                               1.22607\r\n",
      "trainer/Q1 Predictions Min                              -3.43242\r\n",
      "trainer/Q2 Predictions Mean                             -1.07468\r\n",
      "trainer/Q2 Predictions Std                               0.864412\r\n",
      "trainer/Q2 Predictions Max                               1.21117\r\n",
      "trainer/Q2 Predictions Min                              -3.44985\r\n",
      "trainer/Q Targets Mean                                  -1.0712\r\n",
      "trainer/Q Targets Std                                    0.864888\r\n",
      "trainer/Q Targets Max                                    1.18584\r\n",
      "trainer/Q Targets Min                                   -3.47762\r\n",
      "trainer/Log Pis Mean                                     1.83648\r\n",
      "trainer/Log Pis Std                                      1.27768\r\n",
      "trainer/Log Pis Max                                      4.11516\r\n",
      "trainer/Log Pis Min                                     -2.48852\r\n",
      "trainer/Policy mu Mean                                   0.0303272\r\n",
      "trainer/Policy mu Std                                    0.295716\r\n",
      "trainer/Policy mu Max                                    1.64069\r\n",
      "trainer/Policy mu Min                                   -1.30455\r\n",
      "trainer/Policy log std Mean                             -2.23488\r\n",
      "trainer/Policy log std Std                               0.608597\r\n",
      "trainer/Policy log std Max                              -0.319722\r\n",
      "trainer/Policy log std Min                              -3.23815\r\n",
      "trainer/Alpha                                            0.0226957\r\n",
      "trainer/Alpha Loss                                      -0.61891\r\n",
      "exploration/num steps total                           8200\r\n",
      "exploration/num paths total                            410\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0808401\r\n",
      "exploration/Rewards Std                                  0.0718923\r\n",
      "exploration/Rewards Max                                  0.0353937\r\n",
      "exploration/Rewards Min                                 -0.594055\r\n",
      "exploration/Returns Mean                                -1.6168\r\n",
      "exploration/Returns Std                                  0.512901\r\n",
      "exploration/Returns Max                                 -1.06121\r\n",
      "exploration/Returns Min                                 -2.54159\r\n",
      "exploration/Actions Mean                                 0.00823882\r\n",
      "exploration/Actions Std                                  0.133173\r\n",
      "exploration/Actions Max                                  0.578739\r\n",
      "exploration/Actions Min                                 -0.773362\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.6168\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.164572\r\n",
      "exploration/env_infos/final/reward_dist Std              0.202397\r\n",
      "exploration/env_infos/final/reward_dist Max              0.44223\r\n",
      "exploration/env_infos/final/reward_dist Min              1.97065e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0120119\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0199204\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0515621\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000344107\r\n",
      "exploration/env_infos/reward_dist Mean                   0.152894\r\n",
      "exploration/env_infos/reward_dist Std                    0.239948\r\n",
      "exploration/env_infos/reward_dist Max                    0.934106\r\n",
      "exploration/env_infos/reward_dist Min                    7.55207e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.124072\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0615268\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0430175\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.229763\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.404912\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.316674\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0300679\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.814621\r\n",
      "exploration/env_infos/reward_energy Mean                -0.128832\r\n",
      "exploration/env_infos/reward_energy Std                  0.137869\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0180296\r\n",
      "exploration/env_infos/reward_energy Min                 -0.814621\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0361425\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.24588\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.512964\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.434238\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00278875\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0179588\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0222173\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0386681\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00388596\r\n",
      "exploration/env_infos/end_effector_loc Std               0.181597\r\n",
      "exploration/env_infos/end_effector_loc Max               0.512964\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.469832\r\n",
      "evaluation/num steps total                           72000\r\n",
      "evaluation/num paths total                            3600\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0512251\r\n",
      "evaluation/Rewards Std                                   0.079413\r\n",
      "evaluation/Rewards Max                                   0.149412\r\n",
      "evaluation/Rewards Min                                  -0.694786\r\n",
      "evaluation/Returns Mean                                 -1.0245\r\n",
      "evaluation/Returns Std                                   1.13961\r\n",
      "evaluation/Returns Max                                   2.11206\r\n",
      "evaluation/Returns Min                                  -5.17624\r\n",
      "evaluation/Actions Mean                                  0.00355475\r\n",
      "evaluation/Actions Std                                   0.0797162\r\n",
      "evaluation/Actions Max                                   0.661872\r\n",
      "evaluation/Actions Min                                  -0.867439\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.0245\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.233588\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.320058\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.988518\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.61387e-97\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00712585\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0106051\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0424511\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.23273e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.21517\r\n",
      "evaluation/env_infos/reward_dist Std                     0.282242\r\n",
      "evaluation/env_infos/reward_dist Max                     1\r\n",
      "evaluation/env_infos/reward_dist Min                     8.61387e-97\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0755723\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.139757\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0040871\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.754334\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.224537\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.224555\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00397889\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.906472\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.064667\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0924814\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000855171\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.906472\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0406184\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.249537\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.414779\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000912248\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0111902\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.025133\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0433719\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.02153\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.153493\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.414779\r\n",
      "time/data storing (s)                                    0.00288802\r\n",
      "time/evaluation sampling (s)                             0.983816\r\n",
      "time/exploration sampling (s)                            0.125545\r\n",
      "time/logging (s)                                         0.0205728\r\n",
      "time/saving (s)                                          0.0294247\r\n",
      "time/training (s)                                       47.9402\r\n",
      "time/epoch (s)                                          49.1024\r\n",
      "time/total (s)                                        3549.45\r\n",
      "Epoch                                                   71\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:24:56.187837 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 72 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000909551\n",
      "trainer/QF2 Loss                                         0.000760931\n",
      "trainer/Policy Loss                                      3.45846\n",
      "trainer/Q1 Predictions Mean                             -1.25861\n",
      "trainer/Q1 Predictions Std                               0.863\n",
      "trainer/Q1 Predictions Max                               0.897449\n",
      "trainer/Q1 Predictions Min                              -3.47107\n",
      "trainer/Q2 Predictions Mean                             -1.25147\n",
      "trainer/Q2 Predictions Std                               0.863416\n",
      "trainer/Q2 Predictions Max                               0.854898\n",
      "trainer/Q2 Predictions Min                              -3.47101\n",
      "trainer/Q Targets Mean                                  -1.24906\n",
      "trainer/Q Targets Std                                    0.861997\n",
      "trainer/Q Targets Max                                    0.873771\n",
      "trainer/Q Targets Min                                   -3.52495\n",
      "trainer/Log Pis Mean                                     2.20937\n",
      "trainer/Log Pis Std                                      1.28884\n",
      "trainer/Log Pis Max                                      4.69985\n",
      "trainer/Log Pis Min                                     -2.17544\n",
      "trainer/Policy mu Mean                                   0.00859512\n",
      "trainer/Policy mu Std                                    0.231722\n",
      "trainer/Policy mu Max                                    1.17068\n",
      "trainer/Policy mu Min                                   -1.55767\n",
      "trainer/Policy log std Mean                             -2.42016\n",
      "trainer/Policy log std Std                               0.550277\n",
      "trainer/Policy log std Max                              -0.70136\n",
      "trainer/Policy log std Min                              -3.42124\n",
      "trainer/Alpha                                            0.0230616\n",
      "trainer/Alpha Loss                                       0.78942\n",
      "exploration/num steps total                           8300\n",
      "exploration/num paths total                            415\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.109283\n",
      "exploration/Rewards Std                                  0.110154\n",
      "exploration/Rewards Max                                  0.0683411\n",
      "exploration/Rewards Min                                 -0.688974\n",
      "exploration/Returns Mean                                -2.18565\n",
      "exploration/Returns Std                                  1.63198\n",
      "exploration/Returns Max                                 -0.429391\n",
      "exploration/Returns Min                                 -5.07478\n",
      "exploration/Actions Mean                                -1.79031e-05\n",
      "exploration/Actions Std                                  0.163321\n",
      "exploration/Actions Max                                  0.650414\n",
      "exploration/Actions Min                                 -0.449145\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.18565\n",
      "exploration/env_infos/final/reward_dist Mean             0.0512322\n",
      "exploration/env_infos/final/reward_dist Std              0.0655489\n",
      "exploration/env_infos/final/reward_dist Max              0.158111\n",
      "exploration/env_infos/final/reward_dist Min              2.38494e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0106987\n",
      "exploration/env_infos/initial/reward_dist Std            0.0113032\n",
      "exploration/env_infos/initial/reward_dist Max            0.0265981\n",
      "exploration/env_infos/initial/reward_dist Min            3.0235e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0773017\n",
      "exploration/env_infos/reward_dist Std                    0.164347\n",
      "exploration/env_infos/reward_dist Max                    0.936217\n",
      "exploration/env_infos/reward_dist Min                    8.95082e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.176516\n",
      "exploration/env_infos/final/reward_energy Std            0.0696335\n",
      "exploration/env_infos/final/reward_energy Max           -0.113575\n",
      "exploration/env_infos/final/reward_energy Min           -0.311935\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.322216\n",
      "exploration/env_infos/initial/reward_energy Std          0.187518\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0427308\n",
      "exploration/env_infos/initial/reward_energy Min         -0.532375\n",
      "exploration/env_infos/reward_energy Mean                -0.195703\n",
      "exploration/env_infos/reward_energy Std                  0.12267\n",
      "exploration/env_infos/reward_energy Max                 -0.0159297\n",
      "exploration/env_infos/reward_energy Min                 -0.685912\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0154771\n",
      "exploration/env_infos/final/end_effector_loc Std         0.263539\n",
      "exploration/env_infos/final/end_effector_loc Max         0.481457\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.445834\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00401514\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0125543\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0232033\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0224572\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00756241\n",
      "exploration/env_infos/end_effector_loc Std               0.200361\n",
      "exploration/env_infos/end_effector_loc Max               0.556836\n",
      "exploration/env_infos/end_effector_loc Min              -0.450517\n",
      "evaluation/num steps total                           73000\n",
      "evaluation/num paths total                            3650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0567901\n",
      "evaluation/Rewards Std                                   0.0760603\n",
      "evaluation/Rewards Max                                   0.129603\n",
      "evaluation/Rewards Min                                  -0.469098\n",
      "evaluation/Returns Mean                                 -1.1358\n",
      "evaluation/Returns Std                                   1.09119\n",
      "evaluation/Returns Max                                   0.678061\n",
      "evaluation/Returns Min                                  -4.64711\n",
      "evaluation/Actions Mean                                  0.00297403\n",
      "evaluation/Actions Std                                   0.0818852\n",
      "evaluation/Actions Max                                   0.692129\n",
      "evaluation/Actions Min                                  -0.798767\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.1358\n",
      "evaluation/env_infos/final/reward_dist Mean              0.165829\n",
      "evaluation/env_infos/final/reward_dist Std               0.242373\n",
      "evaluation/env_infos/final/reward_dist Max               0.910344\n",
      "evaluation/env_infos/final/reward_dist Min               4.32495e-100\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0073267\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0108329\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0364556\n",
      "evaluation/env_infos/initial/reward_dist Min             1.71578e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.205232\n",
      "evaluation/env_infos/reward_dist Std                     0.269432\n",
      "evaluation/env_infos/reward_dist Max                     0.997623\n",
      "evaluation/env_infos/reward_dist Min                     4.32495e-100\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.056135\n",
      "evaluation/env_infos/final/reward_energy Std             0.118099\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00218629\n",
      "evaluation/env_infos/final/reward_energy Min            -0.772063\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.225882\n",
      "evaluation/env_infos/initial/reward_energy Std           0.228103\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00681566\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.848953\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0632312\n",
      "evaluation/env_infos/reward_energy Std                   0.0971075\n",
      "evaluation/env_infos/reward_energy Max                  -0.00117835\n",
      "evaluation/env_infos/reward_energy Min                  -0.848953\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.054149\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.260944\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.6707\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0036676\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107408\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0346064\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0399384\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0395369\n",
      "evaluation/env_infos/end_effector_loc Std                0.168158\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.6707\n",
      "time/data storing (s)                                    0.00311781\n",
      "time/evaluation sampling (s)                             0.947244\n",
      "time/exploration sampling (s)                            0.119744\n",
      "time/logging (s)                                         0.0199005\n",
      "time/saving (s)                                          0.0281707\n",
      "time/training (s)                                       48.084\n",
      "time/epoch (s)                                          49.2022\n",
      "time/total (s)                                        3599.61\n",
      "Epoch                                                   72\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:25:46.845365 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 73 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000566648\n",
      "trainer/QF2 Loss                                         0.000790792\n",
      "trainer/Policy Loss                                      2.92605\n",
      "trainer/Q1 Predictions Mean                             -1.04742\n",
      "trainer/Q1 Predictions Std                               0.865891\n",
      "trainer/Q1 Predictions Max                               1.22881\n",
      "trainer/Q1 Predictions Min                              -3.77677\n",
      "trainer/Q2 Predictions Mean                             -1.05444\n",
      "trainer/Q2 Predictions Std                               0.867626\n",
      "trainer/Q2 Predictions Max                               1.23928\n",
      "trainer/Q2 Predictions Min                              -3.77882\n",
      "trainer/Q Targets Mean                                  -1.04559\n",
      "trainer/Q Targets Std                                    0.860448\n",
      "trainer/Q Targets Max                                    1.20028\n",
      "trainer/Q Targets Min                                   -3.74542\n",
      "trainer/Log Pis Mean                                     1.8762\n",
      "trainer/Log Pis Std                                      1.37929\n",
      "trainer/Log Pis Max                                      4.2\n",
      "trainer/Log Pis Min                                     -3.73588\n",
      "trainer/Policy mu Mean                                   0.0413386\n",
      "trainer/Policy mu Std                                    0.300251\n",
      "trainer/Policy mu Max                                    1.70749\n",
      "trainer/Policy mu Min                                   -1.31965\n",
      "trainer/Policy log std Mean                             -2.2799\n",
      "trainer/Policy log std Std                               0.611822\n",
      "trainer/Policy log std Max                              -0.188869\n",
      "trainer/Policy log std Min                              -3.3546\n",
      "trainer/Alpha                                            0.0222377\n",
      "trainer/Alpha Loss                                      -0.471162\n",
      "exploration/num steps total                           8400\n",
      "exploration/num paths total                            420\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0911932\n",
      "exploration/Rewards Std                                  0.0782359\n",
      "exploration/Rewards Max                                  0.117461\n",
      "exploration/Rewards Min                                 -0.379602\n",
      "exploration/Returns Mean                                -1.82386\n",
      "exploration/Returns Std                                  0.942118\n",
      "exploration/Returns Max                                 -0.260485\n",
      "exploration/Returns Min                                 -2.77934\n",
      "exploration/Actions Mean                                -0.00272344\n",
      "exploration/Actions Std                                  0.0987086\n",
      "exploration/Actions Max                                  0.388581\n",
      "exploration/Actions Min                                 -0.328086\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.82386\n",
      "exploration/env_infos/final/reward_dist Mean             0.0789832\n",
      "exploration/env_infos/final/reward_dist Std              0.106286\n",
      "exploration/env_infos/final/reward_dist Max              0.27457\n",
      "exploration/env_infos/final/reward_dist Min              3.05045e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000150886\n",
      "exploration/env_infos/initial/reward_dist Std            0.00010756\n",
      "exploration/env_infos/initial/reward_dist Max            0.000309439\n",
      "exploration/env_infos/initial/reward_dist Min            1.31151e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.133266\n",
      "exploration/env_infos/reward_dist Std                    0.22121\n",
      "exploration/env_infos/reward_dist Max                    0.992348\n",
      "exploration/env_infos/reward_dist Min                    1.31151e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.129938\n",
      "exploration/env_infos/final/reward_energy Std            0.0880274\n",
      "exploration/env_infos/final/reward_energy Max           -0.0183847\n",
      "exploration/env_infos/final/reward_energy Min           -0.245748\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.212073\n",
      "exploration/env_infos/initial/reward_energy Std          0.110676\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0296867\n",
      "exploration/env_infos/initial/reward_energy Min         -0.365458\n",
      "exploration/env_infos/reward_energy Mean                -0.113404\n",
      "exploration/env_infos/reward_energy Std                  0.0814929\n",
      "exploration/env_infos/reward_energy Max                 -0.00195488\n",
      "exploration/env_infos/reward_energy Min                 -0.43976\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.027224\n",
      "exploration/env_infos/final/end_effector_loc Std         0.230915\n",
      "exploration/env_infos/final/end_effector_loc Max         0.408961\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.408729\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00055035\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00843963\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0164053\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00804759\n",
      "exploration/env_infos/end_effector_loc Mean              0.0278061\n",
      "exploration/env_infos/end_effector_loc Std               0.160744\n",
      "exploration/env_infos/end_effector_loc Max               0.408961\n",
      "exploration/env_infos/end_effector_loc Min              -0.408729\n",
      "evaluation/num steps total                           74000\n",
      "evaluation/num paths total                            3700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0461172\n",
      "evaluation/Rewards Std                                   0.0795324\n",
      "evaluation/Rewards Max                                   0.179078\n",
      "evaluation/Rewards Min                                  -0.459704\n",
      "evaluation/Returns Mean                                 -0.922344\n",
      "evaluation/Returns Std                                   1.20038\n",
      "evaluation/Returns Max                                   2.49339\n",
      "evaluation/Returns Min                                  -3.22894\n",
      "evaluation/Actions Mean                                 -0.00243599\n",
      "evaluation/Actions Std                                   0.0729711\n",
      "evaluation/Actions Max                                   0.599325\n",
      "evaluation/Actions Min                                  -0.821327\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.922344\n",
      "evaluation/env_infos/final/reward_dist Mean              0.141079\n",
      "evaluation/env_infos/final/reward_dist Std               0.187026\n",
      "evaluation/env_infos/final/reward_dist Max               0.761096\n",
      "evaluation/env_infos/final/reward_dist Min               3.85985e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00777009\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0226754\n",
      "evaluation/env_infos/initial/reward_dist Max             0.130532\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97142e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.15891\n",
      "evaluation/env_infos/reward_dist Std                     0.235335\n",
      "evaluation/env_infos/reward_dist Max                     0.996795\n",
      "evaluation/env_infos/reward_dist Min                     3.85985e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.034279\n",
      "evaluation/env_infos/final/reward_energy Std             0.0302135\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00208216\n",
      "evaluation/env_infos/final/reward_energy Min            -0.119497\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.247821\n",
      "evaluation/env_infos/initial/reward_energy Std           0.26772\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0184779\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.03769\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0577981\n",
      "evaluation/env_infos/reward_energy Std                   0.0855618\n",
      "evaluation/env_infos/reward_energy Max                  -0.00174582\n",
      "evaluation/env_infos/reward_energy Min                  -1.03769\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0699772\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.232147\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.582343\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.676979\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00401759\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122564\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0299662\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0410663\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0394361\n",
      "evaluation/env_infos/end_effector_loc Std                0.151999\n",
      "evaluation/env_infos/end_effector_loc Max                0.582343\n",
      "evaluation/env_infos/end_effector_loc Min               -0.676979\n",
      "time/data storing (s)                                    0.00296172\n",
      "time/evaluation sampling (s)                             0.931534\n",
      "time/exploration sampling (s)                            0.120119\n",
      "time/logging (s)                                         0.0254115\n",
      "time/saving (s)                                          0.031027\n",
      "time/training (s)                                       48.6218\n",
      "time/epoch (s)                                          49.7328\n",
      "time/total (s)                                        3650.27\n",
      "Epoch                                                   73\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:26:39.423571 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 74 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000568868\n",
      "trainer/QF2 Loss                                         0.000566029\n",
      "trainer/Policy Loss                                      3.06203\n",
      "trainer/Q1 Predictions Mean                             -1.13993\n",
      "trainer/Q1 Predictions Std                               0.822353\n",
      "trainer/Q1 Predictions Max                               0.853925\n",
      "trainer/Q1 Predictions Min                              -3.6003\n",
      "trainer/Q2 Predictions Mean                             -1.14077\n",
      "trainer/Q2 Predictions Std                               0.821583\n",
      "trainer/Q2 Predictions Max                               0.848635\n",
      "trainer/Q2 Predictions Min                              -3.58567\n",
      "trainer/Q Targets Mean                                  -1.14386\n",
      "trainer/Q Targets Std                                    0.822877\n",
      "trainer/Q Targets Max                                    0.872227\n",
      "trainer/Q Targets Min                                   -3.59972\n",
      "trainer/Log Pis Mean                                     1.92213\n",
      "trainer/Log Pis Std                                      1.30932\n",
      "trainer/Log Pis Max                                      4.41955\n",
      "trainer/Log Pis Min                                     -2.5893\n",
      "trainer/Policy mu Mean                                   0.0255634\n",
      "trainer/Policy mu Std                                    0.274383\n",
      "trainer/Policy mu Max                                    1.68296\n",
      "trainer/Policy mu Min                                   -1.36368\n",
      "trainer/Policy log std Mean                             -2.29775\n",
      "trainer/Policy log std Std                               0.574466\n",
      "trainer/Policy log std Max                              -0.00674558\n",
      "trainer/Policy log std Min                              -3.21397\n",
      "trainer/Alpha                                            0.0240624\n",
      "trainer/Alpha Loss                                      -0.29023\n",
      "exploration/num steps total                           8500\n",
      "exploration/num paths total                            425\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0767073\n",
      "exploration/Rewards Std                                  0.051329\n",
      "exploration/Rewards Max                                  0.0556786\n",
      "exploration/Rewards Min                                 -0.22084\n",
      "exploration/Returns Mean                                -1.53415\n",
      "exploration/Returns Std                                  0.68672\n",
      "exploration/Returns Max                                 -0.287155\n",
      "exploration/Returns Min                                 -2.25788\n",
      "exploration/Actions Mean                                 0.0105182\n",
      "exploration/Actions Std                                  0.0777301\n",
      "exploration/Actions Max                                  0.22853\n",
      "exploration/Actions Min                                 -0.226932\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.53415\n",
      "exploration/env_infos/final/reward_dist Mean             0.188062\n",
      "exploration/env_infos/final/reward_dist Std              0.340725\n",
      "exploration/env_infos/final/reward_dist Max              0.868174\n",
      "exploration/env_infos/final/reward_dist Min              3.90563e-15\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00814117\n",
      "exploration/env_infos/initial/reward_dist Std            0.00963569\n",
      "exploration/env_infos/initial/reward_dist Max            0.0220829\n",
      "exploration/env_infos/initial/reward_dist Min            1.69474e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.167786\n",
      "exploration/env_infos/reward_dist Std                    0.247747\n",
      "exploration/env_infos/reward_dist Max                    0.983608\n",
      "exploration/env_infos/reward_dist Min                    3.90563e-15\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0947197\n",
      "exploration/env_infos/final/reward_energy Std            0.0452672\n",
      "exploration/env_infos/final/reward_energy Max           -0.0295305\n",
      "exploration/env_infos/final/reward_energy Min           -0.160418\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.113964\n",
      "exploration/env_infos/initial/reward_energy Std          0.053806\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0580438\n",
      "exploration/env_infos/initial/reward_energy Min         -0.17813\n",
      "exploration/env_infos/reward_energy Mean                -0.0968238\n",
      "exploration/env_infos/reward_energy Std                  0.0541327\n",
      "exploration/env_infos/reward_energy Max                 -0.00919411\n",
      "exploration/env_infos/reward_energy Min                 -0.228613\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0937939\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275244\n",
      "exploration/env_infos/final/end_effector_loc Max         0.702439\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.326922\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00190791\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00402661\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00724638\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00735162\n",
      "exploration/env_infos/end_effector_loc Mean              0.0319783\n",
      "exploration/env_infos/end_effector_loc Std               0.152127\n",
      "exploration/env_infos/end_effector_loc Max               0.702439\n",
      "exploration/env_infos/end_effector_loc Min              -0.326922\n",
      "evaluation/num steps total                           75000\n",
      "evaluation/num paths total                            3750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0382309\n",
      "evaluation/Rewards Std                                   0.0651618\n",
      "evaluation/Rewards Max                                   0.15911\n",
      "evaluation/Rewards Min                                  -0.436894\n",
      "evaluation/Returns Mean                                 -0.764619\n",
      "evaluation/Returns Std                                   0.919025\n",
      "evaluation/Returns Max                                   1.37177\n",
      "evaluation/Returns Min                                  -2.54442\n",
      "evaluation/Actions Mean                                  0.00235383\n",
      "evaluation/Actions Std                                   0.0746204\n",
      "evaluation/Actions Max                                   0.616858\n",
      "evaluation/Actions Min                                  -0.757315\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.764619\n",
      "evaluation/env_infos/final/reward_dist Mean              0.210313\n",
      "evaluation/env_infos/final/reward_dist Std               0.276512\n",
      "evaluation/env_infos/final/reward_dist Max               0.905114\n",
      "evaluation/env_infos/final/reward_dist Min               1.18453e-08\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00928486\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165834\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0752171\n",
      "evaluation/env_infos/initial/reward_dist Min             9.89679e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.210817\n",
      "evaluation/env_infos/reward_dist Std                     0.260714\n",
      "evaluation/env_infos/reward_dist Max                     0.99045\n",
      "evaluation/env_infos/reward_dist Min                     1.18453e-08\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0487056\n",
      "evaluation/env_infos/final/reward_energy Std             0.0405916\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00350038\n",
      "evaluation/env_infos/final/reward_energy Min            -0.152252\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.235426\n",
      "evaluation/env_infos/initial/reward_energy Std           0.224121\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00763535\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.996583\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0662758\n",
      "evaluation/env_infos/reward_energy Std                   0.0821888\n",
      "evaluation/env_infos/reward_energy Max                  -0.000972626\n",
      "evaluation/env_infos/reward_energy Min                  -0.996583\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0065735\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.219285\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.42146\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.548808\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000209474\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0114902\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0308429\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0378657\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00121764\n",
      "evaluation/env_infos/end_effector_loc Std                0.148723\n",
      "evaluation/env_infos/end_effector_loc Max                0.42146\n",
      "evaluation/env_infos/end_effector_loc Min               -0.548808\n",
      "time/data storing (s)                                    0.00310561\n",
      "time/evaluation sampling (s)                             1.57959\n",
      "time/exploration sampling (s)                            0.128033\n",
      "time/logging (s)                                         0.0193974\n",
      "time/saving (s)                                          0.0272632\n",
      "time/training (s)                                       49.7453\n",
      "time/epoch (s)                                          51.5027\n",
      "time/total (s)                                        3702.84\n",
      "Epoch                                                   74\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:27:35.010192 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 75 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00109703\n",
      "trainer/QF2 Loss                                         0.000609169\n",
      "trainer/Policy Loss                                      3.14343\n",
      "trainer/Q1 Predictions Mean                             -1.14385\n",
      "trainer/Q1 Predictions Std                               0.768\n",
      "trainer/Q1 Predictions Max                               0.365679\n",
      "trainer/Q1 Predictions Min                              -3.70483\n",
      "trainer/Q2 Predictions Mean                             -1.16125\n",
      "trainer/Q2 Predictions Std                               0.767302\n",
      "trainer/Q2 Predictions Max                               0.354781\n",
      "trainer/Q2 Predictions Min                              -3.72756\n",
      "trainer/Q Targets Mean                                  -1.15883\n",
      "trainer/Q Targets Std                                    0.771834\n",
      "trainer/Q Targets Max                                    0.354594\n",
      "trainer/Q Targets Min                                   -3.7169\n",
      "trainer/Log Pis Mean                                     1.98808\n",
      "trainer/Log Pis Std                                      1.31029\n",
      "trainer/Log Pis Max                                      4.10618\n",
      "trainer/Log Pis Min                                     -3.15809\n",
      "trainer/Policy mu Mean                                   0.041226\n",
      "trainer/Policy mu Std                                    0.284205\n",
      "trainer/Policy mu Max                                    2.04668\n",
      "trainer/Policy mu Min                                   -1.55748\n",
      "trainer/Policy log std Mean                             -2.36995\n",
      "trainer/Policy log std Std                               0.548574\n",
      "trainer/Policy log std Max                              -0.190122\n",
      "trainer/Policy log std Min                              -3.15162\n",
      "trainer/Alpha                                            0.0243954\n",
      "trainer/Alpha Loss                                      -0.0442638\n",
      "exploration/num steps total                           8600\n",
      "exploration/num paths total                            430\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0840368\n",
      "exploration/Rewards Std                                  0.0913895\n",
      "exploration/Rewards Max                                  0.160845\n",
      "exploration/Rewards Min                                 -0.345285\n",
      "exploration/Returns Mean                                -1.68074\n",
      "exploration/Returns Std                                  1.32874\n",
      "exploration/Returns Max                                  0.847767\n",
      "exploration/Returns Min                                 -3.05087\n",
      "exploration/Actions Mean                                -0.0022916\n",
      "exploration/Actions Std                                  0.11744\n",
      "exploration/Actions Max                                  0.419474\n",
      "exploration/Actions Min                                 -0.783506\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.68074\n",
      "exploration/env_infos/final/reward_dist Mean             0.0331423\n",
      "exploration/env_infos/final/reward_dist Std              0.0393288\n",
      "exploration/env_infos/final/reward_dist Max              0.0899947\n",
      "exploration/env_infos/final/reward_dist Min              2.43245e-22\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00425004\n",
      "exploration/env_infos/initial/reward_dist Std            0.00508209\n",
      "exploration/env_infos/initial/reward_dist Max            0.0131382\n",
      "exploration/env_infos/initial/reward_dist Min            1.63794e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.178369\n",
      "exploration/env_infos/reward_dist Std                    0.268962\n",
      "exploration/env_infos/reward_dist Max                    0.99865\n",
      "exploration/env_infos/reward_dist Min                    2.43245e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.134306\n",
      "exploration/env_infos/final/reward_energy Std            0.0501627\n",
      "exploration/env_infos/final/reward_energy Max           -0.0656202\n",
      "exploration/env_infos/final/reward_energy Min           -0.216808\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.280322\n",
      "exploration/env_infos/initial/reward_energy Std          0.331166\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0647969\n",
      "exploration/env_infos/initial/reward_energy Min         -0.937381\n",
      "exploration/env_infos/reward_energy Mean                -0.121262\n",
      "exploration/env_infos/reward_energy Std                  0.113535\n",
      "exploration/env_infos/reward_energy Max                 -0.0136057\n",
      "exploration/env_infos/reward_energy Min                 -0.937381\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0499793\n",
      "exploration/env_infos/final/end_effector_loc Std         0.34763\n",
      "exploration/env_infos/final/end_effector_loc Max         0.747365\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.557843\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00705919\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0136192\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00576906\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0391753\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0256254\n",
      "exploration/env_infos/end_effector_loc Std               0.189447\n",
      "exploration/env_infos/end_effector_loc Max               0.747365\n",
      "exploration/env_infos/end_effector_loc Min              -0.557843\n",
      "evaluation/num steps total                           76000\n",
      "evaluation/num paths total                            3800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0405435\n",
      "evaluation/Rewards Std                                   0.0707536\n",
      "evaluation/Rewards Max                                   0.153829\n",
      "evaluation/Rewards Min                                  -0.388672\n",
      "evaluation/Returns Mean                                 -0.810871\n",
      "evaluation/Returns Std                                   1.05435\n",
      "evaluation/Returns Max                                   1.8556\n",
      "evaluation/Returns Min                                  -3.28564\n",
      "evaluation/Actions Mean                                 -0.00171409\n",
      "evaluation/Actions Std                                   0.0537755\n",
      "evaluation/Actions Max                                   0.501258\n",
      "evaluation/Actions Min                                  -0.538252\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.810871\n",
      "evaluation/env_infos/final/reward_dist Mean              0.215551\n",
      "evaluation/env_infos/final/reward_dist Std               0.298207\n",
      "evaluation/env_infos/final/reward_dist Max               0.940517\n",
      "evaluation/env_infos/final/reward_dist Min               3.53455e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00948148\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0148197\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0562403\n",
      "evaluation/env_infos/initial/reward_dist Min             3.31646e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.212013\n",
      "evaluation/env_infos/reward_dist Std                     0.278361\n",
      "evaluation/env_infos/reward_dist Max                     0.998574\n",
      "evaluation/env_infos/reward_dist Min                     3.53455e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.050862\n",
      "evaluation/env_infos/final/reward_energy Std             0.0749639\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0019198\n",
      "evaluation/env_infos/final/reward_energy Min            -0.490521\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.168348\n",
      "evaluation/env_infos/initial/reward_energy Std           0.136164\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0188881\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.5636\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0505823\n",
      "evaluation/env_infos/reward_energy Std                   0.0568412\n",
      "evaluation/env_infos/reward_energy Max                  -0.000631781\n",
      "evaluation/env_infos/reward_energy Min                  -0.5636\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0422619\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.219297\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.526796\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.477871\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000140212\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00765391\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0250629\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0269126\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0182229\n",
      "evaluation/env_infos/end_effector_loc Std                0.141081\n",
      "evaluation/env_infos/end_effector_loc Max                0.526796\n",
      "evaluation/env_infos/end_effector_loc Min               -0.477871\n",
      "time/data storing (s)                                    0.00308916\n",
      "time/evaluation sampling (s)                             1.18316\n",
      "time/exploration sampling (s)                            0.132621\n",
      "time/logging (s)                                         0.0204407\n",
      "time/saving (s)                                          0.0315581\n",
      "time/training (s)                                       53.1797\n",
      "time/epoch (s)                                          54.5506\n",
      "time/total (s)                                        3758.43\n",
      "Epoch                                                   75\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:28:28.492167 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 76 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000699217\r\n",
      "trainer/QF2 Loss                                         0.000724117\r\n",
      "trainer/Policy Loss                                      3.29609\r\n",
      "trainer/Q1 Predictions Mean                             -1.23935\r\n",
      "trainer/Q1 Predictions Std                               0.85254\r\n",
      "trainer/Q1 Predictions Max                               0.582245\r\n",
      "trainer/Q1 Predictions Min                              -3.36701\r\n",
      "trainer/Q2 Predictions Mean                             -1.25216\r\n",
      "trainer/Q2 Predictions Std                               0.856632\r\n",
      "trainer/Q2 Predictions Max                               0.552655\r\n",
      "trainer/Q2 Predictions Min                              -3.38267\r\n",
      "trainer/Q Targets Mean                                  -1.24821\r\n",
      "trainer/Q Targets Std                                    0.857338\r\n",
      "trainer/Q Targets Max                                    0.545001\r\n",
      "trainer/Q Targets Min                                   -3.39643\r\n",
      "trainer/Log Pis Mean                                     2.05215\r\n",
      "trainer/Log Pis Std                                      1.41557\r\n",
      "trainer/Log Pis Max                                      4.62379\r\n",
      "trainer/Log Pis Min                                     -4.26523\r\n",
      "trainer/Policy mu Mean                                   0.0338589\r\n",
      "trainer/Policy mu Std                                    0.293676\r\n",
      "trainer/Policy mu Max                                    1.99545\r\n",
      "trainer/Policy mu Min                                   -1.66\r\n",
      "trainer/Policy log std Mean                             -2.32103\r\n",
      "trainer/Policy log std Std                               0.624684\r\n",
      "trainer/Policy log std Max                              -0.414215\r\n",
      "trainer/Policy log std Min                              -3.30564\r\n",
      "trainer/Alpha                                            0.0240141\r\n",
      "trainer/Alpha Loss                                       0.194536\r\n",
      "exploration/num steps total                           8700\r\n",
      "exploration/num paths total                            435\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0706069\r\n",
      "exploration/Rewards Std                                  0.0788181\r\n",
      "exploration/Rewards Max                                  0.110892\r\n",
      "exploration/Rewards Min                                 -0.23281\r\n",
      "exploration/Returns Mean                                -1.41214\r\n",
      "exploration/Returns Std                                  1.10277\r\n",
      "exploration/Returns Max                                  0.668489\r\n",
      "exploration/Returns Min                                 -2.38336\r\n",
      "exploration/Actions Mean                                 0.00412315\r\n",
      "exploration/Actions Std                                  0.0893249\r\n",
      "exploration/Actions Max                                  0.24145\r\n",
      "exploration/Actions Min                                 -0.27714\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.41214\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.226044\r\n",
      "exploration/env_infos/final/reward_dist Std              0.313037\r\n",
      "exploration/env_infos/final/reward_dist Max              0.840062\r\n",
      "exploration/env_infos/final/reward_dist Min              0.00120318\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00437537\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00825455\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0208673\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.81296e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.211182\r\n",
      "exploration/env_infos/reward_dist Std                    0.2539\r\n",
      "exploration/env_infos/reward_dist Max                    0.950794\r\n",
      "exploration/env_infos/reward_dist Min                    1.81296e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.181793\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0267401\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.145712\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.210818\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.13447\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.074452\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0531016\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.254803\r\n",
      "exploration/env_infos/reward_energy Mean                -0.108633\r\n",
      "exploration/env_infos/reward_energy Std                  0.0647361\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0143442\r\n",
      "exploration/env_infos/reward_energy Min                 -0.387866\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0169698\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.234812\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.466491\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.233426\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000682583\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00539127\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0120725\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00807097\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00419939\r\n",
      "exploration/env_infos/end_effector_loc Std               0.145484\r\n",
      "exploration/env_infos/end_effector_loc Max               0.466491\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.233426\r\n",
      "evaluation/num steps total                           77000\r\n",
      "evaluation/num paths total                            3850\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0459767\r\n",
      "evaluation/Rewards Std                                   0.0657171\r\n",
      "evaluation/Rewards Max                                   0.112751\r\n",
      "evaluation/Rewards Min                                  -0.474306\r\n",
      "evaluation/Returns Mean                                 -0.919534\r\n",
      "evaluation/Returns Std                                   0.918455\r\n",
      "evaluation/Returns Max                                   0.768208\r\n",
      "evaluation/Returns Min                                  -3.58232\r\n",
      "evaluation/Actions Mean                                 -0.0012804\r\n",
      "evaluation/Actions Std                                   0.0665048\r\n",
      "evaluation/Actions Max                                   0.472785\r\n",
      "evaluation/Actions Min                                  -0.673214\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.919534\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.151071\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.222921\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.732301\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.34969e-14\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00692078\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165045\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.104163\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0071e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.174642\r\n",
      "evaluation/env_infos/reward_dist Std                     0.258021\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996708\r\n",
      "evaluation/env_infos/reward_dist Min                     4.34969e-14\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0371784\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0359974\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00222343\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.165411\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.19597\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.192388\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0299668\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.810293\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0572625\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0746328\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00107476\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.810293\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0387727\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.245579\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.415993\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.630214\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0017826\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00954433\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0236392\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0336607\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0241056\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.158245\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.415993\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.630214\r\n",
      "time/data storing (s)                                    0.00308064\r\n",
      "time/evaluation sampling (s)                             1.04852\r\n",
      "time/exploration sampling (s)                            0.142672\r\n",
      "time/logging (s)                                         0.0196173\r\n",
      "time/saving (s)                                          0.0290098\r\n",
      "time/training (s)                                       51.1101\r\n",
      "time/epoch (s)                                          52.353\r\n",
      "time/total (s)                                        3811.9\r\n",
      "Epoch                                                   76\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:29:20.805547 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 77 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000839078\n",
      "trainer/QF2 Loss                                         0.000786215\n",
      "trainer/Policy Loss                                      3.00661\n",
      "trainer/Q1 Predictions Mean                             -1.02398\n",
      "trainer/Q1 Predictions Std                               0.799134\n",
      "trainer/Q1 Predictions Max                               0.4346\n",
      "trainer/Q1 Predictions Min                              -3.49235\n",
      "trainer/Q2 Predictions Mean                             -1.02236\n",
      "trainer/Q2 Predictions Std                               0.795281\n",
      "trainer/Q2 Predictions Max                               0.420096\n",
      "trainer/Q2 Predictions Min                              -3.49057\n",
      "trainer/Q Targets Mean                                  -1.01386\n",
      "trainer/Q Targets Std                                    0.795108\n",
      "trainer/Q Targets Max                                    0.440746\n",
      "trainer/Q Targets Min                                   -3.4655\n",
      "trainer/Log Pis Mean                                     1.98253\n",
      "trainer/Log Pis Std                                      1.32034\n",
      "trainer/Log Pis Max                                      4.11892\n",
      "trainer/Log Pis Min                                     -2.77286\n",
      "trainer/Policy mu Mean                                   0.0152095\n",
      "trainer/Policy mu Std                                    0.193015\n",
      "trainer/Policy mu Max                                    1.3738\n",
      "trainer/Policy mu Min                                   -1.21425\n",
      "trainer/Policy log std Mean                             -2.36466\n",
      "trainer/Policy log std Std                               0.540161\n",
      "trainer/Policy log std Max                               0.159771\n",
      "trainer/Policy log std Min                              -3.19223\n",
      "trainer/Alpha                                            0.024106\n",
      "trainer/Alpha Loss                                      -0.0651017\n",
      "exploration/num steps total                           8800\n",
      "exploration/num paths total                            440\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0572293\n",
      "exploration/Rewards Std                                  0.0767772\n",
      "exploration/Rewards Max                                  0.110654\n",
      "exploration/Rewards Min                                 -0.27328\n",
      "exploration/Returns Mean                                -1.14459\n",
      "exploration/Returns Std                                  1.2199\n",
      "exploration/Returns Max                                  0.964196\n",
      "exploration/Returns Min                                 -2.55233\n",
      "exploration/Actions Mean                                 0.00590121\n",
      "exploration/Actions Std                                  0.200684\n",
      "exploration/Actions Max                                  0.581985\n",
      "exploration/Actions Min                                 -0.914668\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.14459\n",
      "exploration/env_infos/final/reward_dist Mean             0.00433651\n",
      "exploration/env_infos/final/reward_dist Std              0.0046994\n",
      "exploration/env_infos/final/reward_dist Max              0.0128666\n",
      "exploration/env_infos/final/reward_dist Min              1.22444e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000898927\n",
      "exploration/env_infos/initial/reward_dist Std            0.000829877\n",
      "exploration/env_infos/initial/reward_dist Max            0.00219669\n",
      "exploration/env_infos/initial/reward_dist Min            4.14857e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.113666\n",
      "exploration/env_infos/reward_dist Std                    0.180575\n",
      "exploration/env_infos/reward_dist Max                    0.895583\n",
      "exploration/env_infos/reward_dist Min                    1.87166e-09\n",
      "exploration/env_infos/final/reward_energy Mean          -0.141727\n",
      "exploration/env_infos/final/reward_energy Std            0.0803665\n",
      "exploration/env_infos/final/reward_energy Max           -0.0576639\n",
      "exploration/env_infos/final/reward_energy Min           -0.264657\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.38672\n",
      "exploration/env_infos/initial/reward_energy Std          0.285667\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0853373\n",
      "exploration/env_infos/initial/reward_energy Min         -0.728782\n",
      "exploration/env_infos/reward_energy Mean                -0.216293\n",
      "exploration/env_infos/reward_energy Std                  0.183942\n",
      "exploration/env_infos/reward_energy Max                 -0.00853861\n",
      "exploration/env_infos/reward_energy Min                 -1.00752\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0346721\n",
      "exploration/env_infos/final/end_effector_loc Std         0.251208\n",
      "exploration/env_infos/final/end_effector_loc Max         0.305851\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.531605\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00412069\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0164914\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0287606\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0355473\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0271409\n",
      "exploration/env_infos/end_effector_loc Std               0.184134\n",
      "exploration/env_infos/end_effector_loc Max               0.305851\n",
      "exploration/env_infos/end_effector_loc Min              -0.531605\n",
      "evaluation/num steps total                           78000\n",
      "evaluation/num paths total                            3900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0546061\n",
      "evaluation/Rewards Std                                   0.0783644\n",
      "evaluation/Rewards Max                                   0.153748\n",
      "evaluation/Rewards Min                                  -0.565301\n",
      "evaluation/Returns Mean                                 -1.09212\n",
      "evaluation/Returns Std                                   1.07089\n",
      "evaluation/Returns Max                                   1.73441\n",
      "evaluation/Returns Min                                  -4.19288\n",
      "evaluation/Actions Mean                                  0.00436934\n",
      "evaluation/Actions Std                                   0.0662377\n",
      "evaluation/Actions Max                                   0.3883\n",
      "evaluation/Actions Min                                  -0.558574\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.09212\n",
      "evaluation/env_infos/final/reward_dist Mean              0.139764\n",
      "evaluation/env_infos/final/reward_dist Std               0.252627\n",
      "evaluation/env_infos/final/reward_dist Max               0.954868\n",
      "evaluation/env_infos/final/reward_dist Min               3.87648e-39\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00779978\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0180695\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0933218\n",
      "evaluation/env_infos/initial/reward_dist Min             9.96933e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.158876\n",
      "evaluation/env_infos/reward_dist Std                     0.253578\n",
      "evaluation/env_infos/reward_dist Max                     0.995081\n",
      "evaluation/env_infos/reward_dist Min                     1.18593e-39\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0523407\n",
      "evaluation/env_infos/final/reward_energy Std             0.0610357\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00225912\n",
      "evaluation/env_infos/final/reward_energy Min            -0.349962\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.167176\n",
      "evaluation/env_infos/initial/reward_energy Std           0.165676\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0110628\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.680294\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0606432\n",
      "evaluation/env_infos/reward_energy Std                   0.0716621\n",
      "evaluation/env_infos/reward_energy Max                  -0.000703361\n",
      "evaluation/env_infos/reward_energy Min                  -0.680294\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0104652\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.277402\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.713604\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00111212\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00824672\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.019415\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0279287\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00482812\n",
      "evaluation/env_infos/end_effector_loc Std                0.171087\n",
      "evaluation/env_infos/end_effector_loc Max                0.713604\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.0029806\n",
      "time/evaluation sampling (s)                             0.968922\n",
      "time/exploration sampling (s)                            0.126479\n",
      "time/logging (s)                                         0.0215762\n",
      "time/saving (s)                                          0.0296224\n",
      "time/training (s)                                       50.1517\n",
      "time/epoch (s)                                          51.3013\n",
      "time/total (s)                                        3864.22\n",
      "Epoch                                                   77\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:30:13.577452 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 78 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000702262\n",
      "trainer/QF2 Loss                                         0.00057913\n",
      "trainer/Policy Loss                                      3.18069\n",
      "trainer/Q1 Predictions Mean                             -1.15093\n",
      "trainer/Q1 Predictions Std                               0.819447\n",
      "trainer/Q1 Predictions Max                               0.535822\n",
      "trainer/Q1 Predictions Min                              -3.45505\n",
      "trainer/Q2 Predictions Mean                             -1.15526\n",
      "trainer/Q2 Predictions Std                               0.818222\n",
      "trainer/Q2 Predictions Max                               0.53946\n",
      "trainer/Q2 Predictions Min                              -3.45605\n",
      "trainer/Q Targets Mean                                  -1.15542\n",
      "trainer/Q Targets Std                                    0.823061\n",
      "trainer/Q Targets Max                                    0.517948\n",
      "trainer/Q Targets Min                                   -3.49687\n",
      "trainer/Log Pis Mean                                     2.03592\n",
      "trainer/Log Pis Std                                      1.25414\n",
      "trainer/Log Pis Max                                      4.27327\n",
      "trainer/Log Pis Min                                     -3.78618\n",
      "trainer/Policy mu Mean                                   0.0466617\n",
      "trainer/Policy mu Std                                    0.291193\n",
      "trainer/Policy mu Max                                    2.19756\n",
      "trainer/Policy mu Min                                   -1.83475\n",
      "trainer/Policy log std Mean                             -2.33277\n",
      "trainer/Policy log std Std                               0.549802\n",
      "trainer/Policy log std Max                              -0.148577\n",
      "trainer/Policy log std Min                              -3.18623\n",
      "trainer/Alpha                                            0.024881\n",
      "trainer/Alpha Loss                                       0.132717\n",
      "exploration/num steps total                           8900\n",
      "exploration/num paths total                            445\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0776395\n",
      "exploration/Rewards Std                                  0.0922157\n",
      "exploration/Rewards Max                                  0.139795\n",
      "exploration/Rewards Min                                 -0.236431\n",
      "exploration/Returns Mean                                -1.55279\n",
      "exploration/Returns Std                                  1.58656\n",
      "exploration/Returns Max                                  0.358998\n",
      "exploration/Returns Min                                 -3.31004\n",
      "exploration/Actions Mean                                 0.00371972\n",
      "exploration/Actions Std                                  0.204237\n",
      "exploration/Actions Max                                  0.652673\n",
      "exploration/Actions Min                                 -0.936933\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.55279\n",
      "exploration/env_infos/final/reward_dist Mean             0.180429\n",
      "exploration/env_infos/final/reward_dist Std              0.307033\n",
      "exploration/env_infos/final/reward_dist Max              0.793662\n",
      "exploration/env_infos/final/reward_dist Min              0.000534932\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00302532\n",
      "exploration/env_infos/initial/reward_dist Std            0.00410415\n",
      "exploration/env_infos/initial/reward_dist Max            0.0109659\n",
      "exploration/env_infos/initial/reward_dist Min            1.45446e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.166083\n",
      "exploration/env_infos/reward_dist Std                    0.264226\n",
      "exploration/env_infos/reward_dist Max                    0.964446\n",
      "exploration/env_infos/reward_dist Min                    7.11653e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.102476\n",
      "exploration/env_infos/final/reward_energy Std            0.0584363\n",
      "exploration/env_infos/final/reward_energy Max           -0.0197578\n",
      "exploration/env_infos/final/reward_energy Min           -0.19336\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.436664\n",
      "exploration/env_infos/initial/reward_energy Std          0.426903\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0606459\n",
      "exploration/env_infos/initial/reward_energy Min         -1.00639\n",
      "exploration/env_infos/reward_energy Mean                -0.209823\n",
      "exploration/env_infos/reward_energy Std                  0.198564\n",
      "exploration/env_infos/reward_energy Max                 -0.01339\n",
      "exploration/env_infos/reward_energy Min                 -1.00639\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0420519\n",
      "exploration/env_infos/final/end_effector_loc Std         0.197908\n",
      "exploration/env_infos/final/end_effector_loc Max         0.239627\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.321416\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0121834\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0178247\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00459797\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0468467\n",
      "exploration/env_infos/end_effector_loc Mean             -0.058267\n",
      "exploration/env_infos/end_effector_loc Std               0.15824\n",
      "exploration/env_infos/end_effector_loc Max               0.239627\n",
      "exploration/env_infos/end_effector_loc Min              -0.45703\n",
      "evaluation/num steps total                           79000\n",
      "evaluation/num paths total                            3950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.050001\n",
      "evaluation/Rewards Std                                   0.0708965\n",
      "evaluation/Rewards Max                                   0.137244\n",
      "evaluation/Rewards Min                                  -0.373947\n",
      "evaluation/Returns Mean                                 -1.00002\n",
      "evaluation/Returns Std                                   1.06671\n",
      "evaluation/Returns Max                                   1.70195\n",
      "evaluation/Returns Min                                  -3.74396\n",
      "evaluation/Actions Mean                                  0.00160698\n",
      "evaluation/Actions Std                                   0.0748365\n",
      "evaluation/Actions Max                                   0.498003\n",
      "evaluation/Actions Min                                  -0.70241\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.00002\n",
      "evaluation/env_infos/final/reward_dist Mean              0.197982\n",
      "evaluation/env_infos/final/reward_dist Std               0.276561\n",
      "evaluation/env_infos/final/reward_dist Max               0.981777\n",
      "evaluation/env_infos/final/reward_dist Min               8.85074e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00678391\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0105987\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0365472\n",
      "evaluation/env_infos/initial/reward_dist Min             1.44131e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.191003\n",
      "evaluation/env_infos/reward_dist Std                     0.275413\n",
      "evaluation/env_infos/reward_dist Max                     0.991446\n",
      "evaluation/env_infos/reward_dist Min                     8.85074e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0408598\n",
      "evaluation/env_infos/final/reward_energy Std             0.0498846\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00313898\n",
      "evaluation/env_infos/final/reward_energy Min            -0.260334\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.229876\n",
      "evaluation/env_infos/initial/reward_energy Std           0.233432\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0068349\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.950922\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0650972\n",
      "evaluation/env_infos/reward_energy Std                   0.0834776\n",
      "evaluation/env_infos/reward_energy Max                  -0.00113963\n",
      "evaluation/env_infos/reward_energy Min                  -0.950922\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0176422\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.212489\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.461251\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.718104\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00263548\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0112792\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0249002\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0351205\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0187645\n",
      "evaluation/env_infos/end_effector_loc Std                0.144023\n",
      "evaluation/env_infos/end_effector_loc Max                0.461251\n",
      "evaluation/env_infos/end_effector_loc Min               -0.718104\n",
      "time/data storing (s)                                    0.00289643\n",
      "time/evaluation sampling (s)                             0.945086\n",
      "time/exploration sampling (s)                            0.128266\n",
      "time/logging (s)                                         0.0200638\n",
      "time/saving (s)                                          0.0277922\n",
      "time/training (s)                                       50.6125\n",
      "time/epoch (s)                                          51.7366\n",
      "time/total (s)                                        3916.99\n",
      "Epoch                                                   78\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:31:05.077334 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 79 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000796271\n",
      "trainer/QF2 Loss                                         0.000755338\n",
      "trainer/Policy Loss                                      3.13744\n",
      "trainer/Q1 Predictions Mean                             -1.21219\n",
      "trainer/Q1 Predictions Std                               0.947012\n",
      "trainer/Q1 Predictions Max                               0.445473\n",
      "trainer/Q1 Predictions Min                              -4.00425\n",
      "trainer/Q2 Predictions Mean                             -1.21542\n",
      "trainer/Q2 Predictions Std                               0.947659\n",
      "trainer/Q2 Predictions Max                               0.439016\n",
      "trainer/Q2 Predictions Min                              -3.99707\n",
      "trainer/Q Targets Mean                                  -1.21259\n",
      "trainer/Q Targets Std                                    0.94569\n",
      "trainer/Q Targets Max                                    0.435708\n",
      "trainer/Q Targets Min                                   -3.96539\n",
      "trainer/Log Pis Mean                                     1.92253\n",
      "trainer/Log Pis Std                                      1.34187\n",
      "trainer/Log Pis Max                                      4.23194\n",
      "trainer/Log Pis Min                                     -2.98636\n",
      "trainer/Policy mu Mean                                   0.018632\n",
      "trainer/Policy mu Std                                    0.295609\n",
      "trainer/Policy mu Max                                    1.84593\n",
      "trainer/Policy mu Min                                   -1.89781\n",
      "trainer/Policy log std Mean                             -2.30487\n",
      "trainer/Policy log std Std                               0.602611\n",
      "trainer/Policy log std Max                              -0.150855\n",
      "trainer/Policy log std Min                              -3.18923\n",
      "trainer/Alpha                                            0.023201\n",
      "trainer/Alpha Loss                                      -0.291532\n",
      "exploration/num steps total                           9000\n",
      "exploration/num paths total                            450\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0474926\n",
      "exploration/Rewards Std                                  0.0917056\n",
      "exploration/Rewards Max                                  0.162367\n",
      "exploration/Rewards Min                                 -0.228052\n",
      "exploration/Returns Mean                                -0.949851\n",
      "exploration/Returns Std                                  1.64756\n",
      "exploration/Returns Max                                  2.18351\n",
      "exploration/Returns Min                                 -2.34786\n",
      "exploration/Actions Mean                                 0.00446668\n",
      "exploration/Actions Std                                  0.117659\n",
      "exploration/Actions Max                                  0.461918\n",
      "exploration/Actions Min                                 -0.511791\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.949851\n",
      "exploration/env_infos/final/reward_dist Mean             0.209684\n",
      "exploration/env_infos/final/reward_dist Std              0.375911\n",
      "exploration/env_infos/final/reward_dist Max              0.95834\n",
      "exploration/env_infos/final/reward_dist Min              1.36101e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000830026\n",
      "exploration/env_infos/initial/reward_dist Std            0.000582802\n",
      "exploration/env_infos/initial/reward_dist Max            0.00155427\n",
      "exploration/env_infos/initial/reward_dist Min            0.000134896\n",
      "exploration/env_infos/reward_dist Mean                   0.209197\n",
      "exploration/env_infos/reward_dist Std                    0.32168\n",
      "exploration/env_infos/reward_dist Max                    0.99992\n",
      "exploration/env_infos/reward_dist Min                    1.36101e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.105401\n",
      "exploration/env_infos/final/reward_energy Std            0.0727751\n",
      "exploration/env_infos/final/reward_energy Max           -0.0282466\n",
      "exploration/env_infos/final/reward_energy Min           -0.200376\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.156144\n",
      "exploration/env_infos/initial/reward_energy Std          0.0860235\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0617517\n",
      "exploration/env_infos/initial/reward_energy Min         -0.285902\n",
      "exploration/env_infos/reward_energy Mean                -0.122898\n",
      "exploration/env_infos/reward_energy Std                  0.112353\n",
      "exploration/env_infos/reward_energy Max                 -0.00229004\n",
      "exploration/env_infos/reward_energy Min                 -0.580878\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0550092\n",
      "exploration/env_infos/final/end_effector_loc Std         0.233881\n",
      "exploration/env_infos/final/end_effector_loc Max         0.53215\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.307795\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000190916\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00629998\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0112868\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0107924\n",
      "exploration/env_infos/end_effector_loc Mean              0.0261026\n",
      "exploration/env_infos/end_effector_loc Std               0.138634\n",
      "exploration/env_infos/end_effector_loc Max               0.53215\n",
      "exploration/env_infos/end_effector_loc Min              -0.307795\n",
      "evaluation/num steps total                           80000\n",
      "evaluation/num paths total                            4000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0525616\n",
      "evaluation/Rewards Std                                   0.0701012\n",
      "evaluation/Rewards Max                                   0.13698\n",
      "evaluation/Rewards Min                                  -0.367176\n",
      "evaluation/Returns Mean                                 -1.05123\n",
      "evaluation/Returns Std                                   1.0651\n",
      "evaluation/Returns Max                                   1.33051\n",
      "evaluation/Returns Min                                  -3.017\n",
      "evaluation/Actions Mean                                  0.00341609\n",
      "evaluation/Actions Std                                   0.0832639\n",
      "evaluation/Actions Max                                   0.776508\n",
      "evaluation/Actions Min                                  -0.818485\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.05123\n",
      "evaluation/env_infos/final/reward_dist Mean              0.155782\n",
      "evaluation/env_infos/final/reward_dist Std               0.25991\n",
      "evaluation/env_infos/final/reward_dist Max               0.981161\n",
      "evaluation/env_infos/final/reward_dist Min               8.88201e-46\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00572943\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0114806\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0526692\n",
      "evaluation/env_infos/initial/reward_dist Min             9.41027e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.146851\n",
      "evaluation/env_infos/reward_dist Std                     0.23456\n",
      "evaluation/env_infos/reward_dist Max                     0.992819\n",
      "evaluation/env_infos/reward_dist Min                     8.88201e-46\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0610072\n",
      "evaluation/env_infos/final/reward_energy Std             0.0681232\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000749923\n",
      "evaluation/env_infos/final/reward_energy Min            -0.424836\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.236879\n",
      "evaluation/env_infos/initial/reward_energy Std           0.235126\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0182627\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.06816\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0655236\n",
      "evaluation/env_infos/reward_energy Std                   0.097958\n",
      "evaluation/env_infos/reward_energy Max                  -0.000749923\n",
      "evaluation/env_infos/reward_energy Min                  -1.06816\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00957148\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.256363\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.954862\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.603625\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00204662\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116214\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0304915\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0409243\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0145459\n",
      "evaluation/env_infos/end_effector_loc Std                0.165964\n",
      "evaluation/env_infos/end_effector_loc Max                0.954862\n",
      "evaluation/env_infos/end_effector_loc Min               -0.603625\n",
      "time/data storing (s)                                    0.00303468\n",
      "time/evaluation sampling (s)                             1.02708\n",
      "time/exploration sampling (s)                            0.119945\n",
      "time/logging (s)                                         0.0201309\n",
      "time/saving (s)                                          0.0267632\n",
      "time/training (s)                                       49.2097\n",
      "time/epoch (s)                                          50.4067\n",
      "time/total (s)                                        3968.48\n",
      "Epoch                                                   79\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:31:57.878900 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 80 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000679713\r\n",
      "trainer/QF2 Loss                                         0.000766436\r\n",
      "trainer/Policy Loss                                      3.1449\r\n",
      "trainer/Q1 Predictions Mean                             -1.15067\r\n",
      "trainer/Q1 Predictions Std                               0.875893\r\n",
      "trainer/Q1 Predictions Max                               0.573801\r\n",
      "trainer/Q1 Predictions Min                              -3.86465\r\n",
      "trainer/Q2 Predictions Mean                             -1.15048\r\n",
      "trainer/Q2 Predictions Std                               0.875497\r\n",
      "trainer/Q2 Predictions Max                               0.579091\r\n",
      "trainer/Q2 Predictions Min                              -3.84228\r\n",
      "trainer/Q Targets Mean                                  -1.14899\r\n",
      "trainer/Q Targets Std                                    0.871182\r\n",
      "trainer/Q Targets Max                                    0.576972\r\n",
      "trainer/Q Targets Min                                   -3.82047\r\n",
      "trainer/Log Pis Mean                                     1.99743\r\n",
      "trainer/Log Pis Std                                      1.34482\r\n",
      "trainer/Log Pis Max                                      4.17201\r\n",
      "trainer/Log Pis Min                                     -2.50222\r\n",
      "trainer/Policy mu Mean                                   0.0143022\r\n",
      "trainer/Policy mu Std                                    0.265359\r\n",
      "trainer/Policy mu Max                                    1.79566\r\n",
      "trainer/Policy mu Min                                   -1.50122\r\n",
      "trainer/Policy log std Mean                             -2.35807\r\n",
      "trainer/Policy log std Std                               0.556564\r\n",
      "trainer/Policy log std Max                              -0.26023\r\n",
      "trainer/Policy log std Min                              -3.18018\r\n",
      "trainer/Alpha                                            0.023244\r\n",
      "trainer/Alpha Loss                                      -0.0096694\r\n",
      "exploration/num steps total                           9100\r\n",
      "exploration/num paths total                            455\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0730276\r\n",
      "exploration/Rewards Std                                  0.0787583\r\n",
      "exploration/Rewards Max                                  0.0986001\r\n",
      "exploration/Rewards Min                                 -0.222559\r\n",
      "exploration/Returns Mean                                -1.46055\r\n",
      "exploration/Returns Std                                  1.27688\r\n",
      "exploration/Returns Max                                  0.632581\r\n",
      "exploration/Returns Min                                 -3.16492\r\n",
      "exploration/Actions Mean                                -0.00182581\r\n",
      "exploration/Actions Std                                  0.133262\r\n",
      "exploration/Actions Max                                  0.582742\r\n",
      "exploration/Actions Min                                 -0.632351\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.46055\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0970719\r\n",
      "exploration/env_infos/final/reward_dist Std              0.1726\r\n",
      "exploration/env_infos/final/reward_dist Max              0.440554\r\n",
      "exploration/env_infos/final/reward_dist Min              1.0324e-11\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0179606\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0248005\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0633179\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.02383e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.138082\r\n",
      "exploration/env_infos/reward_dist Std                    0.202126\r\n",
      "exploration/env_infos/reward_dist Max                    0.815419\r\n",
      "exploration/env_infos/reward_dist Min                    1.0324e-11\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.125562\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0990056\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0231003\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.304492\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.291661\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.285577\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0290967\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.670648\r\n",
      "exploration/env_infos/reward_energy Mean                -0.135668\r\n",
      "exploration/env_infos/reward_energy Std                  0.130836\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00738137\r\n",
      "exploration/env_infos/reward_energy Min                 -0.670648\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00142423\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.223778\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.344043\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.459281\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00092868\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0144018\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0291371\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0316176\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00646764\r\n",
      "exploration/env_infos/end_effector_loc Std               0.133524\r\n",
      "exploration/env_infos/end_effector_loc Max               0.347011\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.459281\r\n",
      "evaluation/num steps total                           81000\r\n",
      "evaluation/num paths total                            4050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0664632\r\n",
      "evaluation/Rewards Std                                   0.0829162\r\n",
      "evaluation/Rewards Max                                   0.0952115\r\n",
      "evaluation/Rewards Min                                  -0.751546\r\n",
      "evaluation/Returns Mean                                 -1.32926\r\n",
      "evaluation/Returns Std                                   1.15436\r\n",
      "evaluation/Returns Max                                   0.812738\r\n",
      "evaluation/Returns Min                                  -4.04634\r\n",
      "evaluation/Actions Mean                                  0.00422942\r\n",
      "evaluation/Actions Std                                   0.0974431\r\n",
      "evaluation/Actions Max                                   0.751401\r\n",
      "evaluation/Actions Min                                  -0.96724\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.32926\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.206629\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.282226\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.947758\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.08859e-53\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0115702\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0250304\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.112165\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.32712e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.149051\r\n",
      "evaluation/env_infos/reward_dist Std                     0.246926\r\n",
      "evaluation/env_infos/reward_dist Max                     0.99623\r\n",
      "evaluation/env_infos/reward_dist Min                     9.08859e-53\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0554786\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.119005\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00231332\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.838819\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.294981\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.308288\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00978314\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.09223\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0732416\r\n",
      "evaluation/env_infos/reward_energy Std                   0.116883\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00064512\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.09223\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0244347\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.281825\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.66502\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0051289\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0141867\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0211887\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.048362\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.032526\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.175653\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.66502\r\n",
      "time/data storing (s)                                    0.00306501\r\n",
      "time/evaluation sampling (s)                             1.13164\r\n",
      "time/exploration sampling (s)                            0.124464\r\n",
      "time/logging (s)                                         0.0202948\r\n",
      "time/saving (s)                                          0.0288522\r\n",
      "time/training (s)                                       50.3623\r\n",
      "time/epoch (s)                                          51.6706\r\n",
      "time/total (s)                                        4021.28\r\n",
      "Epoch                                                   80\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:32:50.326417 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 81 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000501914\n",
      "trainer/QF2 Loss                                         0.000812817\n",
      "trainer/Policy Loss                                      3.27369\n",
      "trainer/Q1 Predictions Mean                             -1.18551\n",
      "trainer/Q1 Predictions Std                               0.89019\n",
      "trainer/Q1 Predictions Max                               0.300216\n",
      "trainer/Q1 Predictions Min                              -3.56582\n",
      "trainer/Q2 Predictions Mean                             -1.18698\n",
      "trainer/Q2 Predictions Std                               0.889841\n",
      "trainer/Q2 Predictions Max                               0.270419\n",
      "trainer/Q2 Predictions Min                              -3.57703\n",
      "trainer/Q Targets Mean                                  -1.18013\n",
      "trainer/Q Targets Std                                    0.892682\n",
      "trainer/Q Targets Max                                    0.274576\n",
      "trainer/Q Targets Min                                   -3.5356\n",
      "trainer/Log Pis Mean                                     2.0906\n",
      "trainer/Log Pis Std                                      1.40047\n",
      "trainer/Log Pis Max                                      4.41417\n",
      "trainer/Log Pis Min                                     -3.86845\n",
      "trainer/Policy mu Mean                                   0.0195433\n",
      "trainer/Policy mu Std                                    0.262279\n",
      "trainer/Policy mu Max                                    1.68268\n",
      "trainer/Policy mu Min                                   -1.74498\n",
      "trainer/Policy log std Mean                             -2.40118\n",
      "trainer/Policy log std Std                               0.587813\n",
      "trainer/Policy log std Max                              -0.0769494\n",
      "trainer/Policy log std Min                              -3.2624\n",
      "trainer/Alpha                                            0.023733\n",
      "trainer/Alpha Loss                                       0.338905\n",
      "exploration/num steps total                           9200\n",
      "exploration/num paths total                            460\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0937772\n",
      "exploration/Rewards Std                                  0.0677251\n",
      "exploration/Rewards Max                                  0.121206\n",
      "exploration/Rewards Min                                 -0.332113\n",
      "exploration/Returns Mean                                -1.87554\n",
      "exploration/Returns Std                                  0.765732\n",
      "exploration/Returns Max                                 -0.679487\n",
      "exploration/Returns Min                                 -2.98573\n",
      "exploration/Actions Mean                                 0.000957297\n",
      "exploration/Actions Std                                  0.103705\n",
      "exploration/Actions Max                                  0.365997\n",
      "exploration/Actions Min                                 -0.539242\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.87554\n",
      "exploration/env_infos/final/reward_dist Mean             0.165378\n",
      "exploration/env_infos/final/reward_dist Std              0.323246\n",
      "exploration/env_infos/final/reward_dist Max              0.811792\n",
      "exploration/env_infos/final/reward_dist Min              2.70561e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0104887\n",
      "exploration/env_infos/initial/reward_dist Std            0.0130221\n",
      "exploration/env_infos/initial/reward_dist Max            0.032101\n",
      "exploration/env_infos/initial/reward_dist Min            9.66414e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.136261\n",
      "exploration/env_infos/reward_dist Std                    0.25204\n",
      "exploration/env_infos/reward_dist Max                    0.963598\n",
      "exploration/env_infos/reward_dist Min                    2.70561e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.130591\n",
      "exploration/env_infos/final/reward_energy Std            0.0764548\n",
      "exploration/env_infos/final/reward_energy Max           -0.0331466\n",
      "exploration/env_infos/final/reward_energy Min           -0.266098\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.183905\n",
      "exploration/env_infos/initial/reward_energy Std          0.181686\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0421181\n",
      "exploration/env_infos/initial/reward_energy Min         -0.542123\n",
      "exploration/env_infos/reward_energy Mean                -0.112348\n",
      "exploration/env_infos/reward_energy Std                  0.0942817\n",
      "exploration/env_infos/reward_energy Max                 -0.00555508\n",
      "exploration/env_infos/reward_energy Min                 -0.542123\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0360206\n",
      "exploration/env_infos/final/end_effector_loc Std         0.237847\n",
      "exploration/env_infos/final/end_effector_loc Max         0.314664\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.347579\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00416952\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0081335\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00487872\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0269621\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0326138\n",
      "exploration/env_infos/end_effector_loc Std               0.128067\n",
      "exploration/env_infos/end_effector_loc Max               0.314664\n",
      "exploration/env_infos/end_effector_loc Min              -0.347579\n",
      "evaluation/num steps total                           82000\n",
      "evaluation/num paths total                            4100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0548885\n",
      "evaluation/Rewards Std                                   0.100443\n",
      "evaluation/Rewards Max                                   0.15758\n",
      "evaluation/Rewards Min                                  -0.760129\n",
      "evaluation/Returns Mean                                 -1.09777\n",
      "evaluation/Returns Std                                   1.4825\n",
      "evaluation/Returns Max                                   1.63514\n",
      "evaluation/Returns Min                                  -5.67063\n",
      "evaluation/Actions Mean                                  0.0129991\n",
      "evaluation/Actions Std                                   0.114505\n",
      "evaluation/Actions Max                                   0.868372\n",
      "evaluation/Actions Min                                  -0.65571\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.09777\n",
      "evaluation/env_infos/final/reward_dist Mean              0.161199\n",
      "evaluation/env_infos/final/reward_dist Std               0.236473\n",
      "evaluation/env_infos/final/reward_dist Max               0.940226\n",
      "evaluation/env_infos/final/reward_dist Min               1.95073e-105\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0107524\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0161723\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0715394\n",
      "evaluation/env_infos/initial/reward_dist Min             1.31279e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.17005\n",
      "evaluation/env_infos/reward_dist Std                     0.255779\n",
      "evaluation/env_infos/reward_dist Max                     0.995173\n",
      "evaluation/env_infos/reward_dist Min                     1.95073e-105\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0980509\n",
      "evaluation/env_infos/final/reward_energy Std             0.209188\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00194174\n",
      "evaluation/env_infos/final/reward_energy Min            -0.975538\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.23985\n",
      "evaluation/env_infos/initial/reward_energy Std           0.248776\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00581004\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.868034\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0836414\n",
      "evaluation/env_infos/reward_energy Std                   0.139875\n",
      "evaluation/env_infos/reward_energy Max                  -0.000939147\n",
      "evaluation/env_infos/reward_energy Min                  -0.975538\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0351502\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.302549\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.450905\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000970512\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121791\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0371899\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0327855\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00447512\n",
      "evaluation/env_infos/end_effector_loc Std                0.188927\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.518941\n",
      "time/data storing (s)                                    0.00303631\n",
      "time/evaluation sampling (s)                             0.971898\n",
      "time/exploration sampling (s)                            0.128173\n",
      "time/logging (s)                                         0.0235715\n",
      "time/saving (s)                                          0.0289428\n",
      "time/training (s)                                       50.2456\n",
      "time/epoch (s)                                          51.4012\n",
      "time/total (s)                                        4073.73\n",
      "Epoch                                                   81\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:33:44.371001 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 82 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000624508\n",
      "trainer/QF2 Loss                                         0.000726083\n",
      "trainer/Policy Loss                                      3.16976\n",
      "trainer/Q1 Predictions Mean                             -1.20486\n",
      "trainer/Q1 Predictions Std                               0.861983\n",
      "trainer/Q1 Predictions Max                               0.486261\n",
      "trainer/Q1 Predictions Min                              -3.48362\n",
      "trainer/Q2 Predictions Mean                             -1.20125\n",
      "trainer/Q2 Predictions Std                               0.863261\n",
      "trainer/Q2 Predictions Max                               0.490338\n",
      "trainer/Q2 Predictions Min                              -3.47104\n",
      "trainer/Q Targets Mean                                  -1.20341\n",
      "trainer/Q Targets Std                                    0.854031\n",
      "trainer/Q Targets Max                                    0.498528\n",
      "trainer/Q Targets Min                                   -3.48417\n",
      "trainer/Log Pis Mean                                     1.96605\n",
      "trainer/Log Pis Std                                      1.49546\n",
      "trainer/Log Pis Max                                      4.48759\n",
      "trainer/Log Pis Min                                     -4.55692\n",
      "trainer/Policy mu Mean                                   0.0412045\n",
      "trainer/Policy mu Std                                    0.238788\n",
      "trainer/Policy mu Max                                    2.06931\n",
      "trainer/Policy mu Min                                   -1.03584\n",
      "trainer/Policy log std Mean                             -2.33697\n",
      "trainer/Policy log std Std                               0.601505\n",
      "trainer/Policy log std Max                               0.133116\n",
      "trainer/Policy log std Min                              -3.18393\n",
      "trainer/Alpha                                            0.0246573\n",
      "trainer/Alpha Loss                                      -0.125703\n",
      "exploration/num steps total                           9300\n",
      "exploration/num paths total                            465\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0991734\n",
      "exploration/Rewards Std                                  0.0640419\n",
      "exploration/Rewards Max                                  0.0164013\n",
      "exploration/Rewards Min                                 -0.283169\n",
      "exploration/Returns Mean                                -1.98347\n",
      "exploration/Returns Std                                  0.675818\n",
      "exploration/Returns Max                                 -1.04208\n",
      "exploration/Returns Min                                 -2.91907\n",
      "exploration/Actions Mean                                -0.0094017\n",
      "exploration/Actions Std                                  0.160359\n",
      "exploration/Actions Max                                  0.546498\n",
      "exploration/Actions Min                                 -0.507404\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.98347\n",
      "exploration/env_infos/final/reward_dist Mean             0.0826754\n",
      "exploration/env_infos/final/reward_dist Std              0.164924\n",
      "exploration/env_infos/final/reward_dist Max              0.412522\n",
      "exploration/env_infos/final/reward_dist Min              1.13822e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000378723\n",
      "exploration/env_infos/initial/reward_dist Std            0.000609849\n",
      "exploration/env_infos/initial/reward_dist Max            0.0015895\n",
      "exploration/env_infos/initial/reward_dist Min            1.07075e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.093575\n",
      "exploration/env_infos/reward_dist Std                    0.23091\n",
      "exploration/env_infos/reward_dist Max                    0.986114\n",
      "exploration/env_infos/reward_dist Min                    1.13822e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.18314\n",
      "exploration/env_infos/final/reward_energy Std            0.0727217\n",
      "exploration/env_infos/final/reward_energy Max           -0.0807634\n",
      "exploration/env_infos/final/reward_energy Min           -0.260826\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.17704\n",
      "exploration/env_infos/initial/reward_energy Std          0.0770149\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0936821\n",
      "exploration/env_infos/initial/reward_energy Min         -0.287202\n",
      "exploration/env_infos/reward_energy Mean                -0.189661\n",
      "exploration/env_infos/reward_energy Std                  0.125042\n",
      "exploration/env_infos/reward_energy Max                 -0.0120968\n",
      "exploration/env_infos/reward_energy Min                 -0.574931\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0204503\n",
      "exploration/env_infos/final/end_effector_loc Std         0.193362\n",
      "exploration/env_infos/final/end_effector_loc Max         0.243918\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.464843\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00415736\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00541381\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0132426\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00476134\n",
      "exploration/env_infos/end_effector_loc Mean              0.0431561\n",
      "exploration/env_infos/end_effector_loc Std               0.150079\n",
      "exploration/env_infos/end_effector_loc Max               0.367726\n",
      "exploration/env_infos/end_effector_loc Min              -0.465\n",
      "evaluation/num steps total                           83000\n",
      "evaluation/num paths total                            4150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0620006\n",
      "evaluation/Rewards Std                                   0.108704\n",
      "evaluation/Rewards Max                                   0.123184\n",
      "evaluation/Rewards Min                                  -0.807144\n",
      "evaluation/Returns Mean                                 -1.24001\n",
      "evaluation/Returns Std                                   1.5477\n",
      "evaluation/Returns Max                                   1.22329\n",
      "evaluation/Returns Min                                  -8.129\n",
      "evaluation/Actions Mean                                  0.0204754\n",
      "evaluation/Actions Std                                   0.143312\n",
      "evaluation/Actions Max                                   0.98739\n",
      "evaluation/Actions Min                                  -0.988171\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.24001\n",
      "evaluation/env_infos/final/reward_dist Mean              0.152226\n",
      "evaluation/env_infos/final/reward_dist Std               0.213322\n",
      "evaluation/env_infos/final/reward_dist Max               0.748834\n",
      "evaluation/env_infos/final/reward_dist Min               8.07708e-120\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00715763\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0138692\n",
      "evaluation/env_infos/initial/reward_dist Max             0.076537\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97017e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.120622\n",
      "evaluation/env_infos/reward_dist Std                     0.200044\n",
      "evaluation/env_infos/reward_dist Max                     0.982176\n",
      "evaluation/env_infos/reward_dist Min                     2.9298e-123\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.116247\n",
      "evaluation/env_infos/final/reward_energy Std             0.232813\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00414608\n",
      "evaluation/env_infos/final/reward_energy Min            -1.33831\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.306321\n",
      "evaluation/env_infos/initial/reward_energy Std           0.32445\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.000217716\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.25535\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0984051\n",
      "evaluation/env_infos/reward_energy Std                   0.179531\n",
      "evaluation/env_infos/reward_energy Max                  -0.000217716\n",
      "evaluation/env_infos/reward_energy Min                  -1.33831\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.078785\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.30718\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.549252\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00335411\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0154151\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0328633\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0494086\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0129987\n",
      "evaluation/env_infos/end_effector_loc Std                0.205026\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.600959\n",
      "time/data storing (s)                                    0.00293472\n",
      "time/evaluation sampling (s)                             1.21773\n",
      "time/exploration sampling (s)                            0.133259\n",
      "time/logging (s)                                         0.0205646\n",
      "time/saving (s)                                          0.0287646\n",
      "time/training (s)                                       51.4936\n",
      "time/epoch (s)                                          52.8968\n",
      "time/total (s)                                        4127.77\n",
      "Epoch                                                   82\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:34:38.203568 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 83 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000802997\n",
      "trainer/QF2 Loss                                         0.000728589\n",
      "trainer/Policy Loss                                      3.12219\n",
      "trainer/Q1 Predictions Mean                             -1.07807\n",
      "trainer/Q1 Predictions Std                               0.798062\n",
      "trainer/Q1 Predictions Max                               0.625394\n",
      "trainer/Q1 Predictions Min                              -3.13844\n",
      "trainer/Q2 Predictions Mean                             -1.07892\n",
      "trainer/Q2 Predictions Std                               0.798842\n",
      "trainer/Q2 Predictions Max                               0.628263\n",
      "trainer/Q2 Predictions Min                              -3.13457\n",
      "trainer/Q Targets Mean                                  -1.08678\n",
      "trainer/Q Targets Std                                    0.799065\n",
      "trainer/Q Targets Max                                    0.637205\n",
      "trainer/Q Targets Min                                   -3.14338\n",
      "trainer/Log Pis Mean                                     2.04393\n",
      "trainer/Log Pis Std                                      1.47663\n",
      "trainer/Log Pis Max                                      4.45773\n",
      "trainer/Log Pis Min                                     -4.44094\n",
      "trainer/Policy mu Mean                                   0.0555146\n",
      "trainer/Policy mu Std                                    0.293363\n",
      "trainer/Policy mu Max                                    2.06583\n",
      "trainer/Policy mu Min                                   -1.57229\n",
      "trainer/Policy log std Mean                             -2.33591\n",
      "trainer/Policy log std Std                               0.633862\n",
      "trainer/Policy log std Max                               0.102997\n",
      "trainer/Policy log std Min                              -3.20876\n",
      "trainer/Alpha                                            0.0248268\n",
      "trainer/Alpha Loss                                       0.162348\n",
      "exploration/num steps total                           9400\n",
      "exploration/num paths total                            470\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.100622\n",
      "exploration/Rewards Std                                  0.159106\n",
      "exploration/Rewards Max                                  0.105981\n",
      "exploration/Rewards Min                                 -0.701694\n",
      "exploration/Returns Mean                                -2.01245\n",
      "exploration/Returns Std                                  2.03215\n",
      "exploration/Returns Max                                  0.832667\n",
      "exploration/Returns Min                                 -5.46242\n",
      "exploration/Actions Mean                                -0.010638\n",
      "exploration/Actions Std                                  0.20874\n",
      "exploration/Actions Max                                  0.951553\n",
      "exploration/Actions Min                                 -0.971894\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.01245\n",
      "exploration/env_infos/final/reward_dist Mean             0.32504\n",
      "exploration/env_infos/final/reward_dist Std              0.308045\n",
      "exploration/env_infos/final/reward_dist Max              0.800821\n",
      "exploration/env_infos/final/reward_dist Min              1.38855e-63\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00116555\n",
      "exploration/env_infos/initial/reward_dist Std            0.00118257\n",
      "exploration/env_infos/initial/reward_dist Max            0.0026958\n",
      "exploration/env_infos/initial/reward_dist Min            2.9348e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.236133\n",
      "exploration/env_infos/reward_dist Std                    0.313118\n",
      "exploration/env_infos/reward_dist Max                    0.97419\n",
      "exploration/env_infos/reward_dist Min                    1.38855e-63\n",
      "exploration/env_infos/final/reward_energy Mean          -0.104883\n",
      "exploration/env_infos/final/reward_energy Std            0.0513993\n",
      "exploration/env_infos/final/reward_energy Max           -0.0546693\n",
      "exploration/env_infos/final/reward_energy Min           -0.185233\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.287152\n",
      "exploration/env_infos/initial/reward_energy Std          0.218961\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0756704\n",
      "exploration/env_infos/initial/reward_energy Min         -0.564252\n",
      "exploration/env_infos/reward_energy Mean                -0.197064\n",
      "exploration/env_infos/reward_energy Std                  0.220311\n",
      "exploration/env_infos/reward_energy Max                 -0.0115961\n",
      "exploration/env_infos/reward_energy Min                 -1.25978\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.125205\n",
      "exploration/env_infos/final/end_effector_loc Std         0.392401\n",
      "exploration/env_infos/final/end_effector_loc Max         0.304062\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000831577\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.01274\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0266949\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0260856\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0435148\n",
      "exploration/env_infos/end_effector_loc Std               0.249425\n",
      "exploration/env_infos/end_effector_loc Max               0.304062\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           84000\n",
      "evaluation/num paths total                            4200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0515327\n",
      "evaluation/Rewards Std                                   0.0780406\n",
      "evaluation/Rewards Max                                   0.136748\n",
      "evaluation/Rewards Min                                  -0.307914\n",
      "evaluation/Returns Mean                                 -1.03065\n",
      "evaluation/Returns Std                                   1.21853\n",
      "evaluation/Returns Max                                   1.83176\n",
      "evaluation/Returns Min                                  -3.60723\n",
      "evaluation/Actions Mean                                  0.00411539\n",
      "evaluation/Actions Std                                   0.0866956\n",
      "evaluation/Actions Max                                   0.731663\n",
      "evaluation/Actions Min                                  -0.843167\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.03065\n",
      "evaluation/env_infos/final/reward_dist Mean              0.194708\n",
      "evaluation/env_infos/final/reward_dist Std               0.278293\n",
      "evaluation/env_infos/final/reward_dist Max               0.977219\n",
      "evaluation/env_infos/final/reward_dist Min               4.03447e-09\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0072118\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0186561\n",
      "evaluation/env_infos/initial/reward_dist Max             0.117427\n",
      "evaluation/env_infos/initial/reward_dist Min             1.05836e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.199292\n",
      "evaluation/env_infos/reward_dist Std                     0.265534\n",
      "evaluation/env_infos/reward_dist Max                     0.996633\n",
      "evaluation/env_infos/reward_dist Min                     1.5478e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0546571\n",
      "evaluation/env_infos/final/reward_energy Std             0.0667798\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00635457\n",
      "evaluation/env_infos/final/reward_energy Min            -0.389303\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.230729\n",
      "evaluation/env_infos/initial/reward_energy Std           0.223137\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00611514\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.955398\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0672942\n",
      "evaluation/env_infos/reward_energy Std                   0.102653\n",
      "evaluation/env_infos/reward_energy Max                  -0.000464299\n",
      "evaluation/env_infos/reward_energy Min                  -0.955398\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0383224\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.225179\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.479811\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.477778\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000628841\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0113308\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0285488\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0421583\n",
      "evaluation/env_infos/end_effector_loc Mean               0.012545\n",
      "evaluation/env_infos/end_effector_loc Std                0.151521\n",
      "evaluation/env_infos/end_effector_loc Max                0.552201\n",
      "evaluation/env_infos/end_effector_loc Min               -0.477778\n",
      "time/data storing (s)                                    0.00309867\n",
      "time/evaluation sampling (s)                             1.27101\n",
      "time/exploration sampling (s)                            0.146733\n",
      "time/logging (s)                                         0.0211133\n",
      "time/saving (s)                                          0.0269889\n",
      "time/training (s)                                       51.2073\n",
      "time/epoch (s)                                          52.6762\n",
      "time/total (s)                                        4181.6\n",
      "Epoch                                                   83\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:35:31.783162 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 84 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000751667\n",
      "trainer/QF2 Loss                                         0.000705779\n",
      "trainer/Policy Loss                                      3.10635\n",
      "trainer/Q1 Predictions Mean                             -1.17131\n",
      "trainer/Q1 Predictions Std                               0.903222\n",
      "trainer/Q1 Predictions Max                               0.707973\n",
      "trainer/Q1 Predictions Min                              -3.62542\n",
      "trainer/Q2 Predictions Mean                             -1.16984\n",
      "trainer/Q2 Predictions Std                               0.902014\n",
      "trainer/Q2 Predictions Max                               0.693504\n",
      "trainer/Q2 Predictions Min                              -3.61057\n",
      "trainer/Q Targets Mean                                  -1.17083\n",
      "trainer/Q Targets Std                                    0.90419\n",
      "trainer/Q Targets Max                                    0.716348\n",
      "trainer/Q Targets Min                                   -3.58481\n",
      "trainer/Log Pis Mean                                     1.93474\n",
      "trainer/Log Pis Std                                      1.43514\n",
      "trainer/Log Pis Max                                      4.2128\n",
      "trainer/Log Pis Min                                     -2.99211\n",
      "trainer/Policy mu Mean                                   0.0519202\n",
      "trainer/Policy mu Std                                    0.221896\n",
      "trainer/Policy mu Max                                    1.50276\n",
      "trainer/Policy mu Min                                   -0.561848\n",
      "trainer/Policy log std Mean                             -2.34007\n",
      "trainer/Policy log std Std                               0.603693\n",
      "trainer/Policy log std Max                              -0.269094\n",
      "trainer/Policy log std Min                              -3.16929\n",
      "trainer/Alpha                                            0.023956\n",
      "trainer/Alpha Loss                                      -0.243462\n",
      "exploration/num steps total                           9500\n",
      "exploration/num paths total                            475\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0875493\n",
      "exploration/Rewards Std                                  0.0938802\n",
      "exploration/Rewards Max                                  0.0602913\n",
      "exploration/Rewards Min                                 -0.403812\n",
      "exploration/Returns Mean                                -1.75099\n",
      "exploration/Returns Std                                  1.47369\n",
      "exploration/Returns Max                                 -0.114906\n",
      "exploration/Returns Min                                 -4.03441\n",
      "exploration/Actions Mean                                 0.0113549\n",
      "exploration/Actions Std                                  0.14819\n",
      "exploration/Actions Max                                  0.474245\n",
      "exploration/Actions Min                                 -0.531655\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.75099\n",
      "exploration/env_infos/final/reward_dist Mean             0.0894367\n",
      "exploration/env_infos/final/reward_dist Std              0.155276\n",
      "exploration/env_infos/final/reward_dist Max              0.397608\n",
      "exploration/env_infos/final/reward_dist Min              1.1131e-82\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00725961\n",
      "exploration/env_infos/initial/reward_dist Std            0.00615325\n",
      "exploration/env_infos/initial/reward_dist Max            0.0157032\n",
      "exploration/env_infos/initial/reward_dist Min            1.63028e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.165233\n",
      "exploration/env_infos/reward_dist Std                    0.252508\n",
      "exploration/env_infos/reward_dist Max                    0.963572\n",
      "exploration/env_infos/reward_dist Min                    1.1131e-82\n",
      "exploration/env_infos/final/reward_energy Mean          -0.183541\n",
      "exploration/env_infos/final/reward_energy Std            0.0763726\n",
      "exploration/env_infos/final/reward_energy Max           -0.0826769\n",
      "exploration/env_infos/final/reward_energy Min           -0.299847\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.295377\n",
      "exploration/env_infos/initial/reward_energy Std          0.169579\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0554643\n",
      "exploration/env_infos/initial/reward_energy Min         -0.480806\n",
      "exploration/env_infos/reward_energy Mean                -0.171598\n",
      "exploration/env_infos/reward_energy Std                  0.121379\n",
      "exploration/env_infos/reward_energy Max                 -0.0139685\n",
      "exploration/env_infos/reward_energy Min                 -0.590943\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.102304\n",
      "exploration/env_infos/final/end_effector_loc Std         0.448656\n",
      "exploration/env_infos/final/end_effector_loc Max         0.985917\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.439669\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00212934\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0118521\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0237122\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0197018\n",
      "exploration/env_infos/end_effector_loc Mean              0.0377609\n",
      "exploration/env_infos/end_effector_loc Std               0.247678\n",
      "exploration/env_infos/end_effector_loc Max               0.985917\n",
      "exploration/env_infos/end_effector_loc Min              -0.439669\n",
      "evaluation/num steps total                           85000\n",
      "evaluation/num paths total                            4250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0707834\n",
      "evaluation/Rewards Std                                   0.0794524\n",
      "evaluation/Rewards Max                                   0.110934\n",
      "evaluation/Rewards Min                                  -0.475107\n",
      "evaluation/Returns Mean                                 -1.41567\n",
      "evaluation/Returns Std                                   1.25408\n",
      "evaluation/Returns Max                                   0.942785\n",
      "evaluation/Returns Min                                  -6.3838\n",
      "evaluation/Actions Mean                                  0.0109766\n",
      "evaluation/Actions Std                                   0.0944548\n",
      "evaluation/Actions Max                                   0.883904\n",
      "evaluation/Actions Min                                  -0.798296\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.41567\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0441786\n",
      "evaluation/env_infos/final/reward_dist Std               0.0958876\n",
      "evaluation/env_infos/final/reward_dist Max               0.390884\n",
      "evaluation/env_infos/final/reward_dist Min               7.69454e-35\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0074676\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0150878\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0869212\n",
      "evaluation/env_infos/initial/reward_dist Min             2.20235e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.111048\n",
      "evaluation/env_infos/reward_dist Std                     0.212007\n",
      "evaluation/env_infos/reward_dist Max                     0.993214\n",
      "evaluation/env_infos/reward_dist Min                     3.1203e-100\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.066275\n",
      "evaluation/env_infos/final/reward_energy Std             0.0843609\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00353066\n",
      "evaluation/env_infos/final/reward_energy Min            -0.452562\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.166574\n",
      "evaluation/env_infos/initial/reward_energy Std           0.196738\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00902033\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.901841\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0718194\n",
      "evaluation/env_infos/reward_energy Std                   0.113694\n",
      "evaluation/env_infos/reward_energy Max                  -0.000757595\n",
      "evaluation/env_infos/reward_energy Min                  -0.984006\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.116495\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.257772\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.71681\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.5124\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       9.12571e-06\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00911406\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0386118\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0321199\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0484285\n",
      "evaluation/env_infos/end_effector_loc Std                0.17143\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.5124\n",
      "time/data storing (s)                                    0.00285648\n",
      "time/evaluation sampling (s)                             1.01317\n",
      "time/exploration sampling (s)                            0.129844\n",
      "time/logging (s)                                         0.0227733\n",
      "time/saving (s)                                          0.028127\n",
      "time/training (s)                                       51.3197\n",
      "time/epoch (s)                                          52.5165\n",
      "time/total (s)                                        4235.18\n",
      "Epoch                                                   84\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:36:23.825490 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 85 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000855532\n",
      "trainer/QF2 Loss                                         0.000963405\n",
      "trainer/Policy Loss                                      3.16513\n",
      "trainer/Q1 Predictions Mean                             -1.16825\n",
      "trainer/Q1 Predictions Std                               1.00838\n",
      "trainer/Q1 Predictions Max                               0.924561\n",
      "trainer/Q1 Predictions Min                              -4.13899\n",
      "trainer/Q2 Predictions Mean                             -1.1718\n",
      "trainer/Q2 Predictions Std                               1.00159\n",
      "trainer/Q2 Predictions Max                               0.877535\n",
      "trainer/Q2 Predictions Min                              -4.10344\n",
      "trainer/Q Targets Mean                                  -1.17555\n",
      "trainer/Q Targets Std                                    1.00967\n",
      "trainer/Q Targets Max                                    0.891863\n",
      "trainer/Q Targets Min                                   -4.11423\n",
      "trainer/Log Pis Mean                                     2.00454\n",
      "trainer/Log Pis Std                                      1.30367\n",
      "trainer/Log Pis Max                                      6.24955\n",
      "trainer/Log Pis Min                                     -3.44088\n",
      "trainer/Policy mu Mean                                   0.0513605\n",
      "trainer/Policy mu Std                                    0.331835\n",
      "trainer/Policy mu Max                                    2.26121\n",
      "trainer/Policy mu Min                                   -1.19138\n",
      "trainer/Policy log std Mean                             -2.2443\n",
      "trainer/Policy log std Std                               0.6423\n",
      "trainer/Policy log std Max                               0.0606581\n",
      "trainer/Policy log std Min                              -3.18303\n",
      "trainer/Alpha                                            0.0237196\n",
      "trainer/Alpha Loss                                       0.0169725\n",
      "exploration/num steps total                           9600\n",
      "exploration/num paths total                            480\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0940113\n",
      "exploration/Rewards Std                                  0.0877212\n",
      "exploration/Rewards Max                                  0.0834497\n",
      "exploration/Rewards Min                                 -0.327191\n",
      "exploration/Returns Mean                                -1.88023\n",
      "exploration/Returns Std                                  0.997942\n",
      "exploration/Returns Max                                 -0.899154\n",
      "exploration/Returns Min                                 -3.67311\n",
      "exploration/Actions Mean                                -0.00923733\n",
      "exploration/Actions Std                                  0.121246\n",
      "exploration/Actions Max                                  0.454258\n",
      "exploration/Actions Min                                 -0.546875\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.88023\n",
      "exploration/env_infos/final/reward_dist Mean             0.0478526\n",
      "exploration/env_infos/final/reward_dist Std              0.0953706\n",
      "exploration/env_infos/final/reward_dist Max              0.238594\n",
      "exploration/env_infos/final/reward_dist Min              3.66281e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00858526\n",
      "exploration/env_infos/initial/reward_dist Std            0.0138561\n",
      "exploration/env_infos/initial/reward_dist Max            0.036058\n",
      "exploration/env_infos/initial/reward_dist Min            1.24269e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.177428\n",
      "exploration/env_infos/reward_dist Std                    0.265888\n",
      "exploration/env_infos/reward_dist Max                    0.978666\n",
      "exploration/env_infos/reward_dist Min                    3.66281e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.244279\n",
      "exploration/env_infos/final/reward_energy Std            0.175954\n",
      "exploration/env_infos/final/reward_energy Max           -0.0237224\n",
      "exploration/env_infos/final/reward_energy Min           -0.479927\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.241581\n",
      "exploration/env_infos/initial/reward_energy Std          0.134951\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0965781\n",
      "exploration/env_infos/initial/reward_energy Min         -0.46201\n",
      "exploration/env_infos/reward_energy Mean                -0.135361\n",
      "exploration/env_infos/reward_energy Std                  0.106062\n",
      "exploration/env_infos/reward_energy Max                 -0.0184605\n",
      "exploration/env_infos/reward_energy Min                 -0.566015\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.133946\n",
      "exploration/env_infos/final/end_effector_loc Std         0.274934\n",
      "exploration/env_infos/final/end_effector_loc Max         0.295825\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.586996\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000881553\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00974368\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0227129\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0137665\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0456989\n",
      "exploration/env_infos/end_effector_loc Std               0.194054\n",
      "exploration/env_infos/end_effector_loc Max               0.319806\n",
      "exploration/env_infos/end_effector_loc Min              -0.586996\n",
      "evaluation/num steps total                           86000\n",
      "evaluation/num paths total                            4300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0568031\n",
      "evaluation/Rewards Std                                   0.0950642\n",
      "evaluation/Rewards Max                                   0.135685\n",
      "evaluation/Rewards Min                                  -0.500726\n",
      "evaluation/Returns Mean                                 -1.13606\n",
      "evaluation/Returns Std                                   1.47621\n",
      "evaluation/Returns Max                                   1.36832\n",
      "evaluation/Returns Min                                  -6.97271\n",
      "evaluation/Actions Mean                                  0.00514501\n",
      "evaluation/Actions Std                                   0.100675\n",
      "evaluation/Actions Max                                   0.777781\n",
      "evaluation/Actions Min                                  -0.627673\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.13606\n",
      "evaluation/env_infos/final/reward_dist Mean              0.103041\n",
      "evaluation/env_infos/final/reward_dist Std               0.210808\n",
      "evaluation/env_infos/final/reward_dist Max               0.967063\n",
      "evaluation/env_infos/final/reward_dist Min               2.55266e-41\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0090488\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126075\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0471988\n",
      "evaluation/env_infos/initial/reward_dist Min             2.64548e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.181395\n",
      "evaluation/env_infos/reward_dist Std                     0.256165\n",
      "evaluation/env_infos/reward_dist Max                     0.995307\n",
      "evaluation/env_infos/reward_dist Min                     2.55266e-41\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0705127\n",
      "evaluation/env_infos/final/reward_energy Std             0.0663465\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00696136\n",
      "evaluation/env_infos/final/reward_energy Min            -0.443711\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.295688\n",
      "evaluation/env_infos/initial/reward_energy Std           0.206119\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0225626\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.710746\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0864135\n",
      "evaluation/env_infos/reward_energy Std                   0.113387\n",
      "evaluation/env_infos/reward_energy Max                  -0.00316886\n",
      "evaluation/env_infos/reward_energy Min                  -0.844356\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0116608\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.300108\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.816215\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.69929\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000785667\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127192\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0289242\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0305362\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00212452\n",
      "evaluation/env_infos/end_effector_loc Std                0.198984\n",
      "evaluation/env_infos/end_effector_loc Max                0.816215\n",
      "evaluation/env_infos/end_effector_loc Min               -0.69929\n",
      "time/data storing (s)                                    0.00282452\n",
      "time/evaluation sampling (s)                             1.10168\n",
      "time/exploration sampling (s)                            0.122554\n",
      "time/logging (s)                                         0.0195618\n",
      "time/saving (s)                                          0.0294849\n",
      "time/training (s)                                       49.6159\n",
      "time/epoch (s)                                          50.8921\n",
      "time/total (s)                                        4287.22\n",
      "Epoch                                                   85\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:37:15.500305 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 86 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00081931\r\n",
      "trainer/QF2 Loss                                         0.000836693\r\n",
      "trainer/Policy Loss                                      3.03035\r\n",
      "trainer/Q1 Predictions Mean                             -1.125\r\n",
      "trainer/Q1 Predictions Std                               0.936117\r\n",
      "trainer/Q1 Predictions Max                               0.962572\r\n",
      "trainer/Q1 Predictions Min                              -3.92476\r\n",
      "trainer/Q2 Predictions Mean                             -1.12644\r\n",
      "trainer/Q2 Predictions Std                               0.939584\r\n",
      "trainer/Q2 Predictions Max                               0.991322\r\n",
      "trainer/Q2 Predictions Min                              -3.92007\r\n",
      "trainer/Q Targets Mean                                  -1.12588\r\n",
      "trainer/Q Targets Std                                    0.937968\r\n",
      "trainer/Q Targets Max                                    1.01837\r\n",
      "trainer/Q Targets Min                                   -3.9435\r\n",
      "trainer/Log Pis Mean                                     1.9083\r\n",
      "trainer/Log Pis Std                                      1.50904\r\n",
      "trainer/Log Pis Max                                      4.32252\r\n",
      "trainer/Log Pis Min                                     -4.30051\r\n",
      "trainer/Policy mu Mean                                   0.0687053\r\n",
      "trainer/Policy mu Std                                    0.350528\r\n",
      "trainer/Policy mu Max                                    2.23132\r\n",
      "trainer/Policy mu Min                                   -1.80152\r\n",
      "trainer/Policy log std Mean                             -2.25008\r\n",
      "trainer/Policy log std Std                               0.668669\r\n",
      "trainer/Policy log std Max                               0.00763177\r\n",
      "trainer/Policy log std Min                              -3.25716\r\n",
      "trainer/Alpha                                            0.0246952\r\n",
      "trainer/Alpha Loss                                      -0.339355\r\n",
      "exploration/num steps total                           9700\r\n",
      "exploration/num paths total                            485\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.156608\r\n",
      "exploration/Rewards Std                                  0.110325\r\n",
      "exploration/Rewards Max                                  0.0296039\r\n",
      "exploration/Rewards Min                                 -0.477681\r\n",
      "exploration/Returns Mean                                -3.13217\r\n",
      "exploration/Returns Std                                  1.36198\r\n",
      "exploration/Returns Max                                 -2.04634\r\n",
      "exploration/Returns Min                                 -5.73329\r\n",
      "exploration/Actions Mean                                 0.0104222\r\n",
      "exploration/Actions Std                                  0.203443\r\n",
      "exploration/Actions Max                                  0.698733\r\n",
      "exploration/Actions Min                                 -0.811316\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -3.13217\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.107084\r\n",
      "exploration/env_infos/final/reward_dist Std              0.194842\r\n",
      "exploration/env_infos/final/reward_dist Max              0.495542\r\n",
      "exploration/env_infos/final/reward_dist Min              4.65517e-38\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00777185\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0118895\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0312708\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.91535e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.103352\r\n",
      "exploration/env_infos/reward_dist Std                    0.236001\r\n",
      "exploration/env_infos/reward_dist Max                    0.948472\r\n",
      "exploration/env_infos/reward_dist Min                    4.65517e-38\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.260993\r\n",
      "exploration/env_infos/final/reward_energy Std            0.166338\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0816403\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.553158\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.319056\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.152971\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0723563\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.50494\r\n",
      "exploration/env_infos/reward_energy Mean                -0.223699\r\n",
      "exploration/env_infos/reward_energy Std                  0.181533\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00675088\r\n",
      "exploration/env_infos/reward_energy Min                 -0.886753\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.225965\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.297271\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.602847\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.290822\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00702379\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0103519\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0252356\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00408863\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.142583\r\n",
      "exploration/env_infos/end_effector_loc Std               0.197046\r\n",
      "exploration/env_infos/end_effector_loc Max               0.602847\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.290822\r\n",
      "evaluation/num steps total                           87000\r\n",
      "evaluation/num paths total                            4350\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0453707\r\n",
      "evaluation/Rewards Std                                   0.0742961\r\n",
      "evaluation/Rewards Max                                   0.153327\r\n",
      "evaluation/Rewards Min                                  -0.512343\r\n",
      "evaluation/Returns Mean                                 -0.907414\r\n",
      "evaluation/Returns Std                                   1.11827\r\n",
      "evaluation/Returns Max                                   1.85325\r\n",
      "evaluation/Returns Min                                  -2.95858\r\n",
      "evaluation/Actions Mean                                  0.0069006\r\n",
      "evaluation/Actions Std                                   0.0809212\r\n",
      "evaluation/Actions Max                                   0.673162\r\n",
      "evaluation/Actions Min                                  -0.797242\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.907414\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.170603\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.258682\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.936887\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.71062e-48\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00608185\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0122879\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0648421\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.44074e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.206823\r\n",
      "evaluation/env_infos/reward_dist Std                     0.285879\r\n",
      "evaluation/env_infos/reward_dist Max                     0.99968\r\n",
      "evaluation/env_infos/reward_dist Min                     2.71062e-48\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0763778\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.140963\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00133194\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.772978\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.240526\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.216141\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00680561\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.05712\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0683702\r\n",
      "evaluation/env_infos/reward_energy Std                   0.092289\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00089725\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.05712\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0712399\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.246841\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.976319\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.50621\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000429362\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0114249\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.023658\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0398621\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0285739\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.162503\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.976319\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.50621\r\n",
      "time/data storing (s)                                    0.00304001\r\n",
      "time/evaluation sampling (s)                             0.994811\r\n",
      "time/exploration sampling (s)                            0.124561\r\n",
      "time/logging (s)                                         0.0197373\r\n",
      "time/saving (s)                                          0.0278067\r\n",
      "time/training (s)                                       49.3938\r\n",
      "time/epoch (s)                                          50.5638\r\n",
      "time/total (s)                                        4338.89\r\n",
      "Epoch                                                   86\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:38:06.713764 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 87 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000967391\r\n",
      "trainer/QF2 Loss                                         0.000740711\r\n",
      "trainer/Policy Loss                                      3.10805\r\n",
      "trainer/Q1 Predictions Mean                             -1.09719\r\n",
      "trainer/Q1 Predictions Std                               0.898056\r\n",
      "trainer/Q1 Predictions Max                               1.07411\r\n",
      "trainer/Q1 Predictions Min                              -3.83963\r\n",
      "trainer/Q2 Predictions Mean                             -1.09717\r\n",
      "trainer/Q2 Predictions Std                               0.895508\r\n",
      "trainer/Q2 Predictions Max                               1.04275\r\n",
      "trainer/Q2 Predictions Min                              -3.82989\r\n",
      "trainer/Q Targets Mean                                  -1.09238\r\n",
      "trainer/Q Targets Std                                    0.894355\r\n",
      "trainer/Q Targets Max                                    1.06338\r\n",
      "trainer/Q Targets Min                                   -3.83702\r\n",
      "trainer/Log Pis Mean                                     2.01512\r\n",
      "trainer/Log Pis Std                                      1.28843\r\n",
      "trainer/Log Pis Max                                      4.52608\r\n",
      "trainer/Log Pis Min                                     -2.25408\r\n",
      "trainer/Policy mu Mean                                   0.0118531\r\n",
      "trainer/Policy mu Std                                    0.341171\r\n",
      "trainer/Policy mu Max                                    2.08294\r\n",
      "trainer/Policy mu Min                                   -1.97468\r\n",
      "trainer/Policy log std Mean                             -2.26922\r\n",
      "trainer/Policy log std Std                               0.592561\r\n",
      "trainer/Policy log std Max                              -0.0713712\r\n",
      "trainer/Policy log std Min                              -3.24393\r\n",
      "trainer/Alpha                                            0.0245906\r\n",
      "trainer/Alpha Loss                                       0.0560372\r\n",
      "exploration/num steps total                           9800\r\n",
      "exploration/num paths total                            490\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0348848\r\n",
      "exploration/Rewards Std                                  0.0751926\r\n",
      "exploration/Rewards Max                                  0.119248\r\n",
      "exploration/Rewards Min                                 -0.182206\r\n",
      "exploration/Returns Mean                                -0.697696\r\n",
      "exploration/Returns Std                                  0.987555\r\n",
      "exploration/Returns Max                                  0.731453\r\n",
      "exploration/Returns Min                                 -1.96429\r\n",
      "exploration/Actions Mean                                 0.00423574\r\n",
      "exploration/Actions Std                                  0.153079\r\n",
      "exploration/Actions Max                                  0.551274\r\n",
      "exploration/Actions Min                                 -0.551642\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.697696\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.137535\r\n",
      "exploration/env_infos/final/reward_dist Std              0.154808\r\n",
      "exploration/env_infos/final/reward_dist Max              0.382346\r\n",
      "exploration/env_infos/final/reward_dist Min              1.24052e-07\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000601893\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000720201\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00189981\r\n",
      "exploration/env_infos/initial/reward_dist Min            7.73048e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.192923\r\n",
      "exploration/env_infos/reward_dist Std                    0.240579\r\n",
      "exploration/env_infos/reward_dist Max                    0.905998\r\n",
      "exploration/env_infos/reward_dist Min                    1.24052e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.12808\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0328142\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.08518\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.167604\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.305314\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.185088\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0502204\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.526035\r\n",
      "exploration/env_infos/reward_energy Mean                -0.175031\r\n",
      "exploration/env_infos/reward_energy Std                  0.127539\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0066996\r\n",
      "exploration/env_infos/reward_energy Min                 -0.66289\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00151801\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.245833\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.260676\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.515441\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00348638\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0121321\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0252777\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00882859\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00734038\r\n",
      "exploration/env_infos/end_effector_loc Std               0.164489\r\n",
      "exploration/env_infos/end_effector_loc Max               0.261854\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.515441\r\n",
      "evaluation/num steps total                           88000\r\n",
      "evaluation/num paths total                            4400\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.044048\r\n",
      "evaluation/Rewards Std                                   0.0780151\r\n",
      "evaluation/Rewards Max                                   0.10574\r\n",
      "evaluation/Rewards Min                                  -0.667833\r\n",
      "evaluation/Returns Mean                                 -0.88096\r\n",
      "evaluation/Returns Std                                   1.25006\r\n",
      "evaluation/Returns Max                                   1.12782\r\n",
      "evaluation/Returns Min                                  -4.8447\r\n",
      "evaluation/Actions Mean                                  0.00322819\r\n",
      "evaluation/Actions Std                                   0.0753415\r\n",
      "evaluation/Actions Max                                   0.608989\r\n",
      "evaluation/Actions Min                                  -0.763478\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.88096\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.20437\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.281357\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.993613\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.60494e-32\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00625269\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00972274\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0331669\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.58418e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.174688\r\n",
      "evaluation/env_infos/reward_dist Std                     0.244664\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993613\r\n",
      "evaluation/env_infos/reward_dist Min                     4.60494e-32\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.044362\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0413011\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00322027\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.190079\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.222912\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.213851\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0125951\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.02969\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.063092\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0859821\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00104715\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.02969\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0707715\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.244738\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.698589\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.488588\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00167996\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107914\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0304494\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0381739\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0376834\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/end_effector_loc Std                0.154626\n",
      "evaluation/env_infos/end_effector_loc Max                0.698589\n",
      "evaluation/env_infos/end_effector_loc Min               -0.488588\n",
      "time/data storing (s)                                    0.00314935\n",
      "time/evaluation sampling (s)                             0.977253\n",
      "time/exploration sampling (s)                            0.129601\n",
      "time/logging (s)                                         0.0202238\n",
      "time/saving (s)                                          0.028472\n",
      "time/training (s)                                       48.9125\n",
      "time/epoch (s)                                          50.0712\n",
      "time/total (s)                                        4390.1\n",
      "Epoch                                                   87\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 11:38:58.231733 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 88 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000713256\n",
      "trainer/QF2 Loss                                         0.000946556\n",
      "trainer/Policy Loss                                      2.854\n",
      "trainer/Q1 Predictions Mean                             -1.0402\n",
      "trainer/Q1 Predictions Std                               0.95404\n",
      "trainer/Q1 Predictions Max                               1.12228\n",
      "trainer/Q1 Predictions Min                              -3.69971\n",
      "trainer/Q2 Predictions Mean                             -1.03471\n",
      "trainer/Q2 Predictions Std                               0.95488\n",
      "trainer/Q2 Predictions Max                               1.11517\n",
      "trainer/Q2 Predictions Min                              -3.68875\n",
      "trainer/Q Targets Mean                                  -1.04437\n",
      "trainer/Q Targets Std                                    0.957498\n",
      "trainer/Q Targets Max                                    1.15716\n",
      "trainer/Q Targets Min                                   -3.75101\n",
      "trainer/Log Pis Mean                                     1.82518\n",
      "trainer/Log Pis Std                                      1.5307\n",
      "trainer/Log Pis Max                                      5.01542\n",
      "trainer/Log Pis Min                                     -3.70905\n",
      "trainer/Policy mu Mean                                   0.0473441\n",
      "trainer/Policy mu Std                                    0.407791\n",
      "trainer/Policy mu Max                                    2.20331\n",
      "trainer/Policy mu Min                                   -2.30514\n",
      "trainer/Policy log std Mean                             -2.20436\n",
      "trainer/Policy log std Std                               0.671218\n",
      "trainer/Policy log std Max                              -0.0405939\n",
      "trainer/Policy log std Min                              -3.3227\n",
      "trainer/Alpha                                            0.022816\n",
      "trainer/Alpha Loss                                      -0.660891\n",
      "exploration/num steps total                           9900\n",
      "exploration/num paths total                            495\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117967\n",
      "exploration/Rewards Std                                  0.0972992\n",
      "exploration/Rewards Max                                  0.0613822\n",
      "exploration/Rewards Min                                 -0.719589\n",
      "exploration/Returns Mean                                -2.35934\n",
      "exploration/Returns Std                                  1.22194\n",
      "exploration/Returns Max                                 -0.00465625\n",
      "exploration/Returns Min                                 -3.42246\n",
      "exploration/Actions Mean                                -0.00316595\n",
      "exploration/Actions Std                                  0.151526\n",
      "exploration/Actions Max                                  0.397889\n",
      "exploration/Actions Min                                 -0.900657\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.35934\n",
      "exploration/env_infos/final/reward_dist Mean             0.0628594\n",
      "exploration/env_infos/final/reward_dist Std              0.058378\n",
      "exploration/env_infos/final/reward_dist Max              0.15015\n",
      "exploration/env_infos/final/reward_dist Min              4.10023e-20\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00260435\n",
      "exploration/env_infos/initial/reward_dist Std            0.00458815\n",
      "exploration/env_infos/initial/reward_dist Max            0.0117664\n",
      "exploration/env_infos/initial/reward_dist Min            1.06785e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.124466\n",
      "exploration/env_infos/reward_dist Std                    0.168553\n",
      "exploration/env_infos/reward_dist Max                    0.740134\n",
      "exploration/env_infos/reward_dist Min                    4.10023e-20\n",
      "exploration/env_infos/final/reward_energy Mean          -0.15136\n",
      "exploration/env_infos/final/reward_energy Std            0.0512278\n",
      "exploration/env_infos/final/reward_energy Max           -0.0827304\n",
      "exploration/env_infos/final/reward_energy Min           -0.225133\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.392085\n",
      "exploration/env_infos/initial/reward_energy Std          0.427589\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0630191\n",
      "exploration/env_infos/initial/reward_energy Min         -1.22093\n",
      "exploration/env_infos/reward_energy Mean                -0.155471\n",
      "exploration/env_infos/reward_energy Std                  0.147543\n",
      "exploration/env_infos/reward_energy Max                 -0.0175455\n",
      "exploration/env_infos/reward_energy Min                 -1.22093\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.108977\n",
      "exploration/env_infos/final/end_effector_loc Std         0.307836\n",
      "exploration/env_infos/final/end_effector_loc Max         0.44749\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.594712\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0115781\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0169308\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00295757\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0450329\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0840313\n",
      "exploration/env_infos/end_effector_loc Std               0.174826\n",
      "exploration/env_infos/end_effector_loc Max               0.44749\n",
      "exploration/env_infos/end_effector_loc Min              -0.594712\n",
      "evaluation/num steps total                           89000\n",
      "evaluation/num paths total                            4450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0585888\n",
      "evaluation/Rewards Std                                   0.0703203\n",
      "evaluation/Rewards Max                                   0.127061\n",
      "evaluation/Rewards Min                                  -0.45308\n",
      "evaluation/Returns Mean                                 -1.17178\n",
      "evaluation/Returns Std                                   1.04895\n",
      "evaluation/Returns Max                                   1.83083\n",
      "evaluation/Returns Min                                  -3.29048\n",
      "evaluation/Actions Mean                                  0.00175714\n",
      "evaluation/Actions Std                                   0.0733458\n",
      "evaluation/Actions Max                                   0.668374\n",
      "evaluation/Actions Min                                  -0.448036\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.17178\n",
      "evaluation/env_infos/final/reward_dist Mean              0.173446\n",
      "evaluation/env_infos/final/reward_dist Std               0.266338\n",
      "evaluation/env_infos/final/reward_dist Max               0.999222\n",
      "evaluation/env_infos/final/reward_dist Min               3.16517e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00684491\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126979\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0512853\n",
      "evaluation/env_infos/initial/reward_dist Min             9.39962e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.153595\n",
      "evaluation/env_infos/reward_dist Std                     0.243147\n",
      "evaluation/env_infos/reward_dist Max                     0.999222\n",
      "evaluation/env_infos/reward_dist Min                     3.16517e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0876882\n",
      "evaluation/env_infos/final/reward_energy Std             0.11302\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00258433\n",
      "evaluation/env_infos/final/reward_energy Min            -0.630141\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.232505\n",
      "evaluation/env_infos/initial/reward_energy Std           0.213519\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00914598\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.725212\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0619488\n",
      "evaluation/env_infos/reward_energy Std                   0.0832329\n",
      "evaluation/env_infos/reward_energy Max                  -0.000496725\n",
      "evaluation/env_infos/reward_energy Min                  -0.725212\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0474509\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.252252\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.531435\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.620855\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00325979\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010674\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0334187\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0224018\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0292928\n",
      "evaluation/env_infos/end_effector_loc Std                0.158389\n",
      "evaluation/env_infos/end_effector_loc Max                0.531435\n",
      "evaluation/env_infos/end_effector_loc Min               -0.620855\n",
      "time/data storing (s)                                    0.00364018\n",
      "time/evaluation sampling (s)                             1.08489\n",
      "time/exploration sampling (s)                            0.130115\n",
      "time/logging (s)                                         0.0192907\n",
      "time/saving (s)                                          0.0272917\n",
      "time/training (s)                                       49.1073\n",
      "time/epoch (s)                                          50.3725\n",
      "time/total (s)                                        4441.61\n",
      "Epoch                                                   88\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:39:49.719206 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 89 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000661742\r\n",
      "trainer/QF2 Loss                                         0.000607599\r\n",
      "trainer/Policy Loss                                      3.14147\r\n",
      "trainer/Q1 Predictions Mean                             -1.11789\r\n",
      "trainer/Q1 Predictions Std                               0.877175\r\n",
      "trainer/Q1 Predictions Max                               1.19313\r\n",
      "trainer/Q1 Predictions Min                              -3.03903\r\n",
      "trainer/Q2 Predictions Mean                             -1.11696\r\n",
      "trainer/Q2 Predictions Std                               0.878045\r\n",
      "trainer/Q2 Predictions Max                               1.19455\r\n",
      "trainer/Q2 Predictions Min                              -3.04007\r\n",
      "trainer/Q Targets Mean                                  -1.12013\r\n",
      "trainer/Q Targets Std                                    0.882249\r\n",
      "trainer/Q Targets Max                                    1.20659\r\n",
      "trainer/Q Targets Min                                   -3.06234\r\n",
      "trainer/Log Pis Mean                                     2.03584\r\n",
      "trainer/Log Pis Std                                      1.33571\r\n",
      "trainer/Log Pis Max                                      4.46727\r\n",
      "trainer/Log Pis Min                                     -2.39148\r\n",
      "trainer/Policy mu Mean                                   0.0127224\r\n",
      "trainer/Policy mu Std                                    0.411846\r\n",
      "trainer/Policy mu Max                                    2.15832\r\n",
      "trainer/Policy mu Min                                   -2.48651\r\n",
      "trainer/Policy log std Mean                             -2.29916\r\n",
      "trainer/Policy log std Std                               0.636434\r\n",
      "trainer/Policy log std Max                              -0.265467\r\n",
      "trainer/Policy log std Min                              -3.27702\r\n",
      "trainer/Alpha                                            0.0228393\r\n",
      "trainer/Alpha Loss                                       0.135478\r\n",
      "exploration/num steps total                          10000\r\n",
      "exploration/num paths total                            500\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0561464\r\n",
      "exploration/Rewards Std                                  0.107047\r\n",
      "exploration/Rewards Max                                  0.15085\r\n",
      "exploration/Rewards Min                                 -0.300006\r\n",
      "exploration/Returns Mean                                -1.12293\r\n",
      "exploration/Returns Std                                  1.91971\r\n",
      "exploration/Returns Max                                  2.27685\r\n",
      "exploration/Returns Min                                 -3.14705\r\n",
      "exploration/Actions Mean                                -0.00717381\r\n",
      "exploration/Actions Std                                  0.124177\r\n",
      "exploration/Actions Max                                  0.43078\r\n",
      "exploration/Actions Min                                 -0.661325\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.12293\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0169231\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0334529\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0838278\r\n",
      "exploration/env_infos/final/reward_dist Min              1.07735e-17\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103564\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.015862\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0412912\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.06136e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.145638\r\n",
      "exploration/env_infos/reward_dist Std                    0.290544\r\n",
      "exploration/env_infos/reward_dist Max                    0.953906\r\n",
      "exploration/env_infos/reward_dist Min                    1.07735e-17\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.122873\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0596587\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.059025\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.212737\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.283125\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.217481\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0480824\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.661942\r\n",
      "exploration/env_infos/reward_energy Mean                -0.132783\r\n",
      "exploration/env_infos/reward_energy Std                  0.115375\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00545208\r\n",
      "exploration/env_infos/reward_energy Min                 -0.782788\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0390409\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.26013\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.548649\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.347223\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00457411\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0117643\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00857293\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0330662\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0162528\r\n",
      "exploration/env_infos/end_effector_loc Std               0.153439\r\n",
      "exploration/env_infos/end_effector_loc Max               0.548649\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.347223\r\n",
      "evaluation/num steps total                           90000\r\n",
      "evaluation/num paths total                            4500\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0394548\r\n",
      "evaluation/Rewards Std                                   0.0750897\r\n",
      "evaluation/Rewards Max                                   0.169366\r\n",
      "evaluation/Rewards Min                                  -0.551534\r\n",
      "evaluation/Returns Mean                                 -0.789095\r\n",
      "evaluation/Returns Std                                   1.19663\r\n",
      "evaluation/Returns Max                                   1.68729\r\n",
      "evaluation/Returns Min                                  -2.82151\r\n",
      "evaluation/Actions Mean                                  0.00187652\r\n",
      "evaluation/Actions Std                                   0.0733587\r\n",
      "evaluation/Actions Max                                   0.840842\r\n",
      "evaluation/Actions Min                                  -0.720185\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.789095\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.221816\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.281053\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.912149\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.19618e-12\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00697503\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117158\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0662914\r\n",
      "evaluation/env_infos/initial/reward_dist Min             4.62636e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.213293\r\n",
      "evaluation/env_infos/reward_dist Std                     0.287505\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996289\r\n",
      "evaluation/env_infos/reward_dist Min                     2.19618e-12\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0533888\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0544848\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0078962\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.234725\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.242182\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.220842\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00525646\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.909047\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0628743\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0825642\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000219416\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.909047\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0358187\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23201\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.486271\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.559394\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00122375\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0115231\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0420421\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0360092\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0185866\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.157884\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.486271\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.559394\r\n",
      "time/data storing (s)                                    0.00298995\r\n",
      "time/evaluation sampling (s)                             0.94986\r\n",
      "time/exploration sampling (s)                            0.220384\r\n",
      "time/logging (s)                                         0.0189447\r\n",
      "time/saving (s)                                          0.0282875\r\n",
      "time/training (s)                                       49.1325\r\n",
      "time/epoch (s)                                          50.3529\r\n",
      "time/total (s)                                        4493.1\r\n",
      "Epoch                                                   89\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:40:40.688407 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 90 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000640028\r\n",
      "trainer/QF2 Loss                                         0.00113842\r\n",
      "trainer/Policy Loss                                      2.83636\r\n",
      "trainer/Q1 Predictions Mean                             -0.984102\r\n",
      "trainer/Q1 Predictions Std                               0.858803\r\n",
      "trainer/Q1 Predictions Max                               0.801073\r\n",
      "trainer/Q1 Predictions Min                              -2.88309\r\n",
      "trainer/Q2 Predictions Mean                             -0.974686\r\n",
      "trainer/Q2 Predictions Std                               0.847579\r\n",
      "trainer/Q2 Predictions Max                               0.757766\r\n",
      "trainer/Q2 Predictions Min                              -2.8244\r\n",
      "trainer/Q Targets Mean                                  -0.987131\r\n",
      "trainer/Q Targets Std                                    0.859702\r\n",
      "trainer/Q Targets Max                                    0.828441\r\n",
      "trainer/Q Targets Min                                   -2.92039\r\n",
      "trainer/Log Pis Mean                                     1.85798\r\n",
      "trainer/Log Pis Std                                      1.42351\r\n",
      "trainer/Log Pis Max                                      4.24953\r\n",
      "trainer/Log Pis Min                                     -4.17863\r\n",
      "trainer/Policy mu Mean                                  -0.0211141\r\n",
      "trainer/Policy mu Std                                    0.307442\r\n",
      "trainer/Policy mu Max                                    2.31925\r\n",
      "trainer/Policy mu Min                                   -1.68744\r\n",
      "trainer/Policy log std Mean                             -2.27155\r\n",
      "trainer/Policy log std Std                               0.598817\r\n",
      "trainer/Policy log std Max                              -0.0942147\r\n",
      "trainer/Policy log std Min                              -3.14763\r\n",
      "trainer/Alpha                                            0.0239412\r\n",
      "trainer/Alpha Loss                                      -0.529912\r\n",
      "exploration/num steps total                          10100\r\n",
      "exploration/num paths total                            505\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.106559\r\n",
      "exploration/Rewards Std                                  0.211285\r\n",
      "exploration/Rewards Max                                  0.1533\r\n",
      "exploration/Rewards Min                                 -0.963324\r\n",
      "exploration/Returns Mean                                -2.13119\r\n",
      "exploration/Returns Std                                  3.08466\r\n",
      "exploration/Returns Max                                  1.3905\r\n",
      "exploration/Returns Min                                 -7.73321\r\n",
      "exploration/Actions Mean                                 0.0245213\r\n",
      "exploration/Actions Std                                  0.170214\r\n",
      "exploration/Actions Max                                  0.60834\r\n",
      "exploration/Actions Min                                 -0.722439\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.13119\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.240022\r\n",
      "exploration/env_infos/final/reward_dist Std              0.362824\r\n",
      "exploration/env_infos/final/reward_dist Max              0.948812\r\n",
      "exploration/env_infos/final/reward_dist Min              4.3996e-59\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00914037\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0139875\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0368588\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.49888e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.235362\r\n",
      "exploration/env_infos/reward_dist Std                    0.279659\r\n",
      "exploration/env_infos/reward_dist Max                    0.979063\r\n",
      "exploration/env_infos/reward_dist Min                    7.70115e-61\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.180492\r\n",
      "exploration/env_infos/final/reward_energy Std            0.119609\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0493929\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.401154\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.47752\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.283066\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0116296\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.791655\r\n",
      "exploration/env_infos/reward_energy Mean                -0.18824\r\n",
      "exploration/env_infos/reward_energy Std                  0.153992\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0110388\r\n",
      "exploration/env_infos/reward_energy Min                 -0.791655\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.205558\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.313639\r\n",
      "exploration/env_infos/final/end_effector_loc Max         1\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.0995369\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00377049\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0192606\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0285209\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0361219\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0861278\r\n",
      "exploration/env_infos/end_effector_loc Std               0.216575\r\n",
      "exploration/env_infos/end_effector_loc Max               1\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.208321\r\n",
      "evaluation/num steps total                           91000\r\n",
      "evaluation/num paths total                            4550\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0375306\r\n",
      "evaluation/Rewards Std                                   0.0779658\r\n",
      "evaluation/Rewards Max                                   0.162847\r\n",
      "evaluation/Rewards Min                                  -0.38362\r\n",
      "evaluation/Returns Mean                                 -0.750612\r\n",
      "evaluation/Returns Std                                   1.24582\r\n",
      "evaluation/Returns Max                                   2.71314\r\n",
      "evaluation/Returns Min                                  -2.97882\r\n",
      "evaluation/Actions Mean                                  0.00120107\r\n",
      "evaluation/Actions Std                                   0.0802581\r\n",
      "evaluation/Actions Max                                   0.66231\r\n",
      "evaluation/Actions Min                                  -0.935465\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.750612\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.247381\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.317342\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.939502\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.18724e-13\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0101378\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0179332\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0896233\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.68309e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.240723\r\n",
      "evaluation/env_infos/reward_dist Std                     0.302109\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996951\r\n",
      "evaluation/env_infos/reward_dist Min                     1.18724e-13\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0598011\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0655014\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0053453\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.301835\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.279602\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.248965\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0106258\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.988779\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0660358\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0923303\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000516442\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.988779\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0069542\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23478\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.57791\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.577618\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000544904\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0132251\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0331155\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0467733\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.000728956\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.156388\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.57791\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.577618\r\n",
      "time/data storing (s)                                    0.0028797\r\n",
      "time/evaluation sampling (s)                             0.94833\r\n",
      "time/exploration sampling (s)                            0.119104\r\n",
      "time/logging (s)                                         0.0196386\r\n",
      "time/saving (s)                                          0.0273992\r\n",
      "time/training (s)                                       48.7729\r\n",
      "time/epoch (s)                                          49.8903\r\n",
      "time/total (s)                                        4544.07\r\n",
      "Epoch                                                   90\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:41:32.818036 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 91 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00123428\n",
      "trainer/QF2 Loss                                         0.00085233\n",
      "trainer/Policy Loss                                      2.77402\n",
      "trainer/Q1 Predictions Mean                             -0.906798\n",
      "trainer/Q1 Predictions Std                               0.890154\n",
      "trainer/Q1 Predictions Max                               1.06924\n",
      "trainer/Q1 Predictions Min                              -3.41769\n",
      "trainer/Q2 Predictions Mean                             -0.903045\n",
      "trainer/Q2 Predictions Std                               0.892602\n",
      "trainer/Q2 Predictions Max                               1.09302\n",
      "trainer/Q2 Predictions Min                              -3.37066\n",
      "trainer/Q Targets Mean                                  -0.90397\n",
      "trainer/Q Targets Std                                    0.89054\n",
      "trainer/Q Targets Max                                    1.09966\n",
      "trainer/Q Targets Min                                   -3.38674\n",
      "trainer/Log Pis Mean                                     1.87763\n",
      "trainer/Log Pis Std                                      1.52987\n",
      "trainer/Log Pis Max                                      9.41667\n",
      "trainer/Log Pis Min                                     -6.19847\n",
      "trainer/Policy mu Mean                                   0.0032631\n",
      "trainer/Policy mu Std                                    0.341327\n",
      "trainer/Policy mu Max                                    2.13775\n",
      "trainer/Policy mu Min                                   -1.54728\n",
      "trainer/Policy log std Mean                             -2.23084\n",
      "trainer/Policy log std Std                               0.649008\n",
      "trainer/Policy log std Max                               1.11785\n",
      "trainer/Policy log std Min                              -3.22023\n",
      "trainer/Alpha                                            0.0232713\n",
      "trainer/Alpha Loss                                      -0.460019\n",
      "exploration/num steps total                          10200\n",
      "exploration/num paths total                            510\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0790683\n",
      "exploration/Rewards Std                                  0.0506442\n",
      "exploration/Rewards Max                                 -0.00658312\n",
      "exploration/Rewards Min                                 -0.261524\n",
      "exploration/Returns Mean                                -1.58137\n",
      "exploration/Returns Std                                  0.478011\n",
      "exploration/Returns Max                                 -0.897225\n",
      "exploration/Returns Min                                 -2.15701\n",
      "exploration/Actions Mean                                -0.00417638\n",
      "exploration/Actions Std                                  0.133492\n",
      "exploration/Actions Max                                  0.45813\n",
      "exploration/Actions Min                                 -0.880313\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.58137\n",
      "exploration/env_infos/final/reward_dist Mean             0.117311\n",
      "exploration/env_infos/final/reward_dist Std              0.187085\n",
      "exploration/env_infos/final/reward_dist Max              0.487298\n",
      "exploration/env_infos/final/reward_dist Min              1.2911e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00467156\n",
      "exploration/env_infos/initial/reward_dist Std            0.00505045\n",
      "exploration/env_infos/initial/reward_dist Max            0.0114577\n",
      "exploration/env_infos/initial/reward_dist Min            1.49405e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0451016\n",
      "exploration/env_infos/reward_dist Std                    0.0803597\n",
      "exploration/env_infos/reward_dist Max                    0.487298\n",
      "exploration/env_infos/reward_dist Min                    1.2911e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.116114\n",
      "exploration/env_infos/final/reward_energy Std            0.0544181\n",
      "exploration/env_infos/final/reward_energy Max           -0.0143198\n",
      "exploration/env_infos/final/reward_energy Min           -0.161671\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.295228\n",
      "exploration/env_infos/initial/reward_energy Std          0.327942\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0518224\n",
      "exploration/env_infos/initial/reward_energy Min         -0.932794\n",
      "exploration/env_infos/reward_energy Mean                -0.134848\n",
      "exploration/env_infos/reward_energy Std                  0.132255\n",
      "exploration/env_infos/reward_energy Max                 -0.008866\n",
      "exploration/env_infos/reward_energy Min                 -0.932794\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.12443\n",
      "exploration/env_infos/final/end_effector_loc Std         0.137382\n",
      "exploration/env_infos/final/end_effector_loc Max         0.11283\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.340223\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00459428\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0149089\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0139562\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0440156\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0770302\n",
      "exploration/env_infos/end_effector_loc Std               0.131847\n",
      "exploration/env_infos/end_effector_loc Max               0.124984\n",
      "exploration/env_infos/end_effector_loc Min              -0.442337\n",
      "evaluation/num steps total                           92000\n",
      "evaluation/num paths total                            4600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0562163\n",
      "evaluation/Rewards Std                                   0.0951759\n",
      "evaluation/Rewards Max                                   0.151251\n",
      "evaluation/Rewards Min                                  -0.563385\n",
      "evaluation/Returns Mean                                 -1.12433\n",
      "evaluation/Returns Std                                   1.51144\n",
      "evaluation/Returns Max                                   1.95285\n",
      "evaluation/Returns Min                                  -4.04166\n",
      "evaluation/Actions Mean                                 -0.00244832\n",
      "evaluation/Actions Std                                   0.103738\n",
      "evaluation/Actions Max                                   0.913266\n",
      "evaluation/Actions Min                                  -0.946562\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.12433\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0929317\n",
      "evaluation/env_infos/final/reward_dist Std               0.204088\n",
      "evaluation/env_infos/final/reward_dist Max               0.990081\n",
      "evaluation/env_infos/final/reward_dist Min               1.09003e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705997\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0140316\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0866816\n",
      "evaluation/env_infos/initial/reward_dist Min             2.25059e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.199076\n",
      "evaluation/env_infos/reward_dist Std                     0.281397\n",
      "evaluation/env_infos/reward_dist Max                     0.990081\n",
      "evaluation/env_infos/reward_dist Min                     1.09003e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.063027\n",
      "evaluation/env_infos/final/reward_energy Std             0.0678059\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00729689\n",
      "evaluation/env_infos/final/reward_energy Min            -0.296924\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.386898\n",
      "evaluation/env_infos/initial/reward_energy Std           0.257223\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00744011\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.07643\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0887136\n",
      "evaluation/env_infos/reward_energy Std                   0.116897\n",
      "evaluation/env_infos/reward_energy Max                  -0.000436344\n",
      "evaluation/env_infos/reward_energy Min                  -1.07643\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.020869\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.278568\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.65079\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.599551\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00065268\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0164131\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0456633\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0473281\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0100973\n",
      "evaluation/env_infos/end_effector_loc Std                0.190104\n",
      "evaluation/env_infos/end_effector_loc Max                0.65079\n",
      "evaluation/env_infos/end_effector_loc Min               -0.599551\n",
      "time/data storing (s)                                    0.0032028\n",
      "time/evaluation sampling (s)                             1.01234\n",
      "time/exploration sampling (s)                            0.126626\n",
      "time/logging (s)                                         0.0217783\n",
      "time/saving (s)                                          0.0292397\n",
      "time/training (s)                                       49.7921\n",
      "time/epoch (s)                                          50.9852\n",
      "time/total (s)                                        4596.2\n",
      "Epoch                                                   91\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:42:24.140497 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 92 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000881785\n",
      "trainer/QF2 Loss                                         0.000763298\n",
      "trainer/Policy Loss                                      3.13001\n",
      "trainer/Q1 Predictions Mean                             -1.04225\n",
      "trainer/Q1 Predictions Std                               0.92153\n",
      "trainer/Q1 Predictions Max                               1.11958\n",
      "trainer/Q1 Predictions Min                              -3.44071\n",
      "trainer/Q2 Predictions Mean                             -1.03701\n",
      "trainer/Q2 Predictions Std                               0.923219\n",
      "trainer/Q2 Predictions Max                               1.11549\n",
      "trainer/Q2 Predictions Min                              -3.45125\n",
      "trainer/Q Targets Mean                                  -1.03499\n",
      "trainer/Q Targets Std                                    0.930849\n",
      "trainer/Q Targets Max                                    1.13908\n",
      "trainer/Q Targets Min                                   -3.41586\n",
      "trainer/Log Pis Mean                                     2.09011\n",
      "trainer/Log Pis Std                                      1.33522\n",
      "trainer/Log Pis Max                                      4.28459\n",
      "trainer/Log Pis Min                                     -2.80409\n",
      "trainer/Policy mu Mean                                   0.0103411\n",
      "trainer/Policy mu Std                                    0.287976\n",
      "trainer/Policy mu Max                                    1.80597\n",
      "trainer/Policy mu Min                                   -1.43025\n",
      "trainer/Policy log std Mean                             -2.35636\n",
      "trainer/Policy log std Std                               0.597603\n",
      "trainer/Policy log std Max                              -0.468701\n",
      "trainer/Policy log std Min                              -3.19407\n",
      "trainer/Alpha                                            0.0229874\n",
      "trainer/Alpha Loss                                       0.34003\n",
      "exploration/num steps total                          10300\n",
      "exploration/num paths total                            515\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.070474\n",
      "exploration/Rewards Std                                  0.0577322\n",
      "exploration/Rewards Max                                  0.0465409\n",
      "exploration/Rewards Min                                 -0.249857\n",
      "exploration/Returns Mean                                -1.40948\n",
      "exploration/Returns Std                                  0.645698\n",
      "exploration/Returns Max                                 -0.88944\n",
      "exploration/Returns Min                                 -2.65808\n",
      "exploration/Actions Mean                                -0.00174995\n",
      "exploration/Actions Std                                  0.0904617\n",
      "exploration/Actions Max                                  0.27762\n",
      "exploration/Actions Min                                 -0.328585\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.40948\n",
      "exploration/env_infos/final/reward_dist Mean             0.272191\n",
      "exploration/env_infos/final/reward_dist Std              0.327954\n",
      "exploration/env_infos/final/reward_dist Max              0.778744\n",
      "exploration/env_infos/final/reward_dist Min              0.00208809\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00869101\n",
      "exploration/env_infos/initial/reward_dist Std            0.0128147\n",
      "exploration/env_infos/initial/reward_dist Max            0.0337604\n",
      "exploration/env_infos/initial/reward_dist Min            5.45648e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.153023\n",
      "exploration/env_infos/reward_dist Std                    0.228222\n",
      "exploration/env_infos/reward_dist Max                    0.946448\n",
      "exploration/env_infos/reward_dist Min                    5.45648e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.174964\n",
      "exploration/env_infos/final/reward_energy Std            0.133364\n",
      "exploration/env_infos/final/reward_energy Max           -0.0463026\n",
      "exploration/env_infos/final/reward_energy Min           -0.428768\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.16898\n",
      "exploration/env_infos/initial/reward_energy Std          0.108237\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0724433\n",
      "exploration/env_infos/initial/reward_energy Min         -0.316997\n",
      "exploration/env_infos/reward_energy Mean                -0.103157\n",
      "exploration/env_infos/reward_energy Std                  0.0757057\n",
      "exploration/env_infos/reward_energy Max                 -0.0190504\n",
      "exploration/env_infos/reward_energy Min                 -0.428768\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00691004\n",
      "exploration/env_infos/final/end_effector_loc Std         0.186741\n",
      "exploration/env_infos/final/end_effector_loc Max         0.256686\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.274634\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00187988\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00684127\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0129462\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00914399\n",
      "exploration/env_infos/end_effector_loc Mean              0.0185676\n",
      "exploration/env_infos/end_effector_loc Std               0.122167\n",
      "exploration/env_infos/end_effector_loc Max               0.303053\n",
      "exploration/env_infos/end_effector_loc Min              -0.274634\n",
      "evaluation/num steps total                           93000\n",
      "evaluation/num paths total                            4650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0471932\n",
      "evaluation/Rewards Std                                   0.0850357\n",
      "evaluation/Rewards Max                                   0.144433\n",
      "evaluation/Rewards Min                                  -0.635974\n",
      "evaluation/Returns Mean                                 -0.943864\n",
      "evaluation/Returns Std                                   1.21566\n",
      "evaluation/Returns Max                                   1.71129\n",
      "evaluation/Returns Min                                  -4.3987\n",
      "evaluation/Actions Mean                                 -0.00117127\n",
      "evaluation/Actions Std                                   0.0931722\n",
      "evaluation/Actions Max                                   0.833919\n",
      "evaluation/Actions Min                                  -0.872828\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.943864\n",
      "evaluation/env_infos/final/reward_dist Mean              0.213278\n",
      "evaluation/env_infos/final/reward_dist Std               0.303675\n",
      "evaluation/env_infos/final/reward_dist Max               0.969478\n",
      "evaluation/env_infos/final/reward_dist Min               9.06188e-28\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0141376\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0292777\n",
      "evaluation/env_infos/initial/reward_dist Max             0.14953\n",
      "evaluation/env_infos/initial/reward_dist Min             2.87403e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.20467\n",
      "evaluation/env_infos/reward_dist Std                     0.283211\n",
      "evaluation/env_infos/reward_dist Max                     0.996765\n",
      "evaluation/env_infos/reward_dist Min                     9.06188e-28\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0422569\n",
      "evaluation/env_infos/final/reward_energy Std             0.0367209\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000947462\n",
      "evaluation/env_infos/final/reward_energy Min            -0.149969\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.324137\n",
      "evaluation/env_infos/initial/reward_energy Std           0.301542\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00288932\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.20717\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0737012\n",
      "evaluation/env_infos/reward_energy Std                   0.109238\n",
      "evaluation/env_infos/reward_energy Max                  -0.000906137\n",
      "evaluation/env_infos/reward_energy Min                  -1.20717\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00574169\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.26625\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.692232\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.771108\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000364004\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0156479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.041696\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0436414\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00187669\n",
      "evaluation/env_infos/end_effector_loc Std                0.179506\n",
      "evaluation/env_infos/end_effector_loc Max                0.692232\n",
      "evaluation/env_infos/end_effector_loc Min               -0.771108\n",
      "time/data storing (s)                                    0.00292084\n",
      "time/evaluation sampling (s)                             0.990382\n",
      "time/exploration sampling (s)                            0.124531\n",
      "time/logging (s)                                         0.0194273\n",
      "time/saving (s)                                          0.0281375\n",
      "time/training (s)                                       48.9824\n",
      "time/epoch (s)                                          50.1478\n",
      "time/total (s)                                        4647.51\n",
      "Epoch                                                   92\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:43:15.761360 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 93 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000552864\n",
      "trainer/QF2 Loss                                         0.000750813\n",
      "trainer/Policy Loss                                      3.05719\n",
      "trainer/Q1 Predictions Mean                             -1.03226\n",
      "trainer/Q1 Predictions Std                               0.927364\n",
      "trainer/Q1 Predictions Max                               1.24941\n",
      "trainer/Q1 Predictions Min                              -3.33057\n",
      "trainer/Q2 Predictions Mean                             -1.03732\n",
      "trainer/Q2 Predictions Std                               0.92654\n",
      "trainer/Q2 Predictions Max                               1.2585\n",
      "trainer/Q2 Predictions Min                              -3.32075\n",
      "trainer/Q Targets Mean                                  -1.02803\n",
      "trainer/Q Targets Std                                    0.927139\n",
      "trainer/Q Targets Max                                    1.27629\n",
      "trainer/Q Targets Min                                   -3.34463\n",
      "trainer/Log Pis Mean                                     2.03596\n",
      "trainer/Log Pis Std                                      1.29991\n",
      "trainer/Log Pis Max                                      4.26897\n",
      "trainer/Log Pis Min                                     -3.0998\n",
      "trainer/Policy mu Mean                                  -0.00950598\n",
      "trainer/Policy mu Std                                    0.351936\n",
      "trainer/Policy mu Max                                    2.28394\n",
      "trainer/Policy mu Min                                   -1.84048\n",
      "trainer/Policy log std Mean                             -2.29362\n",
      "trainer/Policy log std Std                               0.646054\n",
      "trainer/Policy log std Max                              -0.123959\n",
      "trainer/Policy log std Min                              -3.19659\n",
      "trainer/Alpha                                            0.0244861\n",
      "trainer/Alpha Loss                                       0.133436\n",
      "exploration/num steps total                          10400\n",
      "exploration/num paths total                            520\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0474226\n",
      "exploration/Rewards Std                                  0.0751239\n",
      "exploration/Rewards Max                                  0.0642233\n",
      "exploration/Rewards Min                                 -0.232944\n",
      "exploration/Returns Mean                                -0.948451\n",
      "exploration/Returns Std                                  1.09127\n",
      "exploration/Returns Max                                  0.162239\n",
      "exploration/Returns Min                                 -2.98206\n",
      "exploration/Actions Mean                                -0.000756202\n",
      "exploration/Actions Std                                  0.143882\n",
      "exploration/Actions Max                                  0.557134\n",
      "exploration/Actions Min                                 -0.768123\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.948451\n",
      "exploration/env_infos/final/reward_dist Mean             0.279471\n",
      "exploration/env_infos/final/reward_dist Std              0.198886\n",
      "exploration/env_infos/final/reward_dist Max              0.579468\n",
      "exploration/env_infos/final/reward_dist Min              1.35962e-09\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00176598\n",
      "exploration/env_infos/initial/reward_dist Std            0.0029237\n",
      "exploration/env_infos/initial/reward_dist Max            0.00758152\n",
      "exploration/env_infos/initial/reward_dist Min            6.02475e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.313747\n",
      "exploration/env_infos/reward_dist Std                    0.331051\n",
      "exploration/env_infos/reward_dist Max                    0.994138\n",
      "exploration/env_infos/reward_dist Min                    1.35962e-09\n",
      "exploration/env_infos/final/reward_energy Mean          -0.186314\n",
      "exploration/env_infos/final/reward_energy Std            0.107127\n",
      "exploration/env_infos/final/reward_energy Max           -0.0519045\n",
      "exploration/env_infos/final/reward_energy Min           -0.378223\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.273843\n",
      "exploration/env_infos/initial/reward_energy Std          0.272559\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0638855\n",
      "exploration/env_infos/initial/reward_energy Min         -0.77913\n",
      "exploration/env_infos/reward_energy Mean                -0.154393\n",
      "exploration/env_infos/reward_energy Std                  0.132545\n",
      "exploration/env_infos/reward_energy Max                 -0.00433585\n",
      "exploration/env_infos/reward_energy Min                 -0.77913\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0196306\n",
      "exploration/env_infos/final/end_effector_loc Std         0.209946\n",
      "exploration/env_infos/final/end_effector_loc Max         0.279164\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.281417\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0018457\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0135348\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0160664\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0384062\n",
      "exploration/env_infos/end_effector_loc Mean              0.0166589\n",
      "exploration/env_infos/end_effector_loc Std               0.140372\n",
      "exploration/env_infos/end_effector_loc Max               0.296906\n",
      "exploration/env_infos/end_effector_loc Min              -0.281417\n",
      "evaluation/num steps total                           94000\n",
      "evaluation/num paths total                            4700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.041777\n",
      "evaluation/Rewards Std                                   0.0771634\n",
      "evaluation/Rewards Max                                   0.142204\n",
      "evaluation/Rewards Min                                  -0.439577\n",
      "evaluation/Returns Mean                                 -0.83554\n",
      "evaluation/Returns Std                                   1.14921\n",
      "evaluation/Returns Max                                   1.20556\n",
      "evaluation/Returns Min                                  -3.84012\n",
      "evaluation/Actions Mean                                  0.000223204\n",
      "evaluation/Actions Std                                   0.0739557\n",
      "evaluation/Actions Max                                   0.662821\n",
      "evaluation/Actions Min                                  -0.750245\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.83554\n",
      "evaluation/env_infos/final/reward_dist Mean              0.243994\n",
      "evaluation/env_infos/final/reward_dist Std               0.300124\n",
      "evaluation/env_infos/final/reward_dist Max               0.948336\n",
      "evaluation/env_infos/final/reward_dist Min               6.78386e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00569732\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0100974\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0407956\n",
      "evaluation/env_infos/initial/reward_dist Min             1.26062e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.197057\n",
      "evaluation/env_infos/reward_dist Std                     0.285154\n",
      "evaluation/env_infos/reward_dist Max                     0.993238\n",
      "evaluation/env_infos/reward_dist Min                     6.78386e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0403232\n",
      "evaluation/env_infos/final/reward_energy Std             0.03679\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00255966\n",
      "evaluation/env_infos/final/reward_energy Min            -0.172251\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.21221\n",
      "evaluation/env_infos/initial/reward_energy Std           0.214899\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.014303\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.75515\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0571961\n",
      "evaluation/env_infos/reward_energy Std                   0.0875648\n",
      "evaluation/env_infos/reward_energy Max                  -0.000656482\n",
      "evaluation/env_infos/reward_energy Min                  -0.812033\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0063101\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.243653\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.700578\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.44369\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000201977\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010676\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0301827\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0375122\n",
      "evaluation/env_infos/end_effector_loc Mean               0.000471489\n",
      "evaluation/env_infos/end_effector_loc Std                0.154048\n",
      "evaluation/env_infos/end_effector_loc Max                0.700578\n",
      "evaluation/env_infos/end_effector_loc Min               -0.44369\n",
      "time/data storing (s)                                    0.00327324\n",
      "time/evaluation sampling (s)                             0.944376\n",
      "time/exploration sampling (s)                            0.133575\n",
      "time/logging (s)                                         0.0201781\n",
      "time/saving (s)                                          0.0274239\n",
      "time/training (s)                                       49.2946\n",
      "time/epoch (s)                                          50.4234\n",
      "time/total (s)                                        4699.13\n",
      "Epoch                                                   93\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:44:07.471348 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 94 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000688004\n",
      "trainer/QF2 Loss                                         0.000717312\n",
      "trainer/Policy Loss                                      3.06034\n",
      "trainer/Q1 Predictions Mean                             -0.982229\n",
      "trainer/Q1 Predictions Std                               0.874711\n",
      "trainer/Q1 Predictions Max                               1.2721\n",
      "trainer/Q1 Predictions Min                              -3.23544\n",
      "trainer/Q2 Predictions Mean                             -0.973025\n",
      "trainer/Q2 Predictions Std                               0.871957\n",
      "trainer/Q2 Predictions Max                               1.28832\n",
      "trainer/Q2 Predictions Min                              -3.29083\n",
      "trainer/Q Targets Mean                                  -0.974871\n",
      "trainer/Q Targets Std                                    0.872212\n",
      "trainer/Q Targets Max                                    1.26761\n",
      "trainer/Q Targets Min                                   -3.25212\n",
      "trainer/Log Pis Mean                                     2.08728\n",
      "trainer/Log Pis Std                                      1.50328\n",
      "trainer/Log Pis Max                                      4.72616\n",
      "trainer/Log Pis Min                                     -4.13195\n",
      "trainer/Policy mu Mean                                   0.0239979\n",
      "trainer/Policy mu Std                                    0.342394\n",
      "trainer/Policy mu Max                                    2.52072\n",
      "trainer/Policy mu Min                                   -2.29813\n",
      "trainer/Policy log std Mean                             -2.39093\n",
      "trainer/Policy log std Std                               0.687371\n",
      "trainer/Policy log std Max                              -0.0266174\n",
      "trainer/Policy log std Min                              -3.29567\n",
      "trainer/Alpha                                            0.0238707\n",
      "trainer/Alpha Loss                                       0.325977\n",
      "exploration/num steps total                          10500\n",
      "exploration/num paths total                            525\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0745564\n",
      "exploration/Rewards Std                                  0.0731778\n",
      "exploration/Rewards Max                                  0.0799721\n",
      "exploration/Rewards Min                                 -0.332349\n",
      "exploration/Returns Mean                                -1.49113\n",
      "exploration/Returns Std                                  0.672955\n",
      "exploration/Returns Max                                 -0.819372\n",
      "exploration/Returns Min                                 -2.76734\n",
      "exploration/Actions Mean                                -0.00314385\n",
      "exploration/Actions Std                                  0.166792\n",
      "exploration/Actions Max                                  0.669795\n",
      "exploration/Actions Min                                 -0.663214\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.49113\n",
      "exploration/env_infos/final/reward_dist Mean             0.118917\n",
      "exploration/env_infos/final/reward_dist Std              0.205142\n",
      "exploration/env_infos/final/reward_dist Max              0.527486\n",
      "exploration/env_infos/final/reward_dist Min              3.49265e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00799545\n",
      "exploration/env_infos/initial/reward_dist Std            0.00992791\n",
      "exploration/env_infos/initial/reward_dist Max            0.0273758\n",
      "exploration/env_infos/initial/reward_dist Min            0.000600852\n",
      "exploration/env_infos/reward_dist Mean                   0.269861\n",
      "exploration/env_infos/reward_dist Std                    0.304526\n",
      "exploration/env_infos/reward_dist Max                    0.999244\n",
      "exploration/env_infos/reward_dist Min                    3.49265e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.110215\n",
      "exploration/env_infos/final/reward_energy Std            0.0768455\n",
      "exploration/env_infos/final/reward_energy Max           -0.0276589\n",
      "exploration/env_infos/final/reward_energy Min           -0.248298\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.382988\n",
      "exploration/env_infos/initial/reward_energy Std          0.269855\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0916288\n",
      "exploration/env_infos/initial/reward_energy Min         -0.721265\n",
      "exploration/env_infos/reward_energy Mean                -0.162365\n",
      "exploration/env_infos/reward_energy Std                  0.171163\n",
      "exploration/env_infos/reward_energy Max                 -0.0109869\n",
      "exploration/env_infos/reward_energy Min                 -0.851149\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0351003\n",
      "exploration/env_infos/final/end_effector_loc Std         0.252962\n",
      "exploration/env_infos/final/end_effector_loc Max         0.411933\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.369028\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000762092\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0165468\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0334898\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0331607\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0133455\n",
      "exploration/env_infos/end_effector_loc Std               0.168502\n",
      "exploration/env_infos/end_effector_loc Max               0.411933\n",
      "exploration/env_infos/end_effector_loc Min              -0.369028\n",
      "evaluation/num steps total                           95000\n",
      "evaluation/num paths total                            4750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0551627\n",
      "evaluation/Rewards Std                                   0.0801043\n",
      "evaluation/Rewards Max                                   0.151601\n",
      "evaluation/Rewards Min                                  -0.481389\n",
      "evaluation/Returns Mean                                 -1.10325\n",
      "evaluation/Returns Std                                   1.14044\n",
      "evaluation/Returns Max                                   1.56293\n",
      "evaluation/Returns Min                                  -3.84451\n",
      "evaluation/Actions Mean                                  0.00349118\n",
      "evaluation/Actions Std                                   0.0815461\n",
      "evaluation/Actions Max                                   0.59823\n",
      "evaluation/Actions Min                                  -0.820185\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.10325\n",
      "evaluation/env_infos/final/reward_dist Mean              0.136655\n",
      "evaluation/env_infos/final/reward_dist Std               0.241591\n",
      "evaluation/env_infos/final/reward_dist Max               0.787572\n",
      "evaluation/env_infos/final/reward_dist Min               6.50086e-66\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00696124\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0129431\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0651122\n",
      "evaluation/env_infos/initial/reward_dist Min             2.6294e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.170711\n",
      "evaluation/env_infos/reward_dist Std                     0.271852\n",
      "evaluation/env_infos/reward_dist Max                     0.999358\n",
      "evaluation/env_infos/reward_dist Min                     6.50086e-66\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0459941\n",
      "evaluation/env_infos/final/reward_energy Std             0.0810337\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00257424\n",
      "evaluation/env_infos/final/reward_energy Min            -0.572593\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.255375\n",
      "evaluation/env_infos/initial/reward_energy Std           0.241591\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.000679267\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.01518\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0648959\n",
      "evaluation/env_infos/reward_energy Std                   0.095459\n",
      "evaluation/env_infos/reward_energy Max                  -0.00066192\n",
      "evaluation/env_infos/reward_energy Min                  -1.01518\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0190783\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.280253\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.558225\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00185366\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122899\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0299115\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0410092\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00182413\n",
      "evaluation/env_infos/end_effector_loc Std                0.175094\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.558225\n",
      "time/data storing (s)                                    0.00556827\n",
      "time/evaluation sampling (s)                             0.954993\n",
      "time/exploration sampling (s)                            0.131897\n",
      "time/logging (s)                                         0.0202891\n",
      "time/saving (s)                                          0.0273594\n",
      "time/training (s)                                       49.4755\n",
      "time/epoch (s)                                          50.6157\n",
      "time/total (s)                                        4750.84\n",
      "Epoch                                                   94\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:44:59.746889 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 95 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000758044\r\n",
      "trainer/QF2 Loss                                         0.00111567\r\n",
      "trainer/Policy Loss                                      2.8698\r\n",
      "trainer/Q1 Predictions Mean                             -0.940152\r\n",
      "trainer/Q1 Predictions Std                               0.885712\r\n",
      "trainer/Q1 Predictions Max                               1.19003\r\n",
      "trainer/Q1 Predictions Min                              -3.26018\r\n",
      "trainer/Q2 Predictions Mean                             -0.937716\r\n",
      "trainer/Q2 Predictions Std                               0.89208\r\n",
      "trainer/Q2 Predictions Max                               1.20477\r\n",
      "trainer/Q2 Predictions Min                              -3.26447\r\n",
      "trainer/Q Targets Mean                                  -0.943889\r\n",
      "trainer/Q Targets Std                                    0.892841\r\n",
      "trainer/Q Targets Max                                    1.17051\r\n",
      "trainer/Q Targets Min                                   -3.26833\r\n",
      "trainer/Log Pis Mean                                     1.93788\r\n",
      "trainer/Log Pis Std                                      1.44957\r\n",
      "trainer/Log Pis Max                                      4.32512\r\n",
      "trainer/Log Pis Min                                     -4.86589\r\n",
      "trainer/Policy mu Mean                                   0.00882974\r\n",
      "trainer/Policy mu Std                                    0.324555\r\n",
      "trainer/Policy mu Max                                    2.94644\r\n",
      "trainer/Policy mu Min                                   -1.5805\r\n",
      "trainer/Policy log std Mean                             -2.29702\r\n",
      "trainer/Policy log std Std                               0.636705\r\n",
      "trainer/Policy log std Max                               0.21114\r\n",
      "trainer/Policy log std Min                              -3.34425\r\n",
      "trainer/Alpha                                            0.0245923\r\n",
      "trainer/Alpha Loss                                      -0.230166\r\n",
      "exploration/num steps total                          10600\r\n",
      "exploration/num paths total                            530\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0627204\r\n",
      "exploration/Rewards Std                                  0.105086\r\n",
      "exploration/Rewards Max                                  0.13097\r\n",
      "exploration/Rewards Min                                 -0.345744\r\n",
      "exploration/Returns Mean                                -1.25441\r\n",
      "exploration/Returns Std                                  1.6589\r\n",
      "exploration/Returns Max                                  1.1709\r\n",
      "exploration/Returns Min                                 -3.80937\r\n",
      "exploration/Actions Mean                                -0.00101617\r\n",
      "exploration/Actions Std                                  0.212225\r\n",
      "exploration/Actions Max                                  0.834595\r\n",
      "exploration/Actions Min                                 -0.786222\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.25441\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.267103\r\n",
      "exploration/env_infos/final/reward_dist Std              0.248473\r\n",
      "exploration/env_infos/final/reward_dist Max              0.721079\r\n",
      "exploration/env_infos/final/reward_dist Min              0.000264099\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0123459\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0112297\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0291787\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.1233e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.224949\r\n",
      "exploration/env_infos/reward_dist Std                    0.277695\r\n",
      "exploration/env_infos/reward_dist Max                    0.994677\r\n",
      "exploration/env_infos/reward_dist Min                    2.1233e-05\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.17428\r\n",
      "exploration/env_infos/final/reward_energy Std            0.107365\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0334332\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.335858\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.380144\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.15034\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.121184\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.537139\r\n",
      "exploration/env_infos/reward_energy Mean                -0.235651\r\n",
      "exploration/env_infos/reward_energy Std                  0.185875\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0220533\r\n",
      "exploration/env_infos/reward_energy Min                 -0.85963\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0543693\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.228366\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.453216\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.31111\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00406145\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0138706\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0217428\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0130313\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0610448\r\n",
      "exploration/env_infos/end_effector_loc Std               0.170153\r\n",
      "exploration/env_infos/end_effector_loc Max               0.477645\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.31111\r\n",
      "evaluation/num steps total                           96000\r\n",
      "evaluation/num paths total                            4800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0584013\r\n",
      "evaluation/Rewards Std                                   0.0963796\r\n",
      "evaluation/Rewards Max                                   0.185632\r\n",
      "evaluation/Rewards Min                                  -1.05927\r\n",
      "evaluation/Returns Mean                                 -1.16803\r\n",
      "evaluation/Returns Std                                   1.39482\r\n",
      "evaluation/Returns Max                                   2.27492\r\n",
      "evaluation/Returns Min                                  -5.34732\r\n",
      "evaluation/Actions Mean                                  0.00361708\r\n",
      "evaluation/Actions Std                                   0.0877271\r\n",
      "evaluation/Actions Max                                   0.827899\r\n",
      "evaluation/Actions Min                                  -0.77107\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.16803\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.121693\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.200953\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.809957\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.32103e-76\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00487069\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00696911\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0228596\r\n",
      "evaluation/env_infos/initial/reward_dist Min             3.31288e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.190191\r\n",
      "evaluation/env_infos/reward_dist Std                     0.27434\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997352\r\n",
      "evaluation/env_infos/reward_dist Min                     1.32103e-76\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0513059\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.114378\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00146481\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.814317\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.263047\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.231751\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0151685\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.977458\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.06431\r\n",
      "evaluation/env_infos/reward_energy Std                   0.106219\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00146481\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.16607\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0224897\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.305299\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.889316\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000159913\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123936\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0312294\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0385535\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00742768\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.182973\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.889316\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00287587\r\n",
      "time/evaluation sampling (s)                             1.28538\r\n",
      "time/exploration sampling (s)                            0.127847\r\n",
      "time/logging (s)                                         0.0208127\r\n",
      "time/saving (s)                                          0.0269602\r\n",
      "time/training (s)                                       49.4971\r\n",
      "time/epoch (s)                                          50.961\r\n",
      "time/total (s)                                        4803.11\r\n",
      "Epoch                                                   95\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:45:52.006662 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 96 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000856302\n",
      "trainer/QF2 Loss                                         0.00066757\n",
      "trainer/Policy Loss                                      3.04197\n",
      "trainer/Q1 Predictions Mean                             -0.914488\n",
      "trainer/Q1 Predictions Std                               0.89333\n",
      "trainer/Q1 Predictions Max                               1.21323\n",
      "trainer/Q1 Predictions Min                              -3.05841\n",
      "trainer/Q2 Predictions Mean                             -0.922874\n",
      "trainer/Q2 Predictions Std                               0.894385\n",
      "trainer/Q2 Predictions Max                               1.17264\n",
      "trainer/Q2 Predictions Min                              -3.08942\n",
      "trainer/Q Targets Mean                                  -0.92274\n",
      "trainer/Q Targets Std                                    0.896049\n",
      "trainer/Q Targets Max                                    1.18823\n",
      "trainer/Q Targets Min                                   -3.14664\n",
      "trainer/Log Pis Mean                                     2.1362\n",
      "trainer/Log Pis Std                                      1.43315\n",
      "trainer/Log Pis Max                                      4.63741\n",
      "trainer/Log Pis Min                                     -4.19714\n",
      "trainer/Policy mu Mean                                   0.000297535\n",
      "trainer/Policy mu Std                                    0.280818\n",
      "trainer/Policy mu Max                                    2.38247\n",
      "trainer/Policy mu Min                                   -1.83082\n",
      "trainer/Policy log std Mean                             -2.38514\n",
      "trainer/Policy log std Std                               0.584461\n",
      "trainer/Policy log std Max                              -0.67978\n",
      "trainer/Policy log std Min                              -3.30892\n",
      "trainer/Alpha                                            0.0237794\n",
      "trainer/Alpha Loss                                       0.50932\n",
      "exploration/num steps total                          10700\n",
      "exploration/num paths total                            535\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0418833\n",
      "exploration/Rewards Std                                  0.0852786\n",
      "exploration/Rewards Max                                  0.0890087\n",
      "exploration/Rewards Min                                 -0.379224\n",
      "exploration/Returns Mean                                -0.837666\n",
      "exploration/Returns Std                                  1.36166\n",
      "exploration/Returns Max                                  0.510004\n",
      "exploration/Returns Min                                 -2.82186\n",
      "exploration/Actions Mean                                 0.00518576\n",
      "exploration/Actions Std                                  0.120675\n",
      "exploration/Actions Max                                  0.456777\n",
      "exploration/Actions Min                                 -0.609062\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.837666\n",
      "exploration/env_infos/final/reward_dist Mean             0.207554\n",
      "exploration/env_infos/final/reward_dist Std              0.320219\n",
      "exploration/env_infos/final/reward_dist Max              0.829304\n",
      "exploration/env_infos/final/reward_dist Min              0.000199841\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0133204\n",
      "exploration/env_infos/initial/reward_dist Std            0.0165284\n",
      "exploration/env_infos/initial/reward_dist Max            0.0458353\n",
      "exploration/env_infos/initial/reward_dist Min            0.0029938\n",
      "exploration/env_infos/reward_dist Mean                   0.213549\n",
      "exploration/env_infos/reward_dist Std                    0.269068\n",
      "exploration/env_infos/reward_dist Max                    0.959488\n",
      "exploration/env_infos/reward_dist Min                    0.000199841\n",
      "exploration/env_infos/final/reward_energy Mean          -0.141519\n",
      "exploration/env_infos/final/reward_energy Std            0.108185\n",
      "exploration/env_infos/final/reward_energy Max           -0.0165354\n",
      "exploration/env_infos/final/reward_energy Min           -0.323003\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.35154\n",
      "exploration/env_infos/initial/reward_energy Std          0.177474\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0829441\n",
      "exploration/env_infos/initial/reward_energy Min         -0.61416\n",
      "exploration/env_infos/reward_energy Mean                -0.131261\n",
      "exploration/env_infos/reward_energy Std                  0.109312\n",
      "exploration/env_infos/reward_energy Max                 -0.00862862\n",
      "exploration/env_infos/reward_energy Min                 -0.61416\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0545678\n",
      "exploration/env_infos/final/end_effector_loc Std         0.146088\n",
      "exploration/env_infos/final/end_effector_loc Max         0.263877\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.152186\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00190476\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.013792\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0228388\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0304531\n",
      "exploration/env_infos/end_effector_loc Mean              0.0201514\n",
      "exploration/env_infos/end_effector_loc Std               0.124326\n",
      "exploration/env_infos/end_effector_loc Max               0.263877\n",
      "exploration/env_infos/end_effector_loc Min              -0.24456\n",
      "evaluation/num steps total                           97000\n",
      "evaluation/num paths total                            4850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0304878\n",
      "evaluation/Rewards Std                                   0.073711\n",
      "evaluation/Rewards Max                                   0.158272\n",
      "evaluation/Rewards Min                                  -0.30184\n",
      "evaluation/Returns Mean                                 -0.609756\n",
      "evaluation/Returns Std                                   1.1213\n",
      "evaluation/Returns Max                                   2.30432\n",
      "evaluation/Returns Min                                  -2.77697\n",
      "evaluation/Actions Mean                                  0.00240792\n",
      "evaluation/Actions Std                                   0.0757135\n",
      "evaluation/Actions Max                                   0.569313\n",
      "evaluation/Actions Min                                  -0.833519\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.609756\n",
      "evaluation/env_infos/final/reward_dist Mean              0.141798\n",
      "evaluation/env_infos/final/reward_dist Std               0.234228\n",
      "evaluation/env_infos/final/reward_dist Max               0.86428\n",
      "evaluation/env_infos/final/reward_dist Min               4.64787e-34\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0122134\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0214538\n",
      "evaluation/env_infos/initial/reward_dist Max             0.112471\n",
      "evaluation/env_infos/initial/reward_dist Min             2.29793e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.197493\n",
      "evaluation/env_infos/reward_dist Std                     0.267954\n",
      "evaluation/env_infos/reward_dist Max                     0.997187\n",
      "evaluation/env_infos/reward_dist Min                     4.64787e-34\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.037745\n",
      "evaluation/env_infos/final/reward_energy Std             0.0335232\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00965169\n",
      "evaluation/env_infos/final/reward_energy Min            -0.190099\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.266814\n",
      "evaluation/env_infos/initial/reward_energy Std           0.211506\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.013866\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.944848\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0633629\n",
      "evaluation/env_infos/reward_energy Std                   0.0863817\n",
      "evaluation/env_infos/reward_energy Max                  -0.000306703\n",
      "evaluation/env_infos/reward_energy Min                  -0.944848\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0334023\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.25405\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.680096\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.740994\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000306194\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0120338\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0284656\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0416759\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0140236\n",
      "evaluation/env_infos/end_effector_loc Std                0.164969\n",
      "evaluation/env_infos/end_effector_loc Max                0.680096\n",
      "evaluation/env_infos/end_effector_loc Min               -0.740994\n",
      "time/data storing (s)                                    0.00291565\n",
      "time/evaluation sampling (s)                             0.987612\n",
      "time/exploration sampling (s)                            0.12452\n",
      "time/logging (s)                                         0.0183376\n",
      "time/saving (s)                                          0.02668\n",
      "time/training (s)                                       49.8597\n",
      "time/epoch (s)                                          51.0197\n",
      "time/total (s)                                        4855.37\n",
      "Epoch                                                   96\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:46:43.865828 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 97 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000648741\n",
      "trainer/QF2 Loss                                         0.000595328\n",
      "trainer/Policy Loss                                      2.7864\n",
      "trainer/Q1 Predictions Mean                             -0.879615\n",
      "trainer/Q1 Predictions Std                               0.879684\n",
      "trainer/Q1 Predictions Max                               1.37787\n",
      "trainer/Q1 Predictions Min                              -3.29309\n",
      "trainer/Q2 Predictions Mean                             -0.88585\n",
      "trainer/Q2 Predictions Std                               0.873978\n",
      "trainer/Q2 Predictions Max                               1.36005\n",
      "trainer/Q2 Predictions Min                              -3.19472\n",
      "trainer/Q Targets Mean                                  -0.88556\n",
      "trainer/Q Targets Std                                    0.87643\n",
      "trainer/Q Targets Max                                    1.3235\n",
      "trainer/Q Targets Min                                   -3.1628\n",
      "trainer/Log Pis Mean                                     1.91316\n",
      "trainer/Log Pis Std                                      1.41629\n",
      "trainer/Log Pis Max                                      4.52847\n",
      "trainer/Log Pis Min                                     -2.21281\n",
      "trainer/Policy mu Mean                                   0.00313378\n",
      "trainer/Policy mu Std                                    0.326168\n",
      "trainer/Policy mu Max                                    1.96702\n",
      "trainer/Policy mu Min                                   -2.16815\n",
      "trainer/Policy log std Mean                             -2.28222\n",
      "trainer/Policy log std Std                               0.624367\n",
      "trainer/Policy log std Max                              -0.279602\n",
      "trainer/Policy log std Min                              -3.30772\n",
      "trainer/Alpha                                            0.0231315\n",
      "trainer/Alpha Loss                                      -0.327062\n",
      "exploration/num steps total                          10800\n",
      "exploration/num paths total                            540\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0781071\n",
      "exploration/Rewards Std                                  0.0709478\n",
      "exploration/Rewards Max                                  0.100983\n",
      "exploration/Rewards Min                                 -0.284782\n",
      "exploration/Returns Mean                                -1.56214\n",
      "exploration/Returns Std                                  0.550719\n",
      "exploration/Returns Max                                 -0.734329\n",
      "exploration/Returns Min                                 -2.39623\n",
      "exploration/Actions Mean                                 0.0132154\n",
      "exploration/Actions Std                                  0.137956\n",
      "exploration/Actions Max                                  0.471749\n",
      "exploration/Actions Min                                 -0.605127\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.56214\n",
      "exploration/env_infos/final/reward_dist Mean             0.00683041\n",
      "exploration/env_infos/final/reward_dist Std              0.0125174\n",
      "exploration/env_infos/final/reward_dist Max              0.031852\n",
      "exploration/env_infos/final/reward_dist Min              1.56785e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00742935\n",
      "exploration/env_infos/initial/reward_dist Std            0.00694507\n",
      "exploration/env_infos/initial/reward_dist Max            0.0185226\n",
      "exploration/env_infos/initial/reward_dist Min            5.02147e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.133221\n",
      "exploration/env_infos/reward_dist Std                    0.168488\n",
      "exploration/env_infos/reward_dist Max                    0.585569\n",
      "exploration/env_infos/reward_dist Min                    1.56785e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.149151\n",
      "exploration/env_infos/final/reward_energy Std            0.0601879\n",
      "exploration/env_infos/final/reward_energy Max           -0.0913021\n",
      "exploration/env_infos/final/reward_energy Min           -0.245859\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.355656\n",
      "exploration/env_infos/initial/reward_energy Std          0.142056\n",
      "exploration/env_infos/initial/reward_energy Max         -0.144299\n",
      "exploration/env_infos/initial/reward_energy Min         -0.500478\n",
      "exploration/env_infos/reward_energy Mean                -0.153926\n",
      "exploration/env_infos/reward_energy Std                  0.121324\n",
      "exploration/env_infos/reward_energy Max                 -0.00888746\n",
      "exploration/env_infos/reward_energy Min                 -0.62065\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.155983\n",
      "exploration/env_infos/final/end_effector_loc Std         0.22867\n",
      "exploration/env_infos/final/end_effector_loc Max         0.372841\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.39571\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00550856\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0123691\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0235874\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0161442\n",
      "exploration/env_infos/end_effector_loc Mean              0.0653447\n",
      "exploration/env_infos/end_effector_loc Std               0.160035\n",
      "exploration/env_infos/end_effector_loc Max               0.372841\n",
      "exploration/env_infos/end_effector_loc Min              -0.396622\n",
      "evaluation/num steps total                           98000\n",
      "evaluation/num paths total                            4900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0579093\n",
      "evaluation/Rewards Std                                   0.0724965\n",
      "evaluation/Rewards Max                                   0.149408\n",
      "evaluation/Rewards Min                                  -0.38742\n",
      "evaluation/Returns Mean                                 -1.15819\n",
      "evaluation/Returns Std                                   1.09349\n",
      "evaluation/Returns Max                                   1.4309\n",
      "evaluation/Returns Min                                  -4.1537\n",
      "evaluation/Actions Mean                                  0.00200762\n",
      "evaluation/Actions Std                                   0.0787153\n",
      "evaluation/Actions Max                                   0.584002\n",
      "evaluation/Actions Min                                  -0.696077\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.15819\n",
      "evaluation/env_infos/final/reward_dist Mean              0.105575\n",
      "evaluation/env_infos/final/reward_dist Std               0.221256\n",
      "evaluation/env_infos/final/reward_dist Max               0.891359\n",
      "evaluation/env_infos/final/reward_dist Min               2.12517e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00739259\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0123257\n",
      "evaluation/env_infos/initial/reward_dist Max             0.048806\n",
      "evaluation/env_infos/initial/reward_dist Min             1.20632e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.191279\n",
      "evaluation/env_infos/reward_dist Std                     0.278368\n",
      "evaluation/env_infos/reward_dist Max                     0.995895\n",
      "evaluation/env_infos/reward_dist Min                     2.12517e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0471072\n",
      "evaluation/env_infos/final/reward_energy Std             0.0496696\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00468534\n",
      "evaluation/env_infos/final/reward_energy Min            -0.308491\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.273471\n",
      "evaluation/env_infos/initial/reward_energy Std           0.208606\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0277949\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.908615\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0689311\n",
      "evaluation/env_infos/reward_energy Std                   0.0874572\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178582\n",
      "evaluation/env_infos/reward_energy Min                  -0.908615\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0222133\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.287276\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.724901\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.693056\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000456067\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.012152\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0292001\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0348038\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00675316\n",
      "evaluation/env_infos/end_effector_loc Std                0.187713\n",
      "evaluation/env_infos/end_effector_loc Max                0.724901\n",
      "evaluation/env_infos/end_effector_loc Min               -0.693056\n",
      "time/data storing (s)                                    0.00314444\n",
      "time/evaluation sampling (s)                             0.99411\n",
      "time/exploration sampling (s)                            0.118527\n",
      "time/logging (s)                                         0.0188777\n",
      "time/saving (s)                                          0.0269748\n",
      "time/training (s)                                       49.4968\n",
      "time/epoch (s)                                          50.6584\n",
      "time/total (s)                                        4907.22\n",
      "Epoch                                                   97\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:47:37.424992 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 98 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000979636\r\n",
      "trainer/QF2 Loss                                         0.00145386\r\n",
      "trainer/Policy Loss                                      2.72805\r\n",
      "trainer/Q1 Predictions Mean                             -0.879703\r\n",
      "trainer/Q1 Predictions Std                               0.897893\r\n",
      "trainer/Q1 Predictions Max                               1.43377\r\n",
      "trainer/Q1 Predictions Min                              -3.2412\r\n",
      "trainer/Q2 Predictions Mean                             -0.855599\r\n",
      "trainer/Q2 Predictions Std                               0.888868\r\n",
      "trainer/Q2 Predictions Max                               1.42271\r\n",
      "trainer/Q2 Predictions Min                              -3.21501\r\n",
      "trainer/Q Targets Mean                                  -0.876059\r\n",
      "trainer/Q Targets Std                                    0.894003\r\n",
      "trainer/Q Targets Max                                    1.39547\r\n",
      "trainer/Q Targets Min                                   -3.23343\r\n",
      "trainer/Log Pis Mean                                     1.86358\r\n",
      "trainer/Log Pis Std                                      1.45628\r\n",
      "trainer/Log Pis Max                                      4.58112\r\n",
      "trainer/Log Pis Min                                     -4.68844\r\n",
      "trainer/Policy mu Mean                                   0.0348339\r\n",
      "trainer/Policy mu Std                                    0.310976\r\n",
      "trainer/Policy mu Max                                    2.67831\r\n",
      "trainer/Policy mu Min                                   -1.41446\r\n",
      "trainer/Policy log std Mean                             -2.31506\r\n",
      "trainer/Policy log std Std                               0.600367\r\n",
      "trainer/Policy log std Max                              -0.0463334\r\n",
      "trainer/Policy log std Min                              -3.26277\r\n",
      "trainer/Alpha                                            0.0226434\r\n",
      "trainer/Alpha Loss                                      -0.516654\r\n",
      "exploration/num steps total                          10900\r\n",
      "exploration/num paths total                            545\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0579444\r\n",
      "exploration/Rewards Std                                  0.100761\r\n",
      "exploration/Rewards Max                                  0.107177\r\n",
      "exploration/Rewards Min                                 -0.445149\r\n",
      "exploration/Returns Mean                                -1.15889\r\n",
      "exploration/Returns Std                                  1.56214\r\n",
      "exploration/Returns Max                                  0.804083\r\n",
      "exploration/Returns Min                                 -3.3982\r\n",
      "exploration/Actions Mean                                -0.000897653\r\n",
      "exploration/Actions Std                                  0.197299\r\n",
      "exploration/Actions Max                                  0.554897\r\n",
      "exploration/Actions Min                                 -0.5579\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.15889\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.349541\r\n",
      "exploration/env_infos/final/reward_dist Std              0.304936\r\n",
      "exploration/env_infos/final/reward_dist Max              0.82714\r\n",
      "exploration/env_infos/final/reward_dist Min              0.0011835\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00429136\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00784501\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0199671\r\n",
      "exploration/env_infos/initial/reward_dist Min            9.34723e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.209032\r\n",
      "exploration/env_infos/reward_dist Std                    0.21722\r\n",
      "exploration/env_infos/reward_dist Max                    0.82714\r\n",
      "exploration/env_infos/reward_dist Min                    9.34723e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.242154\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0764205\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.124625\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.314558\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.341317\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.188436\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.120804\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.632633\r\n",
      "exploration/env_infos/reward_energy Mean                -0.233495\r\n",
      "exploration/env_infos/reward_energy Std                  0.15276\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0151134\r\n",
      "exploration/env_infos/reward_energy Min                 -0.645274\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0357071\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.210238\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.336163\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.275738\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000565689\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0137727\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0277095\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0223589\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.029667\r\n",
      "exploration/env_infos/end_effector_loc Std               0.16221\r\n",
      "exploration/env_infos/end_effector_loc Max               0.351197\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.3309\r\n",
      "evaluation/num steps total                           99000\r\n",
      "evaluation/num paths total                            4950\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.042962\r\n",
      "evaluation/Rewards Std                                   0.071765\r\n",
      "evaluation/Rewards Max                                   0.169266\r\n",
      "evaluation/Rewards Min                                  -0.355064\r\n",
      "evaluation/Returns Mean                                 -0.859239\r\n",
      "evaluation/Returns Std                                   1.04224\r\n",
      "evaluation/Returns Max                                   1.67012\r\n",
      "evaluation/Returns Min                                  -2.91675\r\n",
      "evaluation/Actions Mean                                  0.00209217\r\n",
      "evaluation/Actions Std                                   0.0767482\r\n",
      "evaluation/Actions Max                                   0.707976\r\n",
      "evaluation/Actions Min                                  -0.680147\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.859239\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.145953\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.25892\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.957263\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.09889e-19\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00595851\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0112068\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0520305\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.02967e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.179038\r\n",
      "evaluation/env_infos/reward_dist Std                     0.272862\r\n",
      "evaluation/env_infos/reward_dist Max                     0.985024\r\n",
      "evaluation/env_infos/reward_dist Min                     3.09889e-19\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0514498\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0404716\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00476627\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.183021\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.279691\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.213724\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0179589\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.780758\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0671767\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0853031\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000705802\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.780758\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0457971\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.251701\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.619819\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.525132\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00176154\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123198\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0353988\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0340073\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0227388\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.168486\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.619819\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.525132\r\n",
      "time/data storing (s)                                    0.00304988\r\n",
      "time/evaluation sampling (s)                             0.988384\r\n",
      "time/exploration sampling (s)                            0.152148\r\n",
      "time/logging (s)                                         0.019048\r\n",
      "time/saving (s)                                          0.0303942\r\n",
      "time/training (s)                                       51.0956\r\n",
      "time/epoch (s)                                          52.2887\r\n",
      "time/total (s)                                        4960.78\r\n",
      "Epoch                                                   98\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:48:34.386517 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 99 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000759883\n",
      "trainer/QF2 Loss                                          0.000806125\n",
      "trainer/Policy Loss                                       2.87161\n",
      "trainer/Q1 Predictions Mean                              -0.899691\n",
      "trainer/Q1 Predictions Std                                0.815776\n",
      "trainer/Q1 Predictions Max                                1.26907\n",
      "trainer/Q1 Predictions Min                               -2.85473\n",
      "trainer/Q2 Predictions Mean                              -0.894161\n",
      "trainer/Q2 Predictions Std                                0.815116\n",
      "trainer/Q2 Predictions Max                                1.28765\n",
      "trainer/Q2 Predictions Min                               -2.85111\n",
      "trainer/Q Targets Mean                                   -0.895059\n",
      "trainer/Q Targets Std                                     0.814831\n",
      "trainer/Q Targets Max                                     1.2531\n",
      "trainer/Q Targets Min                                    -2.80889\n",
      "trainer/Log Pis Mean                                      1.99265\n",
      "trainer/Log Pis Std                                       1.27234\n",
      "trainer/Log Pis Max                                       4.41099\n",
      "trainer/Log Pis Min                                      -1.37371\n",
      "trainer/Policy mu Mean                                   -0.0114338\n",
      "trainer/Policy mu Std                                     0.335486\n",
      "trainer/Policy mu Max                                     1.93696\n",
      "trainer/Policy mu Min                                    -2.46527\n",
      "trainer/Policy log std Mean                              -2.29367\n",
      "trainer/Policy log std Std                                0.581983\n",
      "trainer/Policy log std Max                               -0.357106\n",
      "trainer/Policy log std Min                               -3.18399\n",
      "trainer/Alpha                                             0.0234676\n",
      "trainer/Alpha Loss                                       -0.0275885\n",
      "exploration/num steps total                           11000\n",
      "exploration/num paths total                             550\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.084779\n",
      "exploration/Rewards Std                                   0.0708489\n",
      "exploration/Rewards Max                                   0.114574\n",
      "exploration/Rewards Min                                  -0.21966\n",
      "exploration/Returns Mean                                 -1.69558\n",
      "exploration/Returns Std                                   1.11728\n",
      "exploration/Returns Max                                   0.419051\n",
      "exploration/Returns Min                                  -2.83494\n",
      "exploration/Actions Mean                                  0.00309165\n",
      "exploration/Actions Std                                   0.149925\n",
      "exploration/Actions Max                                   0.514514\n",
      "exploration/Actions Min                                  -0.800542\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.69558\n",
      "exploration/env_infos/final/reward_dist Mean              0.289366\n",
      "exploration/env_infos/final/reward_dist Std               0.359168\n",
      "exploration/env_infos/final/reward_dist Max               0.817577\n",
      "exploration/env_infos/final/reward_dist Min               1.10764e-15\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00136419\n",
      "exploration/env_infos/initial/reward_dist Std             0.00237832\n",
      "exploration/env_infos/initial/reward_dist Max             0.00609587\n",
      "exploration/env_infos/initial/reward_dist Min             6.72222e-06\n",
      "exploration/env_infos/reward_dist Mean                    0.0600753\n",
      "exploration/env_infos/reward_dist Std                     0.154825\n",
      "exploration/env_infos/reward_dist Max                     0.817577\n",
      "exploration/env_infos/reward_dist Min                     1.10764e-15\n",
      "exploration/env_infos/final/reward_energy Mean           -0.141545\n",
      "exploration/env_infos/final/reward_energy Std             0.106899\n",
      "exploration/env_infos/final/reward_energy Max            -0.0178928\n",
      "exploration/env_infos/final/reward_energy Min            -0.340662\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.188919\n",
      "exploration/env_infos/initial/reward_energy Std           0.118401\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0357281\n",
      "exploration/env_infos/initial/reward_energy Min          -0.347112\n",
      "exploration/env_infos/reward_energy Mean                 -0.15483\n",
      "exploration/env_infos/reward_energy Std                   0.14492\n",
      "exploration/env_infos/reward_energy Max                  -0.0112202\n",
      "exploration/env_infos/reward_energy Min                  -0.829585\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.104239\n",
      "exploration/env_infos/final/end_effector_loc Std          0.27996\n",
      "exploration/env_infos/final/end_effector_loc Max          0.622515\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.166346\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.000658925\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00785508\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0151565\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0163496\n",
      "exploration/env_infos/end_effector_loc Mean               0.0488755\n",
      "exploration/env_infos/end_effector_loc Std                0.177835\n",
      "exploration/env_infos/end_effector_loc Max                0.622515\n",
      "exploration/env_infos/end_effector_loc Min               -0.166346\n",
      "evaluation/num steps total                           100000\n",
      "evaluation/num paths total                             5000\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0555603\n",
      "evaluation/Rewards Std                                    0.0788821\n",
      "evaluation/Rewards Max                                    0.148223\n",
      "evaluation/Rewards Min                                   -0.313124\n",
      "evaluation/Returns Mean                                  -1.11121\n",
      "evaluation/Returns Std                                    1.25351\n",
      "evaluation/Returns Max                                    2.11246\n",
      "evaluation/Returns Min                                   -3.47184\n",
      "evaluation/Actions Mean                                  -0.004054\n",
      "evaluation/Actions Std                                    0.0673975\n",
      "evaluation/Actions Max                                    0.498848\n",
      "evaluation/Actions Min                                   -0.46377\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -1.11121\n",
      "evaluation/env_infos/final/reward_dist Mean               0.15208\n",
      "evaluation/env_infos/final/reward_dist Std                0.220148\n",
      "evaluation/env_infos/final/reward_dist Max                0.778266\n",
      "evaluation/env_infos/final/reward_dist Min                5.51273e-45\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00657164\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0129755\n",
      "evaluation/env_infos/initial/reward_dist Max              0.071424\n",
      "evaluation/env_infos/initial/reward_dist Min              1.89152e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.1936\n",
      "evaluation/env_infos/reward_dist Std                      0.281209\n",
      "evaluation/env_infos/reward_dist Max                      0.986701\n",
      "evaluation/env_infos/reward_dist Min                      7.94104e-47\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.048012\n",
      "evaluation/env_infos/final/reward_energy Std              0.0507591\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00544975\n",
      "evaluation/env_infos/final/reward_energy Min             -0.250308\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.20114\n",
      "evaluation/env_infos/initial/reward_energy Std            0.164995\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.00614818\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.569343\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0603391\n",
      "evaluation/env_infos/reward_energy Std                    0.0740062\n",
      "evaluation/env_infos/reward_energy Max                   -0.000848076\n",
      "evaluation/env_infos/reward_energy Min                   -0.569343\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0113357\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.271391\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.506994\n",
      "evaluation/env_infos/final/end_effector_loc Min          -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000662461\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.009174\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0249424\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0231885\n",
      "evaluation/env_infos/end_effector_loc Mean                0.000605874\n",
      "evaluation/env_infos/end_effector_loc Std                 0.17629\n",
      "evaluation/env_infos/end_effector_loc Max                 0.506994\n",
      "evaluation/env_infos/end_effector_loc Min                -1\n",
      "time/data storing (s)                                     0.00288734\n",
      "time/evaluation sampling (s)                              1.15627\n",
      "time/exploration sampling (s)                             0.126535\n",
      "time/logging (s)                                          0.0208459\n",
      "time/saving (s)                                           0.0283143\n",
      "time/training (s)                                        54.3833\n",
      "time/epoch (s)                                           55.7182\n",
      "time/total (s)                                         5017.74\n",
      "Epoch                                                    99\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[18990]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1b762778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1b7e3740). One of the two will be used. Which one is undefined.\n",
      "objc[18990]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1b762700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1b7e3768). One of the two will be used. Which one is undefined.\n",
      "objc[18990]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1b7627a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1b7e37b8). One of the two will be used. Which one is undefined.\n",
      "objc[18990]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1b762818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1b7e3830). One of the two will be used. Which one is undefined.\n",
      "using sparse reward if specified\n",
      "2021-05-25 11:48:41.577000 PDT | Variant:\n",
      "2021-05-25 11:48:41.577616 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"pointreacherobs\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 20,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 20,\n",
      "    \"num_train_loops_per_epoch\": 5\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.97,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 2000,\n",
      "    \"latent_dim\": 6,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 1,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"sparse_reward\": null,\n",
      "    \"fixed_ratio\": null\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"horizon\": 20\n",
      "  }\n",
      "}\n",
      "pointreacher\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "2021-05-25 11:49:15.531827 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 0 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.672348\n",
      "trainer/QF2 Loss                                        0.67117\n",
      "trainer/Policy Loss                                    -1.37884\n",
      "trainer/Q1 Predictions Mean                            -0.00121823\n",
      "trainer/Q1 Predictions Std                              0.000552746\n",
      "trainer/Q1 Predictions Max                              0.000254711\n",
      "trainer/Q1 Predictions Min                             -0.00227255\n",
      "trainer/Q2 Predictions Mean                            -8.03623e-05\n",
      "trainer/Q2 Predictions Std                              0.000632807\n",
      "trainer/Q2 Predictions Max                              0.00120733\n",
      "trainer/Q2 Predictions Min                             -0.00179445\n",
      "trainer/Q Targets Mean                                  0.537571\n",
      "trainer/Q Targets Std                                   0.618124\n",
      "trainer/Q Targets Max                                   1.71732\n",
      "trainer/Q Targets Min                                  -1.51786\n",
      "trainer/Log Pis Mean                                   -1.38017\n",
      "trainer/Log Pis Std                                     0.289098\n",
      "trainer/Log Pis Max                                    -0.574903\n",
      "trainer/Log Pis Min                                    -2.60542\n",
      "trainer/Policy mu Mean                                  0.000985406\n",
      "trainer/Policy mu Std                                   0.000596871\n",
      "trainer/Policy mu Max                                   0.00221213\n",
      "trainer/Policy mu Min                                   1.31949e-05\n",
      "trainer/Policy log std Mean                             6.00163e-05\n",
      "trainer/Policy log std Std                              0.000636428\n",
      "trainer/Policy log std Max                              0.000963059\n",
      "trainer/Policy log std Min                             -0.00131503\n",
      "trainer/Alpha                                           0.997005\n",
      "trainer/Alpha Loss                                     -0\n",
      "exploration/num steps total                          1100\n",
      "exploration/num paths total                            55\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.779334\n",
      "exploration/Rewards Std                                 0.360297\n",
      "exploration/Rewards Max                                -0.0619928\n",
      "exploration/Rewards Min                                -1.39334\n",
      "exploration/Returns Mean                              -15.5867\n",
      "exploration/Returns Std                                 5.11475\n",
      "exploration/Returns Max                                -5.88001\n",
      "exploration/Returns Min                               -19.5088\n",
      "exploration/Actions Mean                                0.0672882\n",
      "exploration/Actions Std                                 0.585152\n",
      "exploration/Actions Max                                 0.981492\n",
      "exploration/Actions Min                                -0.953663\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -15.5867\n",
      "exploration/env_infos/final/reward_dist Mean            3.5514e-52\n",
      "exploration/env_infos/final/reward_dist Std             7.1028e-52\n",
      "exploration/env_infos/final/reward_dist Max             1.7757e-51\n",
      "exploration/env_infos/final/reward_dist Min             5.23713e-100\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00110881\n",
      "exploration/env_infos/initial/reward_dist Std           0.000799436\n",
      "exploration/env_infos/initial/reward_dist Max           0.00197989\n",
      "exploration/env_infos/initial/reward_dist Min           8.24155e-06\n",
      "exploration/env_infos/reward_dist Mean                  8.78519e-05\n",
      "exploration/env_infos/reward_dist Std                   0.000349362\n",
      "exploration/env_infos/reward_dist Max                   0.00197989\n",
      "exploration/env_infos/reward_dist Min                   5.23713e-100\n",
      "exploration/env_infos/final/reward_energy Mean         -0.78875\n",
      "exploration/env_infos/final/reward_energy Std           0.300753\n",
      "exploration/env_infos/final/reward_energy Max          -0.191105\n",
      "exploration/env_infos/final/reward_energy Min          -1.0009\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.890991\n",
      "exploration/env_infos/initial/reward_energy Std         0.138347\n",
      "exploration/env_infos/initial/reward_energy Max        -0.722018\n",
      "exploration/env_infos/initial/reward_energy Min        -1.11969\n",
      "exploration/env_infos/reward_energy Mean               -0.784903\n",
      "exploration/env_infos/reward_energy Std                 0.278904\n",
      "exploration/env_infos/reward_energy Max                -0.158037\n",
      "exploration/env_infos/reward_energy Min                -1.26039\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.442852\n",
      "exploration/env_infos/final/end_effector_loc Std        0.662506\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.0222619\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.022818\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0474395\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0351271\n",
      "exploration/env_infos/end_effector_loc Mean             0.286389\n",
      "exploration/env_infos/end_effector_loc Std              0.487203\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           1000\n",
      "evaluation/num paths total                             50\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0724238\n",
      "evaluation/Rewards Std                                  0.0456905\n",
      "evaluation/Rewards Max                                  0.0273882\n",
      "evaluation/Rewards Min                                 -0.145049\n",
      "evaluation/Returns Mean                                -1.44848\n",
      "evaluation/Returns Std                                  0.912568\n",
      "evaluation/Returns Max                                  0.437927\n",
      "evaluation/Returns Min                                 -2.87246\n",
      "evaluation/Actions Mean                                 0.000980846\n",
      "evaluation/Actions Std                                  0.000595694\n",
      "evaluation/Actions Max                                  0.00203524\n",
      "evaluation/Actions Min                                  0.000153851\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.44848\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00345347\n",
      "evaluation/env_infos/final/reward_dist Std              0.00716752\n",
      "evaluation/env_infos/final/reward_dist Max              0.0294951\n",
      "evaluation/env_infos/final/reward_dist Min              3.19905e-07\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00264807\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00510559\n",
      "evaluation/env_infos/initial/reward_dist Max            0.020763\n",
      "evaluation/env_infos/initial/reward_dist Min            1.16923e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.00287612\n",
      "evaluation/env_infos/reward_dist Std                    0.00567184\n",
      "evaluation/env_infos/reward_dist Max                    0.0294951\n",
      "evaluation/env_infos/reward_dist Min                    3.19905e-07\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.00164662\n",
      "evaluation/env_infos/final/reward_energy Std            0.000274058\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0011215\n",
      "evaluation/env_infos/final/reward_energy Min           -0.00206738\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.00155927\n",
      "evaluation/env_infos/initial/reward_energy Std          0.000240541\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00113108\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.00193048\n",
      "evaluation/env_infos/reward_energy Mean                -0.00160215\n",
      "evaluation/env_infos/reward_energy Std                  0.000258701\n",
      "evaluation/env_infos/reward_energy Max                 -0.0011215\n",
      "evaluation/env_infos/reward_energy Min                 -0.00206738\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.010226\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.0061497\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.0203188\n",
      "evaluation/env_infos/final/end_effector_loc Min         0.00180004\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      4.80138e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std       2.83926e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Max       9.60617e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Min       7.69257e-06\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00373622\n",
      "evaluation/env_infos/end_effector_loc Std               0.00432289\n",
      "evaluation/env_infos/end_effector_loc Max               0.0203188\n",
      "evaluation/env_infos/end_effector_loc Min               7.69257e-06\n",
      "time/data storing (s)                                   0.00621381\n",
      "time/evaluation sampling (s)                            1.17551\n",
      "time/exploration sampling (s)                           0.137434\n",
      "time/logging (s)                                        0.0204966\n",
      "time/saving (s)                                         0.0732453\n",
      "time/training (s)                                      31.1826\n",
      "time/epoch (s)                                         32.5955\n",
      "time/total (s)                                         37.2896\n",
      "Epoch                                                   0\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:49:56.045638 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 1 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.0158935\n",
      "trainer/QF2 Loss                                        0.018351\n",
      "trainer/Policy Loss                                    -0.853415\n",
      "trainer/Q1 Predictions Mean                            -0.450416\n",
      "trainer/Q1 Predictions Std                              0.639989\n",
      "trainer/Q1 Predictions Max                              0.662978\n",
      "trainer/Q1 Predictions Min                             -2.21416\n",
      "trainer/Q2 Predictions Mean                            -0.365557\n",
      "trainer/Q2 Predictions Std                              0.637782\n",
      "trainer/Q2 Predictions Max                              0.74346\n",
      "trainer/Q2 Predictions Min                             -2.22241\n",
      "trainer/Q Targets Mean                                 -0.394795\n",
      "trainer/Q Targets Std                                   0.674326\n",
      "trainer/Q Targets Max                                   0.865267\n",
      "trainer/Q Targets Min                                  -2.36967\n",
      "trainer/Log Pis Mean                                   -1.21348\n",
      "trainer/Log Pis Std                                     0.438561\n",
      "trainer/Log Pis Max                                    -0.254476\n",
      "trainer/Log Pis Min                                    -3.41673\n",
      "trainer/Policy mu Mean                                  0.0157817\n",
      "trainer/Policy mu Std                                   0.0356279\n",
      "trainer/Policy mu Max                                   0.0823842\n",
      "trainer/Policy mu Min                                  -0.0756194\n",
      "trainer/Policy log std Mean                            -0.464191\n",
      "trainer/Policy log std Std                              0.159663\n",
      "trainer/Policy log std Max                             -0.171675\n",
      "trainer/Policy log std Min                             -0.87282\n",
      "trainer/Alpha                                           0.225871\n",
      "trainer/Alpha Loss                                     -4.77171\n",
      "exploration/num steps total                          1200\n",
      "exploration/num paths total                            60\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.472323\n",
      "exploration/Rewards Std                                 0.320149\n",
      "exploration/Rewards Max                                -0.0845086\n",
      "exploration/Rewards Min                                -1.49455\n",
      "exploration/Returns Mean                               -9.44645\n",
      "exploration/Returns Std                                 4.19507\n",
      "exploration/Returns Max                                -5.82318\n",
      "exploration/Returns Min                               -17.4822\n",
      "exploration/Actions Mean                                0.0315322\n",
      "exploration/Actions Std                                 0.453184\n",
      "exploration/Actions Max                                 0.942808\n",
      "exploration/Actions Min                                -0.895441\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -9.44645\n",
      "exploration/env_infos/final/reward_dist Mean            4.27435e-44\n",
      "exploration/env_infos/final/reward_dist Std             8.5487e-44\n",
      "exploration/env_infos/final/reward_dist Max             2.13717e-43\n",
      "exploration/env_infos/final/reward_dist Min             4.3175e-109\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00930873\n",
      "exploration/env_infos/initial/reward_dist Std           0.0130394\n",
      "exploration/env_infos/initial/reward_dist Max           0.033955\n",
      "exploration/env_infos/initial/reward_dist Min           2.55492e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0409551\n",
      "exploration/env_infos/reward_dist Std                   0.13018\n",
      "exploration/env_infos/reward_dist Max                   0.827212\n",
      "exploration/env_infos/reward_dist Min                   4.3175e-109\n",
      "exploration/env_infos/final/reward_energy Mean         -0.564381\n",
      "exploration/env_infos/final/reward_energy Std           0.153437\n",
      "exploration/env_infos/final/reward_energy Max          -0.410481\n",
      "exploration/env_infos/final/reward_energy Min          -0.81602\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.547129\n",
      "exploration/env_infos/initial/reward_energy Std         0.164637\n",
      "exploration/env_infos/initial/reward_energy Max        -0.381345\n",
      "exploration/env_infos/initial/reward_energy Min        -0.839745\n",
      "exploration/env_infos/reward_energy Mean               -0.587652\n",
      "exploration/env_infos/reward_energy Std                 0.259625\n",
      "exploration/env_infos/reward_energy Max                -0.0608045\n",
      "exploration/env_infos/reward_energy Min                -1.21176\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.217326\n",
      "exploration/env_infos/final/end_effector_loc Std        0.808911\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.000712911\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0201882\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0293443\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0400074\n",
      "exploration/env_infos/end_effector_loc Mean             0.102351\n",
      "exploration/env_infos/end_effector_loc Std              0.511242\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           2000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.121711\n",
      "evaluation/Rewards Std                                  0.0947938\n",
      "evaluation/Rewards Max                                  0.0820133\n",
      "evaluation/Rewards Min                                 -0.674623\n",
      "evaluation/Returns Mean                                -2.43422\n",
      "evaluation/Returns Std                                  1.38812\n",
      "evaluation/Returns Max                                  0.0802905\n",
      "evaluation/Returns Min                                 -5.80616\n",
      "evaluation/Actions Mean                                 0.0243281\n",
      "evaluation/Actions Std                                  0.032682\n",
      "evaluation/Actions Max                                  0.0833411\n",
      "evaluation/Actions Min                                 -0.0667044\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.43422\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0126902\n",
      "evaluation/env_infos/final/reward_dist Std              0.0516867\n",
      "evaluation/env_infos/final/reward_dist Max              0.338091\n",
      "evaluation/env_infos/final/reward_dist Min              1.01821e-83\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00374265\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00663119\n",
      "evaluation/env_infos/initial/reward_dist Max            0.025391\n",
      "evaluation/env_infos/initial/reward_dist Min            8.27134e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0285517\n",
      "evaluation/env_infos/reward_dist Std                    0.0829103\n",
      "evaluation/env_infos/reward_dist Max                    0.633927\n",
      "evaluation/env_infos/reward_dist Min                    1.01821e-83\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.058981\n",
      "evaluation/env_infos/final/reward_energy Std            0.0203501\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0265055\n",
      "evaluation/env_infos/final/reward_energy Min           -0.0952714\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.0502729\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0188656\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0234923\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.0862124\n",
      "evaluation/env_infos/reward_energy Mean                -0.0541446\n",
      "evaluation/env_infos/reward_energy Std                  0.0197054\n",
      "evaluation/env_infos/reward_energy Max                 -0.0234923\n",
      "evaluation/env_infos/reward_energy Min                 -0.0952714\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.236419\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.341325\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.867469\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.628864\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000954822\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00164086\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.00411694\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00333522\n",
      "evaluation/env_infos/end_effector_loc Mean              0.083256\n",
      "evaluation/env_infos/end_effector_loc Std               0.179578\n",
      "evaluation/env_infos/end_effector_loc Max               0.867469\n",
      "evaluation/env_infos/end_effector_loc Min              -0.628864\n",
      "time/data storing (s)                                   0.00616514\n",
      "time/evaluation sampling (s)                            1.09697\n",
      "time/exploration sampling (s)                           0.127657\n",
      "time/logging (s)                                        0.0210535\n",
      "time/saving (s)                                         0.0318642\n",
      "time/training (s)                                      39.0353\n",
      "time/epoch (s)                                         40.319\n",
      "time/total (s)                                         77.8032\n",
      "Epoch                                                   1\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:50:39.244329 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 2 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                   2000\r\n",
      "trainer/QF1 Loss                                        0.0152706\r\n",
      "trainer/QF2 Loss                                        0.0163452\r\n",
      "trainer/Policy Loss                                     1.00282\r\n",
      "trainer/Q1 Predictions Mean                            -1.28647\r\n",
      "trainer/Q1 Predictions Std                              1.02599\r\n",
      "trainer/Q1 Predictions Max                              0.476811\r\n",
      "trainer/Q1 Predictions Min                             -4.35677\r\n",
      "trainer/Q2 Predictions Mean                            -1.25823\r\n",
      "trainer/Q2 Predictions Std                              1.04016\r\n",
      "trainer/Q2 Predictions Max                              0.512977\r\n",
      "trainer/Q2 Predictions Min                             -4.29007\r\n",
      "trainer/Q Targets Mean                                 -1.2498\r\n",
      "trainer/Q Targets Std                                   1.04695\r\n",
      "trainer/Q Targets Max                                   0.463098\r\n",
      "trainer/Q Targets Min                                  -4.41931\r\n",
      "trainer/Log Pis Mean                                   -0.0670519\r\n",
      "trainer/Log Pis Std                                     1.02024\r\n",
      "trainer/Log Pis Max                                     2.39869\r\n",
      "trainer/Log Pis Min                                    -4.59829\r\n",
      "trainer/Policy mu Mean                                  0.0569354\r\n",
      "trainer/Policy mu Std                                   0.322808\r\n",
      "trainer/Policy mu Max                                   1.64144\r\n",
      "trainer/Policy mu Min                                  -1.02321\r\n",
      "trainer/Policy log std Mean                            -1.15017\r\n",
      "trainer/Policy log std Std                              0.404949\r\n",
      "trainer/Policy log std Max                             -0.283445\r\n",
      "trainer/Policy log std Min                             -2.06719\r\n",
      "trainer/Alpha                                           0.0621381\r\n",
      "trainer/Alpha Loss                                     -5.73845\r\n",
      "exploration/num steps total                          1300\r\n",
      "exploration/num paths total                            65\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.286799\r\n",
      "exploration/Rewards Std                                 0.138236\r\n",
      "exploration/Rewards Max                                -0.0648243\r\n",
      "exploration/Rewards Min                                -0.552259\r\n",
      "exploration/Returns Mean                               -5.73598\r\n",
      "exploration/Returns Std                                 1.56042\r\n",
      "exploration/Returns Max                                -3.90122\r\n",
      "exploration/Returns Min                                -8.45994\r\n",
      "exploration/Actions Mean                                0.0188313\r\n",
      "exploration/Actions Std                                 0.251142\r\n",
      "exploration/Actions Max                                 0.80495\r\n",
      "exploration/Actions Min                                -0.72136\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -5.73598\r\n",
      "exploration/env_infos/final/reward_dist Mean            4.80611e-14\r\n",
      "exploration/env_infos/final/reward_dist Std             9.61223e-14\r\n",
      "exploration/env_infos/final/reward_dist Max             2.40306e-13\r\n",
      "exploration/env_infos/final/reward_dist Min             2.00722e-90\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000189909\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.000358006\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.000905668\r\n",
      "exploration/env_infos/initial/reward_dist Min           5.30858e-07\r\n",
      "exploration/env_infos/reward_dist Mean                  0.00105752\r\n",
      "exploration/env_infos/reward_dist Std                   0.00522925\r\n",
      "exploration/env_infos/reward_dist Max                   0.04493\r\n",
      "exploration/env_infos/reward_dist Min                   2.25678e-91\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.194224\r\n",
      "exploration/env_infos/final/reward_energy Std           0.0974876\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.0574928\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.304506\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.486962\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.274816\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.184341\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.980845\r\n",
      "exploration/env_infos/reward_energy Mean               -0.290606\r\n",
      "exploration/env_infos/reward_energy Std                 0.205916\r\n",
      "exploration/env_infos/reward_energy Max                -0.0250696\r\n",
      "exploration/env_infos/reward_energy Min                -0.986171\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.385392\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.539764\r\n",
      "exploration/env_infos/final/end_effector_loc Max        1\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00474241\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0191919\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0360032\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0182805\r\n",
      "exploration/env_infos/end_effector_loc Mean             0.221574\r\n",
      "exploration/env_infos/end_effector_loc Std              0.382089\r\n",
      "exploration/env_infos/end_effector_loc Max              1\r\n",
      "exploration/env_infos/end_effector_loc Min             -1\r\n",
      "evaluation/num steps total                           3000\r\n",
      "evaluation/num paths total                            150\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.229423\r\n",
      "evaluation/Rewards Std                                  0.220588\r\n",
      "evaluation/Rewards Max                                  0.119587\r\n",
      "evaluation/Rewards Min                                 -1.02478\r\n",
      "evaluation/Returns Mean                                -4.58847\r\n",
      "evaluation/Returns Std                                  3.15404\r\n",
      "evaluation/Returns Max                                 -0.0429861\r\n",
      "evaluation/Returns Min                                -12.1634\r\n",
      "evaluation/Actions Mean                                 0.017984\r\n",
      "evaluation/Actions Std                                  0.141246\r\n",
      "evaluation/Actions Max                                  0.76732\r\n",
      "evaluation/Actions Min                                 -0.474665\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -4.58847\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0116957\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.0633309\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.428023\r\n",
      "evaluation/env_infos/final/reward_dist Min              3.03315e-157\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00694228\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0150435\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0892187\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.11902e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0404275\r\n",
      "evaluation/env_infos/reward_dist Std                    0.138081\r\n",
      "evaluation/env_infos/reward_dist Max                    0.993975\r\n",
      "evaluation/env_infos/reward_dist Min                    1.78698e-162\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.184112\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.141627\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0182393\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.621349\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.182862\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.169334\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00709136\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.775226\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.159095\r\n",
      "evaluation/env_infos/reward_energy Std                  0.123435\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00483205\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.775226\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.197605\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.608407\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00346167\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00810293\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.038366\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00774605\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.132323\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.397563\r\n",
      "evaluation/env_infos/end_effector_loc Max               1\r\n",
      "evaluation/env_infos/end_effector_loc Min              -1\r\n",
      "time/data storing (s)                                   0.00640571\r\n",
      "time/evaluation sampling (s)                            1.10215\r\n",
      "time/exploration sampling (s)                           0.12414\r\n",
      "time/logging (s)                                        0.0200475\r\n",
      "time/saving (s)                                         0.0271609\r\n",
      "time/training (s)                                      41.8603\r\n",
      "time/epoch (s)                                         43.1402\r\n",
      "time/total (s)                                        121\r\n",
      "Epoch                                                   2\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:51:24.166264 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 3 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   2000\r\n",
      "trainer/QF1 Loss                                        0.00743615\r\n",
      "trainer/QF2 Loss                                        0.0145448\r\n",
      "trainer/Policy Loss                                     2.66085\r\n",
      "trainer/Q1 Predictions Mean                            -1.9713\r\n",
      "trainer/Q1 Predictions Std                              1.34341\r\n",
      "trainer/Q1 Predictions Max                              0.23657\r\n",
      "trainer/Q1 Predictions Min                             -6.3588\r\n",
      "trainer/Q2 Predictions Mean                            -1.92974\r\n",
      "trainer/Q2 Predictions Std                              1.33664\r\n",
      "trainer/Q2 Predictions Max                              0.309348\r\n",
      "trainer/Q2 Predictions Min                             -6.32205\r\n",
      "trainer/Q Targets Mean                                 -1.97046\r\n",
      "trainer/Q Targets Std                                   1.3545\r\n",
      "trainer/Q Targets Max                                   0.301583\r\n",
      "trainer/Q Targets Min                                  -6.32522\r\n",
      "trainer/Log Pis Mean                                    0.952188\r\n",
      "trainer/Log Pis Std                                     1.23678\r\n",
      "trainer/Log Pis Max                                     5.21669\r\n",
      "trainer/Log Pis Min                                    -2.80981\r\n",
      "trainer/Policy mu Mean                                  0.0489619\r\n",
      "trainer/Policy mu Std                                   0.521926\r\n",
      "trainer/Policy mu Max                                   2.55613\r\n",
      "trainer/Policy mu Min                                  -1.72504\r\n",
      "trainer/Policy log std Mean                            -1.64454\r\n",
      "trainer/Policy log std Std                              0.508096\r\n",
      "trainer/Policy log std Max                             -0.0916855\r\n",
      "trainer/Policy log std Min                             -2.88881\r\n",
      "trainer/Alpha                                           0.0250817\r\n",
      "trainer/Alpha Loss                                     -3.86045\r\n",
      "exploration/num steps total                          1400\r\n",
      "exploration/num paths total                            70\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.157539\r\n",
      "exploration/Rewards Std                                 0.133584\r\n",
      "exploration/Rewards Max                                 0.100013\r\n",
      "exploration/Rewards Min                                -0.604888\r\n",
      "exploration/Returns Mean                               -3.15078\r\n",
      "exploration/Returns Std                                 1.56085\r\n",
      "exploration/Returns Max                                -0.469782\r\n",
      "exploration/Returns Min                                -5.30904\r\n",
      "exploration/Actions Mean                                0.00587114\r\n",
      "exploration/Actions Std                                 0.195478\r\n",
      "exploration/Actions Max                                 0.557101\r\n",
      "exploration/Actions Min                                -0.700095\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -3.15078\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.0211113\r\n",
      "exploration/env_infos/final/reward_dist Std             0.0406629\r\n",
      "exploration/env_infos/final/reward_dist Max             0.1024\r\n",
      "exploration/env_infos/final/reward_dist Min             2.53362e-91\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00675354\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.0118536\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0304343\r\n",
      "exploration/env_infos/initial/reward_dist Min           0.000253309\r\n",
      "exploration/env_infos/reward_dist Mean                  0.0884899\r\n",
      "exploration/env_infos/reward_dist Std                   0.186906\r\n",
      "exploration/env_infos/reward_dist Max                   0.899566\r\n",
      "exploration/env_infos/reward_dist Min                   2.53362e-91\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.379046\r\n",
      "exploration/env_infos/final/reward_energy Std           0.119973\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.222019\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.588564\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.279663\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.13841\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.101393\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.504465\r\n",
      "exploration/env_infos/reward_energy Mean               -0.231289\r\n",
      "exploration/env_infos/reward_energy Std                 0.151651\r\n",
      "exploration/env_infos/reward_energy Max                -0.0281224\r\n",
      "exploration/env_infos/reward_energy Min                -0.704296\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.193287\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.530781\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.991886\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.577216\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00575624\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00941153\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0230343\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0103115\r\n",
      "exploration/env_infos/end_effector_loc Mean             0.124367\r\n",
      "exploration/env_infos/end_effector_loc Std              0.316305\r\n",
      "exploration/env_infos/end_effector_loc Max              1\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.577216\r\n",
      "evaluation/num steps total                           4000\r\n",
      "evaluation/num paths total                            200\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.131483\r\n",
      "evaluation/Rewards Std                                  0.132743\r\n",
      "evaluation/Rewards Max                                  0.13564\r\n",
      "evaluation/Rewards Min                                 -0.777741\r\n",
      "evaluation/Returns Mean                                -2.62966\r\n",
      "evaluation/Returns Std                                  1.70769\r\n",
      "evaluation/Returns Max                                  0.0746871\r\n",
      "evaluation/Returns Min                                 -7.79977\r\n",
      "evaluation/Actions Mean                                -0.0120567\r\n",
      "evaluation/Actions Std                                  0.132851\r\n",
      "evaluation/Actions Max                                  0.63865\r\n",
      "evaluation/Actions Min                                 -0.563438\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -2.62966\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0579879\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.170225\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.863926\r\n",
      "evaluation/env_infos/final/reward_dist Min              7.2263e-61\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00417104\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00688119\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0308881\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.4858e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0857776\r\n",
      "evaluation/env_infos/reward_dist Std                    0.191196\r\n",
      "evaluation/env_infos/reward_dist Max                    0.990913\r\n",
      "evaluation/env_infos/reward_dist Min                    7.2263e-61\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.159823\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.142738\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00632063\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.630265\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.196552\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.155489\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0107947\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.711108\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.133333\r\n",
      "evaluation/env_infos/reward_energy Std                  0.13346\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00197174\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.711108\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.0356862\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.458586\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00276349\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00841874\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0319325\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0106757\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0507872\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.313227\r\n",
      "evaluation/env_infos/end_effector_loc Max               1\r\n",
      "evaluation/env_infos/end_effector_loc Min              -1\r\n",
      "time/data storing (s)                                   0.00585583\r\n",
      "time/evaluation sampling (s)                            0.980161\r\n",
      "time/exploration sampling (s)                           0.11636\r\n",
      "time/logging (s)                                        0.0208298\r\n",
      "time/saving (s)                                         0.0280159\r\n",
      "time/training (s)                                      43.7022\r\n",
      "time/epoch (s)                                         44.8534\r\n",
      "time/total (s)                                        165.922\r\n",
      "Epoch                                                   3\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:52:09.394135 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 4 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00900255\n",
      "trainer/QF2 Loss                                        0.00792023\n",
      "trainer/Policy Loss                                     3.79236\n",
      "trainer/Q1 Predictions Mean                            -2.29525\n",
      "trainer/Q1 Predictions Std                              1.36468\n",
      "trainer/Q1 Predictions Max                              0.155333\n",
      "trainer/Q1 Predictions Min                             -7.41158\n",
      "trainer/Q2 Predictions Mean                            -2.19783\n",
      "trainer/Q2 Predictions Std                              1.33976\n",
      "trainer/Q2 Predictions Max                              0.234088\n",
      "trainer/Q2 Predictions Min                             -7.40944\n",
      "trainer/Q Targets Mean                                 -2.24644\n",
      "trainer/Q Targets Std                                   1.34892\n",
      "trainer/Q Targets Max                                   0.163697\n",
      "trainer/Q Targets Min                                  -7.36343\n",
      "trainer/Log Pis Mean                                    1.79361\n",
      "trainer/Log Pis Std                                     1.46633\n",
      "trainer/Log Pis Max                                    10.4119\n",
      "trainer/Log Pis Min                                    -5.86206\n",
      "trainer/Policy mu Mean                                  0.0139832\n",
      "trainer/Policy mu Std                                   0.686709\n",
      "trainer/Policy mu Max                                   3.57824\n",
      "trainer/Policy mu Min                                  -3.14863\n",
      "trainer/Policy log std Mean                            -1.93881\n",
      "trainer/Policy log std Std                              0.585949\n",
      "trainer/Policy log std Max                             -0.253617\n",
      "trainer/Policy log std Min                             -2.80068\n",
      "trainer/Alpha                                           0.0158476\n",
      "trainer/Alpha Loss                                     -0.855331\n",
      "exploration/num steps total                          1500\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.110888\n",
      "exploration/Rewards Std                                 0.0696856\n",
      "exploration/Rewards Max                                 0.0394911\n",
      "exploration/Rewards Min                                -0.289838\n",
      "exploration/Returns Mean                               -2.21776\n",
      "exploration/Returns Std                                 0.751434\n",
      "exploration/Returns Max                                -1.27306\n",
      "exploration/Returns Min                                -3.22543\n",
      "exploration/Actions Mean                               -0.0102886\n",
      "exploration/Actions Std                                 0.0864903\n",
      "exploration/Actions Max                                 0.268437\n",
      "exploration/Actions Min                                -0.273123\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.21776\n",
      "exploration/env_infos/final/reward_dist Mean            0.0212517\n",
      "exploration/env_infos/final/reward_dist Std             0.0343024\n",
      "exploration/env_infos/final/reward_dist Max             0.0884713\n",
      "exploration/env_infos/final/reward_dist Min             9.1168e-15\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000384663\n",
      "exploration/env_infos/initial/reward_dist Std           0.000681671\n",
      "exploration/env_infos/initial/reward_dist Max           0.00174597\n",
      "exploration/env_infos/initial/reward_dist Min           5.76326e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.130253\n",
      "exploration/env_infos/reward_dist Std                   0.252659\n",
      "exploration/env_infos/reward_dist Max                   0.989593\n",
      "exploration/env_infos/reward_dist Min                   9.1168e-15\n",
      "exploration/env_infos/final/reward_energy Mean         -0.126808\n",
      "exploration/env_infos/final/reward_energy Std           0.0924511\n",
      "exploration/env_infos/final/reward_energy Max          -0.0648077\n",
      "exploration/env_infos/final/reward_energy Min          -0.309886\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.108618\n",
      "exploration/env_infos/initial/reward_energy Std         0.0458186\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0366528\n",
      "exploration/env_infos/initial/reward_energy Min        -0.179025\n",
      "exploration/env_infos/reward_energy Mean               -0.10721\n",
      "exploration/env_infos/reward_energy Std                 0.0606534\n",
      "exploration/env_infos/reward_energy Max                -0.00262183\n",
      "exploration/env_infos/reward_energy Min                -0.309886\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0494958\n",
      "exploration/env_infos/final/end_effector_loc Std        0.277684\n",
      "exploration/env_infos/final/end_effector_loc Max        0.372061\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.428366\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.000262898\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00415963\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00792012\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.00479534\n",
      "exploration/env_infos/end_effector_loc Mean             0.00126472\n",
      "exploration/env_infos/end_effector_loc Std              0.155724\n",
      "exploration/env_infos/end_effector_loc Max              0.372061\n",
      "exploration/env_infos/end_effector_loc Min             -0.428366\n",
      "evaluation/num steps total                           5000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.149924\n",
      "evaluation/Rewards Std                                  0.148945\n",
      "evaluation/Rewards Max                                  0.129102\n",
      "evaluation/Rewards Min                                 -1.06267\n",
      "evaluation/Returns Mean                                -2.99849\n",
      "evaluation/Returns Std                                  1.93408\n",
      "evaluation/Returns Max                                 -0.171615\n",
      "evaluation/Returns Min                                 -9.61362\n",
      "evaluation/Actions Mean                                -0.00934618\n",
      "evaluation/Actions Std                                  0.121605\n",
      "evaluation/Actions Max                                  0.828549\n",
      "evaluation/Actions Min                                 -0.811231\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.99849\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0398358\n",
      "evaluation/env_infos/final/reward_dist Std              0.165901\n",
      "evaluation/env_infos/final/reward_dist Max              0.929791\n",
      "evaluation/env_infos/final/reward_dist Min              8.6986e-113\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.0049856\n",
      "evaluation/env_infos/initial/reward_dist Std            0.008458\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0321189\n",
      "evaluation/env_infos/initial/reward_dist Min            9.01533e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0844899\n",
      "evaluation/env_infos/reward_dist Std                    0.196414\n",
      "evaluation/env_infos/reward_dist Max                    0.996238\n",
      "evaluation/env_infos/reward_dist Min                    8.6986e-113\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.15929\n",
      "evaluation/env_infos/final/reward_energy Std            0.126319\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00916499\n",
      "evaluation/env_infos/final/reward_energy Min           -0.51637\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.271121\n",
      "evaluation/env_infos/initial/reward_energy Std          0.240372\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0183368\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.921877\n",
      "evaluation/env_infos/reward_energy Mean                -0.129561\n",
      "evaluation/env_infos/reward_energy Std                  0.113862\n",
      "evaluation/env_infos/reward_energy Max                 -0.00470734\n",
      "evaluation/env_infos/reward_energy Min                 -0.921877\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0239422\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.501794\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00262822\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0125379\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0414274\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0405615\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0222596\n",
      "evaluation/env_infos/end_effector_loc Std               0.326799\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00623227\n",
      "time/evaluation sampling (s)                            1.14809\n",
      "time/exploration sampling (s)                           0.123522\n",
      "time/logging (s)                                        0.0214727\n",
      "time/saving (s)                                         0.0259325\n",
      "time/training (s)                                      43.8123\n",
      "time/epoch (s)                                         45.1376\n",
      "time/total (s)                                        211.15\n",
      "Epoch                                                   4\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:52:56.385889 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 5 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00567169\n",
      "trainer/QF2 Loss                                        0.0122032\n",
      "trainer/Policy Loss                                     4.39703\n",
      "trainer/Q1 Predictions Mean                            -2.47431\n",
      "trainer/Q1 Predictions Std                              1.53245\n",
      "trainer/Q1 Predictions Max                              0.158491\n",
      "trainer/Q1 Predictions Min                             -8.17439\n",
      "trainer/Q2 Predictions Mean                            -2.51741\n",
      "trainer/Q2 Predictions Std                              1.53965\n",
      "trainer/Q2 Predictions Max                              0.067403\n",
      "trainer/Q2 Predictions Min                             -8.12406\n",
      "trainer/Q Targets Mean                                 -2.48117\n",
      "trainer/Q Targets Std                                   1.53311\n",
      "trainer/Q Targets Max                                   0.0825881\n",
      "trainer/Q Targets Min                                  -8.08491\n",
      "trainer/Log Pis Mean                                    2.15063\n",
      "trainer/Log Pis Std                                     1.38034\n",
      "trainer/Log Pis Max                                     8.85853\n",
      "trainer/Log Pis Min                                    -2.79126\n",
      "trainer/Policy mu Mean                                  0.0549099\n",
      "trainer/Policy mu Std                                   0.637185\n",
      "trainer/Policy mu Max                                   3.22923\n",
      "trainer/Policy mu Min                                  -2.26771\n",
      "trainer/Policy log std Mean                            -2.12076\n",
      "trainer/Policy log std Std                              0.614233\n",
      "trainer/Policy log std Max                             -0.237867\n",
      "trainer/Policy log std Min                             -3.07506\n",
      "trainer/Alpha                                           0.0144181\n",
      "trainer/Alpha Loss                                      0.63856\n",
      "exploration/num steps total                          1600\n",
      "exploration/num paths total                            80\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.130305\n",
      "exploration/Rewards Std                                 0.0792142\n",
      "exploration/Rewards Max                                 0.0260418\n",
      "exploration/Rewards Min                                -0.309706\n",
      "exploration/Returns Mean                               -2.6061\n",
      "exploration/Returns Std                                 1.2684\n",
      "exploration/Returns Max                                -1.07159\n",
      "exploration/Returns Min                                -4.505\n",
      "exploration/Actions Mean                               -0.0140483\n",
      "exploration/Actions Std                                 0.205331\n",
      "exploration/Actions Max                                 0.754753\n",
      "exploration/Actions Min                                -0.662271\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.6061\n",
      "exploration/env_infos/final/reward_dist Mean            3.65199e-05\n",
      "exploration/env_infos/final/reward_dist Std             4.54696e-05\n",
      "exploration/env_infos/final/reward_dist Max             0.000107332\n",
      "exploration/env_infos/final/reward_dist Min             4.2556e-86\n",
      "exploration/env_infos/initial/reward_dist Mean          0.015449\n",
      "exploration/env_infos/initial/reward_dist Std           0.019222\n",
      "exploration/env_infos/initial/reward_dist Max           0.0525612\n",
      "exploration/env_infos/initial/reward_dist Min           2.19166e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0751058\n",
      "exploration/env_infos/reward_dist Std                   0.185626\n",
      "exploration/env_infos/reward_dist Max                   0.907345\n",
      "exploration/env_infos/reward_dist Min                   4.2556e-86\n",
      "exploration/env_infos/final/reward_energy Mean         -0.283911\n",
      "exploration/env_infos/final/reward_energy Std           0.0609173\n",
      "exploration/env_infos/final/reward_energy Max          -0.194715\n",
      "exploration/env_infos/final/reward_energy Min          -0.363324\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.354308\n",
      "exploration/env_infos/initial/reward_energy Std         0.206757\n",
      "exploration/env_infos/initial/reward_energy Max        -0.151482\n",
      "exploration/env_infos/initial/reward_energy Min        -0.720095\n",
      "exploration/env_infos/reward_energy Mean               -0.238556\n",
      "exploration/env_infos/reward_energy Std                 0.166755\n",
      "exploration/env_infos/reward_energy Max                -0.019065\n",
      "exploration/env_infos/reward_energy Min                -0.90766\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.143686\n",
      "exploration/env_infos/final/end_effector_loc Std        0.392308\n",
      "exploration/env_infos/final/end_effector_loc Max        0.414712\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.835596\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.000419141\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0144975\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0355738\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0208871\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0312969\n",
      "exploration/env_infos/end_effector_loc Std              0.27247\n",
      "exploration/env_infos/end_effector_loc Max              0.45454\n",
      "exploration/env_infos/end_effector_loc Min             -0.835596\n",
      "evaluation/num steps total                           6000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.118701\n",
      "evaluation/Rewards Std                                  0.126882\n",
      "evaluation/Rewards Max                                  0.121322\n",
      "evaluation/Rewards Min                                 -0.778763\n",
      "evaluation/Returns Mean                                -2.37402\n",
      "evaluation/Returns Std                                  1.81265\n",
      "evaluation/Returns Max                                 -0.0820162\n",
      "evaluation/Returns Min                                 -7.85345\n",
      "evaluation/Actions Mean                                -0.00571539\n",
      "evaluation/Actions Std                                  0.0785998\n",
      "evaluation/Actions Max                                  0.541266\n",
      "evaluation/Actions Min                                 -0.449876\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.37402\n",
      "evaluation/env_infos/final/reward_dist Mean             0.052454\n",
      "evaluation/env_infos/final/reward_dist Std              0.134443\n",
      "evaluation/env_infos/final/reward_dist Max              0.65142\n",
      "evaluation/env_infos/final/reward_dist Min              1.44847e-68\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00742581\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0107273\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0522268\n",
      "evaluation/env_infos/initial/reward_dist Min            1.13364e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0885291\n",
      "evaluation/env_infos/reward_dist Std                    0.186356\n",
      "evaluation/env_infos/reward_dist Max                    0.998247\n",
      "evaluation/env_infos/reward_dist Min                    1.44847e-68\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0744104\n",
      "evaluation/env_infos/final/reward_energy Std            0.0639856\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00553929\n",
      "evaluation/env_infos/final/reward_energy Min           -0.310044\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.185687\n",
      "evaluation/env_infos/initial/reward_energy Std          0.171221\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0204229\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.667717\n",
      "evaluation/env_infos/reward_energy Mean                -0.0807099\n",
      "evaluation/env_infos/reward_energy Std                  0.0768576\n",
      "evaluation/env_infos/reward_energy Max                 -0.00371764\n",
      "evaluation/env_infos/reward_energy Min                 -0.667717\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0262751\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.42951\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00209328\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0086812\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0270633\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0224938\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00496048\n",
      "evaluation/env_infos/end_effector_loc Std               0.265103\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00583848\n",
      "time/evaluation sampling (s)                            0.996723\n",
      "time/exploration sampling (s)                           0.130271\n",
      "time/logging (s)                                        0.0192504\n",
      "time/saving (s)                                         0.0273568\n",
      "time/training (s)                                      45.7176\n",
      "time/epoch (s)                                         46.897\n",
      "time/total (s)                                        258.139\n",
      "Epoch                                                   5\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:53:42.316920 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 6 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00633279\n",
      "trainer/QF2 Loss                                        0.00661089\n",
      "trainer/Policy Loss                                     4.54233\n",
      "trainer/Q1 Predictions Mean                            -2.59909\n",
      "trainer/Q1 Predictions Std                              1.8217\n",
      "trainer/Q1 Predictions Max                             -0.0421654\n",
      "trainer/Q1 Predictions Min                            -10.3377\n",
      "trainer/Q2 Predictions Mean                            -2.56751\n",
      "trainer/Q2 Predictions Std                              1.80243\n",
      "trainer/Q2 Predictions Max                             -0.0236893\n",
      "trainer/Q2 Predictions Min                            -10.2991\n",
      "trainer/Q Targets Mean                                 -2.58349\n",
      "trainer/Q Targets Std                                   1.81972\n",
      "trainer/Q Targets Max                                  -0.0724789\n",
      "trainer/Q Targets Min                                 -10.3058\n",
      "trainer/Log Pis Mean                                    2.15655\n",
      "trainer/Log Pis Std                                     1.6873\n",
      "trainer/Log Pis Max                                    10.0157\n",
      "trainer/Log Pis Min                                    -2.76369\n",
      "trainer/Policy mu Mean                                 -0.00182546\n",
      "trainer/Policy mu Std                                   0.830559\n",
      "trainer/Policy mu Max                                   3.61494\n",
      "trainer/Policy mu Min                                  -3.16031\n",
      "trainer/Policy log std Mean                            -2.08212\n",
      "trainer/Policy log std Std                              0.709946\n",
      "trainer/Policy log std Max                             -0.0453137\n",
      "trainer/Policy log std Min                             -3.09719\n",
      "trainer/Alpha                                           0.0150135\n",
      "trainer/Alpha Loss                                      0.657339\n",
      "exploration/num steps total                          1700\n",
      "exploration/num paths total                            85\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.1943\n",
      "exploration/Rewards Std                                 0.151057\n",
      "exploration/Rewards Max                                 0.107483\n",
      "exploration/Rewards Min                                -0.79885\n",
      "exploration/Returns Mean                               -3.886\n",
      "exploration/Returns Std                                 2.36637\n",
      "exploration/Returns Max                                -0.93731\n",
      "exploration/Returns Min                                -8.14266\n",
      "exploration/Actions Mean                               -0.0135752\n",
      "exploration/Actions Std                                 0.134729\n",
      "exploration/Actions Max                                 0.359998\n",
      "exploration/Actions Min                                -0.541396\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -3.886\n",
      "exploration/env_infos/final/reward_dist Mean            3.07272e-11\n",
      "exploration/env_infos/final/reward_dist Std             6.14545e-11\n",
      "exploration/env_infos/final/reward_dist Max             1.53636e-10\n",
      "exploration/env_infos/final/reward_dist Min             1.13219e-75\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00113706\n",
      "exploration/env_infos/initial/reward_dist Std           0.0011281\n",
      "exploration/env_infos/initial/reward_dist Max           0.00302697\n",
      "exploration/env_infos/initial/reward_dist Min           1.98745e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0305783\n",
      "exploration/env_infos/reward_dist Std                   0.120119\n",
      "exploration/env_infos/reward_dist Max                   0.740737\n",
      "exploration/env_infos/reward_dist Min                   1.13219e-75\n",
      "exploration/env_infos/final/reward_energy Mean         -0.170091\n",
      "exploration/env_infos/final/reward_energy Std           0.12207\n",
      "exploration/env_infos/final/reward_energy Max          -0.0640755\n",
      "exploration/env_infos/final/reward_energy Min          -0.40762\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.22864\n",
      "exploration/env_infos/initial/reward_energy Std         0.145261\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0525122\n",
      "exploration/env_infos/initial/reward_energy Min        -0.489391\n",
      "exploration/env_infos/reward_energy Mean               -0.155159\n",
      "exploration/env_infos/reward_energy Std                 0.112241\n",
      "exploration/env_infos/reward_energy Max                -0.0121795\n",
      "exploration/env_infos/reward_energy Min                -0.542801\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.195889\n",
      "exploration/env_infos/final/end_effector_loc Std        0.477377\n",
      "exploration/env_infos/final/end_effector_loc Max        0.394653\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00479282\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00829158\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0116731\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0205173\n",
      "exploration/env_infos/end_effector_loc Mean            -0.109625\n",
      "exploration/env_infos/end_effector_loc Std              0.280183\n",
      "exploration/env_infos/end_effector_loc Max              0.394653\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           7000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0792104\n",
      "evaluation/Rewards Std                                  0.0736306\n",
      "evaluation/Rewards Max                                  0.102641\n",
      "evaluation/Rewards Min                                 -0.407788\n",
      "evaluation/Returns Mean                                -1.58421\n",
      "evaluation/Returns Std                                  1.05238\n",
      "evaluation/Returns Max                                  0.0967754\n",
      "evaluation/Returns Min                                 -4.2172\n",
      "evaluation/Actions Mean                                -0.00535401\n",
      "evaluation/Actions Std                                  0.0743881\n",
      "evaluation/Actions Max                                  0.525742\n",
      "evaluation/Actions Min                                 -0.738899\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.58421\n",
      "evaluation/env_infos/final/reward_dist Mean             0.162834\n",
      "evaluation/env_infos/final/reward_dist Std              0.271171\n",
      "evaluation/env_infos/final/reward_dist Max              0.931191\n",
      "evaluation/env_infos/final/reward_dist Min              6.76781e-54\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00672518\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0118008\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0442445\n",
      "evaluation/env_infos/initial/reward_dist Min            3.0342e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.147249\n",
      "evaluation/env_infos/reward_dist Std                    0.252514\n",
      "evaluation/env_infos/reward_dist Max                    0.999118\n",
      "evaluation/env_infos/reward_dist Min                    6.76781e-54\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0668445\n",
      "evaluation/env_infos/final/reward_energy Std            0.0429974\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0111899\n",
      "evaluation/env_infos/final/reward_energy Min           -0.244395\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.172766\n",
      "evaluation/env_infos/initial/reward_energy Std          0.193862\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00824561\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.769268\n",
      "evaluation/env_infos/reward_energy Mean                -0.0728739\n",
      "evaluation/env_infos/reward_energy Std                  0.076249\n",
      "evaluation/env_infos/reward_energy Max                 -0.000649051\n",
      "evaluation/env_infos/reward_energy Min                 -0.769268\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0831516\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.288532\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.332522\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean     -0.000258487\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00917724\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0262871\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0369449\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0316448\n",
      "evaluation/env_infos/end_effector_loc Std               0.184745\n",
      "evaluation/env_infos/end_effector_loc Max               0.332522\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00589812\n",
      "time/evaluation sampling (s)                            0.903825\n",
      "time/exploration sampling (s)                           0.119275\n",
      "time/logging (s)                                        0.0199945\n",
      "time/saving (s)                                         0.0272606\n",
      "time/training (s)                                      44.7508\n",
      "time/epoch (s)                                         45.827\n",
      "time/total (s)                                        304.07\n",
      "Epoch                                                   6\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:54:28.701213 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 7 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   2000\r\n",
      "trainer/QF1 Loss                                        0.00681199\r\n",
      "trainer/QF2 Loss                                        0.0050364\r\n",
      "trainer/Policy Loss                                     4.57428\r\n",
      "trainer/Q1 Predictions Mean                            -2.73911\r\n",
      "trainer/Q1 Predictions Std                              1.71652\r\n",
      "trainer/Q1 Predictions Max                              0.0181174\r\n",
      "trainer/Q1 Predictions Min                             -9.72226\r\n",
      "trainer/Q2 Predictions Mean                            -2.71108\r\n",
      "trainer/Q2 Predictions Std                              1.68856\r\n",
      "trainer/Q2 Predictions Max                              0.0174201\r\n",
      "trainer/Q2 Predictions Min                             -9.39219\r\n",
      "trainer/Q Targets Mean                                 -2.72035\r\n",
      "trainer/Q Targets Std                                   1.70165\r\n",
      "trainer/Q Targets Max                                   0.0317613\r\n",
      "trainer/Q Targets Min                                  -9.53117\r\n",
      "trainer/Log Pis Mean                                    2.06398\r\n",
      "trainer/Log Pis Std                                     1.49321\r\n",
      "trainer/Log Pis Max                                     8.84349\r\n",
      "trainer/Log Pis Min                                    -2.54917\r\n",
      "trainer/Policy mu Mean                                  0.0559917\r\n",
      "trainer/Policy mu Std                                   0.794381\r\n",
      "trainer/Policy mu Max                                   3.87972\r\n",
      "trainer/Policy mu Min                                  -2.85911\r\n",
      "trainer/Policy log std Mean                            -2.055\r\n",
      "trainer/Policy log std Std                              0.716259\r\n",
      "trainer/Policy log std Max                              0.395696\r\n",
      "trainer/Policy log std Min                             -3.2741\r\n",
      "trainer/Alpha                                           0.0164657\r\n",
      "trainer/Alpha Loss                                      0.262764\r\n",
      "exploration/num steps total                          1800\r\n",
      "exploration/num paths total                            90\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.151768\r\n",
      "exploration/Rewards Std                                 0.056797\r\n",
      "exploration/Rewards Max                                -0.039229\r\n",
      "exploration/Rewards Min                                -0.339839\r\n",
      "exploration/Returns Mean                               -3.03535\r\n",
      "exploration/Returns Std                                 0.581125\r\n",
      "exploration/Returns Max                                -2.21891\r\n",
      "exploration/Returns Min                                -3.8612\r\n",
      "exploration/Actions Mean                               -0.000202929\r\n",
      "exploration/Actions Std                                 0.114183\r\n",
      "exploration/Actions Max                                 0.655157\r\n",
      "exploration/Actions Min                                -0.47409\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -3.03535\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.024186\r\n",
      "exploration/env_infos/final/reward_dist Std             0.046795\r\n",
      "exploration/env_infos/final/reward_dist Max             0.117759\r\n",
      "exploration/env_infos/final/reward_dist Min             9.49089e-20\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00187196\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.0021543\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.00582026\r\n",
      "exploration/env_infos/initial/reward_dist Min           0.000199299\r\n",
      "exploration/env_infos/reward_dist Mean                  0.0505343\r\n",
      "exploration/env_infos/reward_dist Std                   0.128366\r\n",
      "exploration/env_infos/reward_dist Max                   0.775154\r\n",
      "exploration/env_infos/reward_dist Min                   9.49089e-20\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.138562\r\n",
      "exploration/env_infos/final/reward_energy Std           0.0539767\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.0724274\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.224472\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.17544\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.185066\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0431646\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.542937\r\n",
      "exploration/env_infos/reward_energy Mean               -0.122463\r\n",
      "exploration/env_infos/reward_energy Std                 0.105255\r\n",
      "exploration/env_infos/reward_energy Max                -0.0225659\r\n",
      "exploration/env_infos/reward_energy Min                -0.667024\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.0906072\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.253651\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.379516\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.409797\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00114376\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00894301\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0227838\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0147598\r\n",
      "exploration/env_infos/end_effector_loc Mean             0.0573177\r\n",
      "exploration/env_infos/end_effector_loc Std              0.174468\r\n",
      "exploration/env_infos/end_effector_loc Max              0.479329\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.409797\r\n",
      "evaluation/num steps total                           8000\r\n",
      "evaluation/num paths total                            400\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.0976331\r\n",
      "evaluation/Rewards Std                                  0.0820227\r\n",
      "evaluation/Rewards Max                                  0.126871\r\n",
      "evaluation/Rewards Min                                 -0.629454\r\n",
      "evaluation/Returns Mean                                -1.95266\r\n",
      "evaluation/Returns Std                                  1.25017\r\n",
      "evaluation/Returns Max                                  0.473641\r\n",
      "evaluation/Returns Min                                 -4.29378\r\n",
      "evaluation/Actions Mean                                -0.00699374\r\n",
      "evaluation/Actions Std                                  0.062958\r\n",
      "evaluation/Actions Max                                  0.53756\r\n",
      "evaluation/Actions Min                                 -0.317085\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -1.95266\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0256312\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.0708405\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.374638\r\n",
      "evaluation/env_infos/final/reward_dist Min              7.751e-61\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00519144\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00919349\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0353432\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.3794e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0691148\r\n",
      "evaluation/env_infos/reward_dist Std                    0.164294\r\n",
      "evaluation/env_infos/reward_dist Max                    0.995254\r\n",
      "evaluation/env_infos/reward_dist Min                    7.751e-61\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0777651\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.0384106\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00736555\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.21387\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.159041\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.118669\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0232223\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.548265\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.0723762\r\n",
      "evaluation/env_infos/reward_energy Std                  0.0527914\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00281106\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.548265\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.00271826\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.323918\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.864556\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.643009\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.0032554\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00621471\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.026878\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0109632\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0289922\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.18374\r\n",
      "evaluation/env_infos/end_effector_loc Max               0.864556\r\n",
      "evaluation/env_infos/end_effector_loc Min              -0.643009\r\n",
      "time/data storing (s)                                   0.00645136\r\n",
      "time/evaluation sampling (s)                            1.07677\r\n",
      "time/exploration sampling (s)                           0.132517\r\n",
      "time/logging (s)                                        0.018259\r\n",
      "time/saving (s)                                         0.0271046\r\n",
      "time/training (s)                                      45.0058\r\n",
      "time/epoch (s)                                         46.2669\r\n",
      "time/total (s)                                        350.452\r\n",
      "Epoch                                                   7\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:55:17.635194 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 8 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   2000\r\n",
      "trainer/QF1 Loss                                        0.00994483\r\n",
      "trainer/QF2 Loss                                        0.00419931\r\n",
      "trainer/Policy Loss                                     4.35377\r\n",
      "trainer/Q1 Predictions Mean                            -2.58492\r\n",
      "trainer/Q1 Predictions Std                              1.73543\r\n",
      "trainer/Q1 Predictions Max                              0.0519852\r\n",
      "trainer/Q1 Predictions Min                             -9.48846\r\n",
      "trainer/Q2 Predictions Mean                            -2.61422\r\n",
      "trainer/Q2 Predictions Std                              1.72955\r\n",
      "trainer/Q2 Predictions Max                             -0.00511848\r\n",
      "trainer/Q2 Predictions Min                             -9.60227\r\n",
      "trainer/Q Targets Mean                                 -2.58832\r\n",
      "trainer/Q Targets Std                                   1.72475\r\n",
      "trainer/Q Targets Max                                   0.0104756\r\n",
      "trainer/Q Targets Min                                  -9.57905\r\n",
      "trainer/Log Pis Mean                                    1.96961\r\n",
      "trainer/Log Pis Std                                     1.3962\r\n",
      "trainer/Log Pis Max                                     6.36898\r\n",
      "trainer/Log Pis Min                                    -2.26972\r\n",
      "trainer/Policy mu Mean                                 -0.0878941\r\n",
      "trainer/Policy mu Std                                   0.534441\r\n",
      "trainer/Policy mu Max                                   3.22847\r\n",
      "trainer/Policy mu Min                                  -2.42123\r\n",
      "trainer/Policy log std Mean                            -2.18484\r\n",
      "trainer/Policy log std Std                              0.632337\r\n",
      "trainer/Policy log std Max                             -0.0397819\r\n",
      "trainer/Policy log std Min                             -3.29879\r\n",
      "trainer/Alpha                                           0.0169114\r\n",
      "trainer/Alpha Loss                                     -0.123994\r\n",
      "exploration/num steps total                          1900\r\n",
      "exploration/num paths total                            95\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.106803\r\n",
      "exploration/Rewards Std                                 0.0609244\r\n",
      "exploration/Rewards Max                                -0.0105959\r\n",
      "exploration/Rewards Min                                -0.348127\r\n",
      "exploration/Returns Mean                               -2.13606\r\n",
      "exploration/Returns Std                                 0.539058\r\n",
      "exploration/Returns Max                                -1.51124\r\n",
      "exploration/Returns Min                                -3.04686\r\n",
      "exploration/Actions Mean                               -0.0133414\r\n",
      "exploration/Actions Std                                 0.12327\r\n",
      "exploration/Actions Max                                 0.431672\r\n",
      "exploration/Actions Min                                -0.470231\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -2.13606\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.000471733\r\n",
      "exploration/env_infos/final/reward_dist Std             0.000845769\r\n",
      "exploration/env_infos/final/reward_dist Max             0.00215597\r\n",
      "exploration/env_infos/final/reward_dist Min             6.65339e-12\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00374005\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.00684395\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0174226\r\n",
      "exploration/env_infos/initial/reward_dist Min           4.00697e-05\r\n",
      "exploration/env_infos/reward_dist Mean                  0.00685885\r\n",
      "exploration/env_infos/reward_dist Std                   0.0161518\r\n",
      "exploration/env_infos/reward_dist Max                   0.0849804\r\n",
      "exploration/env_infos/reward_dist Min                   6.65339e-12\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.180666\r\n",
      "exploration/env_infos/final/reward_energy Std           0.08952\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.0993884\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.293473\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.157402\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.0959166\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.00479835\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.303022\r\n",
      "exploration/env_infos/reward_energy Mean               -0.135994\r\n",
      "exploration/env_infos/reward_energy Std                 0.110691\r\n",
      "exploration/env_infos/reward_energy Max                -0.00479835\r\n",
      "exploration/env_infos/reward_energy Min                -0.60868\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0754738\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.221463\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.376744\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.322015\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00162107\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00631199\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0140107\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.00810763\r\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0195363\r\n",
      "exploration/env_infos/end_effector_loc Std              0.118954\r\n",
      "exploration/env_infos/end_effector_loc Max              0.376744\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.322015\r\n",
      "evaluation/num steps total                           9000\r\n",
      "evaluation/num paths total                            450\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.0816801\r\n",
      "evaluation/Rewards Std                                  0.0749066\r\n",
      "evaluation/Rewards Max                                  0.152933\r\n",
      "evaluation/Rewards Min                                 -0.401618\r\n",
      "evaluation/Returns Mean                                -1.6336\r\n",
      "evaluation/Returns Std                                  1.26406\r\n",
      "evaluation/Returns Max                                  0.863669\r\n",
      "evaluation/Returns Min                                 -4.98829\r\n",
      "evaluation/Actions Mean                                -0.00633825\r\n",
      "evaluation/Actions Std                                  0.0486527\r\n",
      "evaluation/Actions Max                                  0.484544\r\n",
      "evaluation/Actions Min                                 -0.260106\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -1.6336\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.105194\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.18329\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.751818\r\n",
      "evaluation/env_infos/final/reward_dist Min              2.94776e-25\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00270468\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00647802\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0317762\r\n",
      "evaluation/env_infos/initial/reward_dist Min            9.9632e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0711579\r\n",
      "evaluation/env_infos/reward_dist Std                    0.155203\r\n",
      "evaluation/env_infos/reward_dist Max                    0.982466\r\n",
      "evaluation/env_infos/reward_dist Min                    2.94776e-25\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0535455\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.0336623\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00245482\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.173874\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.119997\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.108016\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00577529\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.52548\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.0537148\r\n",
      "evaluation/env_infos/reward_energy Std                  0.043923\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.000876132\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.52548\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0176926\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.218109\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.43119\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.572256\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00250782\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00512781\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0242272\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00714639\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.01236\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.129452\r\n",
      "evaluation/env_infos/end_effector_loc Max               0.43119\r\n",
      "evaluation/env_infos/end_effector_loc Min              -0.572256\r\n",
      "time/data storing (s)                                   0.00662246\r\n",
      "time/evaluation sampling (s)                            0.973708\r\n",
      "time/exploration sampling (s)                           0.125896\r\n",
      "time/logging (s)                                        0.0198462\r\n",
      "time/saving (s)                                         0.027549\r\n",
      "time/training (s)                                      47.6558\r\n",
      "time/epoch (s)                                         48.8094\r\n",
      "time/total (s)                                        399.387\r\n",
      "Epoch                                                   8\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:56:05.252162 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 9 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00570543\r\n",
      "trainer/QF2 Loss                                         0.0038679\r\n",
      "trainer/Policy Loss                                      4.13852\r\n",
      "trainer/Q1 Predictions Mean                             -2.14128\r\n",
      "trainer/Q1 Predictions Std                               1.41236\r\n",
      "trainer/Q1 Predictions Max                              -0.0427018\r\n",
      "trainer/Q1 Predictions Min                              -7.35702\r\n",
      "trainer/Q2 Predictions Mean                             -2.1649\r\n",
      "trainer/Q2 Predictions Std                               1.42104\r\n",
      "trainer/Q2 Predictions Max                              -0.0447829\r\n",
      "trainer/Q2 Predictions Min                              -7.18682\r\n",
      "trainer/Q Targets Mean                                  -2.15307\r\n",
      "trainer/Q Targets Std                                    1.41476\r\n",
      "trainer/Q Targets Max                                   -0.010761\r\n",
      "trainer/Q Targets Min                                   -7.31529\r\n",
      "trainer/Log Pis Mean                                     2.12815\r\n",
      "trainer/Log Pis Std                                      1.43175\r\n",
      "trainer/Log Pis Max                                      6.80139\r\n",
      "trainer/Log Pis Min                                     -4.51095\r\n",
      "trainer/Policy mu Mean                                  -0.0668423\r\n",
      "trainer/Policy mu Std                                    0.485394\r\n",
      "trainer/Policy mu Max                                    2.75377\r\n",
      "trainer/Policy mu Min                                   -2.80817\r\n",
      "trainer/Policy log std Mean                             -2.32726\r\n",
      "trainer/Policy log std Std                               0.617717\r\n",
      "trainer/Policy log std Max                               0.108484\r\n",
      "trainer/Policy log std Min                              -3.33017\r\n",
      "trainer/Alpha                                            0.0174049\r\n",
      "trainer/Alpha Loss                                       0.519197\r\n",
      "exploration/num steps total                           2000\r\n",
      "exploration/num paths total                            100\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.112829\r\n",
      "exploration/Rewards Std                                  0.0548481\r\n",
      "exploration/Rewards Max                                 -0.00911166\r\n",
      "exploration/Rewards Min                                 -0.264598\r\n",
      "exploration/Returns Mean                                -2.25657\r\n",
      "exploration/Returns Std                                  0.741188\r\n",
      "exploration/Returns Max                                 -1.09547\r\n",
      "exploration/Returns Min                                 -3.31923\r\n",
      "exploration/Actions Mean                                -0.011143\r\n",
      "exploration/Actions Std                                  0.156428\r\n",
      "exploration/Actions Max                                  0.664053\r\n",
      "exploration/Actions Min                                 -0.723389\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.25657\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0296076\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0451877\r\n",
      "exploration/env_infos/final/reward_dist Max              0.116985\r\n",
      "exploration/env_infos/final/reward_dist Min              9.08166e-20\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00153842\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00187138\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0052305\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.00016646\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0813231\r\n",
      "exploration/env_infos/reward_dist Std                    0.220469\r\n",
      "exploration/env_infos/reward_dist Max                    0.934222\r\n",
      "exploration/env_infos/reward_dist Min                    9.08166e-20\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.126982\r\n",
      "exploration/env_infos/final/reward_energy Std            0.076099\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0393261\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.238642\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.281439\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.246975\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0656877\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.67177\r\n",
      "exploration/env_infos/reward_energy Mean                -0.159663\r\n",
      "exploration/env_infos/reward_energy Std                  0.153934\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0131872\r\n",
      "exploration/env_infos/reward_energy Min                 -0.770136\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0358385\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.231671\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.274591\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.445407\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0051081\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0122132\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0332027\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00529771\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00960494\r\n",
      "exploration/env_infos/end_effector_loc Std               0.138768\r\n",
      "exploration/env_infos/end_effector_loc Max               0.274591\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.445407\r\n",
      "evaluation/num steps total                           10000\r\n",
      "evaluation/num paths total                             500\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.101219\r\n",
      "evaluation/Rewards Std                                   0.0766764\r\n",
      "evaluation/Rewards Max                                   0.160141\r\n",
      "evaluation/Rewards Min                                  -0.43223\r\n",
      "evaluation/Returns Mean                                 -2.02438\r\n",
      "evaluation/Returns Std                                   0.940026\r\n",
      "evaluation/Returns Max                                   0.800924\r\n",
      "evaluation/Returns Min                                  -4.48966\r\n",
      "evaluation/Actions Mean                                 -0.0159762\r\n",
      "evaluation/Actions Std                                   0.0635802\r\n",
      "evaluation/Actions Max                                   0.538648\r\n",
      "evaluation/Actions Min                                  -0.73726\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.02438\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.00350949\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.0184681\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.122934\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.09351e-41\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00895051\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126317\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0482769\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.49159e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0545728\r\n",
      "evaluation/env_infos/reward_dist Std                     0.156991\r\n",
      "evaluation/env_infos/reward_dist Max                     0.985992\r\n",
      "evaluation/env_infos/reward_dist Min                     6.09351e-41\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0616133\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0371605\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0129536\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.176696\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.161233\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.161781\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00627345\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.892844\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0699691\r\n",
      "evaluation/env_infos/reward_energy Std                   0.060825\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000186554\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.892844\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.14569\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.316395\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.590499\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.8195\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       1.70531e-05\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00807535\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0269324\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.036863\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.045655\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.198681\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.590499\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.8195\r\n",
      "time/data storing (s)                                    0.00591018\r\n",
      "time/evaluation sampling (s)                             1.09492\r\n",
      "time/exploration sampling (s)                            0.123204\r\n",
      "time/logging (s)                                         0.0203079\r\n",
      "time/saving (s)                                          0.0299642\r\n",
      "time/training (s)                                       46.2004\r\n",
      "time/epoch (s)                                          47.4747\r\n",
      "time/total (s)                                         447.004\r\n",
      "Epoch                                                    9\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:56:53.967325 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 10 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00275141\n",
      "trainer/QF2 Loss                                         0.00233624\n",
      "trainer/Policy Loss                                      3.86375\n",
      "trainer/Q1 Predictions Mean                             -2.05558\n",
      "trainer/Q1 Predictions Std                               1.50876\n",
      "trainer/Q1 Predictions Max                               0.00440224\n",
      "trainer/Q1 Predictions Min                              -8.77215\n",
      "trainer/Q2 Predictions Mean                             -2.04684\n",
      "trainer/Q2 Predictions Std                               1.50415\n",
      "trainer/Q2 Predictions Max                               0.0158683\n",
      "trainer/Q2 Predictions Min                              -8.63884\n",
      "trainer/Q Targets Mean                                  -2.05325\n",
      "trainer/Q Targets Std                                    1.51062\n",
      "trainer/Q Targets Max                                    0.0489913\n",
      "trainer/Q Targets Min                                   -8.68765\n",
      "trainer/Log Pis Mean                                     1.95703\n",
      "trainer/Log Pis Std                                      1.42759\n",
      "trainer/Log Pis Max                                      7.95479\n",
      "trainer/Log Pis Min                                     -2.57103\n",
      "trainer/Policy mu Mean                                  -0.0398943\n",
      "trainer/Policy mu Std                                    0.530987\n",
      "trainer/Policy mu Max                                    3.23659\n",
      "trainer/Policy mu Min                                   -3.07985\n",
      "trainer/Policy log std Mean                             -2.16581\n",
      "trainer/Policy log std Std                               0.633642\n",
      "trainer/Policy log std Max                              -0.330272\n",
      "trainer/Policy log std Min                              -3.27722\n",
      "trainer/Alpha                                            0.0184243\n",
      "trainer/Alpha Loss                                      -0.171646\n",
      "exploration/num steps total                           2100\n",
      "exploration/num paths total                            105\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.135718\n",
      "exploration/Rewards Std                                  0.0651935\n",
      "exploration/Rewards Max                                 -0.000324864\n",
      "exploration/Rewards Min                                 -0.34746\n",
      "exploration/Returns Mean                                -2.71436\n",
      "exploration/Returns Std                                  0.96212\n",
      "exploration/Returns Max                                 -1.76606\n",
      "exploration/Returns Min                                 -4.4496\n",
      "exploration/Actions Mean                                -0.00734716\n",
      "exploration/Actions Std                                  0.153289\n",
      "exploration/Actions Max                                  0.409148\n",
      "exploration/Actions Min                                 -0.645\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.71436\n",
      "exploration/env_infos/final/reward_dist Mean             0.101659\n",
      "exploration/env_infos/final/reward_dist Std              0.127008\n",
      "exploration/env_infos/final/reward_dist Max              0.299808\n",
      "exploration/env_infos/final/reward_dist Min              5.06065e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00107079\n",
      "exploration/env_infos/initial/reward_dist Std            0.00117183\n",
      "exploration/env_infos/initial/reward_dist Max            0.00330836\n",
      "exploration/env_infos/initial/reward_dist Min            5.26536e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.168609\n",
      "exploration/env_infos/reward_dist Std                    0.268213\n",
      "exploration/env_infos/reward_dist Max                    0.992729\n",
      "exploration/env_infos/reward_dist Min                    2.74707e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.156667\n",
      "exploration/env_infos/final/reward_energy Std            0.0277529\n",
      "exploration/env_infos/final/reward_energy Max           -0.120024\n",
      "exploration/env_infos/final/reward_energy Min           -0.19781\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.178458\n",
      "exploration/env_infos/initial/reward_energy Std          0.108537\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0664656\n",
      "exploration/env_infos/initial/reward_energy Min         -0.369194\n",
      "exploration/env_infos/reward_energy Mean                -0.17214\n",
      "exploration/env_infos/reward_energy Std                  0.132176\n",
      "exploration/env_infos/reward_energy Max                 -0.0144251\n",
      "exploration/env_infos/reward_energy Min                 -0.690698\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00573991\n",
      "exploration/env_infos/final/end_effector_loc Std         0.230338\n",
      "exploration/env_infos/final/end_effector_loc Max         0.537435\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.371753\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00323686\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00663756\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0160918\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00990321\n",
      "exploration/env_infos/end_effector_loc Mean              0.0445197\n",
      "exploration/env_infos/end_effector_loc Std               0.184137\n",
      "exploration/env_infos/end_effector_loc Max               0.551264\n",
      "exploration/env_infos/end_effector_loc Min              -0.371753\n",
      "evaluation/num steps total                           11000\n",
      "evaluation/num paths total                             550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0799058\n",
      "evaluation/Rewards Std                                   0.0726554\n",
      "evaluation/Rewards Max                                   0.115023\n",
      "evaluation/Rewards Min                                  -0.368163\n",
      "evaluation/Returns Mean                                 -1.59812\n",
      "evaluation/Returns Std                                   1.09957\n",
      "evaluation/Returns Max                                   0.645098\n",
      "evaluation/Returns Min                                  -4.40284\n",
      "evaluation/Actions Mean                                 -0.00821679\n",
      "evaluation/Actions Std                                   0.0867123\n",
      "evaluation/Actions Max                                   0.68083\n",
      "evaluation/Actions Min                                  -0.699399\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.59812\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0404476\n",
      "evaluation/env_infos/final/reward_dist Std               0.106274\n",
      "evaluation/env_infos/final/reward_dist Max               0.441244\n",
      "evaluation/env_infos/final/reward_dist Min               1.36793e-64\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00503769\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00993891\n",
      "evaluation/env_infos/initial/reward_dist Max             0.045878\n",
      "evaluation/env_infos/initial/reward_dist Min             1.58702e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0798276\n",
      "evaluation/env_infos/reward_dist Std                     0.182767\n",
      "evaluation/env_infos/reward_dist Max                     0.997547\n",
      "evaluation/env_infos/reward_dist Min                     1.11471e-64\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0892516\n",
      "evaluation/env_infos/final/reward_energy Std             0.058326\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00289591\n",
      "evaluation/env_infos/final/reward_energy Min            -0.256876\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.173628\n",
      "evaluation/env_infos/initial/reward_energy Std           0.195496\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.010487\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.877439\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0806106\n",
      "evaluation/env_infos/reward_energy Std                   0.0931397\n",
      "evaluation/env_infos/reward_energy Max                  -0.000882694\n",
      "evaluation/env_infos/reward_energy Min                  -0.877439\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0506597\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.304094\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.587053\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000760987\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0092129\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0340415\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0276754\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.000761707\n",
      "evaluation/env_infos/end_effector_loc Std                0.199944\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.587053\n",
      "time/data storing (s)                                    0.00626315\n",
      "time/evaluation sampling (s)                             1.34677\n",
      "time/exploration sampling (s)                            0.149854\n",
      "time/logging (s)                                         0.0183086\n",
      "time/saving (s)                                          0.0259565\n",
      "time/training (s)                                       46.9968\n",
      "time/epoch (s)                                          48.5439\n",
      "time/total (s)                                         495.717\n",
      "Epoch                                                   10\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:57:42.038544 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 11 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00366848\n",
      "trainer/QF2 Loss                                         0.00334031\n",
      "trainer/Policy Loss                                      3.62385\n",
      "trainer/Q1 Predictions Mean                             -1.61424\n",
      "trainer/Q1 Predictions Std                               0.926969\n",
      "trainer/Q1 Predictions Max                              -0.0116021\n",
      "trainer/Q1 Predictions Min                              -4.71361\n",
      "trainer/Q2 Predictions Mean                             -1.63588\n",
      "trainer/Q2 Predictions Std                               0.929444\n",
      "trainer/Q2 Predictions Max                              -0.069615\n",
      "trainer/Q2 Predictions Min                              -4.73076\n",
      "trainer/Q Targets Mean                                  -1.65231\n",
      "trainer/Q Targets Std                                    0.931405\n",
      "trainer/Q Targets Max                                   -0.0383528\n",
      "trainer/Q Targets Min                                   -4.78057\n",
      "trainer/Log Pis Mean                                     2.0443\n",
      "trainer/Log Pis Std                                      1.38234\n",
      "trainer/Log Pis Max                                      6.58164\n",
      "trainer/Log Pis Min                                     -3.66613\n",
      "trainer/Policy mu Mean                                  -0.0466545\n",
      "trainer/Policy mu Std                                    0.463625\n",
      "trainer/Policy mu Max                                    2.33464\n",
      "trainer/Policy mu Min                                   -2.32429\n",
      "trainer/Policy log std Mean                             -2.24442\n",
      "trainer/Policy log std Std                               0.66171\n",
      "trainer/Policy log std Max                              -0.120501\n",
      "trainer/Policy log std Min                              -3.23926\n",
      "trainer/Alpha                                            0.0188339\n",
      "trainer/Alpha Loss                                       0.175949\n",
      "exploration/num steps total                           2200\n",
      "exploration/num paths total                            110\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.132709\n",
      "exploration/Rewards Std                                  0.0760235\n",
      "exploration/Rewards Max                                  0.0584705\n",
      "exploration/Rewards Min                                 -0.342048\n",
      "exploration/Returns Mean                                -2.65418\n",
      "exploration/Returns Std                                  0.632381\n",
      "exploration/Returns Max                                 -1.72654\n",
      "exploration/Returns Min                                 -3.47055\n",
      "exploration/Actions Mean                                -0.0175046\n",
      "exploration/Actions Std                                  0.10759\n",
      "exploration/Actions Max                                  0.427881\n",
      "exploration/Actions Min                                 -0.307822\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.65418\n",
      "exploration/env_infos/final/reward_dist Mean             0.000222043\n",
      "exploration/env_infos/final/reward_dist Std              0.000444086\n",
      "exploration/env_infos/final/reward_dist Max              0.00111021\n",
      "exploration/env_infos/final/reward_dist Min              1.63818e-25\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000620251\n",
      "exploration/env_infos/initial/reward_dist Std            0.00113197\n",
      "exploration/env_infos/initial/reward_dist Max            0.00288239\n",
      "exploration/env_infos/initial/reward_dist Min            4.90761e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0237388\n",
      "exploration/env_infos/reward_dist Std                    0.0778706\n",
      "exploration/env_infos/reward_dist Max                    0.50168\n",
      "exploration/env_infos/reward_dist Min                    1.63818e-25\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135957\n",
      "exploration/env_infos/final/reward_energy Std            0.0913599\n",
      "exploration/env_infos/final/reward_energy Max           -0.0586493\n",
      "exploration/env_infos/final/reward_energy Min           -0.313338\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.186909\n",
      "exploration/env_infos/initial/reward_energy Std          0.0374193\n",
      "exploration/env_infos/initial/reward_energy Max         -0.130316\n",
      "exploration/env_infos/initial/reward_energy Min         -0.24\n",
      "exploration/env_infos/reward_energy Mean                -0.122963\n",
      "exploration/env_infos/reward_energy Std                  0.0929749\n",
      "exploration/env_infos/reward_energy Max                 -0.00114141\n",
      "exploration/env_infos/reward_energy Min                 -0.454508\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.222504\n",
      "exploration/env_infos/final/end_effector_loc Std         0.351814\n",
      "exploration/env_infos/final/end_effector_loc Max         0.403262\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.674855\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00245695\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00627554\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00836874\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00934002\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0859449\n",
      "exploration/env_infos/end_effector_loc Std               0.208746\n",
      "exploration/env_infos/end_effector_loc Max               0.403262\n",
      "exploration/env_infos/end_effector_loc Min              -0.674855\n",
      "evaluation/num steps total                           12000\n",
      "evaluation/num paths total                             600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0925927\n",
      "evaluation/Rewards Std                                   0.0980895\n",
      "evaluation/Rewards Max                                   0.131017\n",
      "evaluation/Rewards Min                                  -0.778961\n",
      "evaluation/Returns Mean                                 -1.85185\n",
      "evaluation/Returns Std                                   1.12701\n",
      "evaluation/Returns Max                                   0.837028\n",
      "evaluation/Returns Min                                  -4.55695\n",
      "evaluation/Actions Mean                                 -0.0142128\n",
      "evaluation/Actions Std                                   0.0659355\n",
      "evaluation/Actions Max                                   0.778425\n",
      "evaluation/Actions Min                                  -0.866108\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.85185\n",
      "evaluation/env_infos/final/reward_dist Mean              0.00835731\n",
      "evaluation/env_infos/final/reward_dist Std               0.0457097\n",
      "evaluation/env_infos/final/reward_dist Max               0.326953\n",
      "evaluation/env_infos/final/reward_dist Min               6.31363e-67\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00552242\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111337\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0567216\n",
      "evaluation/env_infos/initial/reward_dist Min             1.00926e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0752299\n",
      "evaluation/env_infos/reward_dist Std                     0.166338\n",
      "evaluation/env_infos/reward_dist Max                     0.963448\n",
      "evaluation/env_infos/reward_dist Min                     6.31363e-67\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.060563\n",
      "evaluation/env_infos/final/reward_energy Std             0.0351525\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0119427\n",
      "evaluation/env_infos/final/reward_energy Min            -0.197346\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.177346\n",
      "evaluation/env_infos/initial/reward_energy Std           0.196418\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00582724\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.16451\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0647304\n",
      "evaluation/env_infos/reward_energy Std                   0.070064\n",
      "evaluation/env_infos/reward_energy Max                  -0.00111532\n",
      "evaluation/env_infos/reward_energy Min                  -1.16451\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.165976\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.366543\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.97527\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.893346\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0015304\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00923024\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0389212\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0433054\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0669858\n",
      "evaluation/env_infos/end_effector_loc Std                0.22251\n",
      "evaluation/env_infos/end_effector_loc Max                0.97527\n",
      "evaluation/env_infos/end_effector_loc Min               -0.913898\n",
      "time/data storing (s)                                    0.00607391\n",
      "time/evaluation sampling (s)                             0.98911\n",
      "time/exploration sampling (s)                            0.126382\n",
      "time/logging (s)                                         0.0201478\n",
      "time/saving (s)                                          0.0286661\n",
      "time/training (s)                                       46.7412\n",
      "time/epoch (s)                                          47.9116\n",
      "time/total (s)                                         543.789\n",
      "Epoch                                                   11\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:58:30.140414 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 12 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00250185\r\n",
      "trainer/QF2 Loss                                         0.00245036\r\n",
      "trainer/Policy Loss                                      3.53172\r\n",
      "trainer/Q1 Predictions Mean                             -1.63861\r\n",
      "trainer/Q1 Predictions Std                               0.978148\r\n",
      "trainer/Q1 Predictions Max                               0.018596\r\n",
      "trainer/Q1 Predictions Min                              -4.5449\r\n",
      "trainer/Q2 Predictions Mean                             -1.65775\r\n",
      "trainer/Q2 Predictions Std                               0.983787\r\n",
      "trainer/Q2 Predictions Max                               0.00572579\r\n",
      "trainer/Q2 Predictions Min                              -4.72638\r\n",
      "trainer/Q Targets Mean                                  -1.63617\r\n",
      "trainer/Q Targets Std                                    0.991383\r\n",
      "trainer/Q Targets Max                                    0.0232618\r\n",
      "trainer/Q Targets Min                                   -4.72126\r\n",
      "trainer/Log Pis Mean                                     1.91835\r\n",
      "trainer/Log Pis Std                                      1.22256\r\n",
      "trainer/Log Pis Max                                      4.3025\r\n",
      "trainer/Log Pis Min                                     -2.38101\r\n",
      "trainer/Policy mu Mean                                  -0.00719113\r\n",
      "trainer/Policy mu Std                                    0.432368\r\n",
      "trainer/Policy mu Max                                    2.44071\r\n",
      "trainer/Policy mu Min                                   -2.07361\r\n",
      "trainer/Policy log std Mean                             -2.18339\r\n",
      "trainer/Policy log std Std                               0.648827\r\n",
      "trainer/Policy log std Max                              -0.290477\r\n",
      "trainer/Policy log std Min                              -3.17749\r\n",
      "trainer/Alpha                                            0.0206643\r\n",
      "trainer/Alpha Loss                                      -0.316761\r\n",
      "exploration/num steps total                           2300\r\n",
      "exploration/num paths total                            115\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.122056\r\n",
      "exploration/Rewards Std                                  0.0509369\r\n",
      "exploration/Rewards Max                                 -0.0182913\r\n",
      "exploration/Rewards Min                                 -0.277372\r\n",
      "exploration/Returns Mean                                -2.44111\r\n",
      "exploration/Returns Std                                  0.597463\r\n",
      "exploration/Returns Max                                 -1.76044\r\n",
      "exploration/Returns Min                                 -3.31742\r\n",
      "exploration/Actions Mean                                -0.00590489\r\n",
      "exploration/Actions Std                                  0.0759155\r\n",
      "exploration/Actions Max                                  0.191585\r\n",
      "exploration/Actions Min                                 -0.227421\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.44111\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0616053\r\n",
      "exploration/env_infos/final/reward_dist Std              0.109231\r\n",
      "exploration/env_infos/final/reward_dist Max              0.279107\r\n",
      "exploration/env_infos/final/reward_dist Min              7.89324e-14\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00139932\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00240292\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00616464\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.86494e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0879937\r\n",
      "exploration/env_infos/reward_dist Std                    0.16292\r\n",
      "exploration/env_infos/reward_dist Max                    0.683931\r\n",
      "exploration/env_infos/reward_dist Min                    7.89324e-14\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0709055\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0198201\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0367124\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.0899354\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.134594\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0604049\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0515494\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.229082\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0950897\r\n",
      "exploration/env_infos/reward_energy Std                  0.0505372\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00785275\r\n",
      "exploration/env_infos/reward_energy Min                 -0.258165\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.028851\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.235563\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.286523\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.364299\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00190716\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0048547\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0085407\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00763239\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00127603\r\n",
      "exploration/env_infos/end_effector_loc Std               0.133346\r\n",
      "exploration/env_infos/end_effector_loc Max               0.286523\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.364299\r\n",
      "evaluation/num steps total                           13000\r\n",
      "evaluation/num paths total                             650\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0920834\r\n",
      "evaluation/Rewards Std                                   0.109941\r\n",
      "evaluation/Rewards Max                                   0.140065\r\n",
      "evaluation/Rewards Min                                  -0.856727\r\n",
      "evaluation/Returns Mean                                 -1.84167\r\n",
      "evaluation/Returns Std                                   1.45906\r\n",
      "evaluation/Returns Max                                   0.826059\r\n",
      "evaluation/Returns Min                                  -6.02818\r\n",
      "evaluation/Actions Mean                                 -0.0087239\r\n",
      "evaluation/Actions Std                                   0.0769694\r\n",
      "evaluation/Actions Max                                   0.987338\r\n",
      "evaluation/Actions Min                                  -0.558298\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.84167\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0692281\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.184134\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.951861\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.83709e-77\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.010355\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0178232\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0759857\r\n",
      "evaluation/env_infos/initial/reward_dist Min             8.01056e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0729793\r\n",
      "evaluation/env_infos/reward_dist Std                     0.169926\r\n",
      "evaluation/env_infos/reward_dist Max                     0.989288\r\n",
      "evaluation/env_infos/reward_dist Min                     2.94199e-77\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0830241\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0832029\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00773428\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.460519\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.200531\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.236028\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00774446\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.06212\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0696662\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0845423\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00276939\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.06212\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0601074\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.361944\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.877023\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000923082\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010911\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0493669\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0279149\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00911576\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.218\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.877023\r\n",
      "time/data storing (s)                                    0.00598071\r\n",
      "time/evaluation sampling (s)                             1.04782\r\n",
      "time/exploration sampling (s)                            0.132728\r\n",
      "time/logging (s)                                         0.0218943\r\n",
      "time/saving (s)                                          0.0273118\r\n",
      "time/training (s)                                       46.684\r\n",
      "time/epoch (s)                                          47.9197\r\n",
      "time/total (s)                                         591.892\r\n",
      "Epoch                                                   12\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:59:15.694623 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 13 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0012955\n",
      "trainer/QF2 Loss                                         0.00228723\n",
      "trainer/Policy Loss                                      3.61876\n",
      "trainer/Q1 Predictions Mean                             -1.7005\n",
      "trainer/Q1 Predictions Std                               1.01655\n",
      "trainer/Q1 Predictions Max                               0.14465\n",
      "trainer/Q1 Predictions Min                              -4.4941\n",
      "trainer/Q2 Predictions Mean                             -1.6974\n",
      "trainer/Q2 Predictions Std                               1.01252\n",
      "trainer/Q2 Predictions Max                               0.170289\n",
      "trainer/Q2 Predictions Min                              -4.45331\n",
      "trainer/Q Targets Mean                                  -1.68377\n",
      "trainer/Q Targets Std                                    1.01025\n",
      "trainer/Q Targets Max                                    0.144828\n",
      "trainer/Q Targets Min                                   -4.45422\n",
      "trainer/Log Pis Mean                                     1.94956\n",
      "trainer/Log Pis Std                                      1.2712\n",
      "trainer/Log Pis Max                                      4.27738\n",
      "trainer/Log Pis Min                                     -2.89218\n",
      "trainer/Policy mu Mean                                   0.0254888\n",
      "trainer/Policy mu Std                                    0.358754\n",
      "trainer/Policy mu Max                                    2.94945\n",
      "trainer/Policy mu Min                                   -1.92481\n",
      "trainer/Policy log std Mean                             -2.29035\n",
      "trainer/Policy log std Std                               0.570859\n",
      "trainer/Policy log std Max                              -0.158431\n",
      "trainer/Policy log std Min                              -3.17641\n",
      "trainer/Alpha                                            0.0216763\n",
      "trainer/Alpha Loss                                      -0.193248\n",
      "exploration/num steps total                           2400\n",
      "exploration/num paths total                            120\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.106439\n",
      "exploration/Rewards Std                                  0.0852174\n",
      "exploration/Rewards Max                                  0.0463165\n",
      "exploration/Rewards Min                                 -0.313761\n",
      "exploration/Returns Mean                                -2.12877\n",
      "exploration/Returns Std                                  1.3658\n",
      "exploration/Returns Max                                 -0.0863483\n",
      "exploration/Returns Min                                 -3.88847\n",
      "exploration/Actions Mean                                -0.0257116\n",
      "exploration/Actions Std                                  0.108557\n",
      "exploration/Actions Max                                  0.265853\n",
      "exploration/Actions Min                                 -0.344811\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.12877\n",
      "exploration/env_infos/final/reward_dist Mean             0.00109224\n",
      "exploration/env_infos/final/reward_dist Std              0.00218447\n",
      "exploration/env_infos/final/reward_dist Max              0.00546118\n",
      "exploration/env_infos/final/reward_dist Min              3.75861e-37\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000726822\n",
      "exploration/env_infos/initial/reward_dist Std            0.00136319\n",
      "exploration/env_infos/initial/reward_dist Max            0.00345033\n",
      "exploration/env_infos/initial/reward_dist Min            3.42271e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0387978\n",
      "exploration/env_infos/reward_dist Std                    0.0841243\n",
      "exploration/env_infos/reward_dist Max                    0.357431\n",
      "exploration/env_infos/reward_dist Min                    3.75861e-37\n",
      "exploration/env_infos/final/reward_energy Mean          -0.128203\n",
      "exploration/env_infos/final/reward_energy Std            0.0969465\n",
      "exploration/env_infos/final/reward_energy Max           -0.0127765\n",
      "exploration/env_infos/final/reward_energy Min           -0.271541\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.153828\n",
      "exploration/env_infos/initial/reward_energy Std          0.0605926\n",
      "exploration/env_infos/initial/reward_energy Max         -0.087363\n",
      "exploration/env_infos/initial/reward_energy Min         -0.262014\n",
      "exploration/env_infos/reward_energy Mean                -0.141965\n",
      "exploration/env_infos/reward_energy Std                  0.0688281\n",
      "exploration/env_infos/reward_energy Max                 -0.0127765\n",
      "exploration/env_infos/reward_energy Min                 -0.362064\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.174265\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275938\n",
      "exploration/env_infos/final/end_effector_loc Max         0.121613\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.850144\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00100584\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00575816\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00732828\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0100697\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0492843\n",
      "exploration/env_infos/end_effector_loc Std               0.160273\n",
      "exploration/env_infos/end_effector_loc Max               0.140062\n",
      "exploration/env_infos/end_effector_loc Min              -0.850144\n",
      "evaluation/num steps total                           14000\n",
      "evaluation/num paths total                             700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.075538\n",
      "evaluation/Rewards Std                                   0.085953\n",
      "evaluation/Rewards Max                                   0.119086\n",
      "evaluation/Rewards Min                                  -0.711455\n",
      "evaluation/Returns Mean                                 -1.51076\n",
      "evaluation/Returns Std                                   1.18248\n",
      "evaluation/Returns Max                                   1.21209\n",
      "evaluation/Returns Min                                  -4.98247\n",
      "evaluation/Actions Mean                                 -0.00611581\n",
      "evaluation/Actions Std                                   0.0489828\n",
      "evaluation/Actions Max                                   0.633947\n",
      "evaluation/Actions Min                                  -0.38649\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.51076\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0690326\n",
      "evaluation/env_infos/final/reward_dist Std               0.153619\n",
      "evaluation/env_infos/final/reward_dist Max               0.621887\n",
      "evaluation/env_infos/final/reward_dist Min               5.68517e-61\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00561521\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117103\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0611377\n",
      "evaluation/env_infos/initial/reward_dist Min             1.72567e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0739372\n",
      "evaluation/env_infos/reward_dist Std                     0.166291\n",
      "evaluation/env_infos/reward_dist Max                     0.996378\n",
      "evaluation/env_infos/reward_dist Min                     5.68517e-61\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0475937\n",
      "evaluation/env_infos/final/reward_energy Std             0.0376235\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0015326\n",
      "evaluation/env_infos/final/reward_energy Min            -0.197719\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.13271\n",
      "evaluation/env_infos/initial/reward_energy Std           0.13167\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0116795\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.704346\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0478596\n",
      "evaluation/env_infos/reward_energy Std                   0.0508222\n",
      "evaluation/env_infos/reward_energy Max                  -0.0015326\n",
      "evaluation/env_infos/reward_energy Min                  -0.704346\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.036182\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.302191\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.850356\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.747504\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00011247\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00660858\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0316973\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0193245\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00905184\n",
      "evaluation/env_infos/end_effector_loc Std                0.175257\n",
      "evaluation/env_infos/end_effector_loc Max                0.850356\n",
      "evaluation/env_infos/end_effector_loc Min               -0.76347\n",
      "time/data storing (s)                                    0.00597621\n",
      "time/evaluation sampling (s)                             1.05476\n",
      "time/exploration sampling (s)                            0.120857\n",
      "time/logging (s)                                         0.0206122\n",
      "time/saving (s)                                          0.0275598\n",
      "time/training (s)                                       44.1305\n",
      "time/epoch (s)                                          45.3603\n",
      "time/total (s)                                         637.444\n",
      "Epoch                                                   13\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:00:04.757501 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 14 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00181051\r\n",
      "trainer/QF2 Loss                                         0.00182463\r\n",
      "trainer/Policy Loss                                      3.22203\r\n",
      "trainer/Q1 Predictions Mean                             -1.41327\r\n",
      "trainer/Q1 Predictions Std                               0.929847\r\n",
      "trainer/Q1 Predictions Max                               0.392673\r\n",
      "trainer/Q1 Predictions Min                              -4.37261\r\n",
      "trainer/Q2 Predictions Mean                             -1.42522\r\n",
      "trainer/Q2 Predictions Std                               0.933988\r\n",
      "trainer/Q2 Predictions Max                               0.358654\r\n",
      "trainer/Q2 Predictions Min                              -4.38777\r\n",
      "trainer/Q Targets Mean                                  -1.43195\r\n",
      "trainer/Q Targets Std                                    0.928018\r\n",
      "trainer/Q Targets Max                                    0.38298\r\n",
      "trainer/Q Targets Min                                   -4.3368\r\n",
      "trainer/Log Pis Mean                                     1.81635\r\n",
      "trainer/Log Pis Std                                      1.17275\r\n",
      "trainer/Log Pis Max                                      4.80277\r\n",
      "trainer/Log Pis Min                                     -4.33275\r\n",
      "trainer/Policy mu Mean                                   0.0180556\r\n",
      "trainer/Policy mu Std                                    0.383494\r\n",
      "trainer/Policy mu Max                                    2.78075\r\n",
      "trainer/Policy mu Min                                   -2.87214\r\n",
      "trainer/Policy log std Mean                             -2.20339\r\n",
      "trainer/Policy log std Std                               0.561963\r\n",
      "trainer/Policy log std Max                               0.188872\r\n",
      "trainer/Policy log std Min                              -3.06758\r\n",
      "trainer/Alpha                                            0.0227824\r\n",
      "trainer/Alpha Loss                                      -0.69449\r\n",
      "exploration/num steps total                           2500\r\n",
      "exploration/num paths total                            125\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0757075\r\n",
      "exploration/Rewards Std                                  0.076615\r\n",
      "exploration/Rewards Max                                  0.0949481\r\n",
      "exploration/Rewards Min                                 -0.291941\r\n",
      "exploration/Returns Mean                                -1.51415\r\n",
      "exploration/Returns Std                                  1.00194\r\n",
      "exploration/Returns Max                                  0.267395\r\n",
      "exploration/Returns Min                                 -2.52804\r\n",
      "exploration/Actions Mean                                -0.0050435\r\n",
      "exploration/Actions Std                                  0.116531\r\n",
      "exploration/Actions Max                                  0.45589\r\n",
      "exploration/Actions Min                                 -0.51704\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.51415\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0937383\r\n",
      "exploration/env_infos/final/reward_dist Std              0.151445\r\n",
      "exploration/env_infos/final/reward_dist Max              0.394497\r\n",
      "exploration/env_infos/final/reward_dist Min              1.46855e-15\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000808648\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000899637\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0024144\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.49588e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.114047\r\n",
      "exploration/env_infos/reward_dist Std                    0.183324\r\n",
      "exploration/env_infos/reward_dist Max                    0.917515\r\n",
      "exploration/env_infos/reward_dist Min                    1.46855e-15\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.119847\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0494116\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0568998\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.169972\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.273707\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.227588\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0366449\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.689323\r\n",
      "exploration/env_infos/reward_energy Mean                -0.134522\r\n",
      "exploration/env_infos/reward_energy Std                  0.0954656\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00270826\r\n",
      "exploration/env_infos/reward_energy Min                 -0.689323\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.114727\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.304558\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.439828\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.539369\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00131355\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0125166\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0227945\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.025852\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0415503\r\n",
      "exploration/env_infos/end_effector_loc Std               0.187017\r\n",
      "exploration/env_infos/end_effector_loc Max               0.439828\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.539369\r\n",
      "evaluation/num steps total                           15000\r\n",
      "evaluation/num paths total                             750\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0595792\r\n",
      "evaluation/Rewards Std                                   0.086126\r\n",
      "evaluation/Rewards Max                                   0.146024\r\n",
      "evaluation/Rewards Min                                  -0.674654\r\n",
      "evaluation/Returns Mean                                 -1.19158\r\n",
      "evaluation/Returns Std                                   1.35183\r\n",
      "evaluation/Returns Max                                   1.80243\r\n",
      "evaluation/Returns Min                                  -5.7267\r\n",
      "evaluation/Actions Mean                                 -0.00577141\r\n",
      "evaluation/Actions Std                                   0.0614444\r\n",
      "evaluation/Actions Max                                   0.614999\r\n",
      "evaluation/Actions Min                                  -0.48992\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.19158\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.108049\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.193935\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.749327\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.23041e-53\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00463693\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00950946\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.037191\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.18329e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.134875\r\n",
      "evaluation/env_infos/reward_dist Std                     0.249295\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993754\r\n",
      "evaluation/env_infos/reward_dist Min                     4.23041e-53\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0648689\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0516182\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0104374\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.285619\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.180814\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.159509\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0111088\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.786285\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0589785\r\n",
      "evaluation/env_infos/reward_energy Std                   0.064335\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000889358\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.786285\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.024599\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.292994\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.81883\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.639178\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00194982\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00829874\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0307499\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.024496\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0269428\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.175939\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.81883\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.639178\r\n",
      "time/data storing (s)                                    0.00587525\r\n",
      "time/evaluation sampling (s)                             0.994243\r\n",
      "time/exploration sampling (s)                            0.130059\r\n",
      "time/logging (s)                                         0.0196424\r\n",
      "time/saving (s)                                          0.0276784\r\n",
      "time/training (s)                                       47.6923\r\n",
      "time/epoch (s)                                          48.8698\r\n",
      "time/total (s)                                         686.505\r\n",
      "Epoch                                                   14\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:00:51.452506 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 15 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00277326\n",
      "trainer/QF2 Loss                                         0.000970414\n",
      "trainer/Policy Loss                                      3.57136\n",
      "trainer/Q1 Predictions Mean                             -1.66031\n",
      "trainer/Q1 Predictions Std                               0.956826\n",
      "trainer/Q1 Predictions Max                               0.521594\n",
      "trainer/Q1 Predictions Min                              -4.53476\n",
      "trainer/Q2 Predictions Mean                             -1.64403\n",
      "trainer/Q2 Predictions Std                               0.956267\n",
      "trainer/Q2 Predictions Max                               0.566101\n",
      "trainer/Q2 Predictions Min                              -4.57008\n",
      "trainer/Q Targets Mean                                  -1.64067\n",
      "trainer/Q Targets Std                                    0.955151\n",
      "trainer/Q Targets Max                                    0.528125\n",
      "trainer/Q Targets Min                                   -4.59541\n",
      "trainer/Log Pis Mean                                     1.9558\n",
      "trainer/Log Pis Std                                      1.5056\n",
      "trainer/Log Pis Max                                      6.02999\n",
      "trainer/Log Pis Min                                     -3.13218\n",
      "trainer/Policy mu Mean                                  -0.00800923\n",
      "trainer/Policy mu Std                                    0.524707\n",
      "trainer/Policy mu Max                                    2.54125\n",
      "trainer/Policy mu Min                                   -2.21569\n",
      "trainer/Policy log std Mean                             -2.21315\n",
      "trainer/Policy log std Std                               0.681987\n",
      "trainer/Policy log std Max                              -0.383481\n",
      "trainer/Policy log std Min                              -3.29557\n",
      "trainer/Alpha                                            0.0209531\n",
      "trainer/Alpha Loss                                      -0.170863\n",
      "exploration/num steps total                           2600\n",
      "exploration/num paths total                            130\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.128907\n",
      "exploration/Rewards Std                                  0.100879\n",
      "exploration/Rewards Max                                  0.0614824\n",
      "exploration/Rewards Min                                 -0.518588\n",
      "exploration/Returns Mean                                -2.57815\n",
      "exploration/Returns Std                                  1.31873\n",
      "exploration/Returns Max                                 -1.53506\n",
      "exploration/Returns Min                                 -5.18085\n",
      "exploration/Actions Mean                                -0.0138985\n",
      "exploration/Actions Std                                  0.0993334\n",
      "exploration/Actions Max                                  0.190473\n",
      "exploration/Actions Min                                 -0.325474\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.57815\n",
      "exploration/env_infos/final/reward_dist Mean             0.0313652\n",
      "exploration/env_infos/final/reward_dist Std              0.0578837\n",
      "exploration/env_infos/final/reward_dist Max              0.146876\n",
      "exploration/env_infos/final/reward_dist Min              2.61663e-46\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00441858\n",
      "exploration/env_infos/initial/reward_dist Std            0.00572628\n",
      "exploration/env_infos/initial/reward_dist Max            0.014922\n",
      "exploration/env_infos/initial/reward_dist Min            7.50913e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0292307\n",
      "exploration/env_infos/reward_dist Std                    0.0389047\n",
      "exploration/env_infos/reward_dist Max                    0.146876\n",
      "exploration/env_infos/reward_dist Min                    2.61663e-46\n",
      "exploration/env_infos/final/reward_energy Mean          -0.134446\n",
      "exploration/env_infos/final/reward_energy Std            0.0963201\n",
      "exploration/env_infos/final/reward_energy Max           -0.047824\n",
      "exploration/env_infos/final/reward_energy Min           -0.308935\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.234741\n",
      "exploration/env_infos/initial/reward_energy Std          0.0736362\n",
      "exploration/env_infos/initial/reward_energy Max         -0.14076\n",
      "exploration/env_infos/initial/reward_energy Min         -0.327565\n",
      "exploration/env_infos/reward_energy Mean                -0.119621\n",
      "exploration/env_infos/reward_energy Std                  0.0762331\n",
      "exploration/env_infos/reward_energy Max                 -0.0098298\n",
      "exploration/env_infos/reward_energy Min                 -0.327565\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.293016\n",
      "exploration/env_infos/final/end_effector_loc Std         0.289738\n",
      "exploration/env_infos/final/end_effector_loc Max         0.0540421\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.766223\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00659816\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00566757\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00184742\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0162737\n",
      "exploration/env_infos/end_effector_loc Mean             -0.148447\n",
      "exploration/env_infos/end_effector_loc Std               0.189907\n",
      "exploration/env_infos/end_effector_loc Max               0.0540421\n",
      "exploration/env_infos/end_effector_loc Min              -0.766223\n",
      "evaluation/num steps total                           16000\n",
      "evaluation/num paths total                             800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0605825\n",
      "evaluation/Rewards Std                                   0.0755916\n",
      "evaluation/Rewards Max                                   0.125512\n",
      "evaluation/Rewards Min                                  -0.458727\n",
      "evaluation/Returns Mean                                 -1.21165\n",
      "evaluation/Returns Std                                   1.06017\n",
      "evaluation/Returns Max                                   0.548262\n",
      "evaluation/Returns Min                                  -4.05787\n",
      "evaluation/Actions Mean                                 -0.00963009\n",
      "evaluation/Actions Std                                   0.0577809\n",
      "evaluation/Actions Max                                   0.629255\n",
      "evaluation/Actions Min                                  -0.443908\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.21165\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0977195\n",
      "evaluation/env_infos/final/reward_dist Std               0.193678\n",
      "evaluation/env_infos/final/reward_dist Max               0.836427\n",
      "evaluation/env_infos/final/reward_dist Min               3.29192e-57\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00525647\n",
      "evaluation/env_infos/initial/reward_dist Std             0.010511\n",
      "evaluation/env_infos/initial/reward_dist Max             0.046556\n",
      "evaluation/env_infos/initial/reward_dist Min             9.47634e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.140093\n",
      "evaluation/env_infos/reward_dist Std                     0.236228\n",
      "evaluation/env_infos/reward_dist Max                     0.99714\n",
      "evaluation/env_infos/reward_dist Min                     3.29192e-57\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0484118\n",
      "evaluation/env_infos/final/reward_energy Std             0.0307158\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0031536\n",
      "evaluation/env_infos/final/reward_energy Min            -0.126922\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.190791\n",
      "evaluation/env_infos/initial/reward_energy Std           0.162611\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0187188\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.706021\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0543842\n",
      "evaluation/env_infos/reward_energy Std                   0.0624908\n",
      "evaluation/env_infos/reward_energy Max                  -0.00086461\n",
      "evaluation/env_infos/reward_energy Min                  -0.706021\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0889162\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.300137\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.568703\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.910919\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000696054\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00883572\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0314628\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0221954\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0336897\n",
      "evaluation/env_infos/end_effector_loc Std                0.181575\n",
      "evaluation/env_infos/end_effector_loc Max                0.568703\n",
      "evaluation/env_infos/end_effector_loc Min               -0.910919\n",
      "time/data storing (s)                                    0.00605415\n",
      "time/evaluation sampling (s)                             1.03343\n",
      "time/exploration sampling (s)                            0.129127\n",
      "time/logging (s)                                         0.0195848\n",
      "time/saving (s)                                          0.0289864\n",
      "time/training (s)                                       45.2538\n",
      "time/epoch (s)                                          46.471\n",
      "time/total (s)                                         733.2\n",
      "Epoch                                                   15\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:01:37.101412 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 16 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00123056\r\n",
      "trainer/QF2 Loss                                         0.00269707\r\n",
      "trainer/Policy Loss                                      3.52899\r\n",
      "trainer/Q1 Predictions Mean                             -1.49893\r\n",
      "trainer/Q1 Predictions Std                               0.865558\r\n",
      "trainer/Q1 Predictions Max                               0.475852\r\n",
      "trainer/Q1 Predictions Min                              -3.8082\r\n",
      "trainer/Q2 Predictions Mean                             -1.50323\r\n",
      "trainer/Q2 Predictions Std                               0.863113\r\n",
      "trainer/Q2 Predictions Max                               0.44836\r\n",
      "trainer/Q2 Predictions Min                              -3.785\r\n",
      "trainer/Q Targets Mean                                  -1.5\r\n",
      "trainer/Q Targets Std                                    0.865332\r\n",
      "trainer/Q Targets Max                                    0.51668\r\n",
      "trainer/Q Targets Min                                   -3.78273\r\n",
      "trainer/Log Pis Mean                                     2.04227\r\n",
      "trainer/Log Pis Std                                      1.29832\r\n",
      "trainer/Log Pis Max                                      5.18898\r\n",
      "trainer/Log Pis Min                                     -3.19504\r\n",
      "trainer/Policy mu Mean                                   0.00604618\r\n",
      "trainer/Policy mu Std                                    0.347756\r\n",
      "trainer/Policy mu Max                                    2.11813\r\n",
      "trainer/Policy mu Min                                   -2.34388\r\n",
      "trainer/Policy log std Mean                             -2.34092\r\n",
      "trainer/Policy log std Std                               0.566149\r\n",
      "trainer/Policy log std Max                              -0.312728\r\n",
      "trainer/Policy log std Min                              -3.1446\r\n",
      "trainer/Alpha                                            0.0211978\r\n",
      "trainer/Alpha Loss                                       0.16291\r\n",
      "exploration/num steps total                           2700\r\n",
      "exploration/num paths total                            135\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.083206\r\n",
      "exploration/Rewards Std                                  0.0545607\r\n",
      "exploration/Rewards Max                                  0.0277943\r\n",
      "exploration/Rewards Min                                 -0.259958\r\n",
      "exploration/Returns Mean                                -1.66412\r\n",
      "exploration/Returns Std                                  0.586489\r\n",
      "exploration/Returns Max                                 -0.961497\r\n",
      "exploration/Returns Min                                 -2.39466\r\n",
      "exploration/Actions Mean                                -0.00835734\r\n",
      "exploration/Actions Std                                  0.0995884\r\n",
      "exploration/Actions Max                                  0.265459\r\n",
      "exploration/Actions Min                                 -0.324954\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.66412\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0267341\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0431758\r\n",
      "exploration/env_infos/final/reward_dist Max              0.11253\r\n",
      "exploration/env_infos/final/reward_dist Min              1.01719e-10\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00978989\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0131884\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0335263\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.2084e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.152842\r\n",
      "exploration/env_infos/reward_dist Std                    0.260857\r\n",
      "exploration/env_infos/reward_dist Max                    0.9982\r\n",
      "exploration/env_infos/reward_dist Min                    1.01719e-10\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.121068\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0679377\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0564694\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.237966\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.155952\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0650332\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0538064\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.22722\r\n",
      "exploration/env_infos/reward_energy Mean                -0.121617\r\n",
      "exploration/env_infos/reward_energy Std                  0.072004\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0104296\r\n",
      "exploration/env_infos/reward_energy Min                 -0.357868\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00739384\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.26803\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.389754\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.544665\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00192915\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00565389\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0110147\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0070626\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0245908\r\n",
      "exploration/env_infos/end_effector_loc Std               0.154516\r\n",
      "exploration/env_infos/end_effector_loc Max               0.404808\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.544665\r\n",
      "evaluation/num steps total                           17000\r\n",
      "evaluation/num paths total                             850\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.064096\r\n",
      "evaluation/Rewards Std                                   0.0837111\r\n",
      "evaluation/Rewards Max                                   0.131871\r\n",
      "evaluation/Rewards Min                                  -0.526999\r\n",
      "evaluation/Returns Mean                                 -1.28192\r\n",
      "evaluation/Returns Std                                   1.23631\r\n",
      "evaluation/Returns Max                                   1.46437\r\n",
      "evaluation/Returns Min                                  -5.21812\r\n",
      "evaluation/Actions Mean                                 -0.00530405\r\n",
      "evaluation/Actions Std                                   0.0517836\r\n",
      "evaluation/Actions Max                                   0.526352\r\n",
      "evaluation/Actions Min                                  -0.540149\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.28192\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.111104\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.196236\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.876803\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.86751e-38\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00378181\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00618214\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0268315\r\n",
      "evaluation/env_infos/initial/reward_dist Min             9.96477e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.133917\r\n",
      "evaluation/env_infos/reward_dist Std                     0.232853\r\n",
      "evaluation/env_infos/reward_dist Max                     0.992322\r\n",
      "evaluation/env_infos/reward_dist Min                     6.86751e-38\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0418091\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0276087\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000492808\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.12755\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.157054\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.14499\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.010563\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.754193\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0463584\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0571861\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000349595\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.754193\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0532069\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.297945\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.643744\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.729811\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000857952\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00750825\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0263176\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0270074\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0226866\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.181689\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.643744\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.729811\r\n",
      "time/data storing (s)                                    0.00607866\r\n",
      "time/evaluation sampling (s)                             1.07669\r\n",
      "time/exploration sampling (s)                            0.122511\r\n",
      "time/logging (s)                                         0.0188583\r\n",
      "time/saving (s)                                          0.0268283\r\n",
      "time/training (s)                                       44.1625\r\n",
      "time/epoch (s)                                          45.4134\r\n",
      "time/total (s)                                         778.846\r\n",
      "Epoch                                                   16\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:02:23.731561 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 17 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00118488\n",
      "trainer/QF2 Loss                                         0.0019496\n",
      "trainer/Policy Loss                                      3.48351\n",
      "trainer/Q1 Predictions Mean                             -1.53569\n",
      "trainer/Q1 Predictions Std                               0.897245\n",
      "trainer/Q1 Predictions Max                               0.619683\n",
      "trainer/Q1 Predictions Min                              -3.59456\n",
      "trainer/Q2 Predictions Mean                             -1.53977\n",
      "trainer/Q2 Predictions Std                               0.89658\n",
      "trainer/Q2 Predictions Max                               0.665325\n",
      "trainer/Q2 Predictions Min                              -3.6545\n",
      "trainer/Q Targets Mean                                  -1.54036\n",
      "trainer/Q Targets Std                                    0.898592\n",
      "trainer/Q Targets Max                                    0.576275\n",
      "trainer/Q Targets Min                                   -3.64088\n",
      "trainer/Log Pis Mean                                     1.95723\n",
      "trainer/Log Pis Std                                      1.33026\n",
      "trainer/Log Pis Max                                      5.38398\n",
      "trainer/Log Pis Min                                     -2.69093\n",
      "trainer/Policy mu Mean                                   0.0248645\n",
      "trainer/Policy mu Std                                    0.321117\n",
      "trainer/Policy mu Max                                    2.2345\n",
      "trainer/Policy mu Min                                   -2.18881\n",
      "trainer/Policy log std Mean                             -2.29733\n",
      "trainer/Policy log std Std                               0.542157\n",
      "trainer/Policy log std Max                              -0.11855\n",
      "trainer/Policy log std Min                              -3.06765\n",
      "trainer/Alpha                                            0.0223933\n",
      "trainer/Alpha Loss                                      -0.162482\n",
      "exploration/num steps total                           2800\n",
      "exploration/num paths total                            140\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.119368\n",
      "exploration/Rewards Std                                  0.0999341\n",
      "exploration/Rewards Max                                  0.0493868\n",
      "exploration/Rewards Min                                 -0.499875\n",
      "exploration/Returns Mean                                -2.38735\n",
      "exploration/Returns Std                                  1.60355\n",
      "exploration/Returns Max                                 -0.343054\n",
      "exploration/Returns Min                                 -5.08693\n",
      "exploration/Actions Mean                                 0.00176743\n",
      "exploration/Actions Std                                  0.103133\n",
      "exploration/Actions Max                                  0.423771\n",
      "exploration/Actions Min                                 -0.319901\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.38735\n",
      "exploration/env_infos/final/reward_dist Mean             0.00393639\n",
      "exploration/env_infos/final/reward_dist Std              0.00520675\n",
      "exploration/env_infos/final/reward_dist Max              0.0131264\n",
      "exploration/env_infos/final/reward_dist Min              6.09496e-26\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00644419\n",
      "exploration/env_infos/initial/reward_dist Std            0.0117976\n",
      "exploration/env_infos/initial/reward_dist Max            0.0300057\n",
      "exploration/env_infos/initial/reward_dist Min            1.30042e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.042669\n",
      "exploration/env_infos/reward_dist Std                    0.114188\n",
      "exploration/env_infos/reward_dist Max                    0.652825\n",
      "exploration/env_infos/reward_dist Min                    6.09496e-26\n",
      "exploration/env_infos/final/reward_energy Mean          -0.18793\n",
      "exploration/env_infos/final/reward_energy Std            0.0706174\n",
      "exploration/env_infos/final/reward_energy Max           -0.0738408\n",
      "exploration/env_infos/final/reward_energy Min           -0.296284\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.231646\n",
      "exploration/env_infos/initial/reward_energy Std          0.145642\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0657487\n",
      "exploration/env_infos/initial/reward_energy Min         -0.460326\n",
      "exploration/env_infos/reward_energy Mean                -0.122982\n",
      "exploration/env_infos/reward_energy Std                  0.0784511\n",
      "exploration/env_infos/reward_energy Max                 -0.0104719\n",
      "exploration/env_infos/reward_energy Min                 -0.460326\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0559756\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275753\n",
      "exploration/env_infos/final/end_effector_loc Max         0.587253\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.345684\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000746047\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00964534\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0211886\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.015995\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0395083\n",
      "exploration/env_infos/end_effector_loc Std               0.173875\n",
      "exploration/env_infos/end_effector_loc Max               0.587253\n",
      "exploration/env_infos/end_effector_loc Min              -0.345684\n",
      "evaluation/num steps total                           18000\n",
      "evaluation/num paths total                             900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0807408\n",
      "evaluation/Rewards Std                                   0.0630596\n",
      "evaluation/Rewards Max                                   0.108094\n",
      "evaluation/Rewards Min                                  -0.391963\n",
      "evaluation/Returns Mean                                 -1.61482\n",
      "evaluation/Returns Std                                   0.764863\n",
      "evaluation/Returns Max                                  -0.329045\n",
      "evaluation/Returns Min                                  -3.37887\n",
      "evaluation/Actions Mean                                 -0.00619995\n",
      "evaluation/Actions Std                                   0.0597743\n",
      "evaluation/Actions Max                                   0.538442\n",
      "evaluation/Actions Min                                  -0.42138\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.61482\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0814475\n",
      "evaluation/env_infos/final/reward_dist Std               0.217888\n",
      "evaluation/env_infos/final/reward_dist Max               0.978652\n",
      "evaluation/env_infos/final/reward_dist Min               4.68286e-27\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00763451\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0173048\n",
      "evaluation/env_infos/initial/reward_dist Max             0.102757\n",
      "evaluation/env_infos/initial/reward_dist Min             1.1386e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0784803\n",
      "evaluation/env_infos/reward_dist Std                     0.182557\n",
      "evaluation/env_infos/reward_dist Max                     0.991178\n",
      "evaluation/env_infos/reward_dist Min                     4.68286e-27\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0466677\n",
      "evaluation/env_infos/final/reward_energy Std             0.0378934\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00638429\n",
      "evaluation/env_infos/final/reward_energy Min            -0.191335\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.179533\n",
      "evaluation/env_infos/initial/reward_energy Std           0.158326\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0216958\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.633417\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0573903\n",
      "evaluation/env_infos/reward_energy Std                   0.062683\n",
      "evaluation/env_infos/reward_energy Max                  -0.00101918\n",
      "evaluation/env_infos/reward_energy Min                  -0.633417\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.118682\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.288291\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.456481\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.734072\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00119378\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00837848\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0269221\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.021069\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0555048\n",
      "evaluation/env_infos/end_effector_loc Std                0.183945\n",
      "evaluation/env_infos/end_effector_loc Max                0.456481\n",
      "evaluation/env_infos/end_effector_loc Min               -0.734072\n",
      "time/data storing (s)                                    0.00590469\n",
      "time/evaluation sampling (s)                             0.924502\n",
      "time/exploration sampling (s)                            0.119434\n",
      "time/logging (s)                                         0.0208496\n",
      "time/saving (s)                                          0.0268899\n",
      "time/training (s)                                       45.3059\n",
      "time/epoch (s)                                          46.4035\n",
      "time/total (s)                                         825.478\n",
      "Epoch                                                   17\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:03:09.597262 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 18 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00150498\n",
      "trainer/QF2 Loss                                         0.00122892\n",
      "trainer/Policy Loss                                      3.252\n",
      "trainer/Q1 Predictions Mean                             -1.35116\n",
      "trainer/Q1 Predictions Std                               0.809809\n",
      "trainer/Q1 Predictions Max                               0.502\n",
      "trainer/Q1 Predictions Min                              -3.42509\n",
      "trainer/Q2 Predictions Mean                             -1.34401\n",
      "trainer/Q2 Predictions Std                               0.812181\n",
      "trainer/Q2 Predictions Max                               0.507038\n",
      "trainer/Q2 Predictions Min                              -3.47069\n",
      "trainer/Q Targets Mean                                  -1.33686\n",
      "trainer/Q Targets Std                                    0.817907\n",
      "trainer/Q Targets Max                                    0.525133\n",
      "trainer/Q Targets Min                                   -3.48436\n",
      "trainer/Log Pis Mean                                     1.91974\n",
      "trainer/Log Pis Std                                      1.38306\n",
      "trainer/Log Pis Max                                      6.56416\n",
      "trainer/Log Pis Min                                     -3.09956\n",
      "trainer/Policy mu Mean                                   0.0322128\n",
      "trainer/Policy mu Std                                    0.33968\n",
      "trainer/Policy mu Max                                    2.35408\n",
      "trainer/Policy mu Min                                   -1.59029\n",
      "trainer/Policy log std Mean                             -2.28701\n",
      "trainer/Policy log std Std                               0.630383\n",
      "trainer/Policy log std Max                              -0.386389\n",
      "trainer/Policy log std Min                              -3.33265\n",
      "trainer/Alpha                                            0.0230253\n",
      "trainer/Alpha Loss                                      -0.302664\n",
      "exploration/num steps total                           2900\n",
      "exploration/num paths total                            145\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.159361\n",
      "exploration/Rewards Std                                  0.0903307\n",
      "exploration/Rewards Max                                 -0.0441467\n",
      "exploration/Rewards Min                                 -0.496406\n",
      "exploration/Returns Mean                                -3.18722\n",
      "exploration/Returns Std                                  0.944072\n",
      "exploration/Returns Max                                 -2.24459\n",
      "exploration/Returns Min                                 -4.8319\n",
      "exploration/Actions Mean                                -0.00371153\n",
      "exploration/Actions Std                                  0.117313\n",
      "exploration/Actions Max                                  0.847764\n",
      "exploration/Actions Min                                 -0.447572\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.18722\n",
      "exploration/env_infos/final/reward_dist Mean             0.000215734\n",
      "exploration/env_infos/final/reward_dist Std              0.000420409\n",
      "exploration/env_infos/final/reward_dist Max              0.00105637\n",
      "exploration/env_infos/final/reward_dist Min              1.5976e-36\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000315199\n",
      "exploration/env_infos/initial/reward_dist Std            0.000548964\n",
      "exploration/env_infos/initial/reward_dist Max            0.00141025\n",
      "exploration/env_infos/initial/reward_dist Min            1.15692e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0146625\n",
      "exploration/env_infos/reward_dist Std                    0.0428016\n",
      "exploration/env_infos/reward_dist Max                    0.209224\n",
      "exploration/env_infos/reward_dist Min                    1.5976e-36\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135078\n",
      "exploration/env_infos/final/reward_energy Std            0.0397578\n",
      "exploration/env_infos/final/reward_energy Max           -0.0878669\n",
      "exploration/env_infos/final/reward_energy Min           -0.196617\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.378071\n",
      "exploration/env_infos/initial/reward_energy Std          0.315776\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0110384\n",
      "exploration/env_infos/initial/reward_energy Min         -0.958658\n",
      "exploration/env_infos/reward_energy Mean                -0.120089\n",
      "exploration/env_infos/reward_energy Std                  0.11459\n",
      "exploration/env_infos/reward_energy Max                 -0.00556208\n",
      "exploration/env_infos/reward_energy Min                 -0.958658\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.019996\n",
      "exploration/env_infos/final/end_effector_loc Std         0.428426\n",
      "exploration/env_infos/final/end_effector_loc Max         0.664213\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.656201\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00216609\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0172807\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0423882\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0223786\n",
      "exploration/env_infos/end_effector_loc Mean              0.00204464\n",
      "exploration/env_infos/end_effector_loc Std               0.275093\n",
      "exploration/env_infos/end_effector_loc Max               0.664213\n",
      "exploration/env_infos/end_effector_loc Min              -0.656201\n",
      "evaluation/num steps total                           19000\n",
      "evaluation/num paths total                             950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0534568\n",
      "evaluation/Rewards Std                                   0.0706701\n",
      "evaluation/Rewards Max                                   0.154256\n",
      "evaluation/Rewards Min                                  -0.426258\n",
      "evaluation/Returns Mean                                 -1.06914\n",
      "evaluation/Returns Std                                   1.15358\n",
      "evaluation/Returns Max                                   1.62931\n",
      "evaluation/Returns Min                                  -3.45487\n",
      "evaluation/Actions Mean                                  0.00042405\n",
      "evaluation/Actions Std                                   0.0523938\n",
      "evaluation/Actions Max                                   0.63152\n",
      "evaluation/Actions Min                                  -0.36422\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.06914\n",
      "evaluation/env_infos/final/reward_dist Mean              0.122617\n",
      "evaluation/env_infos/final/reward_dist Std               0.228504\n",
      "evaluation/env_infos/final/reward_dist Max               0.852651\n",
      "evaluation/env_infos/final/reward_dist Min               1.75537e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00580764\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00999649\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0410897\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97762e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.132602\n",
      "evaluation/env_infos/reward_dist Std                     0.23402\n",
      "evaluation/env_infos/reward_dist Max                     0.998177\n",
      "evaluation/env_infos/reward_dist Min                     1.75537e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0452249\n",
      "evaluation/env_infos/final/reward_energy Std             0.0387573\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00347885\n",
      "evaluation/env_infos/final/reward_energy Min            -0.18251\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.167477\n",
      "evaluation/env_infos/initial/reward_energy Std           0.157236\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0281445\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.729022\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0489295\n",
      "evaluation/env_infos/reward_energy Std                   0.055646\n",
      "evaluation/env_infos/reward_energy Max                  -0.000942319\n",
      "evaluation/env_infos/reward_energy Min                  -0.729022\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0685221\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.21689\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.526428\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.442613\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00252572\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00771915\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.031576\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.018211\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0411444\n",
      "evaluation/env_infos/end_effector_loc Std                0.139712\n",
      "evaluation/env_infos/end_effector_loc Max                0.526428\n",
      "evaluation/env_infos/end_effector_loc Min               -0.442613\n",
      "time/data storing (s)                                    0.00636472\n",
      "time/evaluation sampling (s)                             0.954192\n",
      "time/exploration sampling (s)                            0.118211\n",
      "time/logging (s)                                         0.0192126\n",
      "time/saving (s)                                          0.0286687\n",
      "time/training (s)                                       44.4866\n",
      "time/epoch (s)                                          45.6132\n",
      "time/total (s)                                         871.341\n",
      "Epoch                                                   18\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:03:56.969130 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 19 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00161783\n",
      "trainer/QF2 Loss                                         0.00198163\n",
      "trainer/Policy Loss                                      3.43035\n",
      "trainer/Q1 Predictions Mean                             -1.50707\n",
      "trainer/Q1 Predictions Std                               0.85379\n",
      "trainer/Q1 Predictions Max                               0.466398\n",
      "trainer/Q1 Predictions Min                              -3.43156\n",
      "trainer/Q2 Predictions Mean                             -1.50864\n",
      "trainer/Q2 Predictions Std                               0.86707\n",
      "trainer/Q2 Predictions Max                               0.488732\n",
      "trainer/Q2 Predictions Min                              -3.53777\n",
      "trainer/Q Targets Mean                                  -1.5082\n",
      "trainer/Q Targets Std                                    0.855981\n",
      "trainer/Q Targets Max                                    0.4941\n",
      "trainer/Q Targets Min                                   -3.43997\n",
      "trainer/Log Pis Mean                                     1.93666\n",
      "trainer/Log Pis Std                                      1.30728\n",
      "trainer/Log Pis Max                                      3.97185\n",
      "trainer/Log Pis Min                                     -3.26538\n",
      "trainer/Policy mu Mean                                   0.00831211\n",
      "trainer/Policy mu Std                                    0.32811\n",
      "trainer/Policy mu Max                                    2.15442\n",
      "trainer/Policy mu Min                                   -1.8996\n",
      "trainer/Policy log std Mean                             -2.30068\n",
      "trainer/Policy log std Std                               0.536401\n",
      "trainer/Policy log std Max                              -0.54953\n",
      "trainer/Policy log std Min                              -3.0854\n",
      "trainer/Alpha                                            0.0236532\n",
      "trainer/Alpha Loss                                      -0.237114\n",
      "exploration/num steps total                           3000\n",
      "exploration/num paths total                            150\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0827982\n",
      "exploration/Rewards Std                                  0.0719379\n",
      "exploration/Rewards Max                                  0.0954964\n",
      "exploration/Rewards Min                                 -0.254946\n",
      "exploration/Returns Mean                                -1.65596\n",
      "exploration/Returns Std                                  0.833508\n",
      "exploration/Returns Max                                 -0.221282\n",
      "exploration/Returns Min                                 -2.67581\n",
      "exploration/Actions Mean                                 0.0106815\n",
      "exploration/Actions Std                                  0.0962861\n",
      "exploration/Actions Max                                  0.301675\n",
      "exploration/Actions Min                                 -0.282484\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.65596\n",
      "exploration/env_infos/final/reward_dist Mean             0.155098\n",
      "exploration/env_infos/final/reward_dist Std              0.240052\n",
      "exploration/env_infos/final/reward_dist Max              0.627571\n",
      "exploration/env_infos/final/reward_dist Min              3.61895e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00817356\n",
      "exploration/env_infos/initial/reward_dist Std            0.0156863\n",
      "exploration/env_infos/initial/reward_dist Max            0.0395312\n",
      "exploration/env_infos/initial/reward_dist Min            1.57767e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.180189\n",
      "exploration/env_infos/reward_dist Std                    0.277318\n",
      "exploration/env_infos/reward_dist Max                    0.977447\n",
      "exploration/env_infos/reward_dist Min                    3.61895e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.178237\n",
      "exploration/env_infos/final/reward_energy Std            0.0815347\n",
      "exploration/env_infos/final/reward_energy Max           -0.100573\n",
      "exploration/env_infos/final/reward_energy Min           -0.33541\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.136559\n",
      "exploration/env_infos/initial/reward_energy Std          0.0874879\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0442782\n",
      "exploration/env_infos/initial/reward_energy Min         -0.302058\n",
      "exploration/env_infos/reward_energy Mean                -0.117514\n",
      "exploration/env_infos/reward_energy Std                  0.0704314\n",
      "exploration/env_infos/reward_energy Max                 -0.00418087\n",
      "exploration/env_infos/reward_energy Min                 -0.355774\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0558282\n",
      "exploration/env_infos/final/end_effector_loc Std         0.256535\n",
      "exploration/env_infos/final/end_effector_loc Max         0.399526\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.327523\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -3.11039e-05\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00573385\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00902734\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.012108\n",
      "exploration/env_infos/end_effector_loc Mean              0.0169535\n",
      "exploration/env_infos/end_effector_loc Std               0.145655\n",
      "exploration/env_infos/end_effector_loc Max               0.399526\n",
      "exploration/env_infos/end_effector_loc Min              -0.327523\n",
      "evaluation/num steps total                           20000\n",
      "evaluation/num paths total                            1000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0613251\n",
      "evaluation/Rewards Std                                   0.0616856\n",
      "evaluation/Rewards Max                                   0.11689\n",
      "evaluation/Rewards Min                                  -0.258682\n",
      "evaluation/Returns Mean                                 -1.2265\n",
      "evaluation/Returns Std                                   0.995149\n",
      "evaluation/Returns Max                                   0.585285\n",
      "evaluation/Returns Min                                  -3.50533\n",
      "evaluation/Actions Mean                                 -0.00382638\n",
      "evaluation/Actions Std                                   0.0503679\n",
      "evaluation/Actions Max                                   0.563096\n",
      "evaluation/Actions Min                                  -0.353367\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.2265\n",
      "evaluation/env_infos/final/reward_dist Mean              0.230017\n",
      "evaluation/env_infos/final/reward_dist Std               0.319426\n",
      "evaluation/env_infos/final/reward_dist Max               0.939632\n",
      "evaluation/env_infos/final/reward_dist Min               6.36538e-31\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0044869\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00836858\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0343153\n",
      "evaluation/env_infos/initial/reward_dist Min             1.04295e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.141433\n",
      "evaluation/env_infos/reward_dist Std                     0.24514\n",
      "evaluation/env_infos/reward_dist Max                     0.998445\n",
      "evaluation/env_infos/reward_dist Min                     6.36538e-31\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0395917\n",
      "evaluation/env_infos/final/reward_energy Std             0.0365556\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00439756\n",
      "evaluation/env_infos/final/reward_energy Min            -0.147981\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.141413\n",
      "evaluation/env_infos/initial/reward_energy Std           0.131664\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00640207\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.640777\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0451783\n",
      "evaluation/env_infos/reward_energy Std                   0.0553359\n",
      "evaluation/env_infos/reward_energy Max                  -0.000592003\n",
      "evaluation/env_infos/reward_energy Min                  -0.640777\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0438336\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.218363\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.298367\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.611402\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000443829\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00681684\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0281548\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0176684\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0183627\n",
      "evaluation/env_infos/end_effector_loc Std                0.140574\n",
      "evaluation/env_infos/end_effector_loc Max                0.336411\n",
      "evaluation/env_infos/end_effector_loc Min               -0.611402\n",
      "time/data storing (s)                                    0.00631855\n",
      "time/evaluation sampling (s)                             0.954425\n",
      "time/exploration sampling (s)                            0.124909\n",
      "time/logging (s)                                         0.0196136\n",
      "time/saving (s)                                          0.028889\n",
      "time/training (s)                                       45.9768\n",
      "time/epoch (s)                                          47.1109\n",
      "time/total (s)                                         918.713\n",
      "Epoch                                                   19\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:04:43.832151 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 20 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000897544\n",
      "trainer/QF2 Loss                                         0.000980844\n",
      "trainer/Policy Loss                                      3.44877\n",
      "trainer/Q1 Predictions Mean                             -1.46989\n",
      "trainer/Q1 Predictions Std                               0.897256\n",
      "trainer/Q1 Predictions Max                               0.56655\n",
      "trainer/Q1 Predictions Min                              -3.8649\n",
      "trainer/Q2 Predictions Mean                             -1.46535\n",
      "trainer/Q2 Predictions Std                               0.89333\n",
      "trainer/Q2 Predictions Max                               0.584369\n",
      "trainer/Q2 Predictions Min                              -3.82116\n",
      "trainer/Q Targets Mean                                  -1.46843\n",
      "trainer/Q Targets Std                                    0.895015\n",
      "trainer/Q Targets Max                                    0.588035\n",
      "trainer/Q Targets Min                                   -3.84961\n",
      "trainer/Log Pis Mean                                     2.00088\n",
      "trainer/Log Pis Std                                      1.31252\n",
      "trainer/Log Pis Max                                      3.96472\n",
      "trainer/Log Pis Min                                     -3.13089\n",
      "trainer/Policy mu Mean                                   0.0396763\n",
      "trainer/Policy mu Std                                    0.364597\n",
      "trainer/Policy mu Max                                    2.68924\n",
      "trainer/Policy mu Min                                   -2.23711\n",
      "trainer/Policy log std Mean                             -2.3173\n",
      "trainer/Policy log std Std                               0.577136\n",
      "trainer/Policy log std Max                              -0.0299917\n",
      "trainer/Policy log std Min                              -3.11487\n",
      "trainer/Alpha                                            0.0245698\n",
      "trainer/Alpha Loss                                       0.00324391\n",
      "exploration/num steps total                           3100\n",
      "exploration/num paths total                            155\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0772978\n",
      "exploration/Rewards Std                                  0.0536013\n",
      "exploration/Rewards Max                                  0.0448934\n",
      "exploration/Rewards Min                                 -0.244306\n",
      "exploration/Returns Mean                                -1.54596\n",
      "exploration/Returns Std                                  0.68575\n",
      "exploration/Returns Max                                 -0.333234\n",
      "exploration/Returns Min                                 -2.44429\n",
      "exploration/Actions Mean                                -0.00697372\n",
      "exploration/Actions Std                                  0.0990313\n",
      "exploration/Actions Max                                  0.52794\n",
      "exploration/Actions Min                                 -0.325076\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.54596\n",
      "exploration/env_infos/final/reward_dist Mean             0.320859\n",
      "exploration/env_infos/final/reward_dist Std              0.28432\n",
      "exploration/env_infos/final/reward_dist Max              0.737238\n",
      "exploration/env_infos/final/reward_dist Min              1.61569e-29\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00464842\n",
      "exploration/env_infos/initial/reward_dist Std            0.00670226\n",
      "exploration/env_infos/initial/reward_dist Max            0.0175268\n",
      "exploration/env_infos/initial/reward_dist Min            1.11756e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.177415\n",
      "exploration/env_infos/reward_dist Std                    0.264702\n",
      "exploration/env_infos/reward_dist Max                    0.872118\n",
      "exploration/env_infos/reward_dist Min                    1.61569e-29\n",
      "exploration/env_infos/final/reward_energy Mean          -0.137386\n",
      "exploration/env_infos/final/reward_energy Std            0.0592887\n",
      "exploration/env_infos/final/reward_energy Max           -0.0558374\n",
      "exploration/env_infos/final/reward_energy Min           -0.227024\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.134332\n",
      "exploration/env_infos/initial/reward_energy Std          0.0672137\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0521952\n",
      "exploration/env_infos/initial/reward_energy Min         -0.234762\n",
      "exploration/env_infos/reward_energy Mean                -0.106843\n",
      "exploration/env_infos/reward_energy Std                  0.0910836\n",
      "exploration/env_infos/reward_energy Max                 -0.0138754\n",
      "exploration/env_infos/reward_energy Min                 -0.619996\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.106667\n",
      "exploration/env_infos/final/end_effector_loc Std         0.265129\n",
      "exploration/env_infos/final/end_effector_loc Max         0.155942\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.695171\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000240089\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00530525\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0114582\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00930831\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0404992\n",
      "exploration/env_infos/end_effector_loc Std               0.160123\n",
      "exploration/env_infos/end_effector_loc Max               0.239981\n",
      "exploration/env_infos/end_effector_loc Min              -0.695171\n",
      "evaluation/num steps total                           21000\n",
      "evaluation/num paths total                            1050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.066468\n",
      "evaluation/Rewards Std                                   0.0890923\n",
      "evaluation/Rewards Max                                   0.11175\n",
      "evaluation/Rewards Min                                  -0.582351\n",
      "evaluation/Returns Mean                                 -1.32936\n",
      "evaluation/Returns Std                                   1.44848\n",
      "evaluation/Returns Max                                   1.08794\n",
      "evaluation/Returns Min                                  -7.18042\n",
      "evaluation/Actions Mean                                 -0.00508389\n",
      "evaluation/Actions Std                                   0.0617578\n",
      "evaluation/Actions Max                                   0.435373\n",
      "evaluation/Actions Min                                  -0.313501\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.32936\n",
      "evaluation/env_infos/final/reward_dist Mean              0.145784\n",
      "evaluation/env_infos/final/reward_dist Std               0.271277\n",
      "evaluation/env_infos/final/reward_dist Max               0.984055\n",
      "evaluation/env_infos/final/reward_dist Min               1.95128e-42\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00433709\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00701358\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0346539\n",
      "evaluation/env_infos/initial/reward_dist Min             4.32e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.125927\n",
      "evaluation/env_infos/reward_dist Std                     0.233009\n",
      "evaluation/env_infos/reward_dist Max                     0.996826\n",
      "evaluation/env_infos/reward_dist Min                     1.95128e-42\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.043692\n",
      "evaluation/env_infos/final/reward_energy Std             0.0455449\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00163116\n",
      "evaluation/env_infos/final/reward_energy Min            -0.296735\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.191216\n",
      "evaluation/env_infos/initial/reward_energy Std           0.123893\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0103387\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.471108\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0598879\n",
      "evaluation/env_infos/reward_energy Std                   0.063978\n",
      "evaluation/env_infos/reward_energy Max                  -0.000404139\n",
      "evaluation/env_infos/reward_energy Min                  -0.471108\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0620879\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27902\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.640575\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.678454\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       6.88251e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00805522\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0217686\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.015675\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0199377\n",
      "evaluation/env_infos/end_effector_loc Std                0.182279\n",
      "evaluation/env_infos/end_effector_loc Max                0.65785\n",
      "evaluation/env_infos/end_effector_loc Min               -0.678454\n",
      "time/data storing (s)                                    0.00587552\n",
      "time/evaluation sampling (s)                             0.952333\n",
      "time/exploration sampling (s)                            0.121694\n",
      "time/logging (s)                                         0.0184168\n",
      "time/saving (s)                                          0.0261869\n",
      "time/training (s)                                       45.4632\n",
      "time/epoch (s)                                          46.5877\n",
      "time/total (s)                                         965.573\n",
      "Epoch                                                   20\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:05:30.476833 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 21 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00204049\r\n",
      "trainer/QF2 Loss                                         0.00309024\r\n",
      "trainer/Policy Loss                                      3.70651\r\n",
      "trainer/Q1 Predictions Mean                             -1.56299\r\n",
      "trainer/Q1 Predictions Std                               0.882018\r\n",
      "trainer/Q1 Predictions Max                               0.116234\r\n",
      "trainer/Q1 Predictions Min                              -3.96084\r\n",
      "trainer/Q2 Predictions Mean                             -1.56997\r\n",
      "trainer/Q2 Predictions Std                               0.882042\r\n",
      "trainer/Q2 Predictions Max                               0.127523\r\n",
      "trainer/Q2 Predictions Min                              -3.9961\r\n",
      "trainer/Q Targets Mean                                  -1.57284\r\n",
      "trainer/Q Targets Std                                    0.876173\r\n",
      "trainer/Q Targets Max                                    0.098966\r\n",
      "trainer/Q Targets Min                                   -3.93142\r\n",
      "trainer/Log Pis Mean                                     2.16102\r\n",
      "trainer/Log Pis Std                                      1.37808\r\n",
      "trainer/Log Pis Max                                      4.37404\r\n",
      "trainer/Log Pis Min                                     -3.51814\r\n",
      "trainer/Policy mu Mean                                   0.0526172\r\n",
      "trainer/Policy mu Std                                    0.370597\r\n",
      "trainer/Policy mu Max                                    2.2973\r\n",
      "trainer/Policy mu Min                                   -1.49954\r\n",
      "trainer/Policy log std Mean                             -2.39325\r\n",
      "trainer/Policy log std Std                               0.537453\r\n",
      "trainer/Policy log std Max                              -0.309504\r\n",
      "trainer/Policy log std Min                              -3.15483\r\n",
      "trainer/Alpha                                            0.0259946\r\n",
      "trainer/Alpha Loss                                       0.587833\r\n",
      "exploration/num steps total                           3200\r\n",
      "exploration/num paths total                            160\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.101063\r\n",
      "exploration/Rewards Std                                  0.0296336\r\n",
      "exploration/Rewards Max                                 -0.0129706\r\n",
      "exploration/Rewards Min                                 -0.187626\r\n",
      "exploration/Returns Mean                                -2.02126\r\n",
      "exploration/Returns Std                                  0.260683\r\n",
      "exploration/Returns Max                                 -1.56061\r\n",
      "exploration/Returns Min                                 -2.33853\r\n",
      "exploration/Actions Mean                                 0.00739796\r\n",
      "exploration/Actions Std                                  0.200874\r\n",
      "exploration/Actions Max                                  0.79552\r\n",
      "exploration/Actions Min                                 -0.644464\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.02126\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.222546\r\n",
      "exploration/env_infos/final/reward_dist Std              0.388021\r\n",
      "exploration/env_infos/final/reward_dist Max              0.993357\r\n",
      "exploration/env_infos/final/reward_dist Min              1.07727e-13\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00885846\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0166148\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0420765\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.17901e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.11493\r\n",
      "exploration/env_infos/reward_dist Std                    0.217072\r\n",
      "exploration/env_infos/reward_dist Max                    0.993357\r\n",
      "exploration/env_infos/reward_dist Min                    1.07727e-13\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135329\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0602911\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0534978\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.211877\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.404996\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.148783\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.239747\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.675015\r\n",
      "exploration/env_infos/reward_energy Mean                -0.23373\r\n",
      "exploration/env_infos/reward_energy Std                  0.161805\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00463098\r\n",
      "exploration/env_infos/reward_energy Min                 -0.859861\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0251057\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.284794\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.452657\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.462867\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00176203\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0151523\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0174387\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0322232\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0120921\r\n",
      "exploration/env_infos/end_effector_loc Std               0.186404\r\n",
      "exploration/env_infos/end_effector_loc Max               0.452657\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.462867\r\n",
      "evaluation/num steps total                           22000\r\n",
      "evaluation/num paths total                            1100\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0513766\r\n",
      "evaluation/Rewards Std                                   0.0727486\r\n",
      "evaluation/Rewards Max                                   0.168788\r\n",
      "evaluation/Rewards Min                                  -0.353831\r\n",
      "evaluation/Returns Mean                                 -1.02753\r\n",
      "evaluation/Returns Std                                   1.15543\r\n",
      "evaluation/Returns Max                                   0.879219\r\n",
      "evaluation/Returns Min                                  -5.1487\r\n",
      "evaluation/Actions Mean                                 -0.00238988\r\n",
      "evaluation/Actions Std                                   0.0524615\r\n",
      "evaluation/Actions Max                                   0.32518\r\n",
      "evaluation/Actions Min                                  -0.387393\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.02753\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.126988\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.223045\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.997458\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.96092e-19\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00413803\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00746633\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0308571\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.53338e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.112473\r\n",
      "evaluation/env_infos/reward_dist Std                     0.203163\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997458\r\n",
      "evaluation/env_infos/reward_dist Min                     2.96092e-19\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0475785\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0358115\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00442165\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.155142\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.1448\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.106006\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0204471\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.436365\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0529265\r\n",
      "evaluation/env_infos/reward_energy Std                   0.052102\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000448174\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.436365\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0142311\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.246159\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.575271\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.553795\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000157106\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00634276\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.016259\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0193696\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.000510152\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.156082\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.575271\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.553795\r\n",
      "time/data storing (s)                                    0.00614338\r\n",
      "time/evaluation sampling (s)                             0.962496\r\n",
      "time/exploration sampling (s)                            0.128054\r\n",
      "time/logging (s)                                         0.0194969\r\n",
      "time/saving (s)                                          0.0280033\r\n",
      "time/training (s)                                       45.2289\r\n",
      "time/epoch (s)                                          46.3731\r\n",
      "time/total (s)                                        1012.22\r\n",
      "Epoch                                                   21\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:06:18.179320 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 22 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00296181\n",
      "trainer/QF2 Loss                                         0.00213245\n",
      "trainer/Policy Loss                                      3.5267\n",
      "trainer/Q1 Predictions Mean                             -1.55285\n",
      "trainer/Q1 Predictions Std                               0.879326\n",
      "trainer/Q1 Predictions Max                               0.140949\n",
      "trainer/Q1 Predictions Min                              -3.6417\n",
      "trainer/Q2 Predictions Mean                             -1.55092\n",
      "trainer/Q2 Predictions Std                               0.879609\n",
      "trainer/Q2 Predictions Max                               0.148866\n",
      "trainer/Q2 Predictions Min                              -3.58209\n",
      "trainer/Q Targets Mean                                  -1.5581\n",
      "trainer/Q Targets Std                                    0.8879\n",
      "trainer/Q Targets Max                                    0.142767\n",
      "trainer/Q Targets Min                                   -3.75147\n",
      "trainer/Log Pis Mean                                     2.00271\n",
      "trainer/Log Pis Std                                      1.49338\n",
      "trainer/Log Pis Max                                      4.7554\n",
      "trainer/Log Pis Min                                     -3.5922\n",
      "trainer/Policy mu Mean                                   0.0691371\n",
      "trainer/Policy mu Std                                    0.335435\n",
      "trainer/Policy mu Max                                    2.40052\n",
      "trainer/Policy mu Min                                   -1.77037\n",
      "trainer/Policy log std Mean                             -2.32596\n",
      "trainer/Policy log std Std                               0.60153\n",
      "trainer/Policy log std Max                              -0.549178\n",
      "trainer/Policy log std Min                              -3.26435\n",
      "trainer/Alpha                                            0.0232098\n",
      "trainer/Alpha Loss                                       0.0101936\n",
      "exploration/num steps total                           3300\n",
      "exploration/num paths total                            165\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.084211\n",
      "exploration/Rewards Std                                  0.116139\n",
      "exploration/Rewards Max                                  0.107273\n",
      "exploration/Rewards Min                                 -0.329728\n",
      "exploration/Returns Mean                                -1.68422\n",
      "exploration/Returns Std                                  1.95324\n",
      "exploration/Returns Max                                  0.725302\n",
      "exploration/Returns Min                                 -4.31059\n",
      "exploration/Actions Mean                                 0.00504356\n",
      "exploration/Actions Std                                  0.132859\n",
      "exploration/Actions Max                                  0.487748\n",
      "exploration/Actions Min                                 -0.347111\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.68422\n",
      "exploration/env_infos/final/reward_dist Mean             0.165412\n",
      "exploration/env_infos/final/reward_dist Std              0.274803\n",
      "exploration/env_infos/final/reward_dist Max              0.707391\n",
      "exploration/env_infos/final/reward_dist Min              5.05068e-24\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00144733\n",
      "exploration/env_infos/initial/reward_dist Std            0.00213209\n",
      "exploration/env_infos/initial/reward_dist Max            0.00565559\n",
      "exploration/env_infos/initial/reward_dist Min            5.65212e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.119468\n",
      "exploration/env_infos/reward_dist Std                    0.229019\n",
      "exploration/env_infos/reward_dist Max                    0.896303\n",
      "exploration/env_infos/reward_dist Min                    5.05068e-24\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0983899\n",
      "exploration/env_infos/final/reward_energy Std            0.0494263\n",
      "exploration/env_infos/final/reward_energy Max           -0.0139971\n",
      "exploration/env_infos/final/reward_energy Min           -0.163216\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.230602\n",
      "exploration/env_infos/initial/reward_energy Std          0.117693\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0428981\n",
      "exploration/env_infos/initial/reward_energy Min         -0.374965\n",
      "exploration/env_infos/reward_energy Mean                -0.154031\n",
      "exploration/env_infos/reward_energy Std                  0.107835\n",
      "exploration/env_infos/reward_energy Max                 -0.00580927\n",
      "exploration/env_infos/reward_energy Min                 -0.5297\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0599175\n",
      "exploration/env_infos/final/end_effector_loc Std         0.2728\n",
      "exploration/env_infos/final/end_effector_loc Max         0.642195\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.371126\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000474052\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0091412\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0135315\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0149138\n",
      "exploration/env_infos/end_effector_loc Mean              0.0115807\n",
      "exploration/env_infos/end_effector_loc Std               0.171824\n",
      "exploration/env_infos/end_effector_loc Max               0.642195\n",
      "exploration/env_infos/end_effector_loc Min              -0.371126\n",
      "evaluation/num steps total                           23000\n",
      "evaluation/num paths total                            1150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0589971\n",
      "evaluation/Rewards Std                                   0.0662042\n",
      "evaluation/Rewards Max                                   0.173691\n",
      "evaluation/Rewards Min                                  -0.285179\n",
      "evaluation/Returns Mean                                 -1.17994\n",
      "evaluation/Returns Std                                   0.99045\n",
      "evaluation/Returns Max                                   0.789956\n",
      "evaluation/Returns Min                                  -3.27785\n",
      "evaluation/Actions Mean                                 -0.000106548\n",
      "evaluation/Actions Std                                   0.054774\n",
      "evaluation/Actions Max                                   0.539577\n",
      "evaluation/Actions Min                                  -0.36557\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.17994\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0639512\n",
      "evaluation/env_infos/final/reward_dist Std               0.142989\n",
      "evaluation/env_infos/final/reward_dist Max               0.735814\n",
      "evaluation/env_infos/final/reward_dist Min               1.2434e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00571345\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0102646\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0463813\n",
      "evaluation/env_infos/initial/reward_dist Min             1.38884e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0990963\n",
      "evaluation/env_infos/reward_dist Std                     0.201439\n",
      "evaluation/env_infos/reward_dist Max                     0.99025\n",
      "evaluation/env_infos/reward_dist Min                     1.2434e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0610445\n",
      "evaluation/env_infos/final/reward_energy Std             0.057435\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00347439\n",
      "evaluation/env_infos/final/reward_energy Min            -0.317079\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.142158\n",
      "evaluation/env_infos/initial/reward_energy Std           0.113287\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0099414\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.551317\n",
      "evaluation/env_infos/reward_energy Mean                 -0.055062\n",
      "evaluation/env_infos/reward_energy Std                   0.0544847\n",
      "evaluation/env_infos/reward_energy Max                  -0.00106816\n",
      "evaluation/env_infos/reward_energy Min                  -0.551317\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0205294\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.242857\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.45867\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.508343\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000655706\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00639325\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0269789\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0170125\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0143327\n",
      "evaluation/env_infos/end_effector_loc Std                0.142112\n",
      "evaluation/env_infos/end_effector_loc Max                0.45867\n",
      "evaluation/env_infos/end_effector_loc Min               -0.508343\n",
      "time/data storing (s)                                    0.006015\n",
      "time/evaluation sampling (s)                             0.926159\n",
      "time/exploration sampling (s)                            0.124119\n",
      "time/logging (s)                                         0.0199704\n",
      "time/saving (s)                                          0.0306809\n",
      "time/training (s)                                       46.2871\n",
      "time/epoch (s)                                          47.3941\n",
      "time/total (s)                                        1059.92\n",
      "Epoch                                                   22\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:07:08.529700 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 23 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00115135\n",
      "trainer/QF2 Loss                                         0.0027805\n",
      "trainer/Policy Loss                                      3.42762\n",
      "trainer/Q1 Predictions Mean                             -1.56506\n",
      "trainer/Q1 Predictions Std                               0.904277\n",
      "trainer/Q1 Predictions Max                               0.147458\n",
      "trainer/Q1 Predictions Min                              -3.66927\n",
      "trainer/Q2 Predictions Mean                             -1.55986\n",
      "trainer/Q2 Predictions Std                               0.905703\n",
      "trainer/Q2 Predictions Max                               0.147768\n",
      "trainer/Q2 Predictions Min                              -3.70043\n",
      "trainer/Q Targets Mean                                  -1.57557\n",
      "trainer/Q Targets Std                                    0.910153\n",
      "trainer/Q Targets Max                                    0.135251\n",
      "trainer/Q Targets Min                                   -3.68229\n",
      "trainer/Log Pis Mean                                     1.89491\n",
      "trainer/Log Pis Std                                      1.40336\n",
      "trainer/Log Pis Max                                      4.30214\n",
      "trainer/Log Pis Min                                     -6.89177\n",
      "trainer/Policy mu Mean                                   0.061298\n",
      "trainer/Policy mu Std                                    0.357396\n",
      "trainer/Policy mu Max                                    2.08198\n",
      "trainer/Policy mu Min                                   -1.87045\n",
      "trainer/Policy log std Mean                             -2.27027\n",
      "trainer/Policy log std Std                               0.631064\n",
      "trainer/Policy log std Max                              -0.071803\n",
      "trainer/Policy log std Min                              -3.29324\n",
      "trainer/Alpha                                            0.0213593\n",
      "trainer/Alpha Loss                                      -0.404197\n",
      "exploration/num steps total                           3400\n",
      "exploration/num paths total                            170\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0865426\n",
      "exploration/Rewards Std                                  0.114005\n",
      "exploration/Rewards Max                                  0.130129\n",
      "exploration/Rewards Min                                 -0.270445\n",
      "exploration/Returns Mean                                -1.73085\n",
      "exploration/Returns Std                                  1.98324\n",
      "exploration/Returns Max                                  0.731914\n",
      "exploration/Returns Min                                 -3.8163\n",
      "exploration/Actions Mean                                 0.00158828\n",
      "exploration/Actions Std                                  0.190727\n",
      "exploration/Actions Max                                  0.468749\n",
      "exploration/Actions Min                                 -0.615168\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.73085\n",
      "exploration/env_infos/final/reward_dist Mean             0.0373579\n",
      "exploration/env_infos/final/reward_dist Std              0.0318289\n",
      "exploration/env_infos/final/reward_dist Max              0.0805723\n",
      "exploration/env_infos/final/reward_dist Min              4.80388e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00840013\n",
      "exploration/env_infos/initial/reward_dist Std            0.0109063\n",
      "exploration/env_infos/initial/reward_dist Max            0.0268406\n",
      "exploration/env_infos/initial/reward_dist Min            2.83938e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.181077\n",
      "exploration/env_infos/reward_dist Std                    0.304643\n",
      "exploration/env_infos/reward_dist Max                    0.977205\n",
      "exploration/env_infos/reward_dist Min                    4.80388e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.218348\n",
      "exploration/env_infos/final/reward_energy Std            0.101468\n",
      "exploration/env_infos/final/reward_energy Max           -0.128902\n",
      "exploration/env_infos/final/reward_energy Min           -0.359777\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.341066\n",
      "exploration/env_infos/initial/reward_energy Std          0.0936779\n",
      "exploration/env_infos/initial/reward_energy Max         -0.212355\n",
      "exploration/env_infos/initial/reward_energy Min         -0.491588\n",
      "exploration/env_infos/reward_energy Mean                -0.222736\n",
      "exploration/env_infos/reward_energy Std                  0.152144\n",
      "exploration/env_infos/reward_energy Max                 -0.0126115\n",
      "exploration/env_infos/reward_energy Min                 -0.651009\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.064119\n",
      "exploration/env_infos/final/end_effector_loc Std         0.210376\n",
      "exploration/env_infos/final/end_effector_loc Max         0.337231\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.334866\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0092938\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00836674\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0234374\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0036429\n",
      "exploration/env_infos/end_effector_loc Mean              0.0702746\n",
      "exploration/env_infos/end_effector_loc Std               0.137809\n",
      "exploration/env_infos/end_effector_loc Max               0.337231\n",
      "exploration/env_infos/end_effector_loc Min              -0.336428\n",
      "evaluation/num steps total                           24000\n",
      "evaluation/num paths total                            1200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0528956\n",
      "evaluation/Rewards Std                                   0.0645184\n",
      "evaluation/Rewards Max                                   0.119325\n",
      "evaluation/Rewards Min                                  -0.327885\n",
      "evaluation/Returns Mean                                 -1.05791\n",
      "evaluation/Returns Std                                   0.938441\n",
      "evaluation/Returns Max                                   1.1236\n",
      "evaluation/Returns Min                                  -3.73534\n",
      "evaluation/Actions Mean                                 -0.00101301\n",
      "evaluation/Actions Std                                   0.055129\n",
      "evaluation/Actions Max                                   0.613384\n",
      "evaluation/Actions Min                                  -0.277254\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.05791\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0997395\n",
      "evaluation/env_infos/final/reward_dist Std               0.154481\n",
      "evaluation/env_infos/final/reward_dist Max               0.672443\n",
      "evaluation/env_infos/final/reward_dist Min               8.88186e-26\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00748517\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0102415\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0361432\n",
      "evaluation/env_infos/initial/reward_dist Min             1.72429e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.149298\n",
      "evaluation/env_infos/reward_dist Std                     0.23323\n",
      "evaluation/env_infos/reward_dist Max                     0.996402\n",
      "evaluation/env_infos/reward_dist Min                     8.88186e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0421956\n",
      "evaluation/env_infos/final/reward_energy Std             0.0451053\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00263871\n",
      "evaluation/env_infos/final/reward_energy Min            -0.236867\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.161849\n",
      "evaluation/env_infos/initial/reward_energy Std           0.165332\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0211589\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.771381\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0493234\n",
      "evaluation/env_infos/reward_energy Std                   0.060396\n",
      "evaluation/env_infos/reward_energy Max                  -0.000504645\n",
      "evaluation/env_infos/reward_energy Min                  -0.771381\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00811838\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.243367\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.39268\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.79027\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00217912\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00788439\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0306692\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0138627\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0151024\n",
      "evaluation/env_infos/end_effector_loc Std                0.147807\n",
      "evaluation/env_infos/end_effector_loc Max                0.39268\n",
      "evaluation/env_infos/end_effector_loc Min               -0.79027\n",
      "time/data storing (s)                                    0.0062339\n",
      "time/evaluation sampling (s)                             1.03216\n",
      "time/exploration sampling (s)                            0.121074\n",
      "time/logging (s)                                         0.0212914\n",
      "time/saving (s)                                          0.0270581\n",
      "time/training (s)                                       48.8167\n",
      "time/epoch (s)                                          50.0245\n",
      "time/total (s)                                        1110.27\n",
      "Epoch                                                   23\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:08:06.932084 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 24 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00101966\n",
      "trainer/QF2 Loss                                         0.000907167\n",
      "trainer/Policy Loss                                      3.36958\n",
      "trainer/Q1 Predictions Mean                             -1.3827\n",
      "trainer/Q1 Predictions Std                               0.84008\n",
      "trainer/Q1 Predictions Max                               0.0436106\n",
      "trainer/Q1 Predictions Min                              -3.58513\n",
      "trainer/Q2 Predictions Mean                             -1.39387\n",
      "trainer/Q2 Predictions Std                               0.85541\n",
      "trainer/Q2 Predictions Max                               0.0455982\n",
      "trainer/Q2 Predictions Min                              -3.61427\n",
      "trainer/Q Targets Mean                                  -1.38985\n",
      "trainer/Q Targets Std                                    0.85565\n",
      "trainer/Q Targets Max                                    0.0314345\n",
      "trainer/Q Targets Min                                   -3.65508\n",
      "trainer/Log Pis Mean                                     2.00658\n",
      "trainer/Log Pis Std                                      1.45038\n",
      "trainer/Log Pis Max                                      4.21204\n",
      "trainer/Log Pis Min                                     -4.38625\n",
      "trainer/Policy mu Mean                                  -0.0152967\n",
      "trainer/Policy mu Std                                    0.340893\n",
      "trainer/Policy mu Max                                    2.16236\n",
      "trainer/Policy mu Min                                   -2.18235\n",
      "trainer/Policy log std Mean                             -2.31033\n",
      "trainer/Policy log std Std                               0.617321\n",
      "trainer/Policy log std Max                              -0.547212\n",
      "trainer/Policy log std Min                              -3.20876\n",
      "trainer/Alpha                                            0.0222501\n",
      "trainer/Alpha Loss                                       0.0250247\n",
      "exploration/num steps total                           3500\n",
      "exploration/num paths total                            175\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.120915\n",
      "exploration/Rewards Std                                  0.0562137\n",
      "exploration/Rewards Max                                  0.0115819\n",
      "exploration/Rewards Min                                 -0.286442\n",
      "exploration/Returns Mean                                -2.4183\n",
      "exploration/Returns Std                                  0.917463\n",
      "exploration/Returns Max                                 -0.800855\n",
      "exploration/Returns Min                                 -3.60569\n",
      "exploration/Actions Mean                                 0.00015411\n",
      "exploration/Actions Std                                  0.0850425\n",
      "exploration/Actions Max                                  0.261787\n",
      "exploration/Actions Min                                 -0.381439\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.4183\n",
      "exploration/env_infos/final/reward_dist Mean             0.0607495\n",
      "exploration/env_infos/final/reward_dist Std              0.0590069\n",
      "exploration/env_infos/final/reward_dist Max              0.159004\n",
      "exploration/env_infos/final/reward_dist Min              3.63961e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000188128\n",
      "exploration/env_infos/initial/reward_dist Std            0.00033667\n",
      "exploration/env_infos/initial/reward_dist Max            0.000860876\n",
      "exploration/env_infos/initial/reward_dist Min            3.55106e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.030254\n",
      "exploration/env_infos/reward_dist Std                    0.0719408\n",
      "exploration/env_infos/reward_dist Max                    0.302173\n",
      "exploration/env_infos/reward_dist Min                    3.63961e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.124939\n",
      "exploration/env_infos/final/reward_energy Std            0.0502728\n",
      "exploration/env_infos/final/reward_energy Max           -0.046158\n",
      "exploration/env_infos/final/reward_energy Min           -0.171943\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0706115\n",
      "exploration/env_infos/initial/reward_energy Std          0.0364744\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0329767\n",
      "exploration/env_infos/initial/reward_energy Min         -0.128875\n",
      "exploration/env_infos/reward_energy Mean                -0.0968713\n",
      "exploration/env_infos/reward_energy Std                  0.0712772\n",
      "exploration/env_infos/reward_energy Max                 -0.00396097\n",
      "exploration/env_infos/reward_energy Min                 -0.39946\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0671781\n",
      "exploration/env_infos/final/end_effector_loc Std         0.130636\n",
      "exploration/env_infos/final/end_effector_loc Max         0.258669\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.141609\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00053758\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00275798\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00398865\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00631387\n",
      "exploration/env_infos/end_effector_loc Mean              0.0315001\n",
      "exploration/env_infos/end_effector_loc Std               0.0852649\n",
      "exploration/env_infos/end_effector_loc Max               0.258669\n",
      "exploration/env_infos/end_effector_loc Min              -0.141609\n",
      "evaluation/num steps total                           25000\n",
      "evaluation/num paths total                            1250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0477699\n",
      "evaluation/Rewards Std                                   0.0680663\n",
      "evaluation/Rewards Max                                   0.130567\n",
      "evaluation/Rewards Min                                  -0.248411\n",
      "evaluation/Returns Mean                                 -0.955399\n",
      "evaluation/Returns Std                                   1.09529\n",
      "evaluation/Returns Max                                   1.09752\n",
      "evaluation/Returns Min                                  -3.33708\n",
      "evaluation/Actions Mean                                 -0.00306304\n",
      "evaluation/Actions Std                                   0.0476677\n",
      "evaluation/Actions Max                                   0.347415\n",
      "evaluation/Actions Min                                  -0.29204\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.955399\n",
      "evaluation/env_infos/final/reward_dist Mean              0.18299\n",
      "evaluation/env_infos/final/reward_dist Std               0.258904\n",
      "evaluation/env_infos/final/reward_dist Max               0.8522\n",
      "evaluation/env_infos/final/reward_dist Min               3.80296e-15\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00456721\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00805797\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0348532\n",
      "evaluation/env_infos/initial/reward_dist Min             6.3253e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.162598\n",
      "evaluation/env_infos/reward_dist Std                     0.236136\n",
      "evaluation/env_infos/reward_dist Max                     0.999303\n",
      "evaluation/env_infos/reward_dist Min                     3.80296e-15\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0454345\n",
      "evaluation/env_infos/final/reward_energy Std             0.040406\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00272688\n",
      "evaluation/env_infos/final/reward_energy Min            -0.186397\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.129924\n",
      "evaluation/env_infos/initial/reward_energy Std           0.107884\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00508582\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.422296\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0464437\n",
      "evaluation/env_infos/reward_energy Std                   0.0490527\n",
      "evaluation/env_infos/reward_energy Max                  -0.00191862\n",
      "evaluation/env_infos/reward_energy Min                  -0.422296\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0347013\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.216086\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.421477\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.635161\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000238013\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00596591\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0173708\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.014602\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0131424\n",
      "evaluation/env_infos/end_effector_loc Std                0.133081\n",
      "evaluation/env_infos/end_effector_loc Max                0.421477\n",
      "evaluation/env_infos/end_effector_loc Min               -0.635161\n",
      "time/data storing (s)                                    0.00590094\n",
      "time/evaluation sampling (s)                             0.970361\n",
      "time/exploration sampling (s)                            0.131786\n",
      "time/logging (s)                                         0.0234659\n",
      "time/saving (s)                                          0.0295916\n",
      "time/training (s)                                       56.8348\n",
      "time/epoch (s)                                          57.9959\n",
      "time/total (s)                                        1168.68\n",
      "Epoch                                                   24\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:09:06.016520 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 25 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00158631\n",
      "trainer/QF2 Loss                                         0.00267337\n",
      "trainer/Policy Loss                                      3.51543\n",
      "trainer/Q1 Predictions Mean                             -1.61229\n",
      "trainer/Q1 Predictions Std                               0.877838\n",
      "trainer/Q1 Predictions Max                               0.222648\n",
      "trainer/Q1 Predictions Min                              -3.49534\n",
      "trainer/Q2 Predictions Mean                             -1.61337\n",
      "trainer/Q2 Predictions Std                               0.878885\n",
      "trainer/Q2 Predictions Max                               0.219926\n",
      "trainer/Q2 Predictions Min                              -3.47018\n",
      "trainer/Q Targets Mean                                  -1.61637\n",
      "trainer/Q Targets Std                                    0.885359\n",
      "trainer/Q Targets Max                                    0.218807\n",
      "trainer/Q Targets Min                                   -3.40441\n",
      "trainer/Log Pis Mean                                     1.92901\n",
      "trainer/Log Pis Std                                      1.25093\n",
      "trainer/Log Pis Max                                      4.26659\n",
      "trainer/Log Pis Min                                     -2.92524\n",
      "trainer/Policy mu Mean                                  -0.00625818\n",
      "trainer/Policy mu Std                                    0.329848\n",
      "trainer/Policy mu Max                                    1.73531\n",
      "trainer/Policy mu Min                                   -2.22944\n",
      "trainer/Policy log std Mean                             -2.27618\n",
      "trainer/Policy log std Std                               0.556591\n",
      "trainer/Policy log std Max                              -0.521209\n",
      "trainer/Policy log std Min                              -3.18397\n",
      "trainer/Alpha                                            0.0217296\n",
      "trainer/Alpha Loss                                      -0.271796\n",
      "exploration/num steps total                           3600\n",
      "exploration/num paths total                            180\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0477943\n",
      "exploration/Rewards Std                                  0.0723519\n",
      "exploration/Rewards Max                                  0.0845044\n",
      "exploration/Rewards Min                                 -0.21998\n",
      "exploration/Returns Mean                                -0.955886\n",
      "exploration/Returns Std                                  0.550725\n",
      "exploration/Returns Max                                 -0.394406\n",
      "exploration/Returns Min                                 -1.89091\n",
      "exploration/Actions Mean                                -0.00515199\n",
      "exploration/Actions Std                                  0.118295\n",
      "exploration/Actions Max                                  0.315103\n",
      "exploration/Actions Min                                 -0.292975\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.955886\n",
      "exploration/env_infos/final/reward_dist Mean             0.0998606\n",
      "exploration/env_infos/final/reward_dist Std              0.127565\n",
      "exploration/env_infos/final/reward_dist Max              0.345667\n",
      "exploration/env_infos/final/reward_dist Min              1.16603e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000283855\n",
      "exploration/env_infos/initial/reward_dist Std            0.000333428\n",
      "exploration/env_infos/initial/reward_dist Max            0.000930873\n",
      "exploration/env_infos/initial/reward_dist Min            1.44263e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.217665\n",
      "exploration/env_infos/reward_dist Std                    0.274554\n",
      "exploration/env_infos/reward_dist Max                    0.982089\n",
      "exploration/env_infos/reward_dist Min                    1.16603e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.152004\n",
      "exploration/env_infos/final/reward_energy Std            0.0569948\n",
      "exploration/env_infos/final/reward_energy Max           -0.0757899\n",
      "exploration/env_infos/final/reward_energy Min           -0.21556\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.180115\n",
      "exploration/env_infos/initial/reward_energy Std          0.111293\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0478308\n",
      "exploration/env_infos/initial/reward_energy Min         -0.317197\n",
      "exploration/env_infos/reward_energy Mean                -0.147731\n",
      "exploration/env_infos/reward_energy Std                  0.0788432\n",
      "exploration/env_infos/reward_energy Max                 -0.0105019\n",
      "exploration/env_infos/reward_energy Min                 -0.348705\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0933747\n",
      "exploration/env_infos/final/end_effector_loc Std         0.13482\n",
      "exploration/env_infos/final/end_effector_loc Max         0.282134\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.158427\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00131118\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00736989\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0157551\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.014151\n",
      "exploration/env_infos/end_effector_loc Mean              0.0693788\n",
      "exploration/env_infos/end_effector_loc Std               0.126879\n",
      "exploration/env_infos/end_effector_loc Max               0.344321\n",
      "exploration/env_infos/end_effector_loc Min              -0.18369\n",
      "evaluation/num steps total                           26000\n",
      "evaluation/num paths total                            1300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0522218\n",
      "evaluation/Rewards Std                                   0.0638551\n",
      "evaluation/Rewards Max                                   0.133847\n",
      "evaluation/Rewards Min                                  -0.325581\n",
      "evaluation/Returns Mean                                 -1.04444\n",
      "evaluation/Returns Std                                   0.965134\n",
      "evaluation/Returns Max                                   1.16098\n",
      "evaluation/Returns Min                                  -3.30275\n",
      "evaluation/Actions Mean                                  0.00154589\n",
      "evaluation/Actions Std                                   0.0430422\n",
      "evaluation/Actions Max                                   0.303963\n",
      "evaluation/Actions Min                                  -0.306416\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.04444\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0658978\n",
      "evaluation/env_infos/final/reward_dist Std               0.168616\n",
      "evaluation/env_infos/final/reward_dist Max               0.907258\n",
      "evaluation/env_infos/final/reward_dist Min               2.37445e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00451445\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00705599\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0262204\n",
      "evaluation/env_infos/initial/reward_dist Min             1.91522e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.111024\n",
      "evaluation/env_infos/reward_dist Std                     0.218484\n",
      "evaluation/env_infos/reward_dist Max                     0.993013\n",
      "evaluation/env_infos/reward_dist Min                     2.37445e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0531378\n",
      "evaluation/env_infos/final/reward_energy Std             0.0337918\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0120508\n",
      "evaluation/env_infos/final/reward_energy Min            -0.145847\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.125301\n",
      "evaluation/env_infos/initial/reward_energy Std           0.0989972\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00306618\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.382844\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0433348\n",
      "evaluation/env_infos/reward_energy Std                   0.0428034\n",
      "evaluation/env_infos/reward_energy Max                  -0.00124256\n",
      "evaluation/env_infos/reward_energy Min                  -0.382844\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.021458\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.244643\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.546189\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.471536\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000246528\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0056405\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0151982\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0153208\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0104386\n",
      "evaluation/env_infos/end_effector_loc Std                0.144891\n",
      "evaluation/env_infos/end_effector_loc Max                0.546189\n",
      "evaluation/env_infos/end_effector_loc Min               -0.471536\n",
      "time/data storing (s)                                    0.00646515\n",
      "time/evaluation sampling (s)                             3.37289\n",
      "time/exploration sampling (s)                            0.146268\n",
      "time/logging (s)                                         0.0234876\n",
      "time/saving (s)                                          0.0300112\n",
      "time/training (s)                                       54.9707\n",
      "time/epoch (s)                                          58.5499\n",
      "time/total (s)                                        1227.76\n",
      "Epoch                                                   25\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:09:57.741851 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 26 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00281357\n",
      "trainer/QF2 Loss                                         0.00193992\n",
      "trainer/Policy Loss                                      3.45541\n",
      "trainer/Q1 Predictions Mean                             -1.4035\n",
      "trainer/Q1 Predictions Std                               0.899455\n",
      "trainer/Q1 Predictions Max                               0.186174\n",
      "trainer/Q1 Predictions Min                              -3.69988\n",
      "trainer/Q2 Predictions Mean                             -1.41697\n",
      "trainer/Q2 Predictions Std                               0.912256\n",
      "trainer/Q2 Predictions Max                               0.22879\n",
      "trainer/Q2 Predictions Min                              -3.75535\n",
      "trainer/Q Targets Mean                                  -1.40688\n",
      "trainer/Q Targets Std                                    0.89604\n",
      "trainer/Q Targets Max                                    0.208038\n",
      "trainer/Q Targets Min                                   -3.69311\n",
      "trainer/Log Pis Mean                                     2.06505\n",
      "trainer/Log Pis Std                                      1.32096\n",
      "trainer/Log Pis Max                                      4.39161\n",
      "trainer/Log Pis Min                                     -2.59249\n",
      "trainer/Policy mu Mean                                  -0.0262723\n",
      "trainer/Policy mu Std                                    0.257945\n",
      "trainer/Policy mu Max                                    1.47092\n",
      "trainer/Policy mu Min                                   -1.90957\n",
      "trainer/Policy log std Mean                             -2.37267\n",
      "trainer/Policy log std Std                               0.560758\n",
      "trainer/Policy log std Max                              -0.486539\n",
      "trainer/Policy log std Min                              -3.25317\n",
      "trainer/Alpha                                            0.0228866\n",
      "trainer/Alpha Loss                                       0.245672\n",
      "exploration/num steps total                           3700\n",
      "exploration/num paths total                            185\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.110746\n",
      "exploration/Rewards Std                                  0.0611082\n",
      "exploration/Rewards Max                                 -0.00288142\n",
      "exploration/Rewards Min                                 -0.259714\n",
      "exploration/Returns Mean                                -2.21491\n",
      "exploration/Returns Std                                  0.778377\n",
      "exploration/Returns Max                                 -1.53478\n",
      "exploration/Returns Min                                 -3.44783\n",
      "exploration/Actions Mean                                -0.00872309\n",
      "exploration/Actions Std                                  0.10779\n",
      "exploration/Actions Max                                  0.347994\n",
      "exploration/Actions Min                                 -0.446452\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.21491\n",
      "exploration/env_infos/final/reward_dist Mean             0.111282\n",
      "exploration/env_infos/final/reward_dist Std              0.221239\n",
      "exploration/env_infos/final/reward_dist Max              0.553755\n",
      "exploration/env_infos/final/reward_dist Min              3.60475e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00147882\n",
      "exploration/env_infos/initial/reward_dist Std            0.00194689\n",
      "exploration/env_infos/initial/reward_dist Max            0.00490746\n",
      "exploration/env_infos/initial/reward_dist Min            7.74825e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.122819\n",
      "exploration/env_infos/reward_dist Std                    0.200081\n",
      "exploration/env_infos/reward_dist Max                    0.847192\n",
      "exploration/env_infos/reward_dist Min                    3.60475e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0849468\n",
      "exploration/env_infos/final/reward_energy Std            0.0465355\n",
      "exploration/env_infos/final/reward_energy Max           -0.0325301\n",
      "exploration/env_infos/final/reward_energy Min           -0.164864\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.151428\n",
      "exploration/env_infos/initial/reward_energy Std          0.102892\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0255414\n",
      "exploration/env_infos/initial/reward_energy Min         -0.288875\n",
      "exploration/env_infos/reward_energy Mean                -0.122558\n",
      "exploration/env_infos/reward_energy Std                  0.091483\n",
      "exploration/env_infos/reward_energy Max                 -0.00485064\n",
      "exploration/env_infos/reward_energy Min                 -0.480661\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0553979\n",
      "exploration/env_infos/final/end_effector_loc Std         0.290941\n",
      "exploration/env_infos/final/end_effector_loc Max         0.377407\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.542564\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000424514\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00645882\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0123513\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0103319\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0124346\n",
      "exploration/env_infos/end_effector_loc Std               0.178204\n",
      "exploration/env_infos/end_effector_loc Max               0.377407\n",
      "exploration/env_infos/end_effector_loc Min              -0.542564\n",
      "evaluation/num steps total                           27000\n",
      "evaluation/num paths total                            1350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0537771\n",
      "evaluation/Rewards Std                                   0.0671158\n",
      "evaluation/Rewards Max                                   0.155416\n",
      "evaluation/Rewards Min                                  -0.400143\n",
      "evaluation/Returns Mean                                 -1.07554\n",
      "evaluation/Returns Std                                   1.08731\n",
      "evaluation/Returns Max                                   2.21729\n",
      "evaluation/Returns Min                                  -3.35766\n",
      "evaluation/Actions Mean                                  0.00213927\n",
      "evaluation/Actions Std                                   0.0468149\n",
      "evaluation/Actions Max                                   0.521767\n",
      "evaluation/Actions Min                                  -0.584724\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.07554\n",
      "evaluation/env_infos/final/reward_dist Mean              0.210902\n",
      "evaluation/env_infos/final/reward_dist Std               0.28455\n",
      "evaluation/env_infos/final/reward_dist Max               0.952993\n",
      "evaluation/env_infos/final/reward_dist Min               1.92998e-32\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00517352\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00849556\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0394728\n",
      "evaluation/env_infos/initial/reward_dist Min             1.57756e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.163706\n",
      "evaluation/env_infos/reward_dist Std                     0.259254\n",
      "evaluation/env_infos/reward_dist Max                     0.988335\n",
      "evaluation/env_infos/reward_dist Min                     1.92998e-32\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0264921\n",
      "evaluation/env_infos/final/reward_energy Std             0.0300192\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00259655\n",
      "evaluation/env_infos/final/reward_energy Min            -0.171701\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.149228\n",
      "evaluation/env_infos/initial/reward_energy Std           0.145097\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0155496\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.783672\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0403244\n",
      "evaluation/env_infos/reward_energy Std                   0.0525963\n",
      "evaluation/env_infos/reward_energy Max                  -0.00104067\n",
      "evaluation/env_infos/reward_energy Min                  -0.783672\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0726844\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23333\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.638073\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.848329\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00121766\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00725742\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0260883\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0292362\n",
      "evaluation/env_infos/end_effector_loc Mean               0.039102\n",
      "evaluation/env_infos/end_effector_loc Std                0.145458\n",
      "evaluation/env_infos/end_effector_loc Max                0.638073\n",
      "evaluation/env_infos/end_effector_loc Min               -0.848329\n",
      "time/data storing (s)                                    0.00654833\n",
      "time/evaluation sampling (s)                             0.985161\n",
      "time/exploration sampling (s)                            0.143464\n",
      "time/logging (s)                                         0.0214943\n",
      "time/saving (s)                                          0.0289814\n",
      "time/training (s)                                       50.155\n",
      "time/epoch (s)                                          51.3407\n",
      "time/total (s)                                        1279.48\n",
      "Epoch                                                   26\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:10:48.313271 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 27 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00149517\n",
      "trainer/QF2 Loss                                         0.00128675\n",
      "trainer/Policy Loss                                      3.51679\n",
      "trainer/Q1 Predictions Mean                             -1.46992\n",
      "trainer/Q1 Predictions Std                               0.911845\n",
      "trainer/Q1 Predictions Max                               0.331786\n",
      "trainer/Q1 Predictions Min                              -3.69809\n",
      "trainer/Q2 Predictions Mean                             -1.46841\n",
      "trainer/Q2 Predictions Std                               0.915455\n",
      "trainer/Q2 Predictions Max                               0.352067\n",
      "trainer/Q2 Predictions Min                              -3.67409\n",
      "trainer/Q Targets Mean                                  -1.44815\n",
      "trainer/Q Targets Std                                    0.91277\n",
      "trainer/Q Targets Max                                    0.394075\n",
      "trainer/Q Targets Min                                   -3.74383\n",
      "trainer/Log Pis Mean                                     2.08005\n",
      "trainer/Log Pis Std                                      1.45196\n",
      "trainer/Log Pis Max                                      4.7279\n",
      "trainer/Log Pis Min                                     -3.92226\n",
      "trainer/Policy mu Mean                                  -0.0399967\n",
      "trainer/Policy mu Std                                    0.334598\n",
      "trainer/Policy mu Max                                    1.88919\n",
      "trainer/Policy mu Min                                   -2.12193\n",
      "trainer/Policy log std Mean                             -2.33928\n",
      "trainer/Policy log std Std                               0.640085\n",
      "trainer/Policy log std Max                              -0.310432\n",
      "trainer/Policy log std Min                              -3.36379\n",
      "trainer/Alpha                                            0.0214406\n",
      "trainer/Alpha Loss                                       0.307637\n",
      "exploration/num steps total                           3800\n",
      "exploration/num paths total                            190\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0714657\n",
      "exploration/Rewards Std                                  0.0459166\n",
      "exploration/Rewards Max                                  0.0611973\n",
      "exploration/Rewards Min                                 -0.211295\n",
      "exploration/Returns Mean                                -1.42931\n",
      "exploration/Returns Std                                  0.490158\n",
      "exploration/Returns Max                                 -0.842778\n",
      "exploration/Returns Min                                 -2.06379\n",
      "exploration/Actions Mean                                -0.00658657\n",
      "exploration/Actions Std                                  0.130849\n",
      "exploration/Actions Max                                  0.381849\n",
      "exploration/Actions Min                                 -0.404022\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.42931\n",
      "exploration/env_infos/final/reward_dist Mean             0.121058\n",
      "exploration/env_infos/final/reward_dist Std              0.225856\n",
      "exploration/env_infos/final/reward_dist Max              0.572183\n",
      "exploration/env_infos/final/reward_dist Min              1.60467e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00828228\n",
      "exploration/env_infos/initial/reward_dist Std            0.00499308\n",
      "exploration/env_infos/initial/reward_dist Max            0.0156493\n",
      "exploration/env_infos/initial/reward_dist Min            1.1238e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0771474\n",
      "exploration/env_infos/reward_dist Std                    0.108383\n",
      "exploration/env_infos/reward_dist Max                    0.572183\n",
      "exploration/env_infos/reward_dist Min                    1.60467e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.15584\n",
      "exploration/env_infos/final/reward_energy Std            0.0857617\n",
      "exploration/env_infos/final/reward_energy Max           -0.080521\n",
      "exploration/env_infos/final/reward_energy Min           -0.301497\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.194325\n",
      "exploration/env_infos/initial/reward_energy Std          0.108193\n",
      "exploration/env_infos/initial/reward_energy Max         -0.060162\n",
      "exploration/env_infos/initial/reward_energy Min         -0.363737\n",
      "exploration/env_infos/reward_energy Mean                -0.153809\n",
      "exploration/env_infos/reward_energy Std                  0.103307\n",
      "exploration/env_infos/reward_energy Max                 -0.010034\n",
      "exploration/env_infos/reward_energy Min                 -0.484408\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0376593\n",
      "exploration/env_infos/final/end_effector_loc Std         0.22252\n",
      "exploration/env_infos/final/end_effector_loc Max         0.363748\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.424205\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000764375\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00782627\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.016425\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0111287\n",
      "exploration/env_infos/end_effector_loc Mean              0.0211245\n",
      "exploration/env_infos/end_effector_loc Std               0.141391\n",
      "exploration/env_infos/end_effector_loc Max               0.363748\n",
      "exploration/env_infos/end_effector_loc Min              -0.424205\n",
      "evaluation/num steps total                           28000\n",
      "evaluation/num paths total                            1400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0465188\n",
      "evaluation/Rewards Std                                   0.064838\n",
      "evaluation/Rewards Max                                   0.149182\n",
      "evaluation/Rewards Min                                  -0.315971\n",
      "evaluation/Returns Mean                                 -0.930375\n",
      "evaluation/Returns Std                                   0.938715\n",
      "evaluation/Returns Max                                   1.82436\n",
      "evaluation/Returns Min                                  -2.34313\n",
      "evaluation/Actions Mean                                  0.00157026\n",
      "evaluation/Actions Std                                   0.0554357\n",
      "evaluation/Actions Max                                   0.395896\n",
      "evaluation/Actions Min                                  -0.480546\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.930375\n",
      "evaluation/env_infos/final/reward_dist Mean              0.144105\n",
      "evaluation/env_infos/final/reward_dist Std               0.216297\n",
      "evaluation/env_infos/final/reward_dist Max               0.978567\n",
      "evaluation/env_infos/final/reward_dist Min               4.30897e-36\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00608377\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0101884\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0460576\n",
      "evaluation/env_infos/initial/reward_dist Min             9.96735e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.154592\n",
      "evaluation/env_infos/reward_dist Std                     0.224089\n",
      "evaluation/env_infos/reward_dist Max                     0.989316\n",
      "evaluation/env_infos/reward_dist Min                     4.30897e-36\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0381194\n",
      "evaluation/env_infos/final/reward_energy Std             0.0253443\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00301004\n",
      "evaluation/env_infos/final/reward_energy Min            -0.109858\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.184462\n",
      "evaluation/env_infos/initial/reward_energy Std           0.145479\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0107161\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.506747\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0499705\n",
      "evaluation/env_infos/reward_energy Std                   0.0604493\n",
      "evaluation/env_infos/reward_energy Max                  -0.000337199\n",
      "evaluation/env_infos/reward_energy Min                  -0.506747\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0022092\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.237641\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.667794\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.890811\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000246438\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00830224\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0197948\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0240273\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00280374\n",
      "evaluation/env_infos/end_effector_loc Std                0.149061\n",
      "evaluation/env_infos/end_effector_loc Max                0.667794\n",
      "evaluation/env_infos/end_effector_loc Min               -0.890811\n",
      "time/data storing (s)                                    0.00613781\n",
      "time/evaluation sampling (s)                             1.18754\n",
      "time/exploration sampling (s)                            0.132732\n",
      "time/logging (s)                                         0.0219499\n",
      "time/saving (s)                                          0.0275108\n",
      "time/training (s)                                       48.7972\n",
      "time/epoch (s)                                          50.1731\n",
      "time/total (s)                                        1330.05\n",
      "Epoch                                                   27\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:11:37.048031 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 28 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00149126\n",
      "trainer/QF2 Loss                                         0.00110868\n",
      "trainer/Policy Loss                                      3.33954\n",
      "trainer/Q1 Predictions Mean                             -1.30032\n",
      "trainer/Q1 Predictions Std                               0.884798\n",
      "trainer/Q1 Predictions Max                               0.437295\n",
      "trainer/Q1 Predictions Min                              -3.43746\n",
      "trainer/Q2 Predictions Mean                             -1.30416\n",
      "trainer/Q2 Predictions Std                               0.881045\n",
      "trainer/Q2 Predictions Max                               0.402495\n",
      "trainer/Q2 Predictions Min                              -3.43913\n",
      "trainer/Q Targets Mean                                  -1.3033\n",
      "trainer/Q Targets Std                                    0.884399\n",
      "trainer/Q Targets Max                                    0.482364\n",
      "trainer/Q Targets Min                                   -3.49073\n",
      "trainer/Log Pis Mean                                     2.06634\n",
      "trainer/Log Pis Std                                      1.28317\n",
      "trainer/Log Pis Max                                      4.99428\n",
      "trainer/Log Pis Min                                     -5.0374\n",
      "trainer/Policy mu Mean                                  -0.00833795\n",
      "trainer/Policy mu Std                                    0.33856\n",
      "trainer/Policy mu Max                                    1.70658\n",
      "trainer/Policy mu Min                                   -1.79332\n",
      "trainer/Policy log std Mean                             -2.33009\n",
      "trainer/Policy log std Std                               0.65416\n",
      "trainer/Policy log std Max                              -0.443372\n",
      "trainer/Policy log std Min                              -3.49159\n",
      "trainer/Alpha                                            0.0197091\n",
      "trainer/Alpha Loss                                       0.260462\n",
      "exploration/num steps total                           3900\n",
      "exploration/num paths total                            195\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0624223\n",
      "exploration/Rewards Std                                  0.0662681\n",
      "exploration/Rewards Max                                  0.113572\n",
      "exploration/Rewards Min                                 -0.286321\n",
      "exploration/Returns Mean                                -1.24845\n",
      "exploration/Returns Std                                  0.747085\n",
      "exploration/Returns Max                                 -0.140806\n",
      "exploration/Returns Min                                 -2.2456\n",
      "exploration/Actions Mean                                 0.010034\n",
      "exploration/Actions Std                                  0.10883\n",
      "exploration/Actions Max                                  0.381322\n",
      "exploration/Actions Min                                 -0.3973\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.24845\n",
      "exploration/env_infos/final/reward_dist Mean             0.11192\n",
      "exploration/env_infos/final/reward_dist Std              0.0957417\n",
      "exploration/env_infos/final/reward_dist Max              0.2325\n",
      "exploration/env_infos/final/reward_dist Min              8.76281e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0159644\n",
      "exploration/env_infos/initial/reward_dist Std            0.0197279\n",
      "exploration/env_infos/initial/reward_dist Max            0.0444057\n",
      "exploration/env_infos/initial/reward_dist Min            2.31993e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.227403\n",
      "exploration/env_infos/reward_dist Std                    0.292256\n",
      "exploration/env_infos/reward_dist Max                    0.989664\n",
      "exploration/env_infos/reward_dist Min                    8.76281e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.127773\n",
      "exploration/env_infos/final/reward_energy Std            0.0748845\n",
      "exploration/env_infos/final/reward_energy Max           -0.0310444\n",
      "exploration/env_infos/final/reward_energy Min           -0.239575\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.202407\n",
      "exploration/env_infos/initial/reward_energy Std          0.0957624\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0771126\n",
      "exploration/env_infos/initial/reward_energy Min         -0.369166\n",
      "exploration/env_infos/reward_energy Mean                -0.11993\n",
      "exploration/env_infos/reward_energy Std                  0.0974981\n",
      "exploration/env_infos/reward_energy Max                 -0.00628512\n",
      "exploration/env_infos/reward_energy Min                 -0.411131\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.201351\n",
      "exploration/env_infos/final/end_effector_loc Std         0.163929\n",
      "exploration/env_infos/final/end_effector_loc Max         0.527473\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.160703\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00582067\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00536596\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0171987\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00348035\n",
      "exploration/env_infos/end_effector_loc Mean              0.106478\n",
      "exploration/env_infos/end_effector_loc Std               0.115491\n",
      "exploration/env_infos/end_effector_loc Max               0.527473\n",
      "exploration/env_infos/end_effector_loc Min              -0.160703\n",
      "evaluation/num steps total                           29000\n",
      "evaluation/num paths total                            1450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0386751\n",
      "evaluation/Rewards Std                                   0.0726434\n",
      "evaluation/Rewards Max                                   0.146765\n",
      "evaluation/Rewards Min                                  -0.315357\n",
      "evaluation/Returns Mean                                 -0.773503\n",
      "evaluation/Returns Std                                   1.16976\n",
      "evaluation/Returns Max                                   1.77575\n",
      "evaluation/Returns Min                                  -4.78987\n",
      "evaluation/Actions Mean                                  0.00216897\n",
      "evaluation/Actions Std                                   0.0609715\n",
      "evaluation/Actions Max                                   0.687285\n",
      "evaluation/Actions Min                                  -0.358177\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.773503\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0967362\n",
      "evaluation/env_infos/final/reward_dist Std               0.17698\n",
      "evaluation/env_infos/final/reward_dist Max               0.771847\n",
      "evaluation/env_infos/final/reward_dist Min               1.26886e-34\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00717228\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0123708\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0470532\n",
      "evaluation/env_infos/initial/reward_dist Min             5.49233e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.142177\n",
      "evaluation/env_infos/reward_dist Std                     0.224587\n",
      "evaluation/env_infos/reward_dist Max                     0.972756\n",
      "evaluation/env_infos/reward_dist Min                     1.26886e-34\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0601311\n",
      "evaluation/env_infos/final/reward_energy Std             0.0422587\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00575438\n",
      "evaluation/env_infos/final/reward_energy Min            -0.170427\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.197834\n",
      "evaluation/env_infos/initial/reward_energy Std           0.157259\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0143579\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.858306\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0567259\n",
      "evaluation/env_infos/reward_energy Std                   0.0650125\n",
      "evaluation/env_infos/reward_energy Max                  -0.000666125\n",
      "evaluation/env_infos/reward_energy Min                  -0.858306\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0879375\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.245279\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.672736\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.511856\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00283415\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00847367\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0343643\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0179089\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0490792\n",
      "evaluation/env_infos/end_effector_loc Std                0.157953\n",
      "evaluation/env_infos/end_effector_loc Max                0.672736\n",
      "evaluation/env_infos/end_effector_loc Min               -0.511856\n",
      "time/data storing (s)                                    0.00623608\n",
      "time/evaluation sampling (s)                             0.956143\n",
      "time/exploration sampling (s)                            0.121411\n",
      "time/logging (s)                                         0.0188281\n",
      "time/saving (s)                                          0.0270198\n",
      "time/training (s)                                       47.2327\n",
      "time/epoch (s)                                          48.3623\n",
      "time/total (s)                                        1378.78\n",
      "Epoch                                                   28\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:12:26.764358 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 29 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000737794\n",
      "trainer/QF2 Loss                                         0.000962618\n",
      "trainer/Policy Loss                                      3.30718\n",
      "trainer/Q1 Predictions Mean                             -1.31359\n",
      "trainer/Q1 Predictions Std                               0.914484\n",
      "trainer/Q1 Predictions Max                               0.180001\n",
      "trainer/Q1 Predictions Min                              -3.57987\n",
      "trainer/Q2 Predictions Mean                             -1.3262\n",
      "trainer/Q2 Predictions Std                               0.906612\n",
      "trainer/Q2 Predictions Max                               0.162486\n",
      "trainer/Q2 Predictions Min                              -3.57553\n",
      "trainer/Q Targets Mean                                  -1.31277\n",
      "trainer/Q Targets Std                                    0.907582\n",
      "trainer/Q Targets Max                                    0.13865\n",
      "trainer/Q Targets Min                                   -3.60129\n",
      "trainer/Log Pis Mean                                     2.01915\n",
      "trainer/Log Pis Std                                      1.50267\n",
      "trainer/Log Pis Max                                      4.64729\n",
      "trainer/Log Pis Min                                     -3.99946\n",
      "trainer/Policy mu Mean                                  -0.0549513\n",
      "trainer/Policy mu Std                                    0.305856\n",
      "trainer/Policy mu Max                                    1.90361\n",
      "trainer/Policy mu Min                                   -1.76062\n",
      "trainer/Policy log std Mean                             -2.31983\n",
      "trainer/Policy log std Std                               0.617152\n",
      "trainer/Policy log std Max                              -0.480606\n",
      "trainer/Policy log std Min                              -3.41705\n",
      "trainer/Alpha                                            0.0196988\n",
      "trainer/Alpha Loss                                       0.0752239\n",
      "exploration/num steps total                           4000\n",
      "exploration/num paths total                            200\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.069123\n",
      "exploration/Rewards Std                                  0.0737118\n",
      "exploration/Rewards Max                                  0.0976863\n",
      "exploration/Rewards Min                                 -0.253213\n",
      "exploration/Returns Mean                                -1.38246\n",
      "exploration/Returns Std                                  1.01684\n",
      "exploration/Returns Max                                 -0.170457\n",
      "exploration/Returns Min                                 -2.80434\n",
      "exploration/Actions Mean                                 0.00475376\n",
      "exploration/Actions Std                                  0.119379\n",
      "exploration/Actions Max                                  0.448093\n",
      "exploration/Actions Min                                 -0.345535\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.38246\n",
      "exploration/env_infos/final/reward_dist Mean             0.131263\n",
      "exploration/env_infos/final/reward_dist Std              0.262384\n",
      "exploration/env_infos/final/reward_dist Max              0.65603\n",
      "exploration/env_infos/final/reward_dist Min              6.68582e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00843955\n",
      "exploration/env_infos/initial/reward_dist Std            0.00522522\n",
      "exploration/env_infos/initial/reward_dist Max            0.0146393\n",
      "exploration/env_infos/initial/reward_dist Min            0.00150331\n",
      "exploration/env_infos/reward_dist Mean                   0.173088\n",
      "exploration/env_infos/reward_dist Std                    0.246194\n",
      "exploration/env_infos/reward_dist Max                    0.977964\n",
      "exploration/env_infos/reward_dist Min                    6.68582e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.109443\n",
      "exploration/env_infos/final/reward_energy Std            0.0905615\n",
      "exploration/env_infos/final/reward_energy Max           -0.0411406\n",
      "exploration/env_infos/final/reward_energy Min           -0.288398\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.239299\n",
      "exploration/env_infos/initial/reward_energy Std          0.111893\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0628123\n",
      "exploration/env_infos/initial/reward_energy Min         -0.362015\n",
      "exploration/env_infos/reward_energy Mean                -0.142241\n",
      "exploration/env_infos/reward_energy Std                  0.0911889\n",
      "exploration/env_infos/reward_energy Max                 -0.0128814\n",
      "exploration/env_infos/reward_energy Min                 -0.456878\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00325094\n",
      "exploration/env_infos/final/end_effector_loc Std         0.294767\n",
      "exploration/env_infos/final/end_effector_loc Max         0.648309\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.480069\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00202822\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00911682\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0145325\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0172767\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0130002\n",
      "exploration/env_infos/end_effector_loc Std               0.191353\n",
      "exploration/env_infos/end_effector_loc Max               0.648309\n",
      "exploration/env_infos/end_effector_loc Min              -0.480069\n",
      "evaluation/num steps total                           30000\n",
      "evaluation/num paths total                            1500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0478344\n",
      "evaluation/Rewards Std                                   0.0781518\n",
      "evaluation/Rewards Max                                   0.154906\n",
      "evaluation/Rewards Min                                  -0.34385\n",
      "evaluation/Returns Mean                                 -0.956688\n",
      "evaluation/Returns Std                                   1.23753\n",
      "evaluation/Returns Max                                   1.38237\n",
      "evaluation/Returns Min                                  -5.42146\n",
      "evaluation/Actions Mean                                 -5.85024e-05\n",
      "evaluation/Actions Std                                   0.0605195\n",
      "evaluation/Actions Max                                   0.585635\n",
      "evaluation/Actions Min                                  -0.571492\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.956688\n",
      "evaluation/env_infos/final/reward_dist Mean              0.174392\n",
      "evaluation/env_infos/final/reward_dist Std               0.244496\n",
      "evaluation/env_infos/final/reward_dist Max               0.925011\n",
      "evaluation/env_infos/final/reward_dist Min               1.6215e-44\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00584048\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0103673\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0423888\n",
      "evaluation/env_infos/initial/reward_dist Min             1.23617e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.165572\n",
      "evaluation/env_infos/reward_dist Std                     0.242717\n",
      "evaluation/env_infos/reward_dist Max                     0.996089\n",
      "evaluation/env_infos/reward_dist Min                     1.6215e-44\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0334933\n",
      "evaluation/env_infos/final/reward_energy Std             0.0260496\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00624763\n",
      "evaluation/env_infos/final/reward_energy Min            -0.12135\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.182071\n",
      "evaluation/env_infos/initial/reward_energy Std           0.157393\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0143253\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.59927\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0538416\n",
      "evaluation/env_infos/reward_energy Std                   0.0665305\n",
      "evaluation/env_infos/reward_energy Max                  -0.00105336\n",
      "evaluation/env_infos/reward_energy Min                  -0.59927\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0156393\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.244082\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.607835\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000664551\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.008483\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0292818\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0285746\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0076434\n",
      "evaluation/env_infos/end_effector_loc Std                0.164779\n",
      "evaluation/env_infos/end_effector_loc Max                0.607835\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00598187\n",
      "time/evaluation sampling (s)                             0.949223\n",
      "time/exploration sampling (s)                            0.13941\n",
      "time/logging (s)                                         0.0189167\n",
      "time/saving (s)                                          0.0635971\n",
      "time/training (s)                                       48.1715\n",
      "time/epoch (s)                                          49.3487\n",
      "time/total (s)                                        1428.5\n",
      "Epoch                                                   29\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:13:15.537347 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 30 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000806388\n",
      "trainer/QF2 Loss                                         0.000678222\n",
      "trainer/Policy Loss                                      3.11431\n",
      "trainer/Q1 Predictions Mean                             -1.24033\n",
      "trainer/Q1 Predictions Std                               0.86354\n",
      "trainer/Q1 Predictions Max                               0.501521\n",
      "trainer/Q1 Predictions Min                              -3.57237\n",
      "trainer/Q2 Predictions Mean                             -1.24972\n",
      "trainer/Q2 Predictions Std                               0.864981\n",
      "trainer/Q2 Predictions Max                               0.484863\n",
      "trainer/Q2 Predictions Min                              -3.55382\n",
      "trainer/Q Targets Mean                                  -1.24472\n",
      "trainer/Q Targets Std                                    0.859898\n",
      "trainer/Q Targets Max                                    0.489754\n",
      "trainer/Q Targets Min                                   -3.58597\n",
      "trainer/Log Pis Mean                                     1.89674\n",
      "trainer/Log Pis Std                                      1.49676\n",
      "trainer/Log Pis Max                                      4.30452\n",
      "trainer/Log Pis Min                                     -4.52215\n",
      "trainer/Policy mu Mean                                  -0.0399033\n",
      "trainer/Policy mu Std                                    0.346213\n",
      "trainer/Policy mu Max                                    1.80276\n",
      "trainer/Policy mu Min                                   -1.84594\n",
      "trainer/Policy log std Mean                             -2.28613\n",
      "trainer/Policy log std Std                               0.638481\n",
      "trainer/Policy log std Max                              -0.433625\n",
      "trainer/Policy log std Min                              -3.39573\n",
      "trainer/Alpha                                            0.01926\n",
      "trainer/Alpha Loss                                      -0.407837\n",
      "exploration/num steps total                           4100\n",
      "exploration/num paths total                            205\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0453252\n",
      "exploration/Rewards Std                                  0.0602498\n",
      "exploration/Rewards Max                                  0.12533\n",
      "exploration/Rewards Min                                 -0.19298\n",
      "exploration/Returns Mean                                -0.906503\n",
      "exploration/Returns Std                                  0.79581\n",
      "exploration/Returns Max                                  0.522048\n",
      "exploration/Returns Min                                 -1.62048\n",
      "exploration/Actions Mean                                -0.00388164\n",
      "exploration/Actions Std                                  0.13216\n",
      "exploration/Actions Max                                  0.885661\n",
      "exploration/Actions Min                                 -0.48715\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.906503\n",
      "exploration/env_infos/final/reward_dist Mean             0.0672868\n",
      "exploration/env_infos/final/reward_dist Std              0.0944888\n",
      "exploration/env_infos/final/reward_dist Max              0.243611\n",
      "exploration/env_infos/final/reward_dist Min              2.59687e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00663059\n",
      "exploration/env_infos/initial/reward_dist Std            0.0074863\n",
      "exploration/env_infos/initial/reward_dist Max            0.0162655\n",
      "exploration/env_infos/initial/reward_dist Min            0.000151264\n",
      "exploration/env_infos/reward_dist Mean                   0.188354\n",
      "exploration/env_infos/reward_dist Std                    0.230437\n",
      "exploration/env_infos/reward_dist Max                    0.829776\n",
      "exploration/env_infos/reward_dist Min                    2.59687e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.16471\n",
      "exploration/env_infos/final/reward_energy Std            0.0686522\n",
      "exploration/env_infos/final/reward_energy Max           -0.0769298\n",
      "exploration/env_infos/final/reward_energy Min           -0.249323\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.23283\n",
      "exploration/env_infos/initial/reward_energy Std          0.176235\n",
      "exploration/env_infos/initial/reward_energy Max         -0.048188\n",
      "exploration/env_infos/initial/reward_energy Min         -0.521752\n",
      "exploration/env_infos/reward_energy Mean                -0.125974\n",
      "exploration/env_infos/reward_energy Std                  0.138178\n",
      "exploration/env_infos/reward_energy Max                 -0.00240383\n",
      "exploration/env_infos/reward_energy Min                 -0.890503\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0533505\n",
      "exploration/env_infos/final/end_effector_loc Std         0.238039\n",
      "exploration/env_infos/final/end_effector_loc Max         0.216591\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.467336\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00178083\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0101693\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0168126\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0199474\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0322419\n",
      "exploration/env_infos/end_effector_loc Std               0.145183\n",
      "exploration/env_infos/end_effector_loc Max               0.216591\n",
      "exploration/env_infos/end_effector_loc Min              -0.467336\n",
      "evaluation/num steps total                           31000\n",
      "evaluation/num paths total                            1550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0417269\n",
      "evaluation/Rewards Std                                   0.0735391\n",
      "evaluation/Rewards Max                                   0.135855\n",
      "evaluation/Rewards Min                                  -0.359124\n",
      "evaluation/Returns Mean                                 -0.834538\n",
      "evaluation/Returns Std                                   1.07338\n",
      "evaluation/Returns Max                                   1.93759\n",
      "evaluation/Returns Min                                  -2.92244\n",
      "evaluation/Actions Mean                                 -0.00369763\n",
      "evaluation/Actions Std                                   0.0872848\n",
      "evaluation/Actions Max                                   0.648241\n",
      "evaluation/Actions Min                                  -0.640422\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.834538\n",
      "evaluation/env_infos/final/reward_dist Mean              0.234028\n",
      "evaluation/env_infos/final/reward_dist Std               0.277087\n",
      "evaluation/env_infos/final/reward_dist Max               0.968167\n",
      "evaluation/env_infos/final/reward_dist Min               3.17456e-22\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00606811\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0110053\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0465563\n",
      "evaluation/env_infos/initial/reward_dist Min             1.55093e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.219774\n",
      "evaluation/env_infos/reward_dist Std                     0.272617\n",
      "evaluation/env_infos/reward_dist Max                     0.999477\n",
      "evaluation/env_infos/reward_dist Min                     3.17456e-22\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0428928\n",
      "evaluation/env_infos/final/reward_energy Std             0.0334178\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00886486\n",
      "evaluation/env_infos/final/reward_energy Min            -0.141487\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.284414\n",
      "evaluation/env_infos/initial/reward_energy Std           0.22018\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0130913\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.778105\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0745813\n",
      "evaluation/env_infos/reward_energy Std                   0.0985001\n",
      "evaluation/env_infos/reward_energy Max                  -0.00194006\n",
      "evaluation/env_infos/reward_energy Min                  -0.778105\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0210741\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.227472\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.533442\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.658893\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0024821\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0124721\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.032412\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0320211\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.000713825\n",
      "evaluation/env_infos/end_effector_loc Std                0.162735\n",
      "evaluation/env_infos/end_effector_loc Max                0.533442\n",
      "evaluation/env_infos/end_effector_loc Min               -0.658893\n",
      "time/data storing (s)                                    0.00591682\n",
      "time/evaluation sampling (s)                             0.916772\n",
      "time/exploration sampling (s)                            0.125538\n",
      "time/logging (s)                                         0.0195633\n",
      "time/saving (s)                                          0.0272133\n",
      "time/training (s)                                       47.3048\n",
      "time/epoch (s)                                          48.3998\n",
      "time/total (s)                                        1477.27\n",
      "Epoch                                                   30\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:14:05.037172 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 31 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000884594\n",
      "trainer/QF2 Loss                                         0.00108613\n",
      "trainer/Policy Loss                                      3.33284\n",
      "trainer/Q1 Predictions Mean                             -1.25824\n",
      "trainer/Q1 Predictions Std                               0.856912\n",
      "trainer/Q1 Predictions Max                               0.389706\n",
      "trainer/Q1 Predictions Min                              -3.50737\n",
      "trainer/Q2 Predictions Mean                             -1.25326\n",
      "trainer/Q2 Predictions Std                               0.848735\n",
      "trainer/Q2 Predictions Max                               0.397129\n",
      "trainer/Q2 Predictions Min                              -3.47905\n",
      "trainer/Q Targets Mean                                  -1.26514\n",
      "trainer/Q Targets Std                                    0.858446\n",
      "trainer/Q Targets Max                                    0.407059\n",
      "trainer/Q Targets Min                                   -3.44814\n",
      "trainer/Log Pis Mean                                     2.11734\n",
      "trainer/Log Pis Std                                      1.35383\n",
      "trainer/Log Pis Max                                      5.02982\n",
      "trainer/Log Pis Min                                     -2.80121\n",
      "trainer/Policy mu Mean                                   0.00415062\n",
      "trainer/Policy mu Std                                    0.385833\n",
      "trainer/Policy mu Max                                    1.58998\n",
      "trainer/Policy mu Min                                   -1.94343\n",
      "trainer/Policy log std Mean                             -2.29805\n",
      "trainer/Policy log std Std                               0.68548\n",
      "trainer/Policy log std Max                              -0.305371\n",
      "trainer/Policy log std Min                              -3.56432\n",
      "trainer/Alpha                                            0.0198684\n",
      "trainer/Alpha Loss                                       0.460009\n",
      "exploration/num steps total                           4200\n",
      "exploration/num paths total                            210\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0734352\n",
      "exploration/Rewards Std                                  0.105338\n",
      "exploration/Rewards Max                                  0.15986\n",
      "exploration/Rewards Min                                 -0.382495\n",
      "exploration/Returns Mean                                -1.4687\n",
      "exploration/Returns Std                                  1.81362\n",
      "exploration/Returns Max                                  1.44669\n",
      "exploration/Returns Min                                 -3.38397\n",
      "exploration/Actions Mean                                 0.000547262\n",
      "exploration/Actions Std                                  0.118203\n",
      "exploration/Actions Max                                  0.49333\n",
      "exploration/Actions Min                                 -0.486355\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.4687\n",
      "exploration/env_infos/final/reward_dist Mean             0.164121\n",
      "exploration/env_infos/final/reward_dist Std              0.324499\n",
      "exploration/env_infos/final/reward_dist Max              0.813112\n",
      "exploration/env_infos/final/reward_dist Min              5.6038e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00695747\n",
      "exploration/env_infos/initial/reward_dist Std            0.00593394\n",
      "exploration/env_infos/initial/reward_dist Max            0.0146395\n",
      "exploration/env_infos/initial/reward_dist Min            1.10996e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.220305\n",
      "exploration/env_infos/reward_dist Std                    0.296604\n",
      "exploration/env_infos/reward_dist Max                    0.94611\n",
      "exploration/env_infos/reward_dist Min                    5.6038e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.173979\n",
      "exploration/env_infos/final/reward_energy Std            0.0819126\n",
      "exploration/env_infos/final/reward_energy Max           -0.0626042\n",
      "exploration/env_infos/final/reward_energy Min           -0.295022\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.271459\n",
      "exploration/env_infos/initial/reward_energy Std          0.185501\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0412249\n",
      "exploration/env_infos/initial/reward_energy Min         -0.507779\n",
      "exploration/env_infos/reward_energy Mean                -0.127927\n",
      "exploration/env_infos/reward_energy Std                  0.107606\n",
      "exploration/env_infos/reward_energy Max                 -0.00223282\n",
      "exploration/env_infos/reward_energy Min                 -0.533501\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0218679\n",
      "exploration/env_infos/final/end_effector_loc Std         0.182288\n",
      "exploration/env_infos/final/end_effector_loc Max         0.341627\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.290567\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000219988\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0116223\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0187819\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0243177\n",
      "exploration/env_infos/end_effector_loc Mean              0.00521938\n",
      "exploration/env_infos/end_effector_loc Std               0.13527\n",
      "exploration/env_infos/end_effector_loc Max               0.341627\n",
      "exploration/env_infos/end_effector_loc Min              -0.290567\n",
      "evaluation/num steps total                           32000\n",
      "evaluation/num paths total                            1600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0693876\n",
      "evaluation/Rewards Std                                   0.0735058\n",
      "evaluation/Rewards Max                                   0.132683\n",
      "evaluation/Rewards Min                                  -0.51136\n",
      "evaluation/Returns Mean                                 -1.38775\n",
      "evaluation/Returns Std                                   1.10512\n",
      "evaluation/Returns Max                                   2.10345\n",
      "evaluation/Returns Min                                  -3.7861\n",
      "evaluation/Actions Mean                                 -0.000973185\n",
      "evaluation/Actions Std                                   0.0714686\n",
      "evaluation/Actions Max                                   0.736724\n",
      "evaluation/Actions Min                                  -0.4598\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.38775\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0773632\n",
      "evaluation/env_infos/final/reward_dist Std               0.166113\n",
      "evaluation/env_infos/final/reward_dist Max               0.772974\n",
      "evaluation/env_infos/final/reward_dist Min               8.16251e-54\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00630042\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0118899\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0585457\n",
      "evaluation/env_infos/initial/reward_dist Min             2.27344e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.118735\n",
      "evaluation/env_infos/reward_dist Std                     0.206005\n",
      "evaluation/env_infos/reward_dist Max                     0.989332\n",
      "evaluation/env_infos/reward_dist Min                     8.16251e-54\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0260379\n",
      "evaluation/env_infos/final/reward_energy Std             0.0359081\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00371172\n",
      "evaluation/env_infos/final/reward_energy Min            -0.244461\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.230784\n",
      "evaluation/env_infos/initial/reward_energy Std           0.212945\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00360494\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.812973\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0569262\n",
      "evaluation/env_infos/reward_energy Std                   0.0835273\n",
      "evaluation/env_infos/reward_energy Max                  -0.000732515\n",
      "evaluation/env_infos/reward_energy Min                  -0.812973\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00531002\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.311665\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.682977\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00237355\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0108455\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0368362\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.02299\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00735527\n",
      "evaluation/env_infos/end_effector_loc Std                0.196821\n",
      "evaluation/env_infos/end_effector_loc Max                0.682977\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00623494\n",
      "time/evaluation sampling (s)                             0.922012\n",
      "time/exploration sampling (s)                            0.123553\n",
      "time/logging (s)                                         0.0200317\n",
      "time/saving (s)                                          0.0288787\n",
      "time/training (s)                                       47.9917\n",
      "time/epoch (s)                                          49.0924\n",
      "time/total (s)                                        1526.77\n",
      "Epoch                                                   31\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:14:56.644613 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 32 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00100445\r\n",
      "trainer/QF2 Loss                                         0.00147892\r\n",
      "trainer/Policy Loss                                      3.23001\r\n",
      "trainer/Q1 Predictions Mean                             -1.16532\r\n",
      "trainer/Q1 Predictions Std                               0.81824\r\n",
      "trainer/Q1 Predictions Max                               0.346757\r\n",
      "trainer/Q1 Predictions Min                              -3.38243\r\n",
      "trainer/Q2 Predictions Mean                             -1.16685\r\n",
      "trainer/Q2 Predictions Std                               0.814735\r\n",
      "trainer/Q2 Predictions Max                               0.337549\r\n",
      "trainer/Q2 Predictions Min                              -3.391\r\n",
      "trainer/Q Targets Mean                                  -1.16197\r\n",
      "trainer/Q Targets Std                                    0.811886\r\n",
      "trainer/Q Targets Max                                    0.323271\r\n",
      "trainer/Q Targets Min                                   -3.33345\r\n",
      "trainer/Log Pis Mean                                     2.10112\r\n",
      "trainer/Log Pis Std                                      1.27778\r\n",
      "trainer/Log Pis Max                                      4.82083\r\n",
      "trainer/Log Pis Min                                     -2.91114\r\n",
      "trainer/Policy mu Mean                                   0.0157166\r\n",
      "trainer/Policy mu Std                                    0.384129\r\n",
      "trainer/Policy mu Max                                    1.62763\r\n",
      "trainer/Policy mu Min                                   -2.247\r\n",
      "trainer/Policy log std Mean                             -2.32181\r\n",
      "trainer/Policy log std Std                               0.644294\r\n",
      "trainer/Policy log std Max                              -0.380948\r\n",
      "trainer/Policy log std Min                              -3.45516\r\n",
      "trainer/Alpha                                            0.0200291\r\n",
      "trainer/Alpha Loss                                       0.395436\r\n",
      "exploration/num steps total                           4300\r\n",
      "exploration/num paths total                            215\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.094197\r\n",
      "exploration/Rewards Std                                  0.0654321\r\n",
      "exploration/Rewards Max                                  0.0514576\r\n",
      "exploration/Rewards Min                                 -0.323984\r\n",
      "exploration/Returns Mean                                -1.88394\r\n",
      "exploration/Returns Std                                  0.709701\r\n",
      "exploration/Returns Max                                 -0.867758\r\n",
      "exploration/Returns Min                                 -2.97779\r\n",
      "exploration/Actions Mean                                 0.00695078\r\n",
      "exploration/Actions Std                                  0.103833\r\n",
      "exploration/Actions Max                                  0.354512\r\n",
      "exploration/Actions Min                                 -0.33994\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.88394\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.128603\r\n",
      "exploration/env_infos/final/reward_dist Std              0.164725\r\n",
      "exploration/env_infos/final/reward_dist Max              0.399021\r\n",
      "exploration/env_infos/final/reward_dist Min              7.37369e-12\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00863269\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00828279\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0202386\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.25207e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.308036\r\n",
      "exploration/env_infos/reward_dist Std                    0.343645\r\n",
      "exploration/env_infos/reward_dist Max                    0.963135\r\n",
      "exploration/env_infos/reward_dist Min                    7.37369e-12\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0665602\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0212364\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0432376\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.0965758\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.271151\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.151259\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0627075\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.458969\r\n",
      "exploration/env_infos/reward_energy Mean                -0.126208\r\n",
      "exploration/env_infos/reward_energy Std                  0.0757015\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00884883\r\n",
      "exploration/env_infos/reward_energy Min                 -0.458969\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.145077\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.22228\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.505597\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.202407\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00460953\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00996269\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0177256\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.016997\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0782557\r\n",
      "exploration/env_infos/end_effector_loc Std               0.149347\r\n",
      "exploration/env_infos/end_effector_loc Max               0.505597\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.239575\r\n",
      "evaluation/num steps total                           33000\r\n",
      "evaluation/num paths total                            1650\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0563586\r\n",
      "evaluation/Rewards Std                                   0.0716252\r\n",
      "evaluation/Rewards Max                                   0.134737\r\n",
      "evaluation/Rewards Min                                  -0.322095\r\n",
      "evaluation/Returns Mean                                 -1.12717\r\n",
      "evaluation/Returns Std                                   1.10615\r\n",
      "evaluation/Returns Max                                   1.35982\r\n",
      "evaluation/Returns Min                                  -3.76792\r\n",
      "evaluation/Actions Mean                                  0.00298158\r\n",
      "evaluation/Actions Std                                   0.0607962\r\n",
      "evaluation/Actions Max                                   0.499005\r\n",
      "evaluation/Actions Min                                  -0.549083\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.12717\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.135859\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.226408\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.916\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.78937e-20\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00454096\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00987043\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0407812\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.24736e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.165967\r\n",
      "evaluation/env_infos/reward_dist Std                     0.25265\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996011\r\n",
      "evaluation/env_infos/reward_dist Min                     2.78937e-20\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0311519\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0190405\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00127073\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0939891\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.20768\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.149295\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0154026\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.560535\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0559363\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0654314\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00042501\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.560535\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0291688\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.258787\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.610059\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.731064\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       3.28527e-05\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0090429\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0249502\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0274542\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00610758\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.16858\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.610059\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.731064\r\n",
      "time/data storing (s)                                    0.00608166\r\n",
      "time/evaluation sampling (s)                             1.0068\r\n",
      "time/exploration sampling (s)                            0.138088\r\n",
      "time/logging (s)                                         0.0186885\r\n",
      "time/saving (s)                                          0.0291984\r\n",
      "time/training (s)                                       49.9333\r\n",
      "time/epoch (s)                                          51.1322\r\n",
      "time/total (s)                                        1578.37\r\n",
      "Epoch                                                   32\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:15:46.201633 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 33 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00106044\r\n",
      "trainer/QF2 Loss                                         0.0014073\r\n",
      "trainer/Policy Loss                                      3.12047\r\n",
      "trainer/Q1 Predictions Mean                             -1.2187\r\n",
      "trainer/Q1 Predictions Std                               0.791736\r\n",
      "trainer/Q1 Predictions Max                               0.129513\r\n",
      "trainer/Q1 Predictions Min                              -3.23138\r\n",
      "trainer/Q2 Predictions Mean                             -1.21433\r\n",
      "trainer/Q2 Predictions Std                               0.790048\r\n",
      "trainer/Q2 Predictions Max                               0.129048\r\n",
      "trainer/Q2 Predictions Min                              -3.20763\r\n",
      "trainer/Q Targets Mean                                  -1.22532\r\n",
      "trainer/Q Targets Std                                    0.796831\r\n",
      "trainer/Q Targets Max                                    0.113375\r\n",
      "trainer/Q Targets Min                                   -3.21638\r\n",
      "trainer/Log Pis Mean                                     1.93625\r\n",
      "trainer/Log Pis Std                                      1.26816\r\n",
      "trainer/Log Pis Max                                      4.37243\r\n",
      "trainer/Log Pis Min                                     -1.85465\r\n",
      "trainer/Policy mu Mean                                   0.0466563\r\n",
      "trainer/Policy mu Std                                    0.445422\r\n",
      "trainer/Policy mu Max                                    1.87489\r\n",
      "trainer/Policy mu Min                                   -2.31076\r\n",
      "trainer/Policy log std Mean                             -2.20558\r\n",
      "trainer/Policy log std Std                               0.652911\r\n",
      "trainer/Policy log std Max                              -0.112452\r\n",
      "trainer/Policy log std Min                              -3.36151\r\n",
      "trainer/Alpha                                            0.0210602\r\n",
      "trainer/Alpha Loss                                      -0.246034\r\n",
      "exploration/num steps total                           4400\r\n",
      "exploration/num paths total                            220\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0936856\r\n",
      "exploration/Rewards Std                                  0.101467\r\n",
      "exploration/Rewards Max                                  0.0817833\r\n",
      "exploration/Rewards Min                                 -0.44266\r\n",
      "exploration/Returns Mean                                -1.87371\r\n",
      "exploration/Returns Std                                  1.50963\r\n",
      "exploration/Returns Max                                 -0.404006\r\n",
      "exploration/Returns Min                                 -4.76915\r\n",
      "exploration/Actions Mean                                -0.00752819\r\n",
      "exploration/Actions Std                                  0.181801\r\n",
      "exploration/Actions Max                                  0.639236\r\n",
      "exploration/Actions Min                                 -0.861012\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.87371\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0804179\r\n",
      "exploration/env_infos/final/reward_dist Std              0.159578\r\n",
      "exploration/env_infos/final/reward_dist Max              0.399567\r\n",
      "exploration/env_infos/final/reward_dist Min              9.65539e-11\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00607225\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00450389\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0137269\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000218032\r\n",
      "exploration/env_infos/reward_dist Mean                   0.123569\r\n",
      "exploration/env_infos/reward_dist Std                    0.2048\r\n",
      "exploration/env_infos/reward_dist Max                    0.81524\r\n",
      "exploration/env_infos/reward_dist Min                    9.65539e-11\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.261126\r\n",
      "exploration/env_infos/final/reward_energy Std            0.116865\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0609239\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.383953\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.424089\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.279185\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.143471\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.918443\r\n",
      "exploration/env_infos/reward_energy Mean                -0.200702\r\n",
      "exploration/env_infos/reward_energy Std                  0.161043\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0133796\r\n",
      "exploration/env_infos/reward_energy Min                 -0.918443\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00805063\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.314269\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.423402\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.582148\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00425016\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0174408\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0239209\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0430506\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00675048\r\n",
      "exploration/env_infos/end_effector_loc Std               0.224813\r\n",
      "exploration/env_infos/end_effector_loc Max               0.423402\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.582148\r\n",
      "evaluation/num steps total                           34000\r\n",
      "evaluation/num paths total                            1700\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0488928\r\n",
      "evaluation/Rewards Std                                   0.0891165\r\n",
      "evaluation/Rewards Max                                   0.143938\r\n",
      "evaluation/Rewards Min                                  -0.526458\r\n",
      "evaluation/Returns Mean                                 -0.977856\r\n",
      "evaluation/Returns Std                                   1.30123\r\n",
      "evaluation/Returns Max                                   1.92304\r\n",
      "evaluation/Returns Min                                  -4.24887\r\n",
      "evaluation/Actions Mean                                  0.00136522\r\n",
      "evaluation/Actions Std                                   0.0864364\r\n",
      "evaluation/Actions Max                                   0.786446\r\n",
      "evaluation/Actions Min                                  -0.621422\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.977856\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.139363\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.239595\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.954197\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.36532e-18\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00930598\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0150009\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0724628\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.78735e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.200337\r\n",
      "evaluation/env_infos/reward_dist Std                     0.270465\r\n",
      "evaluation/env_infos/reward_dist Max                     0.984451\r\n",
      "evaluation/env_infos/reward_dist Min                     4.36532e-18\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0373975\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0347664\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00130011\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.171276\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.344561\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.232667\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0366313\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.01857\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0726324\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0983401\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000558624\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.01857\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.070004\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.269845\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.668553\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.584951\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00422243\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0140798\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0393223\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0310711\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0404176\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.186967\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.668553\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.603298\r\n",
      "time/data storing (s)                                    0.00587633\r\n",
      "time/evaluation sampling (s)                             1.00777\r\n",
      "time/exploration sampling (s)                            0.129952\r\n",
      "time/logging (s)                                         0.0213848\r\n",
      "time/saving (s)                                          0.0281863\r\n",
      "time/training (s)                                       47.9265\r\n",
      "time/epoch (s)                                          49.1197\r\n",
      "time/total (s)                                        1627.93\r\n",
      "Epoch                                                   33\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:16:36.211615 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 34 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000947664\r\n",
      "trainer/QF2 Loss                                         0.00130401\r\n",
      "trainer/Policy Loss                                      3.3904\r\n",
      "trainer/Q1 Predictions Mean                             -1.25506\r\n",
      "trainer/Q1 Predictions Std                               0.872149\r\n",
      "trainer/Q1 Predictions Max                               0.37814\r\n",
      "trainer/Q1 Predictions Min                              -3.51131\r\n",
      "trainer/Q2 Predictions Mean                             -1.26847\r\n",
      "trainer/Q2 Predictions Std                               0.87679\r\n",
      "trainer/Q2 Predictions Max                               0.351458\r\n",
      "trainer/Q2 Predictions Min                              -3.53713\r\n",
      "trainer/Q Targets Mean                                  -1.25614\r\n",
      "trainer/Q Targets Std                                    0.872688\r\n",
      "trainer/Q Targets Max                                    0.399104\r\n",
      "trainer/Q Targets Min                                   -3.53901\r\n",
      "trainer/Log Pis Mean                                     2.16034\r\n",
      "trainer/Log Pis Std                                      1.20828\r\n",
      "trainer/Log Pis Max                                      4.54438\r\n",
      "trainer/Log Pis Min                                     -1.63546\r\n",
      "trainer/Policy mu Mean                                  -0.0144581\r\n",
      "trainer/Policy mu Std                                    0.350627\r\n",
      "trainer/Policy mu Max                                    1.66789\r\n",
      "trainer/Policy mu Min                                   -2.13915\r\n",
      "trainer/Policy log std Mean                             -2.36014\r\n",
      "trainer/Policy log std Std                               0.600207\r\n",
      "trainer/Policy log std Max                              -0.359091\r\n",
      "trainer/Policy log std Min                              -3.36773\r\n",
      "trainer/Alpha                                            0.0207443\r\n",
      "trainer/Alpha Loss                                       0.621761\r\n",
      "exploration/num steps total                           4500\r\n",
      "exploration/num paths total                            225\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0486114\r\n",
      "exploration/Rewards Std                                  0.074497\r\n",
      "exploration/Rewards Max                                  0.117918\r\n",
      "exploration/Rewards Min                                 -0.27305\r\n",
      "exploration/Returns Mean                                -0.972228\r\n",
      "exploration/Returns Std                                  0.744085\r\n",
      "exploration/Returns Max                                  0.270247\r\n",
      "exploration/Returns Min                                 -1.91976\r\n",
      "exploration/Actions Mean                                -0.0127432\r\n",
      "exploration/Actions Std                                  0.119518\r\n",
      "exploration/Actions Max                                  0.330514\r\n",
      "exploration/Actions Min                                 -0.496401\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.972228\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.179477\r\n",
      "exploration/env_infos/final/reward_dist Std              0.322229\r\n",
      "exploration/env_infos/final/reward_dist Max              0.821425\r\n",
      "exploration/env_infos/final/reward_dist Min              4.83076e-07\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000456456\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000486798\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00130943\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.50757e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.220671\r\n",
      "exploration/env_infos/reward_dist Std                    0.300425\r\n",
      "exploration/env_infos/reward_dist Max                    0.976395\r\n",
      "exploration/env_infos/reward_dist Min                    4.83076e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.159269\r\n",
      "exploration/env_infos/final/reward_energy Std            0.135029\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0177648\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.414072\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.293564\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.134112\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.169653\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.54333\r\n",
      "exploration/env_infos/reward_energy Mean                -0.142823\r\n",
      "exploration/env_infos/reward_energy Std                  0.0921707\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0112235\r\n",
      "exploration/env_infos/reward_energy Min                 -0.54333\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.14205\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.250845\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.26878\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.474988\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0031704\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0109616\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0111669\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.02482\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0663948\r\n",
      "exploration/env_infos/end_effector_loc Std               0.170885\r\n",
      "exploration/env_infos/end_effector_loc Max               0.272054\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.474988\r\n",
      "evaluation/num steps total                           35000\r\n",
      "evaluation/num paths total                            1750\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0337231\r\n",
      "evaluation/Rewards Std                                   0.074938\r\n",
      "evaluation/Rewards Max                                   0.181573\r\n",
      "evaluation/Rewards Min                                  -0.311414\r\n",
      "evaluation/Returns Mean                                 -0.674461\r\n",
      "evaluation/Returns Std                                   1.04718\r\n",
      "evaluation/Returns Max                                   2.00606\r\n",
      "evaluation/Returns Min                                  -3.21826\r\n",
      "evaluation/Actions Mean                                 -0.00444924\r\n",
      "evaluation/Actions Std                                   0.0817868\r\n",
      "evaluation/Actions Max                                   0.858753\r\n",
      "evaluation/Actions Min                                  -0.444313\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.674461\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.195676\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.285852\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.964799\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.74705e-26\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00502473\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0119267\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0570003\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.99264e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.222218\r\n",
      "evaluation/env_infos/reward_dist Std                     0.300672\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997578\r\n",
      "evaluation/env_infos/reward_dist Min                     8.74705e-26\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0321243\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0246658\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00396774\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0967522\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.316826\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.233238\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0192873\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.10255\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0672727\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0942981\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00228487\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.10255\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00514291\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.284802\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.750129\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.618433\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00348343\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134662\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0429376\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0222156\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0111962\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.187542\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.750129\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.618433\r\n",
      "time/data storing (s)                                    0.00627562\r\n",
      "time/evaluation sampling (s)                             1.13779\r\n",
      "time/exploration sampling (s)                            0.134324\r\n",
      "time/logging (s)                                         0.020059\r\n",
      "time/saving (s)                                          0.0275026\r\n",
      "time/training (s)                                       48.1795\r\n",
      "time/epoch (s)                                          49.5054\r\n",
      "time/total (s)                                        1677.94\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch                                                   34\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 12:17:25.931259 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 35 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00190679\n",
      "trainer/QF2 Loss                                         0.00268198\n",
      "trainer/Policy Loss                                      2.91858\n",
      "trainer/Q1 Predictions Mean                             -1.17971\n",
      "trainer/Q1 Predictions Std                               0.74391\n",
      "trainer/Q1 Predictions Max                               0.556153\n",
      "trainer/Q1 Predictions Min                              -3.91777\n",
      "trainer/Q2 Predictions Mean                             -1.17172\n",
      "trainer/Q2 Predictions Std                               0.749062\n",
      "trainer/Q2 Predictions Max                               0.506241\n",
      "trainer/Q2 Predictions Min                              -3.87219\n",
      "trainer/Q Targets Mean                                  -1.18017\n",
      "trainer/Q Targets Std                                    0.7481\n",
      "trainer/Q Targets Max                                    0.518286\n",
      "trainer/Q Targets Min                                   -3.87531\n",
      "trainer/Log Pis Mean                                     1.80196\n",
      "trainer/Log Pis Std                                      1.26404\n",
      "trainer/Log Pis Max                                      4.37348\n",
      "trainer/Log Pis Min                                     -3.42962\n",
      "trainer/Policy mu Mean                                   0.0471502\n",
      "trainer/Policy mu Std                                    0.540249\n",
      "trainer/Policy mu Max                                    2.42164\n",
      "trainer/Policy mu Min                                   -2.32116\n",
      "trainer/Policy log std Mean                             -2.05889\n",
      "trainer/Policy log std Std                               0.69908\n",
      "trainer/Policy log std Max                              -0.0372509\n",
      "trainer/Policy log std Min                              -3.31811\n",
      "trainer/Alpha                                            0.0205875\n",
      "trainer/Alpha Loss                                      -0.769038\n",
      "exploration/num steps total                           4600\n",
      "exploration/num paths total                            230\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.113071\n",
      "exploration/Rewards Std                                  0.0579603\n",
      "exploration/Rewards Max                                 -0.0116315\n",
      "exploration/Rewards Min                                 -0.267673\n",
      "exploration/Returns Mean                                -2.26141\n",
      "exploration/Returns Std                                  0.891842\n",
      "exploration/Returns Max                                 -1.3495\n",
      "exploration/Returns Min                                 -3.82297\n",
      "exploration/Actions Mean                                -0.00824267\n",
      "exploration/Actions Std                                  0.139306\n",
      "exploration/Actions Max                                  0.689294\n",
      "exploration/Actions Min                                 -0.445472\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.26141\n",
      "exploration/env_infos/final/reward_dist Mean             0.230588\n",
      "exploration/env_infos/final/reward_dist Std              0.310124\n",
      "exploration/env_infos/final/reward_dist Max              0.832592\n",
      "exploration/env_infos/final/reward_dist Min              0.000207061\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0011087\n",
      "exploration/env_infos/initial/reward_dist Std            0.000936904\n",
      "exploration/env_infos/initial/reward_dist Max            0.00229321\n",
      "exploration/env_infos/initial/reward_dist Min            2.13691e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.171405\n",
      "exploration/env_infos/reward_dist Std                    0.245477\n",
      "exploration/env_infos/reward_dist Max                    0.983759\n",
      "exploration/env_infos/reward_dist Min                    2.13691e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.188041\n",
      "exploration/env_infos/final/reward_energy Std            0.118131\n",
      "exploration/env_infos/final/reward_energy Max           -0.0456716\n",
      "exploration/env_infos/final/reward_energy Min           -0.385621\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.302946\n",
      "exploration/env_infos/initial/reward_energy Std          0.202659\n",
      "exploration/env_infos/initial/reward_energy Max         -0.110631\n",
      "exploration/env_infos/initial/reward_energy Min         -0.69444\n",
      "exploration/env_infos/reward_energy Mean                -0.150068\n",
      "exploration/env_infos/reward_energy Std                  0.128171\n",
      "exploration/env_infos/reward_energy Max                 -0.011233\n",
      "exploration/env_infos/reward_energy Min                 -0.69444\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0615604\n",
      "exploration/env_infos/final/end_effector_loc Std         0.192555\n",
      "exploration/env_infos/final/end_effector_loc Max         0.400334\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.205065\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00717691\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0107028\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0344647\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00515869\n",
      "exploration/env_infos/end_effector_loc Mean              0.0552967\n",
      "exploration/env_infos/end_effector_loc Std               0.126692\n",
      "exploration/env_infos/end_effector_loc Max               0.400334\n",
      "exploration/env_infos/end_effector_loc Min              -0.205065\n",
      "evaluation/num steps total                           36000\n",
      "evaluation/num paths total                            1800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.045246\n",
      "evaluation/Rewards Std                                   0.0782357\n",
      "evaluation/Rewards Max                                   0.166323\n",
      "evaluation/Rewards Min                                  -0.489615\n",
      "evaluation/Returns Mean                                 -0.904921\n",
      "evaluation/Returns Std                                   1.08932\n",
      "evaluation/Returns Max                                   2.16155\n",
      "evaluation/Returns Min                                  -3.0775\n",
      "evaluation/Actions Mean                                 -0.00325499\n",
      "evaluation/Actions Std                                   0.087349\n",
      "evaluation/Actions Max                                   0.83199\n",
      "evaluation/Actions Min                                  -0.329447\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.904921\n",
      "evaluation/env_infos/final/reward_dist Mean              0.200545\n",
      "evaluation/env_infos/final/reward_dist Std               0.272965\n",
      "evaluation/env_infos/final/reward_dist Max               0.932941\n",
      "evaluation/env_infos/final/reward_dist Min               2.26228e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00651917\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0115539\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0631858\n",
      "evaluation/env_infos/initial/reward_dist Min             1.34395e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.203665\n",
      "evaluation/env_infos/reward_dist Std                     0.268313\n",
      "evaluation/env_infos/reward_dist Max                     0.995251\n",
      "evaluation/env_infos/reward_dist Min                     2.26228e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0424279\n",
      "evaluation/env_infos/final/reward_energy Std             0.0359503\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00478242\n",
      "evaluation/env_infos/final/reward_energy Min            -0.183692\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.333312\n",
      "evaluation/env_infos/initial/reward_energy Std           0.243473\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0291867\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.985445\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0759842\n",
      "evaluation/env_infos/reward_energy Std                   0.0975054\n",
      "evaluation/env_infos/reward_energy Max                  -0.00127811\n",
      "evaluation/env_infos/reward_energy Min                  -0.985445\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.053553\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.241308\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.673544\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.344347\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00689034\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0128644\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0415995\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0164724\n",
      "evaluation/env_infos/end_effector_loc Mean               0.052966\n",
      "evaluation/env_infos/end_effector_loc Std                0.168964\n",
      "evaluation/env_infos/end_effector_loc Max                0.673544\n",
      "evaluation/env_infos/end_effector_loc Min               -0.397695\n",
      "time/data storing (s)                                    0.00610836\n",
      "time/evaluation sampling (s)                             0.939589\n",
      "time/exploration sampling (s)                            0.122766\n",
      "time/logging (s)                                         0.0194812\n",
      "time/saving (s)                                          0.027346\n",
      "time/training (s)                                       48.1452\n",
      "time/epoch (s)                                          49.2605\n",
      "time/total (s)                                        1727.66\n",
      "Epoch                                                   35\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:18:16.612031 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 36 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00190681\n",
      "trainer/QF2 Loss                                         0.00156953\n",
      "trainer/Policy Loss                                      3.19579\n",
      "trainer/Q1 Predictions Mean                             -1.20017\n",
      "trainer/Q1 Predictions Std                               0.791786\n",
      "trainer/Q1 Predictions Max                               0.113581\n",
      "trainer/Q1 Predictions Min                              -4.04767\n",
      "trainer/Q2 Predictions Mean                             -1.18771\n",
      "trainer/Q2 Predictions Std                               0.787547\n",
      "trainer/Q2 Predictions Max                               0.116903\n",
      "trainer/Q2 Predictions Min                              -4.02638\n",
      "trainer/Q Targets Mean                                  -1.17398\n",
      "trainer/Q Targets Std                                    0.785521\n",
      "trainer/Q Targets Max                                    0.090237\n",
      "trainer/Q Targets Min                                   -3.99424\n",
      "trainer/Log Pis Mean                                     2.05911\n",
      "trainer/Log Pis Std                                      1.30106\n",
      "trainer/Log Pis Max                                      5.00556\n",
      "trainer/Log Pis Min                                     -2.02847\n",
      "trainer/Policy mu Mean                                  -0.0271965\n",
      "trainer/Policy mu Std                                    0.487471\n",
      "trainer/Policy mu Max                                    1.92382\n",
      "trainer/Policy mu Min                                   -2.22314\n",
      "trainer/Policy log std Mean                             -2.23956\n",
      "trainer/Policy log std Std                               0.672561\n",
      "trainer/Policy log std Max                              -0.440405\n",
      "trainer/Policy log std Min                              -3.4474\n",
      "trainer/Alpha                                            0.0199241\n",
      "trainer/Alpha Loss                                       0.231541\n",
      "exploration/num steps total                           4700\n",
      "exploration/num paths total                            235\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.079983\n",
      "exploration/Rewards Std                                  0.0828428\n",
      "exploration/Rewards Max                                  0.0709775\n",
      "exploration/Rewards Min                                 -0.295581\n",
      "exploration/Returns Mean                                -1.59966\n",
      "exploration/Returns Std                                  0.956904\n",
      "exploration/Returns Max                                 -0.552447\n",
      "exploration/Returns Min                                 -3.13555\n",
      "exploration/Actions Mean                                -0.00851227\n",
      "exploration/Actions Std                                  0.10885\n",
      "exploration/Actions Max                                  0.37047\n",
      "exploration/Actions Min                                 -0.365713\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.59966\n",
      "exploration/env_infos/final/reward_dist Mean             0.177673\n",
      "exploration/env_infos/final/reward_dist Std              0.355197\n",
      "exploration/env_infos/final/reward_dist Max              0.888068\n",
      "exploration/env_infos/final/reward_dist Min              1.59783e-16\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0014385\n",
      "exploration/env_infos/initial/reward_dist Std            0.00203678\n",
      "exploration/env_infos/initial/reward_dist Max            0.00544889\n",
      "exploration/env_infos/initial/reward_dist Min            1.1159e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.139307\n",
      "exploration/env_infos/reward_dist Std                    0.229533\n",
      "exploration/env_infos/reward_dist Max                    0.90769\n",
      "exploration/env_infos/reward_dist Min                    1.59783e-16\n",
      "exploration/env_infos/final/reward_energy Mean          -0.134549\n",
      "exploration/env_infos/final/reward_energy Std            0.0615048\n",
      "exploration/env_infos/final/reward_energy Max           -0.05703\n",
      "exploration/env_infos/final/reward_energy Min           -0.242106\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.217989\n",
      "exploration/env_infos/initial/reward_energy Std          0.0641829\n",
      "exploration/env_infos/initial/reward_energy Max         -0.115566\n",
      "exploration/env_infos/initial/reward_energy Min         -0.30936\n",
      "exploration/env_infos/reward_energy Mean                -0.132558\n",
      "exploration/env_infos/reward_energy Std                  0.0791846\n",
      "exploration/env_infos/reward_energy Max                 -0.0114799\n",
      "exploration/env_infos/reward_energy Min                 -0.416476\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0811839\n",
      "exploration/env_infos/final/end_effector_loc Std         0.336451\n",
      "exploration/env_infos/final/end_effector_loc Max         0.292163\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.677637\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000595201\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00801213\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0129974\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0102224\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0390525\n",
      "exploration/env_infos/end_effector_loc Std               0.203154\n",
      "exploration/env_infos/end_effector_loc Max               0.303477\n",
      "exploration/env_infos/end_effector_loc Min              -0.677637\n",
      "evaluation/num steps total                           37000\n",
      "evaluation/num paths total                            1850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0506229\n",
      "evaluation/Rewards Std                                   0.0697219\n",
      "evaluation/Rewards Max                                   0.150981\n",
      "evaluation/Rewards Min                                  -0.32217\n",
      "evaluation/Returns Mean                                 -1.01246\n",
      "evaluation/Returns Std                                   1.00525\n",
      "evaluation/Returns Max                                   1.7155\n",
      "evaluation/Returns Min                                  -2.98728\n",
      "evaluation/Actions Mean                                 -0.00129044\n",
      "evaluation/Actions Std                                   0.0742394\n",
      "evaluation/Actions Max                                   0.893973\n",
      "evaluation/Actions Min                                  -0.372498\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.01246\n",
      "evaluation/env_infos/final/reward_dist Mean              0.202942\n",
      "evaluation/env_infos/final/reward_dist Std               0.273794\n",
      "evaluation/env_infos/final/reward_dist Max               0.942648\n",
      "evaluation/env_infos/final/reward_dist Min               1.3012e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00805948\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0162103\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0870908\n",
      "evaluation/env_infos/initial/reward_dist Min             1.11585e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.219177\n",
      "evaluation/env_infos/reward_dist Std                     0.293175\n",
      "evaluation/env_infos/reward_dist Max                     0.997997\n",
      "evaluation/env_infos/reward_dist Min                     1.3012e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.035176\n",
      "evaluation/env_infos/final/reward_energy Std             0.03389\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00111647\n",
      "evaluation/env_infos/final/reward_energy Min            -0.180049\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.254227\n",
      "evaluation/env_infos/initial/reward_energy Std           0.237574\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00575773\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.05462\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0618375\n",
      "evaluation/env_infos/reward_energy Std                   0.084867\n",
      "evaluation/env_infos/reward_energy Max                  -0.000642833\n",
      "evaluation/env_infos/reward_energy Min                  -1.05462\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0245184\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.258362\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.762181\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.450878\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00276324\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0119877\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0446986\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0186249\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0210745\n",
      "evaluation/env_infos/end_effector_loc Std                0.171022\n",
      "evaluation/env_infos/end_effector_loc Max                0.762181\n",
      "evaluation/env_infos/end_effector_loc Min               -0.467864\n",
      "time/data storing (s)                                    0.00607971\n",
      "time/evaluation sampling (s)                             1.11466\n",
      "time/exploration sampling (s)                            0.190079\n",
      "time/logging (s)                                         0.0204772\n",
      "time/saving (s)                                          0.0287836\n",
      "time/training (s)                                       48.8488\n",
      "time/epoch (s)                                          50.2089\n",
      "time/total (s)                                        1778.34\n",
      "Epoch                                                   36\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:19:08.889537 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 37 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0012275\n",
      "trainer/QF2 Loss                                         0.0011068\n",
      "trainer/Policy Loss                                      3.03346\n",
      "trainer/Q1 Predictions Mean                             -1.17193\n",
      "trainer/Q1 Predictions Std                               0.794355\n",
      "trainer/Q1 Predictions Max                               0.630812\n",
      "trainer/Q1 Predictions Min                              -3.2881\n",
      "trainer/Q2 Predictions Mean                             -1.15031\n",
      "trainer/Q2 Predictions Std                               0.786135\n",
      "trainer/Q2 Predictions Max                               0.62529\n",
      "trainer/Q2 Predictions Min                              -3.21871\n",
      "trainer/Q Targets Mean                                  -1.15738\n",
      "trainer/Q Targets Std                                    0.784744\n",
      "trainer/Q Targets Max                                    0.635002\n",
      "trainer/Q Targets Min                                   -3.22546\n",
      "trainer/Log Pis Mean                                     1.90624\n",
      "trainer/Log Pis Std                                      1.39768\n",
      "trainer/Log Pis Max                                      4.47403\n",
      "trainer/Log Pis Min                                     -3.2887\n",
      "trainer/Policy mu Mean                                   0.00813147\n",
      "trainer/Policy mu Std                                    0.453109\n",
      "trainer/Policy mu Max                                    2.20221\n",
      "trainer/Policy mu Min                                   -2.4272\n",
      "trainer/Policy log std Mean                             -2.21474\n",
      "trainer/Policy log std Std                               0.68483\n",
      "trainer/Policy log std Max                              -0.11016\n",
      "trainer/Policy log std Min                              -3.30421\n",
      "trainer/Alpha                                            0.0208857\n",
      "trainer/Alpha Loss                                      -0.362661\n",
      "exploration/num steps total                           4800\n",
      "exploration/num paths total                            240\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0986442\n",
      "exploration/Rewards Std                                  0.0732685\n",
      "exploration/Rewards Max                                  0.0597477\n",
      "exploration/Rewards Min                                 -0.250596\n",
      "exploration/Returns Mean                                -1.97288\n",
      "exploration/Returns Std                                  1.22987\n",
      "exploration/Returns Max                                 -0.334852\n",
      "exploration/Returns Min                                 -3.45226\n",
      "exploration/Actions Mean                                -0.0147134\n",
      "exploration/Actions Std                                  0.0811686\n",
      "exploration/Actions Max                                  0.253843\n",
      "exploration/Actions Min                                 -0.237761\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.97288\n",
      "exploration/env_infos/final/reward_dist Mean             0.0871236\n",
      "exploration/env_infos/final/reward_dist Std              0.111589\n",
      "exploration/env_infos/final/reward_dist Max              0.275197\n",
      "exploration/env_infos/final/reward_dist Min              1.38361e-29\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00938514\n",
      "exploration/env_infos/initial/reward_dist Std            0.0127285\n",
      "exploration/env_infos/initial/reward_dist Max            0.0341403\n",
      "exploration/env_infos/initial/reward_dist Min            0.000139259\n",
      "exploration/env_infos/reward_dist Mean                   0.189429\n",
      "exploration/env_infos/reward_dist Std                    0.231945\n",
      "exploration/env_infos/reward_dist Max                    0.845895\n",
      "exploration/env_infos/reward_dist Min                    1.38361e-29\n",
      "exploration/env_infos/final/reward_energy Mean          -0.104393\n",
      "exploration/env_infos/final/reward_energy Std            0.0262641\n",
      "exploration/env_infos/final/reward_energy Max           -0.0689244\n",
      "exploration/env_infos/final/reward_energy Min           -0.140386\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.149366\n",
      "exploration/env_infos/initial/reward_energy Std          0.0573045\n",
      "exploration/env_infos/initial/reward_energy Max         -0.100717\n",
      "exploration/env_infos/initial/reward_energy Min         -0.257243\n",
      "exploration/env_infos/reward_energy Mean                -0.101312\n",
      "exploration/env_infos/reward_energy Std                  0.0578408\n",
      "exploration/env_infos/reward_energy Max                 -0.0109675\n",
      "exploration/env_infos/reward_energy Min                 -0.26666\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.134053\n",
      "exploration/env_infos/final/end_effector_loc Std         0.321202\n",
      "exploration/env_infos/final/end_effector_loc Max         0.324182\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.607665\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00077419\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00560295\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0126921\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00538696\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0426176\n",
      "exploration/env_infos/end_effector_loc Std               0.192469\n",
      "exploration/env_infos/end_effector_loc Max               0.324182\n",
      "exploration/env_infos/end_effector_loc Min              -0.607665\n",
      "evaluation/num steps total                           38000\n",
      "evaluation/num paths total                            1900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0696375\n",
      "evaluation/Rewards Std                                   0.0887552\n",
      "evaluation/Rewards Max                                   0.173111\n",
      "evaluation/Rewards Min                                  -0.604474\n",
      "evaluation/Returns Mean                                 -1.39275\n",
      "evaluation/Returns Std                                   1.43434\n",
      "evaluation/Returns Max                                   2.00497\n",
      "evaluation/Returns Min                                  -5.71016\n",
      "evaluation/Actions Mean                                 -0.00339587\n",
      "evaluation/Actions Std                                   0.0704158\n",
      "evaluation/Actions Max                                   0.692833\n",
      "evaluation/Actions Min                                  -0.419993\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.39275\n",
      "evaluation/env_infos/final/reward_dist Mean              0.132139\n",
      "evaluation/env_infos/final/reward_dist Std               0.246595\n",
      "evaluation/env_infos/final/reward_dist Max               0.972497\n",
      "evaluation/env_infos/final/reward_dist Min               2.76847e-37\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00808336\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0109328\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0434286\n",
      "evaluation/env_infos/initial/reward_dist Min             1.26139e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.18475\n",
      "evaluation/env_infos/reward_dist Std                     0.271559\n",
      "evaluation/env_infos/reward_dist Max                     0.99938\n",
      "evaluation/env_infos/reward_dist Min                     2.76847e-37\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0329717\n",
      "evaluation/env_infos/final/reward_energy Std             0.0218978\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00499213\n",
      "evaluation/env_infos/final/reward_energy Min            -0.102047\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.236522\n",
      "evaluation/env_infos/initial/reward_energy Std           0.162452\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00898368\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.69796\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0631596\n",
      "evaluation/env_infos/reward_energy Std                   0.0771408\n",
      "evaluation/env_infos/reward_energy Max                  -0.000465087\n",
      "evaluation/env_infos/reward_energy Min                  -0.69796\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0328275\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.267141\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.711325\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.599732\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00265138\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00979217\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0346416\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0209997\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0308716\n",
      "evaluation/env_infos/end_effector_loc Std                0.179805\n",
      "evaluation/env_infos/end_effector_loc Max                0.711325\n",
      "evaluation/env_infos/end_effector_loc Min               -0.599732\n",
      "time/data storing (s)                                    0.00616738\n",
      "time/evaluation sampling (s)                             1.23947\n",
      "time/exploration sampling (s)                            0.126401\n",
      "time/logging (s)                                         0.0196587\n",
      "time/saving (s)                                          0.0289687\n",
      "time/training (s)                                       50.2883\n",
      "time/epoch (s)                                          51.709\n",
      "time/total (s)                                        1830.61\n",
      "Epoch                                                   37\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:20:01.998953 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 38 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00169449\r\n",
      "trainer/QF2 Loss                                         0.00117593\r\n",
      "trainer/Policy Loss                                      3.06788\r\n",
      "trainer/Q1 Predictions Mean                             -1.08602\r\n",
      "trainer/Q1 Predictions Std                               0.785696\r\n",
      "trainer/Q1 Predictions Max                               0.491625\r\n",
      "trainer/Q1 Predictions Min                              -3.55427\r\n",
      "trainer/Q2 Predictions Mean                             -1.09752\r\n",
      "trainer/Q2 Predictions Std                               0.785389\r\n",
      "trainer/Q2 Predictions Max                               0.509928\r\n",
      "trainer/Q2 Predictions Min                              -3.59127\r\n",
      "trainer/Q Targets Mean                                  -1.09306\r\n",
      "trainer/Q Targets Std                                    0.786608\r\n",
      "trainer/Q Targets Max                                    0.562517\r\n",
      "trainer/Q Targets Min                                   -3.44697\r\n",
      "trainer/Log Pis Mean                                     2.00164\r\n",
      "trainer/Log Pis Std                                      1.41608\r\n",
      "trainer/Log Pis Max                                      4.91137\r\n",
      "trainer/Log Pis Min                                     -6.32968\r\n",
      "trainer/Policy mu Mean                                  -0.0564979\r\n",
      "trainer/Policy mu Std                                    0.476844\r\n",
      "trainer/Policy mu Max                                    2.10562\r\n",
      "trainer/Policy mu Min                                   -2.4583\r\n",
      "trainer/Policy log std Mean                             -2.26717\r\n",
      "trainer/Policy log std Std                               0.686914\r\n",
      "trainer/Policy log std Max                               0.141248\r\n",
      "trainer/Policy log std Min                              -3.4371\r\n",
      "trainer/Alpha                                            0.0212059\r\n",
      "trainer/Alpha Loss                                       0.00632431\r\n",
      "exploration/num steps total                           4900\r\n",
      "exploration/num paths total                            245\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0833946\r\n",
      "exploration/Rewards Std                                  0.0985882\r\n",
      "exploration/Rewards Max                                  0.0952183\r\n",
      "exploration/Rewards Min                                 -0.35506\r\n",
      "exploration/Returns Mean                                -1.66789\r\n",
      "exploration/Returns Std                                  1.78381\r\n",
      "exploration/Returns Max                                  0.914531\r\n",
      "exploration/Returns Min                                 -4.33486\r\n",
      "exploration/Actions Mean                                 0.00283128\r\n",
      "exploration/Actions Std                                  0.122397\r\n",
      "exploration/Actions Max                                  0.523131\r\n",
      "exploration/Actions Min                                 -0.23875\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.66789\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.10154\r\n",
      "exploration/env_infos/final/reward_dist Std              0.111275\r\n",
      "exploration/env_infos/final/reward_dist Max              0.255585\r\n",
      "exploration/env_infos/final/reward_dist Min              3.62666e-27\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0005687\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000443815\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00102361\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.79755e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.127941\r\n",
      "exploration/env_infos/reward_dist Std                    0.195089\r\n",
      "exploration/env_infos/reward_dist Max                    0.794288\r\n",
      "exploration/env_infos/reward_dist Min                    3.62666e-27\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.146643\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0737586\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0455493\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.230213\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.285778\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.166149\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.117346\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.531401\r\n",
      "exploration/env_infos/reward_energy Mean                -0.146084\r\n",
      "exploration/env_infos/reward_energy Std                  0.0929393\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00766479\r\n",
      "exploration/env_infos/reward_energy Min                 -0.531401\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.108901\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.213433\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.526665\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.214435\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00587725\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.010102\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0261566\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00967151\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.066355\r\n",
      "exploration/env_infos/end_effector_loc Std               0.140951\r\n",
      "exploration/env_infos/end_effector_loc Max               0.526665\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.214435\r\n",
      "evaluation/num steps total                           39000\r\n",
      "evaluation/num paths total                            1950\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0480165\r\n",
      "evaluation/Rewards Std                                   0.0782285\r\n",
      "evaluation/Rewards Max                                   0.126164\r\n",
      "evaluation/Rewards Min                                  -0.41994\r\n",
      "evaluation/Returns Mean                                 -0.96033\r\n",
      "evaluation/Returns Std                                   1.18413\r\n",
      "evaluation/Returns Max                                   1.46737\r\n",
      "evaluation/Returns Min                                  -4.37367\r\n",
      "evaluation/Actions Mean                                 -0.00414721\r\n",
      "evaluation/Actions Std                                   0.0802625\r\n",
      "evaluation/Actions Max                                   0.694843\r\n",
      "evaluation/Actions Min                                  -0.459985\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.96033\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.128172\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.191939\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.687729\r\n",
      "evaluation/env_infos/final/reward_dist Min               7.50658e-55\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00474013\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00666446\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0236359\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.32193e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.21812\r\n",
      "evaluation/env_infos/reward_dist Std                     0.280866\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997752\r\n",
      "evaluation/env_infos/reward_dist Min                     7.50658e-55\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0434787\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0336598\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00432534\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.196545\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.291472\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.192055\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.003761\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.777198\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0723119\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0876899\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.001881\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.777198\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00148153\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.259823\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.916763\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.518816\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00192197\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121905\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0347421\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0229992\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0104959\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.177999\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.916763\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.518816\r\n",
      "time/data storing (s)                                    0.00594254\r\n",
      "time/evaluation sampling (s)                             1.02466\r\n",
      "time/exploration sampling (s)                            0.121786\r\n",
      "time/logging (s)                                         0.0203044\r\n",
      "time/saving (s)                                          0.0430995\r\n",
      "time/training (s)                                       51.3954\r\n",
      "time/epoch (s)                                          52.6112\r\n",
      "time/total (s)                                        1883.73\r\n",
      "Epoch                                                   38\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:20:56.699104 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 39 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00465176\n",
      "trainer/QF2 Loss                                         0.00369048\n",
      "trainer/Policy Loss                                      3.05001\n",
      "trainer/Q1 Predictions Mean                             -1.10591\n",
      "trainer/Q1 Predictions Std                               0.834151\n",
      "trainer/Q1 Predictions Max                               0.562254\n",
      "trainer/Q1 Predictions Min                              -3.53048\n",
      "trainer/Q2 Predictions Mean                             -1.11069\n",
      "trainer/Q2 Predictions Std                               0.838387\n",
      "trainer/Q2 Predictions Max                               0.533187\n",
      "trainer/Q2 Predictions Min                              -3.54544\n",
      "trainer/Q Targets Mean                                  -1.12044\n",
      "trainer/Q Targets Std                                    0.850682\n",
      "trainer/Q Targets Max                                    0.573024\n",
      "trainer/Q Targets Min                                   -3.82018\n",
      "trainer/Log Pis Mean                                     1.96661\n",
      "trainer/Log Pis Std                                      1.29437\n",
      "trainer/Log Pis Max                                      4.51742\n",
      "trainer/Log Pis Min                                     -3.32886\n",
      "trainer/Policy mu Mean                                  -0.0369327\n",
      "trainer/Policy mu Std                                    0.40373\n",
      "trainer/Policy mu Max                                    1.93227\n",
      "trainer/Policy mu Min                                   -1.95297\n",
      "trainer/Policy log std Mean                             -2.26104\n",
      "trainer/Policy log std Std                               0.630909\n",
      "trainer/Policy log std Max                               0.0679319\n",
      "trainer/Policy log std Min                              -3.25211\n",
      "trainer/Alpha                                            0.0211491\n",
      "trainer/Alpha Loss                                      -0.128748\n",
      "exploration/num steps total                           5000\n",
      "exploration/num paths total                            250\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.096611\n",
      "exploration/Rewards Std                                  0.0575284\n",
      "exploration/Rewards Max                                  0.00412414\n",
      "exploration/Rewards Min                                 -0.347596\n",
      "exploration/Returns Mean                                -1.93222\n",
      "exploration/Returns Std                                  0.627367\n",
      "exploration/Returns Max                                 -0.980231\n",
      "exploration/Returns Min                                 -2.92456\n",
      "exploration/Actions Mean                                 0.00284611\n",
      "exploration/Actions Std                                  0.172813\n",
      "exploration/Actions Max                                  0.663004\n",
      "exploration/Actions Min                                 -0.495885\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.93222\n",
      "exploration/env_infos/final/reward_dist Mean             0.0084376\n",
      "exploration/env_infos/final/reward_dist Std              0.00801189\n",
      "exploration/env_infos/final/reward_dist Max              0.0207392\n",
      "exploration/env_infos/final/reward_dist Min              1.21597e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00158155\n",
      "exploration/env_infos/initial/reward_dist Std            0.00215424\n",
      "exploration/env_infos/initial/reward_dist Max            0.00577098\n",
      "exploration/env_infos/initial/reward_dist Min            7.05543e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.144398\n",
      "exploration/env_infos/reward_dist Std                    0.204194\n",
      "exploration/env_infos/reward_dist Max                    0.77883\n",
      "exploration/env_infos/reward_dist Min                    1.21597e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.104574\n",
      "exploration/env_infos/final/reward_energy Std            0.050994\n",
      "exploration/env_infos/final/reward_energy Max           -0.0515202\n",
      "exploration/env_infos/final/reward_energy Min           -0.199176\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.328195\n",
      "exploration/env_infos/initial/reward_energy Std          0.100919\n",
      "exploration/env_infos/initial/reward_energy Max         -0.155307\n",
      "exploration/env_infos/initial/reward_energy Min         -0.455193\n",
      "exploration/env_infos/reward_energy Mean                -0.201688\n",
      "exploration/env_infos/reward_energy Std                  0.138083\n",
      "exploration/env_infos/reward_energy Max                 -0.00987293\n",
      "exploration/env_infos/reward_energy Min                 -0.666892\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0602884\n",
      "exploration/env_infos/final/end_effector_loc Std         0.224888\n",
      "exploration/env_infos/final/end_effector_loc Max         0.461392\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.183168\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00121579\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0120786\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0202118\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0188079\n",
      "exploration/env_infos/end_effector_loc Mean              0.0127256\n",
      "exploration/env_infos/end_effector_loc Std               0.176579\n",
      "exploration/env_infos/end_effector_loc Max               0.461392\n",
      "exploration/env_infos/end_effector_loc Min              -0.326562\n",
      "evaluation/num steps total                           40000\n",
      "evaluation/num paths total                            2000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0448132\n",
      "evaluation/Rewards Std                                   0.0731869\n",
      "evaluation/Rewards Max                                   0.189962\n",
      "evaluation/Rewards Min                                  -0.441261\n",
      "evaluation/Returns Mean                                 -0.896264\n",
      "evaluation/Returns Std                                   1.05082\n",
      "evaluation/Returns Max                                   1.57656\n",
      "evaluation/Returns Min                                  -2.66698\n",
      "evaluation/Actions Mean                                 -0.00319786\n",
      "evaluation/Actions Std                                   0.0712629\n",
      "evaluation/Actions Max                                   0.599774\n",
      "evaluation/Actions Min                                  -0.468887\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.896264\n",
      "evaluation/env_infos/final/reward_dist Mean              0.15323\n",
      "evaluation/env_infos/final/reward_dist Std               0.254933\n",
      "evaluation/env_infos/final/reward_dist Max               0.966732\n",
      "evaluation/env_infos/final/reward_dist Min               2.53813e-38\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0120656\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0164234\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0627214\n",
      "evaluation/env_infos/initial/reward_dist Min             1.44918e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.214486\n",
      "evaluation/env_infos/reward_dist Std                     0.275565\n",
      "evaluation/env_infos/reward_dist Max                     0.999193\n",
      "evaluation/env_infos/reward_dist Min                     2.53813e-38\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.035642\n",
      "evaluation/env_infos/final/reward_energy Std             0.0300165\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00204133\n",
      "evaluation/env_infos/final/reward_energy Min            -0.167954\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.244012\n",
      "evaluation/env_infos/initial/reward_energy Std           0.15927\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0111556\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.613234\n",
      "evaluation/env_infos/reward_energy Mean                 -0.066571\n",
      "evaluation/env_infos/reward_energy Std                   0.0757995\n",
      "evaluation/env_infos/reward_energy Max                  -0.000792848\n",
      "evaluation/env_infos/reward_energy Min                  -0.613234\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0476974\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.266261\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.501465\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.749493\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000235358\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0102995\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0299887\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0234443\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0219597\n",
      "evaluation/env_infos/end_effector_loc Std                0.175221\n",
      "evaluation/env_infos/end_effector_loc Max                0.501465\n",
      "evaluation/env_infos/end_effector_loc Min               -0.749493\n",
      "time/data storing (s)                                    0.00596318\n",
      "time/evaluation sampling (s)                             1.07362\n",
      "time/exploration sampling (s)                            0.145088\n",
      "time/logging (s)                                         0.0195276\n",
      "time/saving (s)                                          0.0277462\n",
      "time/training (s)                                       52.9029\n",
      "time/epoch (s)                                          54.1749\n",
      "time/total (s)                                        1938.41\n",
      "Epoch                                                   39\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:21:47.800613 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 40 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00100869\n",
      "trainer/QF2 Loss                                         0.00102457\n",
      "trainer/Policy Loss                                      3.49966\n",
      "trainer/Q1 Predictions Mean                             -1.2473\n",
      "trainer/Q1 Predictions Std                               0.906431\n",
      "trainer/Q1 Predictions Max                               0.882894\n",
      "trainer/Q1 Predictions Min                              -3.76796\n",
      "trainer/Q2 Predictions Mean                             -1.23101\n",
      "trainer/Q2 Predictions Std                               0.901847\n",
      "trainer/Q2 Predictions Max                               0.860766\n",
      "trainer/Q2 Predictions Min                              -3.76169\n",
      "trainer/Q Targets Mean                                  -1.246\n",
      "trainer/Q Targets Std                                    0.903441\n",
      "trainer/Q Targets Max                                    0.862993\n",
      "trainer/Q Targets Min                                   -3.78016\n",
      "trainer/Log Pis Mean                                     2.30713\n",
      "trainer/Log Pis Std                                      1.35306\n",
      "trainer/Log Pis Max                                      4.91661\n",
      "trainer/Log Pis Min                                     -1.78833\n",
      "trainer/Policy mu Mean                                  -0.0353147\n",
      "trainer/Policy mu Std                                    0.439661\n",
      "trainer/Policy mu Max                                    2.21368\n",
      "trainer/Policy mu Min                                   -2.23778\n",
      "trainer/Policy log std Mean                             -2.33871\n",
      "trainer/Policy log std Std                               0.70751\n",
      "trainer/Policy log std Max                               0.0226979\n",
      "trainer/Policy log std Min                              -3.56064\n",
      "trainer/Alpha                                            0.0202971\n",
      "trainer/Alpha Loss                                       1.19701\n",
      "exploration/num steps total                           5100\n",
      "exploration/num paths total                            255\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.104125\n",
      "exploration/Rewards Std                                  0.0663407\n",
      "exploration/Rewards Max                                  0.0459232\n",
      "exploration/Rewards Min                                 -0.238011\n",
      "exploration/Returns Mean                                -2.0825\n",
      "exploration/Returns Std                                  1.00011\n",
      "exploration/Returns Max                                 -0.312036\n",
      "exploration/Returns Min                                 -3.0732\n",
      "exploration/Actions Mean                                -0.000739965\n",
      "exploration/Actions Std                                  0.109163\n",
      "exploration/Actions Max                                  0.312448\n",
      "exploration/Actions Min                                 -0.436365\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.0825\n",
      "exploration/env_infos/final/reward_dist Mean             0.30907\n",
      "exploration/env_infos/final/reward_dist Std              0.285957\n",
      "exploration/env_infos/final/reward_dist Max              0.652643\n",
      "exploration/env_infos/final/reward_dist Min              2.50373e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000692348\n",
      "exploration/env_infos/initial/reward_dist Std            0.00084336\n",
      "exploration/env_infos/initial/reward_dist Max            0.0022963\n",
      "exploration/env_infos/initial/reward_dist Min            3.01038e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.186254\n",
      "exploration/env_infos/reward_dist Std                    0.300881\n",
      "exploration/env_infos/reward_dist Max                    0.995926\n",
      "exploration/env_infos/reward_dist Min                    2.50373e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.057034\n",
      "exploration/env_infos/final/reward_energy Std            0.0201156\n",
      "exploration/env_infos/final/reward_energy Max           -0.0323047\n",
      "exploration/env_infos/final/reward_energy Min           -0.0921777\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.194034\n",
      "exploration/env_infos/initial/reward_energy Std          0.12361\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0853971\n",
      "exploration/env_infos/initial/reward_energy Min         -0.357288\n",
      "exploration/env_infos/reward_energy Mean                -0.125262\n",
      "exploration/env_infos/reward_energy Std                  0.0902418\n",
      "exploration/env_infos/reward_energy Max                 -0.0116367\n",
      "exploration/env_infos/reward_energy Min                 -0.500425\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0414172\n",
      "exploration/env_infos/final/end_effector_loc Std         0.220476\n",
      "exploration/env_infos/final/end_effector_loc Max         0.30641\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.453635\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00438109\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00685323\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00423981\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0178521\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0405139\n",
      "exploration/env_infos/end_effector_loc Std               0.123005\n",
      "exploration/env_infos/end_effector_loc Max               0.30641\n",
      "exploration/env_infos/end_effector_loc Min              -0.453635\n",
      "evaluation/num steps total                           41000\n",
      "evaluation/num paths total                            2050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0483082\n",
      "evaluation/Rewards Std                                   0.0653311\n",
      "evaluation/Rewards Max                                   0.139353\n",
      "evaluation/Rewards Min                                  -0.270565\n",
      "evaluation/Returns Mean                                 -0.966165\n",
      "evaluation/Returns Std                                   0.966574\n",
      "evaluation/Returns Max                                   1.26566\n",
      "evaluation/Returns Min                                  -3.05738\n",
      "evaluation/Actions Mean                                 -0.00185485\n",
      "evaluation/Actions Std                                   0.0636286\n",
      "evaluation/Actions Max                                   0.726745\n",
      "evaluation/Actions Min                                  -0.43009\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.966165\n",
      "evaluation/env_infos/final/reward_dist Mean              0.216233\n",
      "evaluation/env_infos/final/reward_dist Std               0.270733\n",
      "evaluation/env_infos/final/reward_dist Max               0.789602\n",
      "evaluation/env_infos/final/reward_dist Min               1.14164e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00443151\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00670248\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0310264\n",
      "evaluation/env_infos/initial/reward_dist Min             1.03189e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.183887\n",
      "evaluation/env_infos/reward_dist Std                     0.259339\n",
      "evaluation/env_infos/reward_dist Max                     0.995598\n",
      "evaluation/env_infos/reward_dist Min                     1.14164e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0329307\n",
      "evaluation/env_infos/final/reward_energy Std             0.0367194\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00326253\n",
      "evaluation/env_infos/final/reward_energy Min            -0.181805\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.199967\n",
      "evaluation/env_infos/initial/reward_energy Std           0.181478\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0120818\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.859322\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0528298\n",
      "evaluation/env_infos/reward_energy Std                   0.072891\n",
      "evaluation/env_infos/reward_energy Max                  -0.000620618\n",
      "evaluation/env_infos/reward_energy Min                  -0.859322\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0183471\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.224637\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.5635\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.462296\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00197038\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00934178\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0363372\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0215045\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0196911\n",
      "evaluation/env_infos/end_effector_loc Std                0.147722\n",
      "evaluation/env_infos/end_effector_loc Max                0.5635\n",
      "evaluation/env_infos/end_effector_loc Min               -0.462296\n",
      "time/data storing (s)                                    0.0060424\n",
      "time/evaluation sampling (s)                             1.14703\n",
      "time/exploration sampling (s)                            0.120568\n",
      "time/logging (s)                                         0.023381\n",
      "time/saving (s)                                          0.0345917\n",
      "time/training (s)                                       49.2162\n",
      "time/epoch (s)                                          50.5478\n",
      "time/total (s)                                        1989.52\n",
      "Epoch                                                   40\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:22:38.256958 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 41 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00168396\n",
      "trainer/QF2 Loss                                         0.00220799\n",
      "trainer/Policy Loss                                      3.14408\n",
      "trainer/Q1 Predictions Mean                             -1.28074\n",
      "trainer/Q1 Predictions Std                               0.927691\n",
      "trainer/Q1 Predictions Max                               0.870419\n",
      "trainer/Q1 Predictions Min                              -4.29965\n",
      "trainer/Q2 Predictions Mean                             -1.26605\n",
      "trainer/Q2 Predictions Std                               0.915281\n",
      "trainer/Q2 Predictions Max                               0.847792\n",
      "trainer/Q2 Predictions Min                              -4.25999\n",
      "trainer/Q Targets Mean                                  -1.272\n",
      "trainer/Q Targets Std                                    0.924376\n",
      "trainer/Q Targets Max                                    0.886944\n",
      "trainer/Q Targets Min                                   -4.3959\n",
      "trainer/Log Pis Mean                                     1.92489\n",
      "trainer/Log Pis Std                                      1.38235\n",
      "trainer/Log Pis Max                                      4.72843\n",
      "trainer/Log Pis Min                                     -3.56524\n",
      "trainer/Policy mu Mean                                  -0.0810843\n",
      "trainer/Policy mu Std                                    0.525227\n",
      "trainer/Policy mu Max                                    2.03944\n",
      "trainer/Policy mu Min                                   -2.09329\n",
      "trainer/Policy log std Mean                             -2.15478\n",
      "trainer/Policy log std Std                               0.659679\n",
      "trainer/Policy log std Max                               0.168027\n",
      "trainer/Policy log std Min                              -3.37378\n",
      "trainer/Alpha                                            0.0194595\n",
      "trainer/Alpha Loss                                      -0.295847\n",
      "exploration/num steps total                           5200\n",
      "exploration/num paths total                            260\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0680671\n",
      "exploration/Rewards Std                                  0.0969515\n",
      "exploration/Rewards Max                                  0.155049\n",
      "exploration/Rewards Min                                 -0.385479\n",
      "exploration/Returns Mean                                -1.36134\n",
      "exploration/Returns Std                                  1.52177\n",
      "exploration/Returns Max                                  1.67753\n",
      "exploration/Returns Min                                 -2.23076\n",
      "exploration/Actions Mean                                -0.00343716\n",
      "exploration/Actions Std                                  0.151058\n",
      "exploration/Actions Max                                  0.541168\n",
      "exploration/Actions Min                                 -0.619518\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.36134\n",
      "exploration/env_infos/final/reward_dist Mean             0.107333\n",
      "exploration/env_infos/final/reward_dist Std              0.214655\n",
      "exploration/env_infos/final/reward_dist Max              0.536642\n",
      "exploration/env_infos/final/reward_dist Min              1.91326e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00750519\n",
      "exploration/env_infos/initial/reward_dist Std            0.0095002\n",
      "exploration/env_infos/initial/reward_dist Max            0.0236588\n",
      "exploration/env_infos/initial/reward_dist Min            8.15879e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.184163\n",
      "exploration/env_infos/reward_dist Std                    0.268832\n",
      "exploration/env_infos/reward_dist Max                    0.963553\n",
      "exploration/env_infos/reward_dist Min                    1.91326e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0838806\n",
      "exploration/env_infos/final/reward_energy Std            0.0528358\n",
      "exploration/env_infos/final/reward_energy Max           -0.00993333\n",
      "exploration/env_infos/final/reward_energy Min           -0.171211\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.277121\n",
      "exploration/env_infos/initial/reward_energy Std          0.117164\n",
      "exploration/env_infos/initial/reward_energy Max         -0.136926\n",
      "exploration/env_infos/initial/reward_energy Min         -0.451496\n",
      "exploration/env_infos/reward_energy Mean                -0.164643\n",
      "exploration/env_infos/reward_energy Std                  0.13621\n",
      "exploration/env_infos/reward_energy Max                 -0.00993333\n",
      "exploration/env_infos/reward_energy Min                 -0.779586\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0938977\n",
      "exploration/env_infos/final/end_effector_loc Std         0.334133\n",
      "exploration/env_infos/final/end_effector_loc Max         0.576434\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.528275\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00300656\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0102037\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0174867\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0186464\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0499037\n",
      "exploration/env_infos/end_effector_loc Std               0.214938\n",
      "exploration/env_infos/end_effector_loc Max               0.576434\n",
      "exploration/env_infos/end_effector_loc Min              -0.528275\n",
      "evaluation/num steps total                           42000\n",
      "evaluation/num paths total                            2100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0608711\n",
      "evaluation/Rewards Std                                   0.0660185\n",
      "evaluation/Rewards Max                                   0.150941\n",
      "evaluation/Rewards Min                                  -0.350975\n",
      "evaluation/Returns Mean                                 -1.21742\n",
      "evaluation/Returns Std                                   0.907434\n",
      "evaluation/Returns Max                                   1.55601\n",
      "evaluation/Returns Min                                  -3.78743\n",
      "evaluation/Actions Mean                                 -0.00431184\n",
      "evaluation/Actions Std                                   0.0675774\n",
      "evaluation/Actions Max                                   0.743017\n",
      "evaluation/Actions Min                                  -0.372161\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.21742\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0716519\n",
      "evaluation/env_infos/final/reward_dist Std               0.158639\n",
      "evaluation/env_infos/final/reward_dist Max               0.798045\n",
      "evaluation/env_infos/final/reward_dist Min               1.61712e-34\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00737963\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0131185\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0543333\n",
      "evaluation/env_infos/initial/reward_dist Min             1.39123e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.113314\n",
      "evaluation/env_infos/reward_dist Std                     0.204956\n",
      "evaluation/env_infos/reward_dist Max                     0.989855\n",
      "evaluation/env_infos/reward_dist Min                     1.61712e-34\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0280367\n",
      "evaluation/env_infos/final/reward_energy Std             0.0226103\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00126483\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0994695\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.21972\n",
      "evaluation/env_infos/initial/reward_energy Std           0.187972\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0100306\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.79766\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0576055\n",
      "evaluation/env_infos/reward_energy Std                   0.0764997\n",
      "evaluation/env_infos/reward_energy Max                  -0.00024041\n",
      "evaluation/env_infos/reward_energy Min                  -0.79766\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.052907\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.285949\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.632927\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.834439\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000325518\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010218\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0371508\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0186081\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0206627\n",
      "evaluation/env_infos/end_effector_loc Std                0.189767\n",
      "evaluation/env_infos/end_effector_loc Max                0.632927\n",
      "evaluation/env_infos/end_effector_loc Min               -0.834439\n",
      "time/data storing (s)                                    0.00622733\n",
      "time/evaluation sampling (s)                             1.25528\n",
      "time/exploration sampling (s)                            0.129844\n",
      "time/logging (s)                                         0.0197884\n",
      "time/saving (s)                                          0.028496\n",
      "time/training (s)                                       48.4381\n",
      "time/epoch (s)                                          49.8777\n",
      "time/total (s)                                        2039.97\n",
      "Epoch                                                   41\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:23:28.916986 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 42 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00232765\n",
      "trainer/QF2 Loss                                         0.00196826\n",
      "trainer/Policy Loss                                      3.17058\n",
      "trainer/Q1 Predictions Mean                             -1.23097\n",
      "trainer/Q1 Predictions Std                               0.879344\n",
      "trainer/Q1 Predictions Max                               0.846843\n",
      "trainer/Q1 Predictions Min                              -3.92824\n",
      "trainer/Q2 Predictions Mean                             -1.22047\n",
      "trainer/Q2 Predictions Std                               0.87515\n",
      "trainer/Q2 Predictions Max                               0.877428\n",
      "trainer/Q2 Predictions Min                              -4.02351\n",
      "trainer/Q Targets Mean                                  -1.20691\n",
      "trainer/Q Targets Std                                    0.877011\n",
      "trainer/Q Targets Max                                    0.8791\n",
      "trainer/Q Targets Min                                   -3.98936\n",
      "trainer/Log Pis Mean                                     1.97615\n",
      "trainer/Log Pis Std                                      1.39897\n",
      "trainer/Log Pis Max                                      4.39875\n",
      "trainer/Log Pis Min                                     -6.86042\n",
      "trainer/Policy mu Mean                                  -0.0600019\n",
      "trainer/Policy mu Std                                    0.456447\n",
      "trainer/Policy mu Max                                    1.9632\n",
      "trainer/Policy mu Min                                   -2.34574\n",
      "trainer/Policy log std Mean                             -2.22723\n",
      "trainer/Policy log std Std                               0.651054\n",
      "trainer/Policy log std Max                               0.318542\n",
      "trainer/Policy log std Min                              -3.29417\n",
      "trainer/Alpha                                            0.018932\n",
      "trainer/Alpha Loss                                      -0.0946063\n",
      "exploration/num steps total                           5300\n",
      "exploration/num paths total                            265\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0997929\n",
      "exploration/Rewards Std                                  0.0774042\n",
      "exploration/Rewards Max                                  0.0456645\n",
      "exploration/Rewards Min                                 -0.387664\n",
      "exploration/Returns Mean                                -1.99586\n",
      "exploration/Returns Std                                  0.679285\n",
      "exploration/Returns Max                                 -1.19984\n",
      "exploration/Returns Min                                 -3.1255\n",
      "exploration/Actions Mean                                 0.00322054\n",
      "exploration/Actions Std                                  0.115439\n",
      "exploration/Actions Max                                  0.397203\n",
      "exploration/Actions Min                                 -0.655856\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.99586\n",
      "exploration/env_infos/final/reward_dist Mean             0.173812\n",
      "exploration/env_infos/final/reward_dist Std              0.347593\n",
      "exploration/env_infos/final/reward_dist Max              0.868998\n",
      "exploration/env_infos/final/reward_dist Min              1.30088e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0124925\n",
      "exploration/env_infos/initial/reward_dist Std            0.0182556\n",
      "exploration/env_infos/initial/reward_dist Max            0.048431\n",
      "exploration/env_infos/initial/reward_dist Min            0.00049501\n",
      "exploration/env_infos/reward_dist Mean                   0.149636\n",
      "exploration/env_infos/reward_dist Std                    0.237881\n",
      "exploration/env_infos/reward_dist Max                    0.955355\n",
      "exploration/env_infos/reward_dist Min                    1.30088e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.107685\n",
      "exploration/env_infos/final/reward_energy Std            0.0710361\n",
      "exploration/env_infos/final/reward_energy Max           -0.0456689\n",
      "exploration/env_infos/final/reward_energy Min           -0.238688\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.284382\n",
      "exploration/env_infos/initial/reward_energy Std          0.1371\n",
      "exploration/env_infos/initial/reward_energy Max         -0.101893\n",
      "exploration/env_infos/initial/reward_energy Min         -0.478369\n",
      "exploration/env_infos/reward_energy Mean                -0.11742\n",
      "exploration/env_infos/reward_energy Std                  0.113516\n",
      "exploration/env_infos/reward_energy Max                 -0.00351192\n",
      "exploration/env_infos/reward_energy Min                 -0.67572\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0536026\n",
      "exploration/env_infos/final/end_effector_loc Std         0.30729\n",
      "exploration/env_infos/final/end_effector_loc Max         0.404879\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.582663\n",
      "exploration/env_infos/initial/end_effector_loc Mean      2.41379e-05\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0111618\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0198602\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0154714\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0371578\n",
      "exploration/env_infos/end_effector_loc Std               0.189025\n",
      "exploration/env_infos/end_effector_loc Max               0.404879\n",
      "exploration/env_infos/end_effector_loc Min              -0.582663\n",
      "evaluation/num steps total                           43000\n",
      "evaluation/num paths total                            2150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0407991\n",
      "evaluation/Rewards Std                                   0.0732045\n",
      "evaluation/Rewards Max                                   0.14799\n",
      "evaluation/Rewards Min                                  -0.277262\n",
      "evaluation/Returns Mean                                 -0.815982\n",
      "evaluation/Returns Std                                   1.1519\n",
      "evaluation/Returns Max                                   1.93514\n",
      "evaluation/Returns Min                                  -2.98826\n",
      "evaluation/Actions Mean                                 -0.00215839\n",
      "evaluation/Actions Std                                   0.0556499\n",
      "evaluation/Actions Max                                   0.688897\n",
      "evaluation/Actions Min                                  -0.38372\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.815982\n",
      "evaluation/env_infos/final/reward_dist Mean              0.172249\n",
      "evaluation/env_infos/final/reward_dist Std               0.262115\n",
      "evaluation/env_infos/final/reward_dist Max               0.97352\n",
      "evaluation/env_infos/final/reward_dist Min               3.39549e-08\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00603194\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00955924\n",
      "evaluation/env_infos/initial/reward_dist Max             0.043044\n",
      "evaluation/env_infos/initial/reward_dist Min             3.99494e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.204905\n",
      "evaluation/env_infos/reward_dist Std                     0.269287\n",
      "evaluation/env_infos/reward_dist Max                     0.999032\n",
      "evaluation/env_infos/reward_dist Min                     3.39549e-08\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0324261\n",
      "evaluation/env_infos/final/reward_energy Std             0.0198855\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00538197\n",
      "evaluation/env_infos/final/reward_energy Min            -0.105675\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.19806\n",
      "evaluation/env_infos/initial/reward_energy Std           0.155529\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.013025\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.741966\n",
      "evaluation/env_infos/reward_energy Mean                 -0.05092\n",
      "evaluation/env_infos/reward_energy Std                   0.0600856\n",
      "evaluation/env_infos/reward_energy Max                  -0.000925996\n",
      "evaluation/env_infos/reward_energy Min                  -0.741966\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0327002\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.220242\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.339124\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.468071\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00158127\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00876189\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0344448\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.019186\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0181639\n",
      "evaluation/env_infos/end_effector_loc Std                0.146198\n",
      "evaluation/env_infos/end_effector_loc Max                0.339124\n",
      "evaluation/env_infos/end_effector_loc Min               -0.468071\n",
      "time/data storing (s)                                    0.00585332\n",
      "time/evaluation sampling (s)                             0.928645\n",
      "time/exploration sampling (s)                            0.119738\n",
      "time/logging (s)                                         0.0214827\n",
      "time/saving (s)                                          0.0286493\n",
      "time/training (s)                                       48.9651\n",
      "time/epoch (s)                                          50.0695\n",
      "time/total (s)                                        2090.63\n",
      "Epoch                                                   42\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:24:18.714978 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 43 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00179691\n",
      "trainer/QF2 Loss                                         0.00143547\n",
      "trainer/Policy Loss                                      3.17049\n",
      "trainer/Q1 Predictions Mean                             -1.29286\n",
      "trainer/Q1 Predictions Std                               0.917964\n",
      "trainer/Q1 Predictions Max                               0.309563\n",
      "trainer/Q1 Predictions Min                              -3.29669\n",
      "trainer/Q2 Predictions Mean                             -1.28737\n",
      "trainer/Q2 Predictions Std                               0.914148\n",
      "trainer/Q2 Predictions Max                               0.314662\n",
      "trainer/Q2 Predictions Min                              -3.32294\n",
      "trainer/Q Targets Mean                                  -1.30293\n",
      "trainer/Q Targets Std                                    0.915939\n",
      "trainer/Q Targets Max                                    0.300563\n",
      "trainer/Q Targets Min                                   -3.33379\n",
      "trainer/Log Pis Mean                                     1.91037\n",
      "trainer/Log Pis Std                                      1.38665\n",
      "trainer/Log Pis Max                                      4.43863\n",
      "trainer/Log Pis Min                                     -3.65795\n",
      "trainer/Policy mu Mean                                  -0.0151367\n",
      "trainer/Policy mu Std                                    0.448138\n",
      "trainer/Policy mu Max                                    2.24011\n",
      "trainer/Policy mu Min                                   -1.7517\n",
      "trainer/Policy log std Mean                             -2.19247\n",
      "trainer/Policy log std Std                               0.686893\n",
      "trainer/Policy log std Max                              -0.112907\n",
      "trainer/Policy log std Min                              -3.35074\n",
      "trainer/Alpha                                            0.0197019\n",
      "trainer/Alpha Loss                                      -0.351815\n",
      "exploration/num steps total                           5400\n",
      "exploration/num paths total                            270\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0435731\n",
      "exploration/Rewards Std                                  0.0760558\n",
      "exploration/Rewards Max                                  0.097579\n",
      "exploration/Rewards Min                                 -0.238857\n",
      "exploration/Returns Mean                                -0.871461\n",
      "exploration/Returns Std                                  1.19905\n",
      "exploration/Returns Max                                  0.872201\n",
      "exploration/Returns Min                                 -2.43465\n",
      "exploration/Actions Mean                                 0.000904016\n",
      "exploration/Actions Std                                  0.164234\n",
      "exploration/Actions Max                                  0.665469\n",
      "exploration/Actions Min                                 -0.480784\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.871461\n",
      "exploration/env_infos/final/reward_dist Mean             0.188047\n",
      "exploration/env_infos/final/reward_dist Std              0.165937\n",
      "exploration/env_infos/final/reward_dist Max              0.396649\n",
      "exploration/env_infos/final/reward_dist Min              5.34063e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00587974\n",
      "exploration/env_infos/initial/reward_dist Std            0.00583859\n",
      "exploration/env_infos/initial/reward_dist Max            0.0133625\n",
      "exploration/env_infos/initial/reward_dist Min            0.000241879\n",
      "exploration/env_infos/reward_dist Mean                   0.174153\n",
      "exploration/env_infos/reward_dist Std                    0.222262\n",
      "exploration/env_infos/reward_dist Max                    0.962719\n",
      "exploration/env_infos/reward_dist Min                    5.34063e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.179693\n",
      "exploration/env_infos/final/reward_energy Std            0.16901\n",
      "exploration/env_infos/final/reward_energy Max           -0.055373\n",
      "exploration/env_infos/final/reward_energy Min           -0.503984\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.343107\n",
      "exploration/env_infos/initial/reward_energy Std          0.275672\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0313971\n",
      "exploration/env_infos/initial/reward_energy Min         -0.832677\n",
      "exploration/env_infos/reward_energy Mean                -0.182842\n",
      "exploration/env_infos/reward_energy Std                  0.143234\n",
      "exploration/env_infos/reward_energy Max                 -0.0090116\n",
      "exploration/env_infos/reward_energy Min                 -0.832677\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0251731\n",
      "exploration/env_infos/final/end_effector_loc Std         0.204121\n",
      "exploration/env_infos/final/end_effector_loc Max         0.388341\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.413627\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00813757\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0132638\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0332735\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0137954\n",
      "exploration/env_infos/end_effector_loc Mean              0.0410034\n",
      "exploration/env_infos/end_effector_loc Std               0.154788\n",
      "exploration/env_infos/end_effector_loc Max               0.388341\n",
      "exploration/env_infos/end_effector_loc Min              -0.413627\n",
      "evaluation/num steps total                           44000\n",
      "evaluation/num paths total                            2200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0384854\n",
      "evaluation/Rewards Std                                   0.0824981\n",
      "evaluation/Rewards Max                                   0.128542\n",
      "evaluation/Rewards Min                                  -0.604644\n",
      "evaluation/Returns Mean                                 -0.769708\n",
      "evaluation/Returns Std                                   1.21448\n",
      "evaluation/Returns Max                                   1.70811\n",
      "evaluation/Returns Min                                  -3.72825\n",
      "evaluation/Actions Mean                                  2.85512e-05\n",
      "evaluation/Actions Std                                   0.0737116\n",
      "evaluation/Actions Max                                   0.896664\n",
      "evaluation/Actions Min                                  -0.396134\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.769708\n",
      "evaluation/env_infos/final/reward_dist Mean              0.140401\n",
      "evaluation/env_infos/final/reward_dist Std               0.223613\n",
      "evaluation/env_infos/final/reward_dist Max               0.870201\n",
      "evaluation/env_infos/final/reward_dist Min               3.72543e-48\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00854992\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0133749\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0486601\n",
      "evaluation/env_infos/initial/reward_dist Min             2.04566e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.163145\n",
      "evaluation/env_infos/reward_dist Std                     0.235511\n",
      "evaluation/env_infos/reward_dist Max                     0.991443\n",
      "evaluation/env_infos/reward_dist Min                     3.72543e-48\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0523172\n",
      "evaluation/env_infos/final/reward_energy Std             0.0381168\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0146054\n",
      "evaluation/env_infos/final/reward_energy Min            -0.209813\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.233686\n",
      "evaluation/env_infos/initial/reward_energy Std           0.194663\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0105951\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.934557\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0680871\n",
      "evaluation/env_infos/reward_energy Std                   0.0789364\n",
      "evaluation/env_infos/reward_energy Max                  -0.000381695\n",
      "evaluation/env_infos/reward_energy Min                  -0.934557\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0234846\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.225722\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.35554\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.848057\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00019643\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107513\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0448332\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0198067\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.013138\n",
      "evaluation/env_infos/end_effector_loc Std                0.152205\n",
      "evaluation/env_infos/end_effector_loc Max                0.374533\n",
      "evaluation/env_infos/end_effector_loc Min               -0.848057\n",
      "time/data storing (s)                                    0.00608824\n",
      "time/evaluation sampling (s)                             0.921875\n",
      "time/exploration sampling (s)                            0.122193\n",
      "time/logging (s)                                         0.0199914\n",
      "time/saving (s)                                          0.0312314\n",
      "time/training (s)                                       48.1535\n",
      "time/epoch (s)                                          49.2549\n",
      "time/total (s)                                        2140.43\n",
      "Epoch                                                   43\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:25:08.360525 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 44 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00514804\n",
      "trainer/QF2 Loss                                         0.00209302\n",
      "trainer/Policy Loss                                      3.47248\n",
      "trainer/Q1 Predictions Mean                             -1.26958\n",
      "trainer/Q1 Predictions Std                               0.857462\n",
      "trainer/Q1 Predictions Max                               0.294184\n",
      "trainer/Q1 Predictions Min                              -4.02235\n",
      "trainer/Q2 Predictions Mean                             -1.27729\n",
      "trainer/Q2 Predictions Std                               0.862357\n",
      "trainer/Q2 Predictions Max                               0.307744\n",
      "trainer/Q2 Predictions Min                              -3.8982\n",
      "trainer/Q Targets Mean                                  -1.28189\n",
      "trainer/Q Targets Std                                    0.861757\n",
      "trainer/Q Targets Max                                    0.274353\n",
      "trainer/Q Targets Min                                   -3.87402\n",
      "trainer/Log Pis Mean                                     2.23782\n",
      "trainer/Log Pis Std                                      1.12094\n",
      "trainer/Log Pis Max                                      4.65124\n",
      "trainer/Log Pis Min                                     -1.68367\n",
      "trainer/Policy mu Mean                                  -0.0296607\n",
      "trainer/Policy mu Std                                    0.480574\n",
      "trainer/Policy mu Max                                    1.91554\n",
      "trainer/Policy mu Min                                   -2.22765\n",
      "trainer/Policy log std Mean                             -2.26912\n",
      "trainer/Policy log std Std                               0.647726\n",
      "trainer/Policy log std Max                              -0.330767\n",
      "trainer/Policy log std Min                              -3.42514\n",
      "trainer/Alpha                                            0.0201674\n",
      "trainer/Alpha Loss                                       0.927956\n",
      "exploration/num steps total                           5500\n",
      "exploration/num paths total                            275\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.110316\n",
      "exploration/Rewards Std                                  0.0446111\n",
      "exploration/Rewards Max                                 -0.0405898\n",
      "exploration/Rewards Min                                 -0.305822\n",
      "exploration/Returns Mean                                -2.20633\n",
      "exploration/Returns Std                                  0.463514\n",
      "exploration/Returns Max                                 -1.49697\n",
      "exploration/Returns Min                                 -2.92188\n",
      "exploration/Actions Mean                                -0.00454799\n",
      "exploration/Actions Std                                  0.175202\n",
      "exploration/Actions Max                                  0.892627\n",
      "exploration/Actions Min                                 -0.65201\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.20633\n",
      "exploration/env_infos/final/reward_dist Mean             0.1415\n",
      "exploration/env_infos/final/reward_dist Std              0.220856\n",
      "exploration/env_infos/final/reward_dist Max              0.570545\n",
      "exploration/env_infos/final/reward_dist Min              6.60917e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0265436\n",
      "exploration/env_infos/initial/reward_dist Std            0.0459217\n",
      "exploration/env_infos/initial/reward_dist Max            0.11814\n",
      "exploration/env_infos/initial/reward_dist Min            0.00029892\n",
      "exploration/env_infos/reward_dist Mean                   0.132913\n",
      "exploration/env_infos/reward_dist Std                    0.184864\n",
      "exploration/env_infos/reward_dist Max                    0.962837\n",
      "exploration/env_infos/reward_dist Min                    6.60917e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.127695\n",
      "exploration/env_infos/final/reward_energy Std            0.0778899\n",
      "exploration/env_infos/final/reward_energy Max           -0.01644\n",
      "exploration/env_infos/final/reward_energy Min           -0.241239\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.398364\n",
      "exploration/env_infos/initial/reward_energy Std          0.298205\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0612894\n",
      "exploration/env_infos/initial/reward_energy Min         -0.9541\n",
      "exploration/env_infos/reward_energy Mean                -0.193726\n",
      "exploration/env_infos/reward_energy Std                  0.154606\n",
      "exploration/env_infos/reward_energy Max                 -0.00802377\n",
      "exploration/env_infos/reward_energy Min                 -0.9541\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.075066\n",
      "exploration/env_infos/final/end_effector_loc Std         0.182401\n",
      "exploration/env_infos/final/end_effector_loc Max         0.365415\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.152528\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00705928\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0161149\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0446313\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0110213\n",
      "exploration/env_infos/end_effector_loc Mean              0.0640441\n",
      "exploration/env_infos/end_effector_loc Std               0.143859\n",
      "exploration/env_infos/end_effector_loc Max               0.365415\n",
      "exploration/env_infos/end_effector_loc Min              -0.169934\n",
      "evaluation/num steps total                           45000\n",
      "evaluation/num paths total                            2250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0426256\n",
      "evaluation/Rewards Std                                   0.0701317\n",
      "evaluation/Rewards Max                                   0.133894\n",
      "evaluation/Rewards Min                                  -0.388869\n",
      "evaluation/Returns Mean                                 -0.852513\n",
      "evaluation/Returns Std                                   1.00067\n",
      "evaluation/Returns Max                                   1.44587\n",
      "evaluation/Returns Min                                  -2.6023\n",
      "evaluation/Actions Mean                                 -0.00132102\n",
      "evaluation/Actions Std                                   0.0661709\n",
      "evaluation/Actions Max                                   0.800716\n",
      "evaluation/Actions Min                                  -0.462486\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.852513\n",
      "evaluation/env_infos/final/reward_dist Mean              0.193638\n",
      "evaluation/env_infos/final/reward_dist Std               0.258336\n",
      "evaluation/env_infos/final/reward_dist Max               0.88909\n",
      "evaluation/env_infos/final/reward_dist Min               1.93425e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00652676\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0106117\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0407092\n",
      "evaluation/env_infos/initial/reward_dist Min             2.49471e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.220107\n",
      "evaluation/env_infos/reward_dist Std                     0.275421\n",
      "evaluation/env_infos/reward_dist Max                     0.997014\n",
      "evaluation/env_infos/reward_dist Min                     1.93425e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0314485\n",
      "evaluation/env_infos/final/reward_energy Std             0.0192426\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00661003\n",
      "evaluation/env_infos/final/reward_energy Min            -0.124214\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.234249\n",
      "evaluation/env_infos/initial/reward_energy Std           0.178961\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0132336\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.806485\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0588913\n",
      "evaluation/env_infos/reward_energy Std                   0.0727494\n",
      "evaluation/env_infos/reward_energy Max                  -0.000677946\n",
      "evaluation/env_infos/reward_energy Min                  -0.806485\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0184038\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.237079\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.541836\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.525342\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00182083\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010262\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0400358\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0231243\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0180218\n",
      "evaluation/env_infos/end_effector_loc Std                0.155629\n",
      "evaluation/env_infos/end_effector_loc Max                0.541836\n",
      "evaluation/env_infos/end_effector_loc Min               -0.525342\n",
      "time/data storing (s)                                    0.00620427\n",
      "time/evaluation sampling (s)                             0.990386\n",
      "time/exploration sampling (s)                            0.125543\n",
      "time/logging (s)                                         0.0216346\n",
      "time/saving (s)                                          0.0273052\n",
      "time/training (s)                                       47.849\n",
      "time/epoch (s)                                          49.0201\n",
      "time/total (s)                                        2190.07\n",
      "Epoch                                                   44\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:25:58.227678 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 45 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00133322\r\n",
      "trainer/QF2 Loss                                         0.00180009\r\n",
      "trainer/Policy Loss                                      3.25785\r\n",
      "trainer/Q1 Predictions Mean                             -1.22782\r\n",
      "trainer/Q1 Predictions Std                               0.883555\r\n",
      "trainer/Q1 Predictions Max                               0.866859\r\n",
      "trainer/Q1 Predictions Min                              -3.46851\r\n",
      "trainer/Q2 Predictions Mean                             -1.2348\r\n",
      "trainer/Q2 Predictions Std                               0.88783\r\n",
      "trainer/Q2 Predictions Max                               0.821493\r\n",
      "trainer/Q2 Predictions Min                              -3.57511\r\n",
      "trainer/Q Targets Mean                                  -1.23321\r\n",
      "trainer/Q Targets Std                                    0.88593\r\n",
      "trainer/Q Targets Max                                    0.86131\r\n",
      "trainer/Q Targets Min                                   -3.60887\r\n",
      "trainer/Log Pis Mean                                     2.05675\r\n",
      "trainer/Log Pis Std                                      1.41909\r\n",
      "trainer/Log Pis Max                                      5.49655\r\n",
      "trainer/Log Pis Min                                     -3.23069\r\n",
      "trainer/Policy mu Mean                                  -0.0381263\r\n",
      "trainer/Policy mu Std                                    0.470241\r\n",
      "trainer/Policy mu Max                                    2.36792\r\n",
      "trainer/Policy mu Min                                   -2.658\r\n",
      "trainer/Policy log std Mean                             -2.26212\r\n",
      "trainer/Policy log std Std                               0.687229\r\n",
      "trainer/Policy log std Max                              -0.0742826\r\n",
      "trainer/Policy log std Min                              -3.47998\r\n",
      "trainer/Alpha                                            0.0193016\r\n",
      "trainer/Alpha Loss                                       0.22409\r\n",
      "exploration/num steps total                           5600\r\n",
      "exploration/num paths total                            280\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0914792\r\n",
      "exploration/Rewards Std                                  0.0589347\r\n",
      "exploration/Rewards Max                                  0.0580812\r\n",
      "exploration/Rewards Min                                 -0.203012\r\n",
      "exploration/Returns Mean                                -1.82958\r\n",
      "exploration/Returns Std                                  0.845915\r\n",
      "exploration/Returns Max                                 -0.706328\r\n",
      "exploration/Returns Min                                 -3.14392\r\n",
      "exploration/Actions Mean                                -0.00787733\r\n",
      "exploration/Actions Std                                  0.0753757\r\n",
      "exploration/Actions Max                                  0.281317\r\n",
      "exploration/Actions Min                                 -0.371837\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.82958\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00553192\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00511682\r\n",
      "exploration/env_infos/final/reward_dist Max              0.013592\r\n",
      "exploration/env_infos/final/reward_dist Min              2.02044e-10\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0041646\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00764883\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0194435\r\n",
      "exploration/env_infos/initial/reward_dist Min            8.18441e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0942027\r\n",
      "exploration/env_infos/reward_dist Std                    0.205318\r\n",
      "exploration/env_infos/reward_dist Max                    0.941825\r\n",
      "exploration/env_infos/reward_dist Min                    2.02044e-10\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.104496\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0189439\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0914032\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.141702\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.130774\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.127754\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0377726\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.372451\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0859926\r\n",
      "exploration/env_infos/reward_energy Std                  0.0639717\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0149782\r\n",
      "exploration/env_infos/reward_energy Min                 -0.372451\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0591576\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.220635\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.279071\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.362287\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00294406\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00575424\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00190787\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0185918\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0338638\r\n",
      "exploration/env_infos/end_effector_loc Std               0.130641\r\n",
      "exploration/env_infos/end_effector_loc Max               0.279071\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.362287\r\n",
      "evaluation/num steps total                           46000\r\n",
      "evaluation/num paths total                            2300\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0449257\r\n",
      "evaluation/Rewards Std                                   0.0661493\r\n",
      "evaluation/Rewards Max                                   0.16973\r\n",
      "evaluation/Rewards Min                                  -0.364923\r\n",
      "evaluation/Returns Mean                                 -0.898513\r\n",
      "evaluation/Returns Std                                   0.997173\r\n",
      "evaluation/Returns Max                                   1.18311\r\n",
      "evaluation/Returns Min                                  -3.36304\r\n",
      "evaluation/Actions Mean                                 -0.00485289\r\n",
      "evaluation/Actions Std                                   0.0599349\r\n",
      "evaluation/Actions Max                                   0.759772\r\n",
      "evaluation/Actions Min                                  -0.537021\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.898513\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.1568\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.260258\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.958531\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.36489e-32\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00489317\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00818377\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0374308\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.09653e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.195308\r\n",
      "evaluation/env_infos/reward_dist Std                     0.266423\r\n",
      "evaluation/env_infos/reward_dist Max                     0.99889\r\n",
      "evaluation/env_infos/reward_dist Min                     5.36489e-32\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0332184\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0215359\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00366699\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.110636\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.210003\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.173967\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0063661\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.78301\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0524297\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0669522\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000575897\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.78301\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0365421\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.240943\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.37804\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.78369\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000203515\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00963929\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0379886\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0268511\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0133713\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.150777\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.37804\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.78369\r\n",
      "time/data storing (s)                                    0.00631765\r\n",
      "time/evaluation sampling (s)                             0.93308\r\n",
      "time/exploration sampling (s)                            0.119889\r\n",
      "time/logging (s)                                         0.0194492\r\n",
      "time/saving (s)                                          0.0274583\r\n",
      "time/training (s)                                       48.2002\r\n",
      "time/epoch (s)                                          49.3064\r\n",
      "time/total (s)                                        2239.93\r\n",
      "Epoch                                                   45\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:26:47.692778 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 46 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00180625\r\n",
      "trainer/QF2 Loss                                         0.00121679\r\n",
      "trainer/Policy Loss                                      3.3006\r\n",
      "trainer/Q1 Predictions Mean                             -1.20323\r\n",
      "trainer/Q1 Predictions Std                               0.870797\r\n",
      "trainer/Q1 Predictions Max                               0.777513\r\n",
      "trainer/Q1 Predictions Min                              -3.87698\r\n",
      "trainer/Q2 Predictions Mean                             -1.21083\r\n",
      "trainer/Q2 Predictions Std                               0.869105\r\n",
      "trainer/Q2 Predictions Max                               0.76718\r\n",
      "trainer/Q2 Predictions Min                              -3.88355\r\n",
      "trainer/Q Targets Mean                                  -1.198\r\n",
      "trainer/Q Targets Std                                    0.867148\r\n",
      "trainer/Q Targets Max                                    0.774883\r\n",
      "trainer/Q Targets Min                                   -3.95118\r\n",
      "trainer/Log Pis Mean                                     2.14406\r\n",
      "trainer/Log Pis Std                                      1.41146\r\n",
      "trainer/Log Pis Max                                      4.73335\r\n",
      "trainer/Log Pis Min                                     -4.49968\r\n",
      "trainer/Policy mu Mean                                  -0.0415354\r\n",
      "trainer/Policy mu Std                                    0.414742\r\n",
      "trainer/Policy mu Max                                    2.11765\r\n",
      "trainer/Policy mu Min                                   -2.19126\r\n",
      "trainer/Policy log std Mean                             -2.29704\r\n",
      "trainer/Policy log std Std                               0.659085\r\n",
      "trainer/Policy log std Max                              -0.389866\r\n",
      "trainer/Policy log std Min                              -3.61471\r\n",
      "trainer/Alpha                                            0.0211232\r\n",
      "trainer/Alpha Loss                                       0.555727\r\n",
      "exploration/num steps total                           5700\r\n",
      "exploration/num paths total                            285\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0666996\r\n",
      "exploration/Rewards Std                                  0.0657716\r\n",
      "exploration/Rewards Max                                  0.065223\r\n",
      "exploration/Rewards Min                                 -0.212952\r\n",
      "exploration/Returns Mean                                -1.33399\r\n",
      "exploration/Returns Std                                  0.617276\r\n",
      "exploration/Returns Max                                 -0.55971\r\n",
      "exploration/Returns Min                                 -2.25145\r\n",
      "exploration/Actions Mean                                -0.0073845\r\n",
      "exploration/Actions Std                                  0.07359\r\n",
      "exploration/Actions Max                                  0.235814\r\n",
      "exploration/Actions Min                                 -0.178387\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.33399\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0510983\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0970759\r\n",
      "exploration/env_infos/final/reward_dist Max              0.245111\r\n",
      "exploration/env_infos/final/reward_dist Min              9.92984e-12\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0127167\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0145471\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0309757\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.036e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.216725\r\n",
      "exploration/env_infos/reward_dist Std                    0.260446\r\n",
      "exploration/env_infos/reward_dist Max                    0.945976\r\n",
      "exploration/env_infos/reward_dist Min                    9.92984e-12\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0756615\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0426945\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0260554\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.131171\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.203869\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0787257\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0944211\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.331151\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0895104\r\n",
      "exploration/env_infos/reward_energy Std                  0.0541104\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00175507\r\n",
      "exploration/env_infos/reward_energy Min                 -0.331151\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.089556\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.282327\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.293386\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.530585\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000574965\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00770517\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0117907\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00891935\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0345045\r\n",
      "exploration/env_infos/end_effector_loc Std               0.171707\r\n",
      "exploration/env_infos/end_effector_loc Max               0.293386\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.530585\r\n",
      "evaluation/num steps total                           47000\r\n",
      "evaluation/num paths total                            2350\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0364661\r\n",
      "evaluation/Rewards Std                                   0.0703731\r\n",
      "evaluation/Rewards Max                                   0.137808\r\n",
      "evaluation/Rewards Min                                  -0.473036\r\n",
      "evaluation/Returns Mean                                 -0.729323\r\n",
      "evaluation/Returns Std                                   1.02687\r\n",
      "evaluation/Returns Max                                   1.62319\r\n",
      "evaluation/Returns Min                                  -2.43959\r\n",
      "evaluation/Actions Mean                                 -0.00435476\r\n",
      "evaluation/Actions Std                                   0.0628677\r\n",
      "evaluation/Actions Max                                   0.671049\r\n",
      "evaluation/Actions Min                                  -0.494859\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.729323\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.206855\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.272947\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.881948\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.07557e-21\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00491419\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0113527\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.056656\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.6243e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.228436\r\n",
      "evaluation/env_infos/reward_dist Std                     0.281647\r\n",
      "evaluation/env_infos/reward_dist Max                     0.995058\r\n",
      "evaluation/env_infos/reward_dist Min                     8.07557e-21\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0480047\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0301464\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00888427\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.151865\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.223417\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.182239\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0285926\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.691569\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0576549\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0679598\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000762207\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.691569\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00243726\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.214285\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.537981\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.555696\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00243906\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00989742\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0335524\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0247429\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0160267\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.143022\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.537981\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.555696\r\n",
      "time/data storing (s)                                    0.00585508\r\n",
      "time/evaluation sampling (s)                             0.925254\r\n",
      "time/exploration sampling (s)                            0.116871\r\n",
      "time/logging (s)                                         0.0199369\r\n",
      "time/saving (s)                                          0.0314321\r\n",
      "time/training (s)                                       47.7331\r\n",
      "time/epoch (s)                                          48.8325\r\n",
      "time/total (s)                                        2289.4\r\n",
      "Epoch                                                   46\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:27:37.216364 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 47 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000772312\r\n",
      "trainer/QF2 Loss                                         0.000824431\r\n",
      "trainer/Policy Loss                                      3.16122\r\n",
      "trainer/Q1 Predictions Mean                             -1.25007\r\n",
      "trainer/Q1 Predictions Std                               0.920104\r\n",
      "trainer/Q1 Predictions Max                               0.741039\r\n",
      "trainer/Q1 Predictions Min                              -3.90529\r\n",
      "trainer/Q2 Predictions Mean                             -1.2532\r\n",
      "trainer/Q2 Predictions Std                               0.91853\r\n",
      "trainer/Q2 Predictions Max                               0.721413\r\n",
      "trainer/Q2 Predictions Min                              -3.88922\r\n",
      "trainer/Q Targets Mean                                  -1.25432\r\n",
      "trainer/Q Targets Std                                    0.918556\r\n",
      "trainer/Q Targets Max                                    0.71873\r\n",
      "trainer/Q Targets Min                                   -3.91395\r\n",
      "trainer/Log Pis Mean                                     1.93045\r\n",
      "trainer/Log Pis Std                                      1.3722\r\n",
      "trainer/Log Pis Max                                      4.59248\r\n",
      "trainer/Log Pis Min                                     -3.22309\r\n",
      "trainer/Policy mu Mean                                   0.00498908\r\n",
      "trainer/Policy mu Std                                    0.394771\r\n",
      "trainer/Policy mu Max                                    2.22674\r\n",
      "trainer/Policy mu Min                                   -2.43052\r\n",
      "trainer/Policy log std Mean                             -2.28634\r\n",
      "trainer/Policy log std Std                               0.604496\r\n",
      "trainer/Policy log std Max                              -0.490974\r\n",
      "trainer/Policy log std Min                              -3.37191\r\n",
      "trainer/Alpha                                            0.0225553\r\n",
      "trainer/Alpha Loss                                      -0.263783\r\n",
      "exploration/num steps total                           5800\r\n",
      "exploration/num paths total                            290\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0842748\r\n",
      "exploration/Rewards Std                                  0.0720142\r\n",
      "exploration/Rewards Max                                  0.0878651\r\n",
      "exploration/Rewards Min                                 -0.261893\r\n",
      "exploration/Returns Mean                                -1.6855\r\n",
      "exploration/Returns Std                                  1.09259\r\n",
      "exploration/Returns Max                                  0.118656\r\n",
      "exploration/Returns Min                                 -3.01718\r\n",
      "exploration/Actions Mean                                -0.00881422\r\n",
      "exploration/Actions Std                                  0.0867115\r\n",
      "exploration/Actions Max                                  0.321821\r\n",
      "exploration/Actions Min                                 -0.23753\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.6855\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.129986\r\n",
      "exploration/env_infos/final/reward_dist Std              0.251092\r\n",
      "exploration/env_infos/final/reward_dist Max              0.632008\r\n",
      "exploration/env_infos/final/reward_dist Min              9.39601e-07\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00949262\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0105501\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0263037\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.03933e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0848214\r\n",
      "exploration/env_infos/reward_dist Std                    0.142633\r\n",
      "exploration/env_infos/reward_dist Max                    0.632008\r\n",
      "exploration/env_infos/reward_dist Min                    9.39601e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0858194\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0448204\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0197691\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.155297\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0990303\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0284387\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0636247\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.136522\r\n",
      "exploration/env_infos/reward_energy Mean                -0.103841\r\n",
      "exploration/env_infos/reward_energy Std                  0.0664099\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00906101\r\n",
      "exploration/env_infos/reward_energy Min                 -0.333277\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0395587\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.190541\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.26983\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.271059\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000330027\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00362778\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00662972\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00574206\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00747467\r\n",
      "exploration/env_infos/end_effector_loc Std               0.109412\r\n",
      "exploration/env_infos/end_effector_loc Max               0.26983\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.271059\r\n",
      "evaluation/num steps total                           48000\r\n",
      "evaluation/num paths total                            2400\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0536058\r\n",
      "evaluation/Rewards Std                                   0.0758541\r\n",
      "evaluation/Rewards Max                                   0.141393\r\n",
      "evaluation/Rewards Min                                  -0.29647\r\n",
      "evaluation/Returns Mean                                 -1.07212\r\n",
      "evaluation/Returns Std                                   1.16692\r\n",
      "evaluation/Returns Max                                   1.6342\r\n",
      "evaluation/Returns Min                                  -3.4031\r\n",
      "evaluation/Actions Mean                                 -0.00453724\r\n",
      "evaluation/Actions Std                                   0.0634694\r\n",
      "evaluation/Actions Max                                   0.578566\r\n",
      "evaluation/Actions Min                                  -0.782282\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.07212\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.240359\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.293567\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.911011\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.39676e-11\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00303703\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00524433\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0234547\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.56908e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.217998\r\n",
      "evaluation/env_infos/reward_dist Std                     0.282234\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998804\r\n",
      "evaluation/env_infos/reward_dist Min                     1.39676e-11\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0499546\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0663692\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00765487\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.496414\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.223019\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.181668\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0173532\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.791118\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0552238\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0710509\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00214488\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.791118\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0347935\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.22125\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.454531\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.478069\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000570594\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0101538\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0289283\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0391141\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00812148\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.142849\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.454531\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.478069\r\n",
      "time/data storing (s)                                    0.00623003\r\n",
      "time/evaluation sampling (s)                             1.0821\r\n",
      "time/exploration sampling (s)                            0.121712\r\n",
      "time/logging (s)                                         0.0194352\r\n",
      "time/saving (s)                                          0.029252\r\n",
      "time/training (s)                                       47.665\r\n",
      "time/epoch (s)                                          48.9237\r\n",
      "time/total (s)                                        2338.92\r\n",
      "Epoch                                                   47\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:28:27.445290 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 48 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00140605\r\n",
      "trainer/QF2 Loss                                         0.00189348\r\n",
      "trainer/Policy Loss                                      3.13918\r\n",
      "trainer/Q1 Predictions Mean                             -1.26029\r\n",
      "trainer/Q1 Predictions Std                               0.809146\r\n",
      "trainer/Q1 Predictions Max                               0.676398\r\n",
      "trainer/Q1 Predictions Min                              -3.18065\r\n",
      "trainer/Q2 Predictions Mean                             -1.2412\r\n",
      "trainer/Q2 Predictions Std                               0.818403\r\n",
      "trainer/Q2 Predictions Max                               0.69731\r\n",
      "trainer/Q2 Predictions Min                              -3.19027\r\n",
      "trainer/Q Targets Mean                                  -1.25467\r\n",
      "trainer/Q Targets Std                                    0.812906\r\n",
      "trainer/Q Targets Max                                    0.721645\r\n",
      "trainer/Q Targets Min                                   -3.20633\r\n",
      "trainer/Log Pis Mean                                     1.9091\r\n",
      "trainer/Log Pis Std                                      1.36949\r\n",
      "trainer/Log Pis Max                                      4.26524\r\n",
      "trainer/Log Pis Min                                     -2.93645\r\n",
      "trainer/Policy mu Mean                                   0.0089957\r\n",
      "trainer/Policy mu Std                                    0.38401\r\n",
      "trainer/Policy mu Max                                    2.11745\r\n",
      "trainer/Policy mu Min                                   -2.19406\r\n",
      "trainer/Policy log std Mean                             -2.25356\r\n",
      "trainer/Policy log std Std                               0.615106\r\n",
      "trainer/Policy log std Max                              -0.172213\r\n",
      "trainer/Policy log std Min                              -3.49457\r\n",
      "trainer/Alpha                                            0.0219018\r\n",
      "trainer/Alpha Loss                                      -0.347346\r\n",
      "exploration/num steps total                           5900\r\n",
      "exploration/num paths total                            295\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                 0.0142619\r\n",
      "exploration/Rewards Std                                  0.0700927\r\n",
      "exploration/Rewards Max                                  0.1446\r\n",
      "exploration/Rewards Min                                 -0.198239\r\n",
      "exploration/Returns Mean                                 0.285238\r\n",
      "exploration/Returns Std                                  0.495815\r\n",
      "exploration/Returns Max                                  0.9695\r\n",
      "exploration/Returns Min                                 -0.446173\r\n",
      "exploration/Actions Mean                                -0.00369404\r\n",
      "exploration/Actions Std                                  0.103531\r\n",
      "exploration/Actions Max                                  0.354684\r\n",
      "exploration/Actions Min                                 -0.447581\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                              0.285238\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.212626\r\n",
      "exploration/env_infos/final/reward_dist Std              0.148882\r\n",
      "exploration/env_infos/final/reward_dist Max              0.435638\r\n",
      "exploration/env_infos/final/reward_dist Min              0.031906\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00222309\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00357354\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00930372\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.11583e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.220871\r\n",
      "exploration/env_infos/reward_dist Std                    0.232415\r\n",
      "exploration/env_infos/reward_dist Max                    0.966461\r\n",
      "exploration/env_infos/reward_dist Min                    5.11583e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0884368\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0321576\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0382499\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.130005\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.326467\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0839846\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.238551\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.485947\r\n",
      "exploration/env_infos/reward_energy Mean                -0.118771\r\n",
      "exploration/env_infos/reward_energy Std                  0.0857804\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0118613\r\n",
      "exploration/env_infos/reward_energy Min                 -0.485947\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.063301\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.180458\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.234343\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.329599\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00407139\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0112012\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0116175\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0223791\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0464229\r\n",
      "exploration/env_infos/end_effector_loc Std               0.136903\r\n",
      "exploration/env_infos/end_effector_loc Max               0.234343\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.329599\r\n",
      "evaluation/num steps total                           49000\r\n",
      "evaluation/num paths total                            2450\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0217769\r\n",
      "evaluation/Rewards Std                                   0.0740153\r\n",
      "evaluation/Rewards Max                                   0.163972\r\n",
      "evaluation/Rewards Min                                  -0.31599\r\n",
      "evaluation/Returns Mean                                 -0.435538\r\n",
      "evaluation/Returns Std                                   1.1532\r\n",
      "evaluation/Returns Max                                   1.64226\r\n",
      "evaluation/Returns Min                                  -2.48523\r\n",
      "evaluation/Actions Mean                                 -0.00499238\r\n",
      "evaluation/Actions Std                                   0.0590125\r\n",
      "evaluation/Actions Max                                   0.72745\r\n",
      "evaluation/Actions Min                                  -0.544509\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.435538\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.301201\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.294248\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.98839\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.18478e-18\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00639194\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0129643\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0749154\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.56392e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.273682\r\n",
      "evaluation/env_infos/reward_dist Std                     0.313692\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999557\r\n",
      "evaluation/env_infos/reward_dist Min                     1.18478e-18\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0369287\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0207534\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00740611\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.137138\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.210588\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.17006\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0237958\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.822303\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0544486\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0636407\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00188456\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.822303\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0381344\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.20867\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.42014\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.493696\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000428435\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00956041\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0363725\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0272255\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0134033\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.136622\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.42014\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.493696\r\n",
      "time/data storing (s)                                    0.00637889\r\n",
      "time/evaluation sampling (s)                             0.967533\r\n",
      "time/exploration sampling (s)                            0.124843\r\n",
      "time/logging (s)                                         0.0197451\r\n",
      "time/saving (s)                                          0.029266\r\n",
      "time/training (s)                                       48.48\r\n",
      "time/epoch (s)                                          49.6278\r\n",
      "time/total (s)                                        2389.15\r\n",
      "Epoch                                                   48\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:29:16.519837 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 49 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00148556\n",
      "trainer/QF2 Loss                                         0.00115168\n",
      "trainer/Policy Loss                                      3.09452\n",
      "trainer/Q1 Predictions Mean                             -1.17653\n",
      "trainer/Q1 Predictions Std                               0.813911\n",
      "trainer/Q1 Predictions Max                               0.768927\n",
      "trainer/Q1 Predictions Min                              -2.98511\n",
      "trainer/Q2 Predictions Mean                             -1.17763\n",
      "trainer/Q2 Predictions Std                               0.81243\n",
      "trainer/Q2 Predictions Max                               0.772343\n",
      "trainer/Q2 Predictions Min                              -2.97027\n",
      "trainer/Q Targets Mean                                  -1.18483\n",
      "trainer/Q Targets Std                                    0.810869\n",
      "trainer/Q Targets Max                                    0.767076\n",
      "trainer/Q Targets Min                                   -3.02307\n",
      "trainer/Log Pis Mean                                     1.94984\n",
      "trainer/Log Pis Std                                      1.35955\n",
      "trainer/Log Pis Max                                      4.6352\n",
      "trainer/Log Pis Min                                     -3.95424\n",
      "trainer/Policy mu Mean                                  -0.0339717\n",
      "trainer/Policy mu Std                                    0.434981\n",
      "trainer/Policy mu Max                                    1.96066\n",
      "trainer/Policy mu Min                                   -2.61393\n",
      "trainer/Policy log std Mean                             -2.26316\n",
      "trainer/Policy log std Std                               0.605205\n",
      "trainer/Policy log std Max                              -0.261482\n",
      "trainer/Policy log std Min                              -3.31569\n",
      "trainer/Alpha                                            0.0234111\n",
      "trainer/Alpha Loss                                      -0.188343\n",
      "exploration/num steps total                           6000\n",
      "exploration/num paths total                            300\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0944623\n",
      "exploration/Rewards Std                                  0.0633662\n",
      "exploration/Rewards Max                                  0.0541827\n",
      "exploration/Rewards Min                                 -0.252207\n",
      "exploration/Returns Mean                                -1.88925\n",
      "exploration/Returns Std                                  0.861632\n",
      "exploration/Returns Max                                 -0.342163\n",
      "exploration/Returns Min                                 -3.00147\n",
      "exploration/Actions Mean                                -6.54088e-05\n",
      "exploration/Actions Std                                  0.101417\n",
      "exploration/Actions Max                                  0.502421\n",
      "exploration/Actions Min                                 -0.34216\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.88925\n",
      "exploration/env_infos/final/reward_dist Mean             0.175851\n",
      "exploration/env_infos/final/reward_dist Std              0.341848\n",
      "exploration/env_infos/final/reward_dist Max              0.859398\n",
      "exploration/env_infos/final/reward_dist Min              2.62442e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000422854\n",
      "exploration/env_infos/initial/reward_dist Std            0.000689362\n",
      "exploration/env_infos/initial/reward_dist Max            0.00178991\n",
      "exploration/env_infos/initial/reward_dist Min            6.57361e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.152567\n",
      "exploration/env_infos/reward_dist Std                    0.268384\n",
      "exploration/env_infos/reward_dist Max                    0.985489\n",
      "exploration/env_infos/reward_dist Min                    2.62442e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0930777\n",
      "exploration/env_infos/final/reward_energy Std            0.027806\n",
      "exploration/env_infos/final/reward_energy Max           -0.0555416\n",
      "exploration/env_infos/final/reward_energy Min           -0.133899\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.217908\n",
      "exploration/env_infos/initial/reward_energy Std          0.124444\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0673704\n",
      "exploration/env_infos/initial/reward_energy Min         -0.438353\n",
      "exploration/env_infos/reward_energy Mean                -0.11635\n",
      "exploration/env_infos/reward_energy Std                  0.0838667\n",
      "exploration/env_infos/reward_energy Max                 -0.00797141\n",
      "exploration/env_infos/reward_energy Min                 -0.502748\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00291215\n",
      "exploration/env_infos/final/end_effector_loc Std         0.261752\n",
      "exploration/env_infos/final/end_effector_loc Max         0.50921\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.410664\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000503452\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00885773\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0210718\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0106323\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00103228\n",
      "exploration/env_infos/end_effector_loc Std               0.15476\n",
      "exploration/env_infos/end_effector_loc Max               0.50921\n",
      "exploration/env_infos/end_effector_loc Min              -0.410664\n",
      "evaluation/num steps total                           50000\n",
      "evaluation/num paths total                            2500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0292299\n",
      "evaluation/Rewards Std                                   0.0746181\n",
      "evaluation/Rewards Max                                   0.17562\n",
      "evaluation/Rewards Min                                  -0.389378\n",
      "evaluation/Returns Mean                                 -0.584598\n",
      "evaluation/Returns Std                                   1.15166\n",
      "evaluation/Returns Max                                   2.36187\n",
      "evaluation/Returns Min                                  -3.02301\n",
      "evaluation/Actions Mean                                 -0.00029266\n",
      "evaluation/Actions Std                                   0.0668412\n",
      "evaluation/Actions Max                                   0.72327\n",
      "evaluation/Actions Min                                  -0.639678\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.584598\n",
      "evaluation/env_infos/final/reward_dist Mean              0.254046\n",
      "evaluation/env_infos/final/reward_dist Std               0.285796\n",
      "evaluation/env_infos/final/reward_dist Max               0.962007\n",
      "evaluation/env_infos/final/reward_dist Min               1.55334e-16\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00495188\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0103338\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0628671\n",
      "evaluation/env_infos/initial/reward_dist Min             2.72122e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.2712\n",
      "evaluation/env_infos/reward_dist Std                     0.30397\n",
      "evaluation/env_infos/reward_dist Max                     0.99649\n",
      "evaluation/env_infos/reward_dist Min                     1.55334e-16\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0467803\n",
      "evaluation/env_infos/final/reward_energy Std             0.0483226\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00205352\n",
      "evaluation/env_infos/final/reward_energy Min            -0.21577\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.229702\n",
      "evaluation/env_infos/initial/reward_energy Std           0.199972\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0180527\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.777579\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0598246\n",
      "evaluation/env_infos/reward_energy Std                   0.0731894\n",
      "evaluation/env_infos/reward_energy Max                  -0.00142167\n",
      "evaluation/env_infos/reward_energy Min                  -0.777579\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0054863\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.229811\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.478404\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.526183\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00073003\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107428\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0361635\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0319839\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00597361\n",
      "evaluation/env_infos/end_effector_loc Std                0.150332\n",
      "evaluation/env_infos/end_effector_loc Max                0.478404\n",
      "evaluation/env_infos/end_effector_loc Min               -0.526183\n",
      "time/data storing (s)                                    0.00614742\n",
      "time/evaluation sampling (s)                             0.929181\n",
      "time/exploration sampling (s)                            0.121381\n",
      "time/logging (s)                                         0.0205667\n",
      "time/saving (s)                                          0.0326913\n",
      "time/training (s)                                       47.361\n",
      "time/epoch (s)                                          48.471\n",
      "time/total (s)                                        2438.22\n",
      "Epoch                                                   49\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:30:07.669214 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 50 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000930431\n",
      "trainer/QF2 Loss                                         0.00196105\n",
      "trainer/Policy Loss                                      3.11011\n",
      "trainer/Q1 Predictions Mean                             -1.16912\n",
      "trainer/Q1 Predictions Std                               0.826708\n",
      "trainer/Q1 Predictions Max                               0.738173\n",
      "trainer/Q1 Predictions Min                              -3.66051\n",
      "trainer/Q2 Predictions Mean                             -1.16703\n",
      "trainer/Q2 Predictions Std                               0.82754\n",
      "trainer/Q2 Predictions Max                               0.741095\n",
      "trainer/Q2 Predictions Min                              -3.6351\n",
      "trainer/Q Targets Mean                                  -1.16978\n",
      "trainer/Q Targets Std                                    0.831268\n",
      "trainer/Q Targets Max                                    0.769946\n",
      "trainer/Q Targets Min                                   -3.60348\n",
      "trainer/Log Pis Mean                                     1.94895\n",
      "trainer/Log Pis Std                                      1.35798\n",
      "trainer/Log Pis Max                                      4.52307\n",
      "trainer/Log Pis Min                                     -3.07042\n",
      "trainer/Policy mu Mean                                  -0.00712857\n",
      "trainer/Policy mu Std                                    0.311059\n",
      "trainer/Policy mu Max                                    1.8665\n",
      "trainer/Policy mu Min                                   -2.59518\n",
      "trainer/Policy log std Mean                             -2.3282\n",
      "trainer/Policy log std Std                               0.561481\n",
      "trainer/Policy log std Max                              -0.479059\n",
      "trainer/Policy log std Min                              -3.41356\n",
      "trainer/Alpha                                            0.0232164\n",
      "trainer/Alpha Loss                                      -0.192145\n",
      "exploration/num steps total                           6100\n",
      "exploration/num paths total                            305\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0810045\n",
      "exploration/Rewards Std                                  0.0928655\n",
      "exploration/Rewards Max                                  0.0734728\n",
      "exploration/Rewards Min                                 -0.345576\n",
      "exploration/Returns Mean                                -1.62009\n",
      "exploration/Returns Std                                  1.56618\n",
      "exploration/Returns Max                                 -0.104671\n",
      "exploration/Returns Min                                 -4.5468\n",
      "exploration/Actions Mean                                 0.00283664\n",
      "exploration/Actions Std                                  0.159671\n",
      "exploration/Actions Max                                  0.600139\n",
      "exploration/Actions Min                                 -0.502598\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.62009\n",
      "exploration/env_infos/final/reward_dist Mean             0.19838\n",
      "exploration/env_infos/final/reward_dist Std              0.28641\n",
      "exploration/env_infos/final/reward_dist Max              0.749568\n",
      "exploration/env_infos/final/reward_dist Min              0.0029007\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0144001\n",
      "exploration/env_infos/initial/reward_dist Std            0.018645\n",
      "exploration/env_infos/initial/reward_dist Max            0.0473659\n",
      "exploration/env_infos/initial/reward_dist Min            1.67525e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.211619\n",
      "exploration/env_infos/reward_dist Std                    0.256774\n",
      "exploration/env_infos/reward_dist Max                    0.860062\n",
      "exploration/env_infos/reward_dist Min                    1.67525e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.183333\n",
      "exploration/env_infos/final/reward_energy Std            0.138159\n",
      "exploration/env_infos/final/reward_energy Max           -0.0855159\n",
      "exploration/env_infos/final/reward_energy Min           -0.454748\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.416215\n",
      "exploration/env_infos/initial/reward_energy Std          0.261392\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0135358\n",
      "exploration/env_infos/initial/reward_energy Min         -0.750177\n",
      "exploration/env_infos/reward_energy Mean                -0.178796\n",
      "exploration/env_infos/reward_energy Std                  0.137979\n",
      "exploration/env_infos/reward_energy Max                 -0.0135358\n",
      "exploration/env_infos/reward_energy Min                 -0.750177\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0284484\n",
      "exploration/env_infos/final/end_effector_loc Std         0.186544\n",
      "exploration/env_infos/final/end_effector_loc Max         0.278515\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.272251\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0089079\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0149198\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0300069\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0098107\n",
      "exploration/env_infos/end_effector_loc Mean              0.0221502\n",
      "exploration/env_infos/end_effector_loc Std               0.140647\n",
      "exploration/env_infos/end_effector_loc Max               0.304501\n",
      "exploration/env_infos/end_effector_loc Min              -0.272251\n",
      "evaluation/num steps total                           51000\n",
      "evaluation/num paths total                            2550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0452181\n",
      "evaluation/Rewards Std                                   0.0748472\n",
      "evaluation/Rewards Max                                   0.150032\n",
      "evaluation/Rewards Min                                  -0.358304\n",
      "evaluation/Returns Mean                                 -0.904362\n",
      "evaluation/Returns Std                                   1.11873\n",
      "evaluation/Returns Max                                   2.00225\n",
      "evaluation/Returns Min                                  -4.63484\n",
      "evaluation/Actions Mean                                 -0.000862576\n",
      "evaluation/Actions Std                                   0.0642702\n",
      "evaluation/Actions Max                                   0.848763\n",
      "evaluation/Actions Min                                  -0.481476\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.904362\n",
      "evaluation/env_infos/final/reward_dist Mean              0.24121\n",
      "evaluation/env_infos/final/reward_dist Std               0.300392\n",
      "evaluation/env_infos/final/reward_dist Max               0.871451\n",
      "evaluation/env_infos/final/reward_dist Min               1.10995e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0047505\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0122326\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0677827\n",
      "evaluation/env_infos/initial/reward_dist Min             2.38074e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.224137\n",
      "evaluation/env_infos/reward_dist Std                     0.276229\n",
      "evaluation/env_infos/reward_dist Max                     0.99087\n",
      "evaluation/env_infos/reward_dist Min                     1.10995e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0289703\n",
      "evaluation/env_infos/final/reward_energy Std             0.027025\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00111332\n",
      "evaluation/env_infos/final/reward_energy Min            -0.119721\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.227928\n",
      "evaluation/env_infos/initial/reward_energy Std           0.189499\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0200583\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.918324\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0546202\n",
      "evaluation/env_infos/reward_energy Std                   0.0726597\n",
      "evaluation/env_infos/reward_energy Max                  -0.000357277\n",
      "evaluation/env_infos/reward_energy Min                  -0.918324\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.003238\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.228114\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.499168\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.462797\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00178451\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0103267\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0424382\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0240738\n",
      "evaluation/env_infos/end_effector_loc Mean               0.000761219\n",
      "evaluation/env_infos/end_effector_loc Std                0.146819\n",
      "evaluation/env_infos/end_effector_loc Max                0.499168\n",
      "evaluation/env_infos/end_effector_loc Min               -0.462797\n",
      "time/data storing (s)                                    0.0059458\n",
      "time/evaluation sampling (s)                             1.06906\n",
      "time/exploration sampling (s)                            0.129234\n",
      "time/logging (s)                                         0.020697\n",
      "time/saving (s)                                          0.0507823\n",
      "time/training (s)                                       49.1764\n",
      "time/epoch (s)                                          50.4521\n",
      "time/total (s)                                        2489.37\n",
      "Epoch                                                   50\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:30:58.337709 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 51 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000904333\n",
      "trainer/QF2 Loss                                         0.00117275\n",
      "trainer/Policy Loss                                      3.26863\n",
      "trainer/Q1 Predictions Mean                             -1.21251\n",
      "trainer/Q1 Predictions Std                               0.811331\n",
      "trainer/Q1 Predictions Max                               0.777998\n",
      "trainer/Q1 Predictions Min                              -3.21057\n",
      "trainer/Q2 Predictions Mean                             -1.21649\n",
      "trainer/Q2 Predictions Std                               0.814107\n",
      "trainer/Q2 Predictions Max                               0.821427\n",
      "trainer/Q2 Predictions Min                              -3.20546\n",
      "trainer/Q Targets Mean                                  -1.21778\n",
      "trainer/Q Targets Std                                    0.813465\n",
      "trainer/Q Targets Max                                    0.816371\n",
      "trainer/Q Targets Min                                   -3.22547\n",
      "trainer/Log Pis Mean                                     2.08189\n",
      "trainer/Log Pis Std                                      1.38253\n",
      "trainer/Log Pis Max                                      4.7481\n",
      "trainer/Log Pis Min                                     -4.53578\n",
      "trainer/Policy mu Mean                                  -0.00492311\n",
      "trainer/Policy mu Std                                    0.408224\n",
      "trainer/Policy mu Max                                    2.32458\n",
      "trainer/Policy mu Min                                   -2.40185\n",
      "trainer/Policy log std Mean                             -2.28238\n",
      "trainer/Policy log std Std                               0.644627\n",
      "trainer/Policy log std Max                              -0.33844\n",
      "trainer/Policy log std Min                              -3.33908\n",
      "trainer/Alpha                                            0.0238049\n",
      "trainer/Alpha Loss                                       0.306112\n",
      "exploration/num steps total                           6200\n",
      "exploration/num paths total                            310\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.066813\n",
      "exploration/Rewards Std                                  0.118304\n",
      "exploration/Rewards Max                                  0.108285\n",
      "exploration/Rewards Min                                 -0.316962\n",
      "exploration/Returns Mean                                -1.33626\n",
      "exploration/Returns Std                                  2.14377\n",
      "exploration/Returns Max                                  0.885778\n",
      "exploration/Returns Min                                 -3.94486\n",
      "exploration/Actions Mean                                 0.00984984\n",
      "exploration/Actions Std                                  0.144137\n",
      "exploration/Actions Max                                  0.540499\n",
      "exploration/Actions Min                                 -0.403546\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.33626\n",
      "exploration/env_infos/final/reward_dist Mean             0.210106\n",
      "exploration/env_infos/final/reward_dist Std              0.390984\n",
      "exploration/env_infos/final/reward_dist Max              0.991077\n",
      "exploration/env_infos/final/reward_dist Min              4.75285e-37\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00339216\n",
      "exploration/env_infos/initial/reward_dist Std            0.00326443\n",
      "exploration/env_infos/initial/reward_dist Max            0.00860955\n",
      "exploration/env_infos/initial/reward_dist Min            0.000326628\n",
      "exploration/env_infos/reward_dist Mean                   0.21581\n",
      "exploration/env_infos/reward_dist Std                    0.300116\n",
      "exploration/env_infos/reward_dist Max                    0.991077\n",
      "exploration/env_infos/reward_dist Min                    4.75285e-37\n",
      "exploration/env_infos/final/reward_energy Mean          -0.167861\n",
      "exploration/env_infos/final/reward_energy Std            0.0788281\n",
      "exploration/env_infos/final/reward_energy Max           -0.0787822\n",
      "exploration/env_infos/final/reward_energy Min           -0.301254\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.165379\n",
      "exploration/env_infos/initial/reward_energy Std          0.0586774\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0872216\n",
      "exploration/env_infos/initial/reward_energy Min         -0.224193\n",
      "exploration/env_infos/reward_energy Mean                -0.176492\n",
      "exploration/env_infos/reward_energy Std                  0.102935\n",
      "exploration/env_infos/reward_energy Max                 -0.0223274\n",
      "exploration/env_infos/reward_energy Min                 -0.567271\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0600454\n",
      "exploration/env_infos/final/end_effector_loc Std         0.338078\n",
      "exploration/env_infos/final/end_effector_loc Max         0.934076\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.365077\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0024262\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00571009\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0100606\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0111973\n",
      "exploration/env_infos/end_effector_loc Mean              0.00624268\n",
      "exploration/env_infos/end_effector_loc Std               0.200985\n",
      "exploration/env_infos/end_effector_loc Max               0.934076\n",
      "exploration/env_infos/end_effector_loc Min              -0.436115\n",
      "evaluation/num steps total                           52000\n",
      "evaluation/num paths total                            2600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0407576\n",
      "evaluation/Rewards Std                                   0.0747096\n",
      "evaluation/Rewards Max                                   0.142922\n",
      "evaluation/Rewards Min                                  -0.334146\n",
      "evaluation/Returns Mean                                 -0.815151\n",
      "evaluation/Returns Std                                   1.19759\n",
      "evaluation/Returns Max                                   1.5207\n",
      "evaluation/Returns Min                                  -3.35683\n",
      "evaluation/Actions Mean                                 -0.00261061\n",
      "evaluation/Actions Std                                   0.0629468\n",
      "evaluation/Actions Max                                   0.610671\n",
      "evaluation/Actions Min                                  -0.522691\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.815151\n",
      "evaluation/env_infos/final/reward_dist Mean              0.184067\n",
      "evaluation/env_infos/final/reward_dist Std               0.272561\n",
      "evaluation/env_infos/final/reward_dist Max               0.967197\n",
      "evaluation/env_infos/final/reward_dist Min               2.6597e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00901637\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165273\n",
      "evaluation/env_infos/initial/reward_dist Max             0.099355\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0144e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.202921\n",
      "evaluation/env_infos/reward_dist Std                     0.276672\n",
      "evaluation/env_infos/reward_dist Max                     0.994601\n",
      "evaluation/env_infos/reward_dist Min                     2.6597e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0351837\n",
      "evaluation/env_infos/final/reward_energy Std             0.0218173\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00195016\n",
      "evaluation/env_infos/final/reward_energy Min            -0.114533\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.216192\n",
      "evaluation/env_infos/initial/reward_energy Std           0.170347\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00286727\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.67016\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0577525\n",
      "evaluation/env_infos/reward_energy Std                   0.0678445\n",
      "evaluation/env_infos/reward_energy Max                  -0.000369813\n",
      "evaluation/env_infos/reward_energy Min                  -0.67016\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0293417\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.228897\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.451517\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.710578\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000109087\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0097306\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0305336\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0261346\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0151807\n",
      "evaluation/env_infos/end_effector_loc Std                0.152127\n",
      "evaluation/env_infos/end_effector_loc Max                0.451517\n",
      "evaluation/env_infos/end_effector_loc Min               -0.710578\n",
      "time/data storing (s)                                    0.00593039\n",
      "time/evaluation sampling (s)                             0.944921\n",
      "time/exploration sampling (s)                            0.131909\n",
      "time/logging (s)                                         0.0202564\n",
      "time/saving (s)                                          0.0267373\n",
      "time/training (s)                                       48.9157\n",
      "time/epoch (s)                                          50.0454\n",
      "time/total (s)                                        2540.04\n",
      "Epoch                                                   51\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:31:48.945332 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 52 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000797178\n",
      "trainer/QF2 Loss                                         0.000802408\n",
      "trainer/Policy Loss                                      3.32765\n",
      "trainer/Q1 Predictions Mean                             -1.20978\n",
      "trainer/Q1 Predictions Std                               0.850654\n",
      "trainer/Q1 Predictions Max                               0.615514\n",
      "trainer/Q1 Predictions Min                              -3.37473\n",
      "trainer/Q2 Predictions Mean                             -1.20404\n",
      "trainer/Q2 Predictions Std                               0.849184\n",
      "trainer/Q2 Predictions Max                               0.5924\n",
      "trainer/Q2 Predictions Min                              -3.39909\n",
      "trainer/Q Targets Mean                                  -1.20439\n",
      "trainer/Q Targets Std                                    0.852216\n",
      "trainer/Q Targets Max                                    0.615307\n",
      "trainer/Q Targets Min                                   -3.40374\n",
      "trainer/Log Pis Mean                                     2.13836\n",
      "trainer/Log Pis Std                                      1.40004\n",
      "trainer/Log Pis Max                                      4.878\n",
      "trainer/Log Pis Min                                     -6.29953\n",
      "trainer/Policy mu Mean                                  -0.0219908\n",
      "trainer/Policy mu Std                                    0.289494\n",
      "trainer/Policy mu Max                                    1.76574\n",
      "trainer/Policy mu Min                                   -1.50208\n",
      "trainer/Policy log std Mean                             -2.36337\n",
      "trainer/Policy log std Std                               0.607575\n",
      "trainer/Policy log std Max                              -0.41797\n",
      "trainer/Policy log std Min                              -3.40519\n",
      "trainer/Alpha                                            0.0244192\n",
      "trainer/Alpha Loss                                       0.513728\n",
      "exploration/num steps total                           6300\n",
      "exploration/num paths total                            315\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.11442\n",
      "exploration/Rewards Std                                  0.0682617\n",
      "exploration/Rewards Max                                  0.0495646\n",
      "exploration/Rewards Min                                 -0.24037\n",
      "exploration/Returns Mean                                -2.28839\n",
      "exploration/Returns Std                                  1.03234\n",
      "exploration/Returns Max                                 -0.564303\n",
      "exploration/Returns Min                                 -3.36014\n",
      "exploration/Actions Mean                                -0.00685698\n",
      "exploration/Actions Std                                  0.140205\n",
      "exploration/Actions Max                                  0.475446\n",
      "exploration/Actions Min                                 -0.472318\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.28839\n",
      "exploration/env_infos/final/reward_dist Mean             0.234782\n",
      "exploration/env_infos/final/reward_dist Std              0.265322\n",
      "exploration/env_infos/final/reward_dist Max              0.698243\n",
      "exploration/env_infos/final/reward_dist Min              3.96283e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000980329\n",
      "exploration/env_infos/initial/reward_dist Std            0.00150559\n",
      "exploration/env_infos/initial/reward_dist Max            0.00391369\n",
      "exploration/env_infos/initial/reward_dist Min            1.99762e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.131071\n",
      "exploration/env_infos/reward_dist Std                    0.177697\n",
      "exploration/env_infos/reward_dist Max                    0.698243\n",
      "exploration/env_infos/reward_dist Min                    3.96283e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.204451\n",
      "exploration/env_infos/final/reward_energy Std            0.0928626\n",
      "exploration/env_infos/final/reward_energy Max           -0.0939382\n",
      "exploration/env_infos/final/reward_energy Min           -0.364439\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.188972\n",
      "exploration/env_infos/initial/reward_energy Std          0.111346\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0223325\n",
      "exploration/env_infos/initial/reward_energy Min         -0.296804\n",
      "exploration/env_infos/reward_energy Mean                -0.161384\n",
      "exploration/env_infos/reward_energy Std                  0.115604\n",
      "exploration/env_infos/reward_energy Max                 -0.0223325\n",
      "exploration/env_infos/reward_energy Min                 -0.545517\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0819955\n",
      "exploration/env_infos/final/end_effector_loc Std         0.171436\n",
      "exploration/env_infos/final/end_effector_loc Max         0.158822\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.399789\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00295474\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00716972\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00836537\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0145183\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0430389\n",
      "exploration/env_infos/end_effector_loc Std               0.110306\n",
      "exploration/env_infos/end_effector_loc Max               0.158822\n",
      "exploration/env_infos/end_effector_loc Min              -0.399789\n",
      "evaluation/num steps total                           53000\n",
      "evaluation/num paths total                            2650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0345209\n",
      "evaluation/Rewards Std                                   0.0719625\n",
      "evaluation/Rewards Max                                   0.167016\n",
      "evaluation/Rewards Min                                  -0.262125\n",
      "evaluation/Returns Mean                                 -0.690417\n",
      "evaluation/Returns Std                                   1.07925\n",
      "evaluation/Returns Max                                   1.4289\n",
      "evaluation/Returns Min                                  -2.81398\n",
      "evaluation/Actions Mean                                 -0.00689652\n",
      "evaluation/Actions Std                                   0.0613484\n",
      "evaluation/Actions Max                                   0.713829\n",
      "evaluation/Actions Min                                  -0.407931\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.690417\n",
      "evaluation/env_infos/final/reward_dist Mean              0.22301\n",
      "evaluation/env_infos/final/reward_dist Std               0.316288\n",
      "evaluation/env_infos/final/reward_dist Max               0.987301\n",
      "evaluation/env_infos/final/reward_dist Min               3.21611e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00749662\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126822\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0541257\n",
      "evaluation/env_infos/initial/reward_dist Min             3.63815e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.227714\n",
      "evaluation/env_infos/reward_dist Std                     0.295191\n",
      "evaluation/env_infos/reward_dist Max                     0.99994\n",
      "evaluation/env_infos/reward_dist Min                     3.21611e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0495944\n",
      "evaluation/env_infos/final/reward_energy Std             0.0490761\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00262902\n",
      "evaluation/env_infos/final/reward_energy Min            -0.303293\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.206776\n",
      "evaluation/env_infos/initial/reward_energy Std           0.146424\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0409543\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.77237\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0595007\n",
      "evaluation/env_infos/reward_energy Std                   0.0638909\n",
      "evaluation/env_infos/reward_energy Max                  -0.000880742\n",
      "evaluation/env_infos/reward_energy Min                  -0.77237\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0996025\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.211295\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.384584\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.720414\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00135553\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00885483\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0356915\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0203965\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0485129\n",
      "evaluation/env_infos/end_effector_loc Std                0.143297\n",
      "evaluation/env_infos/end_effector_loc Max                0.384584\n",
      "evaluation/env_infos/end_effector_loc Min               -0.720414\n",
      "time/data storing (s)                                    0.00592283\n",
      "time/evaluation sampling (s)                             0.973389\n",
      "time/exploration sampling (s)                            0.122246\n",
      "time/logging (s)                                         0.0209363\n",
      "time/saving (s)                                          0.0315648\n",
      "time/training (s)                                       48.7493\n",
      "time/epoch (s)                                          49.9033\n",
      "time/total (s)                                        2590.64\n",
      "Epoch                                                   52\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:32:38.947001 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 53 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000889647\n",
      "trainer/QF2 Loss                                         0.000959159\n",
      "trainer/Policy Loss                                      3.11983\n",
      "trainer/Q1 Predictions Mean                             -1.14934\n",
      "trainer/Q1 Predictions Std                               0.899579\n",
      "trainer/Q1 Predictions Max                               0.674168\n",
      "trainer/Q1 Predictions Min                              -3.23604\n",
      "trainer/Q2 Predictions Mean                             -1.15062\n",
      "trainer/Q2 Predictions Std                               0.898039\n",
      "trainer/Q2 Predictions Max                               0.643436\n",
      "trainer/Q2 Predictions Min                              -3.25527\n",
      "trainer/Q Targets Mean                                  -1.1557\n",
      "trainer/Q Targets Std                                    0.901569\n",
      "trainer/Q Targets Max                                    0.674461\n",
      "trainer/Q Targets Min                                   -3.23407\n",
      "trainer/Log Pis Mean                                     1.99344\n",
      "trainer/Log Pis Std                                      1.41008\n",
      "trainer/Log Pis Max                                      4.39949\n",
      "trainer/Log Pis Min                                     -3.50106\n",
      "trainer/Policy mu Mean                                  -0.0350545\n",
      "trainer/Policy mu Std                                    0.38448\n",
      "trainer/Policy mu Max                                    1.96753\n",
      "trainer/Policy mu Min                                   -2.09144\n",
      "trainer/Policy log std Mean                             -2.32457\n",
      "trainer/Policy log std Std                               0.655847\n",
      "trainer/Policy log std Max                              -0.4055\n",
      "trainer/Policy log std Min                              -3.27881\n",
      "trainer/Alpha                                            0.0236831\n",
      "trainer/Alpha Loss                                      -0.0245439\n",
      "exploration/num steps total                           6400\n",
      "exploration/num paths total                            320\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0856305\n",
      "exploration/Rewards Std                                  0.0612731\n",
      "exploration/Rewards Max                                  0.0317994\n",
      "exploration/Rewards Min                                 -0.307927\n",
      "exploration/Returns Mean                                -1.71261\n",
      "exploration/Returns Std                                  0.815306\n",
      "exploration/Returns Max                                 -1.24042\n",
      "exploration/Returns Min                                 -3.33797\n",
      "exploration/Actions Mean                                 0.0057644\n",
      "exploration/Actions Std                                  0.105957\n",
      "exploration/Actions Max                                  0.354285\n",
      "exploration/Actions Min                                 -0.328516\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.71261\n",
      "exploration/env_infos/final/reward_dist Mean             0.0274294\n",
      "exploration/env_infos/final/reward_dist Std              0.0232549\n",
      "exploration/env_infos/final/reward_dist Max              0.0559839\n",
      "exploration/env_infos/final/reward_dist Min              1.73622e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00166328\n",
      "exploration/env_infos/initial/reward_dist Std            0.00305434\n",
      "exploration/env_infos/initial/reward_dist Max            0.00776227\n",
      "exploration/env_infos/initial/reward_dist Min            2.59405e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.101086\n",
      "exploration/env_infos/reward_dist Std                    0.188662\n",
      "exploration/env_infos/reward_dist Max                    0.957581\n",
      "exploration/env_infos/reward_dist Min                    1.73622e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.145106\n",
      "exploration/env_infos/final/reward_energy Std            0.103285\n",
      "exploration/env_infos/final/reward_energy Max           -0.0575602\n",
      "exploration/env_infos/final/reward_energy Min           -0.336434\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.159824\n",
      "exploration/env_infos/initial/reward_energy Std          0.120997\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0432144\n",
      "exploration/env_infos/initial/reward_energy Min         -0.390392\n",
      "exploration/env_infos/reward_energy Mean                -0.123014\n",
      "exploration/env_infos/reward_energy Std                  0.0859521\n",
      "exploration/env_infos/reward_energy Max                 -0.0071946\n",
      "exploration/env_infos/reward_energy Min                 -0.390392\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0877842\n",
      "exploration/env_infos/final/end_effector_loc Std         0.15893\n",
      "exploration/env_infos/final/end_effector_loc Max         0.344685\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.113209\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00336452\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0062378\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0177143\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00513565\n",
      "exploration/env_infos/end_effector_loc Mean              0.0502868\n",
      "exploration/env_infos/end_effector_loc Std               0.0989823\n",
      "exploration/env_infos/end_effector_loc Max               0.344685\n",
      "exploration/env_infos/end_effector_loc Min              -0.117175\n",
      "evaluation/num steps total                           54000\n",
      "evaluation/num paths total                            2700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0352698\n",
      "evaluation/Rewards Std                                   0.0754376\n",
      "evaluation/Rewards Max                                   0.160137\n",
      "evaluation/Rewards Min                                  -0.299178\n",
      "evaluation/Returns Mean                                 -0.705396\n",
      "evaluation/Returns Std                                   1.14416\n",
      "evaluation/Returns Max                                   2.12282\n",
      "evaluation/Returns Min                                  -3.20598\n",
      "evaluation/Actions Mean                                 -0.00130894\n",
      "evaluation/Actions Std                                   0.0631493\n",
      "evaluation/Actions Max                                   0.547755\n",
      "evaluation/Actions Min                                  -0.500634\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.705396\n",
      "evaluation/env_infos/final/reward_dist Mean              0.243749\n",
      "evaluation/env_infos/final/reward_dist Std               0.297426\n",
      "evaluation/env_infos/final/reward_dist Max               0.989333\n",
      "evaluation/env_infos/final/reward_dist Min               5.47785e-08\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00405859\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00961935\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0466112\n",
      "evaluation/env_infos/initial/reward_dist Min             3.09716e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.211643\n",
      "evaluation/env_infos/reward_dist Std                     0.282828\n",
      "evaluation/env_infos/reward_dist Max                     0.989333\n",
      "evaluation/env_infos/reward_dist Min                     5.47785e-08\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.036125\n",
      "evaluation/env_infos/final/reward_energy Std             0.0386398\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00191705\n",
      "evaluation/env_infos/final/reward_energy Min            -0.258554\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.209564\n",
      "evaluation/env_infos/initial/reward_energy Std           0.135322\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0379937\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.549453\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0604533\n",
      "evaluation/env_infos/reward_energy Std                   0.0657609\n",
      "evaluation/env_infos/reward_energy Max                  -0.00100759\n",
      "evaluation/env_infos/reward_energy Min                  -0.549453\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0135416\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.206579\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.388972\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.492167\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       4.04668e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00881957\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0273878\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0250317\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00506232\n",
      "evaluation/env_infos/end_effector_loc Std                0.14432\n",
      "evaluation/env_infos/end_effector_loc Max                0.388972\n",
      "evaluation/env_infos/end_effector_loc Min               -0.492167\n",
      "time/data storing (s)                                    0.00616294\n",
      "time/evaluation sampling (s)                             1.16704\n",
      "time/exploration sampling (s)                            0.12257\n",
      "time/logging (s)                                         0.0201445\n",
      "time/saving (s)                                          0.0264061\n",
      "time/training (s)                                       47.9284\n",
      "time/epoch (s)                                          49.2707\n",
      "time/total (s)                                        2640.64\n",
      "Epoch                                                   53\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:33:29.200411 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 54 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000962548\n",
      "trainer/QF2 Loss                                         0.00127294\n",
      "trainer/Policy Loss                                      3.04722\n",
      "trainer/Q1 Predictions Mean                             -1.09583\n",
      "trainer/Q1 Predictions Std                               0.87141\n",
      "trainer/Q1 Predictions Max                               0.708822\n",
      "trainer/Q1 Predictions Min                              -3.85709\n",
      "trainer/Q2 Predictions Mean                             -1.10058\n",
      "trainer/Q2 Predictions Std                               0.874078\n",
      "trainer/Q2 Predictions Max                               0.677514\n",
      "trainer/Q2 Predictions Min                              -3.82748\n",
      "trainer/Q Targets Mean                                  -1.10428\n",
      "trainer/Q Targets Std                                    0.869968\n",
      "trainer/Q Targets Max                                    0.737687\n",
      "trainer/Q Targets Min                                   -3.83666\n",
      "trainer/Log Pis Mean                                     1.96424\n",
      "trainer/Log Pis Std                                      1.32596\n",
      "trainer/Log Pis Max                                      4.24918\n",
      "trainer/Log Pis Min                                     -3.00118\n",
      "trainer/Policy mu Mean                                   0.0300406\n",
      "trainer/Policy mu Std                                    0.30429\n",
      "trainer/Policy mu Max                                    2.51559\n",
      "trainer/Policy mu Min                                   -1.16532\n",
      "trainer/Policy log std Mean                             -2.29704\n",
      "trainer/Policy log std Std                               0.548778\n",
      "trainer/Policy log std Max                               0.335096\n",
      "trainer/Policy log std Min                              -3.11468\n",
      "trainer/Alpha                                            0.0252713\n",
      "trainer/Alpha Loss                                      -0.131487\n",
      "exploration/num steps total                           6500\n",
      "exploration/num paths total                            325\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0720851\n",
      "exploration/Rewards Std                                  0.0857127\n",
      "exploration/Rewards Max                                  0.125596\n",
      "exploration/Rewards Min                                 -0.287932\n",
      "exploration/Returns Mean                                -1.4417\n",
      "exploration/Returns Std                                  1.30598\n",
      "exploration/Returns Max                                  0.953494\n",
      "exploration/Returns Min                                 -2.7154\n",
      "exploration/Actions Mean                                 0.00526958\n",
      "exploration/Actions Std                                  0.125706\n",
      "exploration/Actions Max                                  0.429613\n",
      "exploration/Actions Min                                 -0.378126\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.4417\n",
      "exploration/env_infos/final/reward_dist Mean             0.21823\n",
      "exploration/env_infos/final/reward_dist Std              0.316596\n",
      "exploration/env_infos/final/reward_dist Max              0.81394\n",
      "exploration/env_infos/final/reward_dist Min              7.6809e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00850705\n",
      "exploration/env_infos/initial/reward_dist Std            0.0116525\n",
      "exploration/env_infos/initial/reward_dist Max            0.0300381\n",
      "exploration/env_infos/initial/reward_dist Min            9.51652e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.137011\n",
      "exploration/env_infos/reward_dist Std                    0.229575\n",
      "exploration/env_infos/reward_dist Max                    0.911513\n",
      "exploration/env_infos/reward_dist Min                    7.6809e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.196526\n",
      "exploration/env_infos/final/reward_energy Std            0.186132\n",
      "exploration/env_infos/final/reward_energy Max           -0.0497952\n",
      "exploration/env_infos/final/reward_energy Min           -0.525635\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.22326\n",
      "exploration/env_infos/initial/reward_energy Std          0.0806646\n",
      "exploration/env_infos/initial/reward_energy Max         -0.105965\n",
      "exploration/env_infos/initial/reward_energy Min         -0.340049\n",
      "exploration/env_infos/reward_energy Mean                -0.151159\n",
      "exploration/env_infos/reward_energy Std                  0.0938652\n",
      "exploration/env_infos/reward_energy Max                 -0.0194159\n",
      "exploration/env_infos/reward_energy Min                 -0.525635\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0404708\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275561\n",
      "exploration/env_infos/final/end_effector_loc Max         0.485724\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.363732\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000195802\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00839056\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0133525\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0120672\n",
      "exploration/env_infos/end_effector_loc Mean              0.0181761\n",
      "exploration/env_infos/end_effector_loc Std               0.180929\n",
      "exploration/env_infos/end_effector_loc Max               0.486604\n",
      "exploration/env_infos/end_effector_loc Min              -0.363732\n",
      "evaluation/num steps total                           55000\n",
      "evaluation/num paths total                            2750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0289836\n",
      "evaluation/Rewards Std                                   0.0701624\n",
      "evaluation/Rewards Max                                   0.149601\n",
      "evaluation/Rewards Min                                  -0.255956\n",
      "evaluation/Returns Mean                                 -0.579673\n",
      "evaluation/Returns Std                                   1.13946\n",
      "evaluation/Returns Max                                   2.02353\n",
      "evaluation/Returns Min                                  -4.24555\n",
      "evaluation/Actions Mean                                 -0.00272927\n",
      "evaluation/Actions Std                                   0.0511113\n",
      "evaluation/Actions Max                                   0.512478\n",
      "evaluation/Actions Min                                  -0.433115\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.579673\n",
      "evaluation/env_infos/final/reward_dist Mean              0.287293\n",
      "evaluation/env_infos/final/reward_dist Std               0.319397\n",
      "evaluation/env_infos/final/reward_dist Max               0.956751\n",
      "evaluation/env_infos/final/reward_dist Min               8.1413e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705835\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00997688\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0535184\n",
      "evaluation/env_infos/initial/reward_dist Min             2.25872e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.233662\n",
      "evaluation/env_infos/reward_dist Std                     0.286773\n",
      "evaluation/env_infos/reward_dist Max                     0.996958\n",
      "evaluation/env_infos/reward_dist Min                     8.1413e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0328541\n",
      "evaluation/env_infos/final/reward_energy Std             0.0237084\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00302008\n",
      "evaluation/env_infos/final/reward_energy Min            -0.135884\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.167543\n",
      "evaluation/env_infos/initial/reward_energy Std           0.147904\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00461402\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.646923\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0465763\n",
      "evaluation/env_infos/reward_energy Std                   0.0554102\n",
      "evaluation/env_infos/reward_energy Max                  -0.000798735\n",
      "evaluation/env_infos/reward_energy Min                  -0.646923\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0190823\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.189965\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.400674\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.511443\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00161684\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00773426\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0256239\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0216558\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0188775\n",
      "evaluation/env_infos/end_effector_loc Std                0.124316\n",
      "evaluation/env_infos/end_effector_loc Max                0.400674\n",
      "evaluation/env_infos/end_effector_loc Min               -0.511443\n",
      "time/data storing (s)                                    0.00618857\n",
      "time/evaluation sampling (s)                             0.942885\n",
      "time/exploration sampling (s)                            0.125751\n",
      "time/logging (s)                                         0.0193036\n",
      "time/saving (s)                                          0.0278495\n",
      "time/training (s)                                       48.4029\n",
      "time/epoch (s)                                          49.5249\n",
      "time/total (s)                                        2690.89\n",
      "Epoch                                                   54\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:34:19.761922 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 55 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00102885\n",
      "trainer/QF2 Loss                                         0.000819663\n",
      "trainer/Policy Loss                                      3.22843\n",
      "trainer/Q1 Predictions Mean                             -1.23474\n",
      "trainer/Q1 Predictions Std                               0.925401\n",
      "trainer/Q1 Predictions Max                               0.725562\n",
      "trainer/Q1 Predictions Min                              -3.12557\n",
      "trainer/Q2 Predictions Mean                             -1.24235\n",
      "trainer/Q2 Predictions Std                               0.928192\n",
      "trainer/Q2 Predictions Max                               0.733514\n",
      "trainer/Q2 Predictions Min                              -3.15993\n",
      "trainer/Q Targets Mean                                  -1.24808\n",
      "trainer/Q Targets Std                                    0.933604\n",
      "trainer/Q Targets Max                                    0.73529\n",
      "trainer/Q Targets Min                                   -3.16942\n",
      "trainer/Log Pis Mean                                     2.0077\n",
      "trainer/Log Pis Std                                      1.30266\n",
      "trainer/Log Pis Max                                      4.25982\n",
      "trainer/Log Pis Min                                     -2.18862\n",
      "trainer/Policy mu Mean                                   0.0101607\n",
      "trainer/Policy mu Std                                    0.242911\n",
      "trainer/Policy mu Max                                    1.4472\n",
      "trainer/Policy mu Min                                   -1.8177\n",
      "trainer/Policy log std Mean                             -2.3369\n",
      "trainer/Policy log std Std                               0.562387\n",
      "trainer/Policy log std Max                              -0.0993328\n",
      "trainer/Policy log std Min                              -3.27176\n",
      "trainer/Alpha                                            0.024771\n",
      "trainer/Alpha Loss                                       0.0284911\n",
      "exploration/num steps total                           6600\n",
      "exploration/num paths total                            330\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.068173\n",
      "exploration/Rewards Std                                  0.0620386\n",
      "exploration/Rewards Max                                  0.0505592\n",
      "exploration/Rewards Min                                 -0.242164\n",
      "exploration/Returns Mean                                -1.36346\n",
      "exploration/Returns Std                                  0.788125\n",
      "exploration/Returns Max                                 -0.653033\n",
      "exploration/Returns Min                                 -2.75536\n",
      "exploration/Actions Mean                                -0.011289\n",
      "exploration/Actions Std                                  0.137039\n",
      "exploration/Actions Max                                  0.439088\n",
      "exploration/Actions Min                                 -0.664701\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.36346\n",
      "exploration/env_infos/final/reward_dist Mean             0.434309\n",
      "exploration/env_infos/final/reward_dist Std              0.406347\n",
      "exploration/env_infos/final/reward_dist Max              0.975398\n",
      "exploration/env_infos/final/reward_dist Min              7.46344e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00309711\n",
      "exploration/env_infos/initial/reward_dist Std            0.00464012\n",
      "exploration/env_infos/initial/reward_dist Max            0.0122256\n",
      "exploration/env_infos/initial/reward_dist Min            5.68639e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.19844\n",
      "exploration/env_infos/reward_dist Std                    0.283594\n",
      "exploration/env_infos/reward_dist Max                    0.975398\n",
      "exploration/env_infos/reward_dist Min                    7.46344e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.162507\n",
      "exploration/env_infos/final/reward_energy Std            0.0444072\n",
      "exploration/env_infos/final/reward_energy Max           -0.0804247\n",
      "exploration/env_infos/final/reward_energy Min           -0.204613\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.141121\n",
      "exploration/env_infos/initial/reward_energy Std          0.0909552\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0425374\n",
      "exploration/env_infos/initial/reward_energy Min         -0.282402\n",
      "exploration/env_infos/reward_energy Mean                -0.155515\n",
      "exploration/env_infos/reward_energy Std                  0.116745\n",
      "exploration/env_infos/reward_energy Max                 -0.00835471\n",
      "exploration/env_infos/reward_energy Min                 -0.678452\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.144016\n",
      "exploration/env_infos/final/end_effector_loc Std         0.244741\n",
      "exploration/env_infos/final/end_effector_loc Max         0.162452\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.734827\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00459959\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00375215\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.000451328\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0121737\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0613593\n",
      "exploration/env_infos/end_effector_loc Std               0.137991\n",
      "exploration/env_infos/end_effector_loc Max               0.167983\n",
      "exploration/env_infos/end_effector_loc Min              -0.734827\n",
      "evaluation/num steps total                           56000\n",
      "evaluation/num paths total                            2800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0400451\n",
      "evaluation/Rewards Std                                   0.0650278\n",
      "evaluation/Rewards Max                                   0.170612\n",
      "evaluation/Rewards Min                                  -0.354082\n",
      "evaluation/Returns Mean                                 -0.800903\n",
      "evaluation/Returns Std                                   0.939539\n",
      "evaluation/Returns Max                                   2.06777\n",
      "evaluation/Returns Min                                  -2.89494\n",
      "evaluation/Actions Mean                                 -0.000836477\n",
      "evaluation/Actions Std                                   0.0549304\n",
      "evaluation/Actions Max                                   0.486804\n",
      "evaluation/Actions Min                                  -0.482494\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.800903\n",
      "evaluation/env_infos/final/reward_dist Mean              0.247987\n",
      "evaluation/env_infos/final/reward_dist Std               0.302858\n",
      "evaluation/env_infos/final/reward_dist Max               0.992416\n",
      "evaluation/env_infos/final/reward_dist Min               3.00595e-13\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00739897\n",
      "evaluation/env_infos/initial/reward_dist Std             0.013757\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0665826\n",
      "evaluation/env_infos/initial/reward_dist Min             1.99076e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.196515\n",
      "evaluation/env_infos/reward_dist Std                     0.275396\n",
      "evaluation/env_infos/reward_dist Max                     0.998573\n",
      "evaluation/env_infos/reward_dist Min                     3.00595e-13\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0383416\n",
      "evaluation/env_infos/final/reward_energy Std             0.0373802\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000383782\n",
      "evaluation/env_infos/final/reward_energy Min            -0.218097\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.178292\n",
      "evaluation/env_infos/initial/reward_energy Std           0.13642\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00652374\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.552565\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0504309\n",
      "evaluation/env_infos/reward_energy Std                   0.0591001\n",
      "evaluation/env_infos/reward_energy Max                  -0.000383782\n",
      "evaluation/env_infos/reward_energy Min                  -0.552565\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0134279\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.20865\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.388189\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.55612\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000408572\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0079266\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0243402\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0241247\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0100764\n",
      "evaluation/env_infos/end_effector_loc Std                0.136591\n",
      "evaluation/env_infos/end_effector_loc Max                0.388189\n",
      "evaluation/env_infos/end_effector_loc Min               -0.55612\n",
      "time/data storing (s)                                    0.00584427\n",
      "time/evaluation sampling (s)                             0.926655\n",
      "time/exploration sampling (s)                            0.118226\n",
      "time/logging (s)                                         0.0208405\n",
      "time/saving (s)                                          0.0294208\n",
      "time/training (s)                                       48.76\n",
      "time/epoch (s)                                          49.861\n",
      "time/total (s)                                        2741.45\n",
      "Epoch                                                   55\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:35:10.135965 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 56 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00157491\n",
      "trainer/QF2 Loss                                         0.00117135\n",
      "trainer/Policy Loss                                      3.15213\n",
      "trainer/Q1 Predictions Mean                             -1.16263\n",
      "trainer/Q1 Predictions Std                               0.950674\n",
      "trainer/Q1 Predictions Max                               0.478922\n",
      "trainer/Q1 Predictions Min                              -3.50658\n",
      "trainer/Q2 Predictions Mean                             -1.15814\n",
      "trainer/Q2 Predictions Std                               0.955007\n",
      "trainer/Q2 Predictions Max                               0.492868\n",
      "trainer/Q2 Predictions Min                              -3.49712\n",
      "trainer/Q Targets Mean                                  -1.16106\n",
      "trainer/Q Targets Std                                    0.953261\n",
      "trainer/Q Targets Max                                    0.491232\n",
      "trainer/Q Targets Min                                   -3.48732\n",
      "trainer/Log Pis Mean                                     2.01035\n",
      "trainer/Log Pis Std                                      1.25948\n",
      "trainer/Log Pis Max                                      4.54169\n",
      "trainer/Log Pis Min                                     -1.51592\n",
      "trainer/Policy mu Mean                                   0.0119769\n",
      "trainer/Policy mu Std                                    0.349966\n",
      "trainer/Policy mu Max                                    2.33042\n",
      "trainer/Policy mu Min                                   -2.07178\n",
      "trainer/Policy log std Mean                             -2.27501\n",
      "trainer/Policy log std Std                               0.604516\n",
      "trainer/Policy log std Max                              -9.02414e-05\n",
      "trainer/Policy log std Min                              -3.19835\n",
      "trainer/Alpha                                            0.023272\n",
      "trainer/Alpha Loss                                       0.0389518\n",
      "exploration/num steps total                           6700\n",
      "exploration/num paths total                            335\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0464959\n",
      "exploration/Rewards Std                                  0.103538\n",
      "exploration/Rewards Max                                  0.12377\n",
      "exploration/Rewards Min                                 -0.462896\n",
      "exploration/Returns Mean                                -0.929918\n",
      "exploration/Returns Std                                  1.01715\n",
      "exploration/Returns Max                                  1.05624\n",
      "exploration/Returns Min                                 -1.81305\n",
      "exploration/Actions Mean                                -0.00393249\n",
      "exploration/Actions Std                                  0.112796\n",
      "exploration/Actions Max                                  0.325751\n",
      "exploration/Actions Min                                 -0.505763\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.929918\n",
      "exploration/env_infos/final/reward_dist Mean             0.112549\n",
      "exploration/env_infos/final/reward_dist Std              0.195326\n",
      "exploration/env_infos/final/reward_dist Max              0.501768\n",
      "exploration/env_infos/final/reward_dist Min              2.51203e-29\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00119381\n",
      "exploration/env_infos/initial/reward_dist Std            0.00142769\n",
      "exploration/env_infos/initial/reward_dist Max            0.00303979\n",
      "exploration/env_infos/initial/reward_dist Min            7.89151e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.115212\n",
      "exploration/env_infos/reward_dist Std                    0.189961\n",
      "exploration/env_infos/reward_dist Max                    0.733262\n",
      "exploration/env_infos/reward_dist Min                    2.51203e-29\n",
      "exploration/env_infos/final/reward_energy Mean          -0.142997\n",
      "exploration/env_infos/final/reward_energy Std            0.0720773\n",
      "exploration/env_infos/final/reward_energy Max           -0.0412584\n",
      "exploration/env_infos/final/reward_energy Min           -0.234376\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.225283\n",
      "exploration/env_infos/initial/reward_energy Std          0.103307\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0749556\n",
      "exploration/env_infos/initial/reward_energy Min         -0.372972\n",
      "exploration/env_infos/reward_energy Mean                -0.125602\n",
      "exploration/env_infos/reward_energy Std                  0.0984943\n",
      "exploration/env_infos/reward_energy Max                 -0.00554143\n",
      "exploration/env_infos/reward_energy Min                 -0.508638\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0617542\n",
      "exploration/env_infos/final/end_effector_loc Std         0.312578\n",
      "exploration/env_infos/final/end_effector_loc Max         0.370501\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.763574\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000208721\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00875998\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00925911\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0155478\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0272883\n",
      "exploration/env_infos/end_effector_loc Std               0.175869\n",
      "exploration/env_infos/end_effector_loc Max               0.370501\n",
      "exploration/env_infos/end_effector_loc Min              -0.763574\n",
      "evaluation/num steps total                           57000\n",
      "evaluation/num paths total                            2850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0252556\n",
      "evaluation/Rewards Std                                   0.0797202\n",
      "evaluation/Rewards Max                                   0.153914\n",
      "evaluation/Rewards Min                                  -0.401524\n",
      "evaluation/Returns Mean                                 -0.505111\n",
      "evaluation/Returns Std                                   1.19445\n",
      "evaluation/Returns Max                                   1.80035\n",
      "evaluation/Returns Min                                  -3.17832\n",
      "evaluation/Actions Mean                                  6.98443e-05\n",
      "evaluation/Actions Std                                   0.0601255\n",
      "evaluation/Actions Max                                   0.539937\n",
      "evaluation/Actions Min                                  -0.470538\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.505111\n",
      "evaluation/env_infos/final/reward_dist Mean              0.18294\n",
      "evaluation/env_infos/final/reward_dist Std               0.248024\n",
      "evaluation/env_infos/final/reward_dist Max               0.877032\n",
      "evaluation/env_infos/final/reward_dist Min               8.65244e-16\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00663239\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0108052\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0497003\n",
      "evaluation/env_infos/initial/reward_dist Min             1.8982e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.222643\n",
      "evaluation/env_infos/reward_dist Std                     0.278685\n",
      "evaluation/env_infos/reward_dist Max                     0.998723\n",
      "evaluation/env_infos/reward_dist Min                     8.65244e-16\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0242486\n",
      "evaluation/env_infos/final/reward_energy Std             0.020891\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00152252\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0993499\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.231996\n",
      "evaluation/env_infos/initial/reward_energy Std           0.144546\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0205511\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.693865\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0539482\n",
      "evaluation/env_infos/reward_energy Std                   0.0657248\n",
      "evaluation/env_infos/reward_energy Max                  -0.000616395\n",
      "evaluation/env_infos/reward_energy Min                  -0.693865\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00781641\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.246606\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.675805\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.508589\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000348321\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0096578\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0269969\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0235269\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00579077\n",
      "evaluation/env_infos/end_effector_loc Std                0.161888\n",
      "evaluation/env_infos/end_effector_loc Max                0.675805\n",
      "evaluation/env_infos/end_effector_loc Min               -0.508589\n",
      "time/data storing (s)                                    0.00612628\n",
      "time/evaluation sampling (s)                             0.964598\n",
      "time/exploration sampling (s)                            0.12095\n",
      "time/logging (s)                                         0.0190855\n",
      "time/saving (s)                                          0.0277266\n",
      "time/training (s)                                       48.5302\n",
      "time/epoch (s)                                          49.6687\n",
      "time/total (s)                                        2791.83\n",
      "Epoch                                                   56\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:36:01.140782 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 57 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00275069\r\n",
      "trainer/QF2 Loss                                         0.00231291\r\n",
      "trainer/Policy Loss                                      2.89529\r\n",
      "trainer/Q1 Predictions Mean                             -1.06349\r\n",
      "trainer/Q1 Predictions Std                               0.945618\r\n",
      "trainer/Q1 Predictions Max                               0.953198\r\n",
      "trainer/Q1 Predictions Min                              -3.5957\r\n",
      "trainer/Q2 Predictions Mean                             -1.06099\r\n",
      "trainer/Q2 Predictions Std                               0.941435\r\n",
      "trainer/Q2 Predictions Max                               0.971961\r\n",
      "trainer/Q2 Predictions Min                              -3.55835\r\n",
      "trainer/Q Targets Mean                                  -1.06335\r\n",
      "trainer/Q Targets Std                                    0.93686\r\n",
      "trainer/Q Targets Max                                    0.928952\r\n",
      "trainer/Q Targets Min                                   -3.56234\r\n",
      "trainer/Log Pis Mean                                     1.85692\r\n",
      "trainer/Log Pis Std                                      1.33347\r\n",
      "trainer/Log Pis Max                                      4.46031\r\n",
      "trainer/Log Pis Min                                     -1.81657\r\n",
      "trainer/Policy mu Mean                                   0.0139534\r\n",
      "trainer/Policy mu Std                                    0.283224\r\n",
      "trainer/Policy mu Max                                    2.00472\r\n",
      "trainer/Policy mu Min                                   -1.44779\r\n",
      "trainer/Policy log std Mean                             -2.28554\r\n",
      "trainer/Policy log std Std                               0.5702\r\n",
      "trainer/Policy log std Max                              -0.334651\r\n",
      "trainer/Policy log std Min                              -3.31153\r\n",
      "trainer/Alpha                                            0.0240695\r\n",
      "trainer/Alpha Loss                                      -0.533004\r\n",
      "exploration/num steps total                           6800\r\n",
      "exploration/num paths total                            340\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.114509\r\n",
      "exploration/Rewards Std                                  0.0843977\r\n",
      "exploration/Rewards Max                                  0.038368\r\n",
      "exploration/Rewards Min                                 -0.348497\r\n",
      "exploration/Returns Mean                                -2.29019\r\n",
      "exploration/Returns Std                                  1.42988\r\n",
      "exploration/Returns Max                                  0.204824\r\n",
      "exploration/Returns Min                                 -4.2106\r\n",
      "exploration/Actions Mean                                 0.0136448\r\n",
      "exploration/Actions Std                                  0.138521\r\n",
      "exploration/Actions Max                                  0.590393\r\n",
      "exploration/Actions Min                                 -0.428866\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.29019\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0207467\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0412265\r\n",
      "exploration/env_infos/final/reward_dist Max              0.103199\r\n",
      "exploration/env_infos/final/reward_dist Min              1.38365e-30\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00431753\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00767973\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0196488\r\n",
      "exploration/env_infos/initial/reward_dist Min            9.62893e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0507233\r\n",
      "exploration/env_infos/reward_dist Std                    0.105087\r\n",
      "exploration/env_infos/reward_dist Max                    0.467848\r\n",
      "exploration/env_infos/reward_dist Min                    1.38365e-30\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0965156\r\n",
      "exploration/env_infos/final/reward_energy Std            0.039809\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.059003\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.172572\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.208815\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.164956\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.068579\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.500196\r\n",
      "exploration/env_infos/reward_energy Mean                -0.1492\r\n",
      "exploration/env_infos/reward_energy Std                  0.128406\r\n",
      "exploration/env_infos/reward_energy Max                 -0.007209\r\n",
      "exploration/env_infos/reward_energy Min                 -0.709183\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0816464\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.309506\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.698407\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.432143\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00295429\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00893255\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0238603\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00749518\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0189547\r\n",
      "exploration/env_infos/end_effector_loc Std               0.189651\r\n",
      "exploration/env_infos/end_effector_loc Max               0.698407\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.432143\r\n",
      "evaluation/num steps total                           58000\r\n",
      "evaluation/num paths total                            2900\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0385933\r\n",
      "evaluation/Rewards Std                                   0.0746077\r\n",
      "evaluation/Rewards Max                                   0.147258\r\n",
      "evaluation/Rewards Min                                  -0.468915\r\n",
      "evaluation/Returns Mean                                 -0.771866\r\n",
      "evaluation/Returns Std                                   1.17149\r\n",
      "evaluation/Returns Max                                   1.66176\r\n",
      "evaluation/Returns Min                                  -3.43558\r\n",
      "evaluation/Actions Mean                                  0.0013187\r\n",
      "evaluation/Actions Std                                   0.072153\r\n",
      "evaluation/Actions Max                                   0.611003\r\n",
      "evaluation/Actions Min                                  -0.671829\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.771866\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.184444\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.24627\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.911879\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.44775e-12\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00619962\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0094042\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0352436\r\n",
      "evaluation/env_infos/initial/reward_dist Min             5.05206e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.19126\r\n",
      "evaluation/env_infos/reward_dist Std                     0.254269\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998387\r\n",
      "evaluation/env_infos/reward_dist Min                     2.44775e-12\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0436377\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0404833\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00572805\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.186249\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.255917\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.194057\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00761029\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.765076\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0626854\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0805365\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000720083\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.765076\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00733826\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.213438\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.622957\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.410639\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00166504\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0112324\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0305501\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0335915\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00836963\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.152515\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.622957\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.516405\r\n",
      "time/data storing (s)                                    0.00619349\r\n",
      "time/evaluation sampling (s)                             0.950932\r\n",
      "time/exploration sampling (s)                            0.123847\r\n",
      "time/logging (s)                                         0.0197732\r\n",
      "time/saving (s)                                          0.0267665\r\n",
      "time/training (s)                                       49.118\r\n",
      "time/epoch (s)                                          50.2456\r\n",
      "time/total (s)                                        2842.83\r\n",
      "Epoch                                                   57\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:36:52.172018 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 58 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00164319\n",
      "trainer/QF2 Loss                                         0.00209785\n",
      "trainer/Policy Loss                                      3.15019\n",
      "trainer/Q1 Predictions Mean                             -1.11974\n",
      "trainer/Q1 Predictions Std                               0.918284\n",
      "trainer/Q1 Predictions Max                               0.734885\n",
      "trainer/Q1 Predictions Min                              -3.40035\n",
      "trainer/Q2 Predictions Mean                             -1.11769\n",
      "trainer/Q2 Predictions Std                               0.921339\n",
      "trainer/Q2 Predictions Max                               0.714749\n",
      "trainer/Q2 Predictions Min                              -3.44487\n",
      "trainer/Q Targets Mean                                  -1.11772\n",
      "trainer/Q Targets Std                                    0.91589\n",
      "trainer/Q Targets Max                                    0.743051\n",
      "trainer/Q Targets Min                                   -3.42247\n",
      "trainer/Log Pis Mean                                     2.05486\n",
      "trainer/Log Pis Std                                      1.3236\n",
      "trainer/Log Pis Max                                      4.31998\n",
      "trainer/Log Pis Min                                     -4.14375\n",
      "trainer/Policy mu Mean                                   0.0210597\n",
      "trainer/Policy mu Std                                    0.288738\n",
      "trainer/Policy mu Max                                    2.71979\n",
      "trainer/Policy mu Min                                   -0.970239\n",
      "trainer/Policy log std Mean                             -2.38724\n",
      "trainer/Policy log std Std                               0.531848\n",
      "trainer/Policy log std Max                              -0.173375\n",
      "trainer/Policy log std Min                              -3.28112\n",
      "trainer/Alpha                                            0.0238687\n",
      "trainer/Alpha Loss                                       0.204923\n",
      "exploration/num steps total                           6900\n",
      "exploration/num paths total                            345\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0859714\n",
      "exploration/Rewards Std                                  0.0536106\n",
      "exploration/Rewards Max                                  0.0532687\n",
      "exploration/Rewards Min                                 -0.27881\n",
      "exploration/Returns Mean                                -1.71943\n",
      "exploration/Returns Std                                  0.639033\n",
      "exploration/Returns Max                                 -0.607264\n",
      "exploration/Returns Min                                 -2.51185\n",
      "exploration/Actions Mean                                -0.0012916\n",
      "exploration/Actions Std                                  0.0990641\n",
      "exploration/Actions Max                                  0.474998\n",
      "exploration/Actions Min                                 -0.448681\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.71943\n",
      "exploration/env_infos/final/reward_dist Mean             0.00539395\n",
      "exploration/env_infos/final/reward_dist Std              0.0107194\n",
      "exploration/env_infos/final/reward_dist Max              0.0268327\n",
      "exploration/env_infos/final/reward_dist Min              1.08748e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0033466\n",
      "exploration/env_infos/initial/reward_dist Std            0.00410521\n",
      "exploration/env_infos/initial/reward_dist Max            0.0105764\n",
      "exploration/env_infos/initial/reward_dist Min            1.55863e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0901495\n",
      "exploration/env_infos/reward_dist Std                    0.19794\n",
      "exploration/env_infos/reward_dist Max                    0.836516\n",
      "exploration/env_infos/reward_dist Min                    1.08748e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.115768\n",
      "exploration/env_infos/final/reward_energy Std            0.075654\n",
      "exploration/env_infos/final/reward_energy Max           -0.0433737\n",
      "exploration/env_infos/final/reward_energy Min           -0.255328\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.197704\n",
      "exploration/env_infos/initial/reward_energy Std          0.159789\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0562673\n",
      "exploration/env_infos/initial/reward_energy Min         -0.507022\n",
      "exploration/env_infos/reward_energy Mean                -0.108724\n",
      "exploration/env_infos/reward_energy Std                  0.088373\n",
      "exploration/env_infos/reward_energy Max                 -0.0136798\n",
      "exploration/env_infos/reward_energy Min                 -0.507022\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.022986\n",
      "exploration/env_infos/final/end_effector_loc Std         0.227693\n",
      "exploration/env_infos/final/end_effector_loc Max         0.423106\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.378734\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000709161\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00895943\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0237499\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00886674\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0106635\n",
      "exploration/env_infos/end_effector_loc Std               0.138647\n",
      "exploration/env_infos/end_effector_loc Max               0.423106\n",
      "exploration/env_infos/end_effector_loc Min              -0.378734\n",
      "evaluation/num steps total                           59000\n",
      "evaluation/num paths total                            2950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0448657\n",
      "evaluation/Rewards Std                                   0.0756484\n",
      "evaluation/Rewards Max                                   0.150122\n",
      "evaluation/Rewards Min                                  -0.471153\n",
      "evaluation/Returns Mean                                 -0.897314\n",
      "evaluation/Returns Std                                   1.12022\n",
      "evaluation/Returns Max                                   2.12075\n",
      "evaluation/Returns Min                                  -3.37421\n",
      "evaluation/Actions Mean                                 -0.0027332\n",
      "evaluation/Actions Std                                   0.066766\n",
      "evaluation/Actions Max                                   0.574976\n",
      "evaluation/Actions Min                                  -0.42351\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.897314\n",
      "evaluation/env_infos/final/reward_dist Mean              0.229852\n",
      "evaluation/env_infos/final/reward_dist Std               0.319736\n",
      "evaluation/env_infos/final/reward_dist Max               0.98053\n",
      "evaluation/env_infos/final/reward_dist Min               7.84886e-09\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00680088\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0107779\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0480318\n",
      "evaluation/env_infos/initial/reward_dist Min             4.7522e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.221785\n",
      "evaluation/env_infos/reward_dist Std                     0.282908\n",
      "evaluation/env_infos/reward_dist Max                     0.999366\n",
      "evaluation/env_infos/reward_dist Min                     7.84886e-09\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0333581\n",
      "evaluation/env_infos/final/reward_energy Std             0.0224749\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0058685\n",
      "evaluation/env_infos/final/reward_energy Min            -0.121765\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.259539\n",
      "evaluation/env_infos/initial/reward_energy Std           0.159973\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0208663\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.595166\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0597231\n",
      "evaluation/env_infos/reward_energy Std                   0.0732358\n",
      "evaluation/env_infos/reward_energy Max                  -0.00114478\n",
      "evaluation/env_infos/reward_energy Min                  -0.595166\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0238759\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.215521\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.540008\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.444285\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00161244\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106578\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0287488\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0211755\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00495946\n",
      "evaluation/env_infos/end_effector_loc Std                0.149934\n",
      "evaluation/env_infos/end_effector_loc Max                0.540008\n",
      "evaluation/env_infos/end_effector_loc Min               -0.444285\n",
      "time/data storing (s)                                    0.00569731\n",
      "time/evaluation sampling (s)                             0.927846\n",
      "time/exploration sampling (s)                            0.123592\n",
      "time/logging (s)                                         0.0208036\n",
      "time/saving (s)                                          0.029133\n",
      "time/training (s)                                       49.2286\n",
      "time/epoch (s)                                          50.3356\n",
      "time/total (s)                                        2893.86\n",
      "Epoch                                                   58\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:37:43.059082 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 59 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00100036\r\n",
      "trainer/QF2 Loss                                         0.00117996\r\n",
      "trainer/Policy Loss                                      3.16391\r\n",
      "trainer/Q1 Predictions Mean                             -1.20832\r\n",
      "trainer/Q1 Predictions Std                               0.884088\r\n",
      "trainer/Q1 Predictions Max                               0.697471\r\n",
      "trainer/Q1 Predictions Min                              -3.30321\r\n",
      "trainer/Q2 Predictions Mean                             -1.20966\r\n",
      "trainer/Q2 Predictions Std                               0.883667\r\n",
      "trainer/Q2 Predictions Max                               0.663405\r\n",
      "trainer/Q2 Predictions Min                              -3.31507\r\n",
      "trainer/Q Targets Mean                                  -1.20493\r\n",
      "trainer/Q Targets Std                                    0.881974\r\n",
      "trainer/Q Targets Max                                    0.703182\r\n",
      "trainer/Q Targets Min                                   -3.30968\r\n",
      "trainer/Log Pis Mean                                     1.98363\r\n",
      "trainer/Log Pis Std                                      1.49457\r\n",
      "trainer/Log Pis Max                                      4.59343\r\n",
      "trainer/Log Pis Min                                     -5.16146\r\n",
      "trainer/Policy mu Mean                                   0.0200297\r\n",
      "trainer/Policy mu Std                                    0.303749\r\n",
      "trainer/Policy mu Max                                    1.90833\r\n",
      "trainer/Policy mu Min                                   -1.58192\r\n",
      "trainer/Policy log std Mean                             -2.29879\r\n",
      "trainer/Policy log std Std                               0.564989\r\n",
      "trainer/Policy log std Max                              -0.647\r\n",
      "trainer/Policy log std Min                              -3.17277\r\n",
      "trainer/Alpha                                            0.0244863\r\n",
      "trainer/Alpha Loss                                      -0.0607216\r\n",
      "exploration/num steps total                           7000\r\n",
      "exploration/num paths total                            350\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0357043\r\n",
      "exploration/Rewards Std                                  0.100909\r\n",
      "exploration/Rewards Max                                  0.153278\r\n",
      "exploration/Rewards Min                                 -0.304826\r\n",
      "exploration/Returns Mean                                -0.714086\r\n",
      "exploration/Returns Std                                  1.50287\r\n",
      "exploration/Returns Max                                  0.966036\r\n",
      "exploration/Returns Min                                 -3.38391\r\n",
      "exploration/Actions Mean                                -0.00125198\r\n",
      "exploration/Actions Std                                  0.121794\r\n",
      "exploration/Actions Max                                  0.403495\r\n",
      "exploration/Actions Min                                 -0.484648\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.714086\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.155929\r\n",
      "exploration/env_infos/final/reward_dist Std              0.27291\r\n",
      "exploration/env_infos/final/reward_dist Max              0.698903\r\n",
      "exploration/env_infos/final/reward_dist Min              2.55365e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00806297\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0100273\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0261061\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000347479\r\n",
      "exploration/env_infos/reward_dist Mean                   0.276461\r\n",
      "exploration/env_infos/reward_dist Std                    0.329264\r\n",
      "exploration/env_infos/reward_dist Max                    0.998494\r\n",
      "exploration/env_infos/reward_dist Min                    1.98604e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.236407\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0858163\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.144897\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.397061\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.338003\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.14878\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.106097\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.532666\r\n",
      "exploration/env_infos/reward_energy Mean                -0.138287\r\n",
      "exploration/env_infos/reward_energy Std                  0.102701\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0114806\r\n",
      "exploration/env_infos/reward_energy Min                 -0.532666\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0355857\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.241019\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.356077\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.343783\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00343107\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0125978\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0190122\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0242324\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0192695\r\n",
      "exploration/env_infos/end_effector_loc Std               0.161608\r\n",
      "exploration/env_infos/end_effector_loc Max               0.358968\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.343783\r\n",
      "evaluation/num steps total                           60000\r\n",
      "evaluation/num paths total                            3000\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0311713\r\n",
      "evaluation/Rewards Std                                   0.0849939\r\n",
      "evaluation/Rewards Max                                   0.175286\r\n",
      "evaluation/Rewards Min                                  -0.605616\r\n",
      "evaluation/Returns Mean                                 -0.623426\r\n",
      "evaluation/Returns Std                                   1.29662\r\n",
      "evaluation/Returns Max                                   2.37875\r\n",
      "evaluation/Returns Min                                  -3.54928\r\n",
      "evaluation/Actions Mean                                 -0.00116192\r\n",
      "evaluation/Actions Std                                   0.0716779\r\n",
      "evaluation/Actions Max                                   0.558293\r\n",
      "evaluation/Actions Min                                  -0.623353\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.623426\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.154981\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.203946\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.945294\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.12328e-10\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00873923\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0164517\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0800767\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.26782e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.217155\r\n",
      "evaluation/env_infos/reward_dist Std                     0.273798\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998314\r\n",
      "evaluation/env_infos/reward_dist Min                     1.12328e-10\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0294734\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0174574\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00167805\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0752896\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.268757\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.189754\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0119763\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.704558\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.061468\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0806215\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.0010991\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.704558\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0182386\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.226561\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.489993\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.556408\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000879087\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0115984\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0279147\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0311676\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00481375\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.157555\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.489993\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.556408\r\n",
      "time/data storing (s)                                    0.00615585\r\n",
      "time/evaluation sampling (s)                             0.94834\r\n",
      "time/exploration sampling (s)                            0.122706\r\n",
      "time/logging (s)                                         0.0195386\r\n",
      "time/saving (s)                                          0.02731\r\n",
      "time/training (s)                                       49.0194\r\n",
      "time/epoch (s)                                          50.1434\r\n",
      "time/total (s)                                        2944.74\r\n",
      "Epoch                                                   59\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:38:33.671008 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 60 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000910253\r\n",
      "trainer/QF2 Loss                                         0.000771204\r\n",
      "trainer/Policy Loss                                      3.29141\r\n",
      "trainer/Q1 Predictions Mean                             -1.25754\r\n",
      "trainer/Q1 Predictions Std                               0.914081\r\n",
      "trainer/Q1 Predictions Max                               0.721091\r\n",
      "trainer/Q1 Predictions Min                              -3.41966\r\n",
      "trainer/Q2 Predictions Mean                             -1.25053\r\n",
      "trainer/Q2 Predictions Std                               0.913657\r\n",
      "trainer/Q2 Predictions Max                               0.692595\r\n",
      "trainer/Q2 Predictions Min                              -3.43692\r\n",
      "trainer/Q Targets Mean                                  -1.25657\r\n",
      "trainer/Q Targets Std                                    0.916918\r\n",
      "trainer/Q Targets Max                                    0.731403\r\n",
      "trainer/Q Targets Min                                   -3.46939\r\n",
      "trainer/Log Pis Mean                                     2.07805\r\n",
      "trainer/Log Pis Std                                      1.33863\r\n",
      "trainer/Log Pis Max                                      4.37033\r\n",
      "trainer/Log Pis Min                                     -4.04538\r\n",
      "trainer/Policy mu Mean                                   0.0267946\r\n",
      "trainer/Policy mu Std                                    0.36244\r\n",
      "trainer/Policy mu Max                                    2.07656\r\n",
      "trainer/Policy mu Min                                   -1.53585\r\n",
      "trainer/Policy log std Mean                             -2.30081\r\n",
      "trainer/Policy log std Std                               0.590523\r\n",
      "trainer/Policy log std Max                              -0.592122\r\n",
      "trainer/Policy log std Min                              -3.19992\r\n",
      "trainer/Alpha                                            0.0247097\r\n",
      "trainer/Alpha Loss                                       0.288865\r\n",
      "exploration/num steps total                           7100\r\n",
      "exploration/num paths total                            355\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0154056\r\n",
      "exploration/Rewards Std                                  0.0825034\r\n",
      "exploration/Rewards Max                                  0.129186\r\n",
      "exploration/Rewards Min                                 -0.210037\r\n",
      "exploration/Returns Mean                                -0.308113\r\n",
      "exploration/Returns Std                                  1.36459\r\n",
      "exploration/Returns Max                                  0.940682\r\n",
      "exploration/Returns Min                                 -2.75529\r\n",
      "exploration/Actions Mean                                -0.00720062\r\n",
      "exploration/Actions Std                                  0.156222\r\n",
      "exploration/Actions Max                                  0.641911\r\n",
      "exploration/Actions Min                                 -0.544515\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.308113\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.396636\r\n",
      "exploration/env_infos/final/reward_dist Std              0.213906\r\n",
      "exploration/env_infos/final/reward_dist Max              0.565241\r\n",
      "exploration/env_infos/final/reward_dist Min              0.00184592\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00601237\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00909332\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0240571\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.56232e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.302789\r\n",
      "exploration/env_infos/reward_dist Std                    0.320021\r\n",
      "exploration/env_infos/reward_dist Max                    0.985848\r\n",
      "exploration/env_infos/reward_dist Min                    1.90783e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.179253\r\n",
      "exploration/env_infos/final/reward_energy Std            0.125143\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0819483\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.411198\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.322828\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.166596\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.176054\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.642803\r\n",
      "exploration/env_infos/reward_energy Mean                -0.174577\r\n",
      "exploration/env_infos/reward_energy Std                  0.135785\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0307951\r\n",
      "exploration/env_infos/reward_energy Min                 -0.674333\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0638332\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.178969\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.252048\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.325716\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00365948\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0123115\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0114623\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0270801\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0279429\r\n",
      "exploration/env_infos/end_effector_loc Std               0.126971\r\n",
      "exploration/env_infos/end_effector_loc Max               0.252048\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.325716\r\n",
      "evaluation/num steps total                           61000\r\n",
      "evaluation/num paths total                            3050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0319247\r\n",
      "evaluation/Rewards Std                                   0.0761666\r\n",
      "evaluation/Rewards Max                                   0.172467\r\n",
      "evaluation/Rewards Min                                  -0.392793\r\n",
      "evaluation/Returns Mean                                 -0.638494\r\n",
      "evaluation/Returns Std                                   1.13474\r\n",
      "evaluation/Returns Max                                   1.85503\r\n",
      "evaluation/Returns Min                                  -3.34657\r\n",
      "evaluation/Actions Mean                                 -0.00172709\r\n",
      "evaluation/Actions Std                                   0.0619821\r\n",
      "evaluation/Actions Max                                   0.547192\r\n",
      "evaluation/Actions Min                                  -0.512221\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.638494\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.192561\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.262467\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.969371\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.55796e-09\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00905353\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0173925\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0820649\r\n",
      "evaluation/env_infos/initial/reward_dist Min             3.68556e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.185854\r\n",
      "evaluation/env_infos/reward_dist Std                     0.262344\r\n",
      "evaluation/env_infos/reward_dist Max                     0.989481\r\n",
      "evaluation/env_infos/reward_dist Min                     1.55796e-09\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0310919\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.023238\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0023828\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.10408\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.242258\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.186106\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00244418\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.649977\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0527523\r\n",
      "evaluation/env_infos/reward_energy Std                   0.070048\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00100867\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.649977\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0133482\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.211717\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.507663\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.438076\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00267867\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0104633\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0273596\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0256111\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0180649\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.146444\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.507663\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.438076\r\n",
      "time/data storing (s)                                    0.00611505\r\n",
      "time/evaluation sampling (s)                             0.948442\r\n",
      "time/exploration sampling (s)                            0.121222\r\n",
      "time/logging (s)                                         0.0202148\r\n",
      "time/saving (s)                                          0.031397\r\n",
      "time/training (s)                                       48.6406\r\n",
      "time/epoch (s)                                          49.768\r\n",
      "time/total (s)                                        2995.35\r\n",
      "Epoch                                                   60\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:39:24.288384 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 61 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00105568\n",
      "trainer/QF2 Loss                                         0.00175368\n",
      "trainer/Policy Loss                                      3.00487\n",
      "trainer/Q1 Predictions Mean                             -1.04616\n",
      "trainer/Q1 Predictions Std                               0.918332\n",
      "trainer/Q1 Predictions Max                               0.54069\n",
      "trainer/Q1 Predictions Min                              -3.35942\n",
      "trainer/Q2 Predictions Mean                             -1.05127\n",
      "trainer/Q2 Predictions Std                               0.920917\n",
      "trainer/Q2 Predictions Max                               0.535654\n",
      "trainer/Q2 Predictions Min                              -3.36752\n",
      "trainer/Q Targets Mean                                  -1.05644\n",
      "trainer/Q Targets Std                                    0.924386\n",
      "trainer/Q Targets Max                                    0.582707\n",
      "trainer/Q Targets Min                                   -3.40541\n",
      "trainer/Log Pis Mean                                     1.97591\n",
      "trainer/Log Pis Std                                      1.28096\n",
      "trainer/Log Pis Max                                      4.2838\n",
      "trainer/Log Pis Min                                     -2.21045\n",
      "trainer/Policy mu Mean                                   0.0130646\n",
      "trainer/Policy mu Std                                    0.277058\n",
      "trainer/Policy mu Max                                    2.25761\n",
      "trainer/Policy mu Min                                   -1.78036\n",
      "trainer/Policy log std Mean                             -2.32234\n",
      "trainer/Policy log std Std                               0.553236\n",
      "trainer/Policy log std Max                              -0.547265\n",
      "trainer/Policy log std Min                              -3.20806\n",
      "trainer/Alpha                                            0.0237785\n",
      "trainer/Alpha Loss                                      -0.0900592\n",
      "exploration/num steps total                           7200\n",
      "exploration/num paths total                            360\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.073905\n",
      "exploration/Rewards Std                                  0.0656004\n",
      "exploration/Rewards Max                                  0.0470261\n",
      "exploration/Rewards Min                                 -0.296308\n",
      "exploration/Returns Mean                                -1.4781\n",
      "exploration/Returns Std                                  0.457388\n",
      "exploration/Returns Max                                 -0.833474\n",
      "exploration/Returns Min                                 -2.03221\n",
      "exploration/Actions Mean                                 0.00757683\n",
      "exploration/Actions Std                                  0.123715\n",
      "exploration/Actions Max                                  0.494467\n",
      "exploration/Actions Min                                 -0.460771\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.4781\n",
      "exploration/env_infos/final/reward_dist Mean             0.147235\n",
      "exploration/env_infos/final/reward_dist Std              0.260573\n",
      "exploration/env_infos/final/reward_dist Max              0.666048\n",
      "exploration/env_infos/final/reward_dist Min              1.53061e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000501368\n",
      "exploration/env_infos/initial/reward_dist Std            0.000903561\n",
      "exploration/env_infos/initial/reward_dist Max            0.00230765\n",
      "exploration/env_infos/initial/reward_dist Min            8.12376e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.253506\n",
      "exploration/env_infos/reward_dist Std                    0.285402\n",
      "exploration/env_infos/reward_dist Max                    0.993824\n",
      "exploration/env_infos/reward_dist Min                    8.12376e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.155918\n",
      "exploration/env_infos/final/reward_energy Std            0.0726233\n",
      "exploration/env_infos/final/reward_energy Max           -0.0419491\n",
      "exploration/env_infos/final/reward_energy Min           -0.221806\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.304519\n",
      "exploration/env_infos/initial/reward_energy Std          0.126373\n",
      "exploration/env_infos/initial/reward_energy Max         -0.122677\n",
      "exploration/env_infos/initial/reward_energy Min         -0.507092\n",
      "exploration/env_infos/reward_energy Mean                -0.143659\n",
      "exploration/env_infos/reward_energy Std                  0.100437\n",
      "exploration/env_infos/reward_energy Max                 -0.0140494\n",
      "exploration/env_infos/reward_energy Min                 -0.507092\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.105632\n",
      "exploration/env_infos/final/end_effector_loc Std         0.248241\n",
      "exploration/env_infos/final/end_effector_loc Max         0.423806\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.382823\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00368603\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0110585\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0247234\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0133906\n",
      "exploration/env_infos/end_effector_loc Mean              0.0542518\n",
      "exploration/env_infos/end_effector_loc Std               0.171138\n",
      "exploration/env_infos/end_effector_loc Max               0.423806\n",
      "exploration/env_infos/end_effector_loc Min              -0.382823\n",
      "evaluation/num steps total                           62000\n",
      "evaluation/num paths total                            3100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0349701\n",
      "evaluation/Rewards Std                                   0.0686943\n",
      "evaluation/Rewards Max                                   0.15767\n",
      "evaluation/Rewards Min                                  -0.334256\n",
      "evaluation/Returns Mean                                 -0.699402\n",
      "evaluation/Returns Std                                   1.02373\n",
      "evaluation/Returns Max                                   1.99234\n",
      "evaluation/Returns Min                                  -2.49243\n",
      "evaluation/Actions Mean                                 -0.00139887\n",
      "evaluation/Actions Std                                   0.0663647\n",
      "evaluation/Actions Max                                   0.484584\n",
      "evaluation/Actions Min                                  -0.54457\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.699402\n",
      "evaluation/env_infos/final/reward_dist Mean              0.308516\n",
      "evaluation/env_infos/final/reward_dist Std               0.336966\n",
      "evaluation/env_infos/final/reward_dist Max               0.977247\n",
      "evaluation/env_infos/final/reward_dist Min               1.26777e-15\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00542546\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111289\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0612433\n",
      "evaluation/env_infos/initial/reward_dist Min             1.69738e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.251645\n",
      "evaluation/env_infos/reward_dist Std                     0.303472\n",
      "evaluation/env_infos/reward_dist Max                     0.997227\n",
      "evaluation/env_infos/reward_dist Min                     1.26777e-15\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0273159\n",
      "evaluation/env_infos/final/reward_energy Std             0.0162696\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00358954\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0737657\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.25675\n",
      "evaluation/env_infos/initial/reward_energy Std           0.176974\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0215772\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.708702\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0569783\n",
      "evaluation/env_infos/reward_energy Std                   0.0746052\n",
      "evaluation/env_infos/reward_energy Max                  -0.00200272\n",
      "evaluation/env_infos/reward_energy Min                  -0.708702\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0229631\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.205326\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.586202\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.384756\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000627761\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0110071\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0242292\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0272285\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00958021\n",
      "evaluation/env_infos/end_effector_loc Std                0.142735\n",
      "evaluation/env_infos/end_effector_loc Max                0.586202\n",
      "evaluation/env_infos/end_effector_loc Min               -0.384756\n",
      "time/data storing (s)                                    0.00590659\n",
      "time/evaluation sampling (s)                             1.07259\n",
      "time/exploration sampling (s)                            0.126069\n",
      "time/logging (s)                                         0.0220156\n",
      "time/saving (s)                                          0.0280948\n",
      "time/training (s)                                       48.5809\n",
      "time/epoch (s)                                          49.8355\n",
      "time/total (s)                                        3045.97\n",
      "Epoch                                                   61\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:40:16.086240 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 62 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00129434\r\n",
      "trainer/QF2 Loss                                         0.000680253\r\n",
      "trainer/Policy Loss                                      2.95074\r\n",
      "trainer/Q1 Predictions Mean                             -1.17644\r\n",
      "trainer/Q1 Predictions Std                               0.899226\r\n",
      "trainer/Q1 Predictions Max                               0.525085\r\n",
      "trainer/Q1 Predictions Min                              -3.44843\r\n",
      "trainer/Q2 Predictions Mean                             -1.18117\r\n",
      "trainer/Q2 Predictions Std                               0.90337\r\n",
      "trainer/Q2 Predictions Max                               0.541824\r\n",
      "trainer/Q2 Predictions Min                              -3.49203\r\n",
      "trainer/Q Targets Mean                                  -1.18365\r\n",
      "trainer/Q Targets Std                                    0.905306\r\n",
      "trainer/Q Targets Max                                    0.573667\r\n",
      "trainer/Q Targets Min                                   -3.47889\r\n",
      "trainer/Log Pis Mean                                     1.79545\r\n",
      "trainer/Log Pis Std                                      1.40121\r\n",
      "trainer/Log Pis Max                                      4.2801\r\n",
      "trainer/Log Pis Min                                     -2.42875\r\n",
      "trainer/Policy mu Mean                                   0.0708358\r\n",
      "trainer/Policy mu Std                                    0.388879\r\n",
      "trainer/Policy mu Max                                    2.06679\r\n",
      "trainer/Policy mu Min                                   -1.72228\r\n",
      "trainer/Policy log std Mean                             -2.23238\r\n",
      "trainer/Policy log std Std                               0.658302\r\n",
      "trainer/Policy log std Max                              -0.300929\r\n",
      "trainer/Policy log std Min                              -3.22385\r\n",
      "trainer/Alpha                                            0.0235042\r\n",
      "trainer/Alpha Loss                                      -0.767003\r\n",
      "exploration/num steps total                           7300\r\n",
      "exploration/num paths total                            365\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0857805\r\n",
      "exploration/Rewards Std                                  0.0709105\r\n",
      "exploration/Rewards Max                                  0.0575597\r\n",
      "exploration/Rewards Min                                 -0.26809\r\n",
      "exploration/Returns Mean                                -1.71561\r\n",
      "exploration/Returns Std                                  0.942379\r\n",
      "exploration/Returns Max                                 -0.344608\r\n",
      "exploration/Returns Min                                 -2.97836\r\n",
      "exploration/Actions Mean                                -0.00522529\r\n",
      "exploration/Actions Std                                  0.0903065\r\n",
      "exploration/Actions Max                                  0.330916\r\n",
      "exploration/Actions Min                                 -0.342629\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.71561\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.176297\r\n",
      "exploration/env_infos/final/reward_dist Std              0.237658\r\n",
      "exploration/env_infos/final/reward_dist Max              0.613579\r\n",
      "exploration/env_infos/final/reward_dist Min              4.47895e-05\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00100493\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00189723\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00479584\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.05865e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.151409\r\n",
      "exploration/env_infos/reward_dist Std                    0.235711\r\n",
      "exploration/env_infos/reward_dist Max                    0.978025\r\n",
      "exploration/env_infos/reward_dist Min                    1.26298e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.126475\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0582611\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0762714\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.240438\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.128808\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.111885\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0296071\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.33972\r\n",
      "exploration/env_infos/reward_energy Mean                -0.107462\r\n",
      "exploration/env_infos/reward_energy Std                  0.0694052\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00721805\r\n",
      "exploration/env_infos/reward_energy Min                 -0.355854\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0158439\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.26492\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.433347\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.399834\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000707502\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00599055\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0165458\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00454253\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00687284\r\n",
      "exploration/env_infos/end_effector_loc Std               0.161924\r\n",
      "exploration/env_infos/end_effector_loc Max               0.433347\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.399834\r\n",
      "evaluation/num steps total                           63000\r\n",
      "evaluation/num paths total                            3150\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0381553\r\n",
      "evaluation/Rewards Std                                   0.0694265\r\n",
      "evaluation/Rewards Max                                   0.145777\r\n",
      "evaluation/Rewards Min                                  -0.266808\r\n",
      "evaluation/Returns Mean                                 -0.763105\r\n",
      "evaluation/Returns Std                                   1.06443\r\n",
      "evaluation/Returns Max                                   1.57341\r\n",
      "evaluation/Returns Min                                  -3.13045\r\n",
      "evaluation/Actions Mean                                  0.00152981\r\n",
      "evaluation/Actions Std                                   0.067083\r\n",
      "evaluation/Actions Max                                   0.669021\r\n",
      "evaluation/Actions Min                                  -0.446875\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.763105\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.173156\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.262628\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.979344\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.53663e-09\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0109861\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0171473\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.083269\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.72922e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.208608\r\n",
      "evaluation/env_infos/reward_dist Std                     0.273029\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996586\r\n",
      "evaluation/env_infos/reward_dist Min                     3.53663e-09\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0461968\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0313036\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00852946\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.137425\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.250803\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.189531\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0056807\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.819806\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0600238\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0734989\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00130816\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.819806\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0620049\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.210418\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.55778\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.401134\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00301884\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106966\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.033451\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0223438\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0375518\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.14463\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.55778\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.401134\r\n",
      "time/data storing (s)                                    0.00590974\r\n",
      "time/evaluation sampling (s)                             1.04159\r\n",
      "time/exploration sampling (s)                            0.124875\r\n",
      "time/logging (s)                                         0.0207194\r\n",
      "time/saving (s)                                          0.0260007\r\n",
      "time/training (s)                                       49.7898\r\n",
      "time/epoch (s)                                          51.0089\r\n",
      "time/total (s)                                        3097.77\r\n",
      "Epoch                                                   62\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:41:06.861259 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 63 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000942235\n",
      "trainer/QF2 Loss                                         0.00117384\n",
      "trainer/Policy Loss                                      3.01789\n",
      "trainer/Q1 Predictions Mean                             -1.07771\n",
      "trainer/Q1 Predictions Std                               0.936465\n",
      "trainer/Q1 Predictions Max                               0.625976\n",
      "trainer/Q1 Predictions Min                              -3.3921\n",
      "trainer/Q2 Predictions Mean                             -1.07853\n",
      "trainer/Q2 Predictions Std                               0.935075\n",
      "trainer/Q2 Predictions Max                               0.610489\n",
      "trainer/Q2 Predictions Min                              -3.32735\n",
      "trainer/Q Targets Mean                                  -1.07353\n",
      "trainer/Q Targets Std                                    0.934164\n",
      "trainer/Q Targets Max                                    0.649058\n",
      "trainer/Q Targets Min                                   -3.4122\n",
      "trainer/Log Pis Mean                                     1.96175\n",
      "trainer/Log Pis Std                                      1.31362\n",
      "trainer/Log Pis Max                                      4.8097\n",
      "trainer/Log Pis Min                                     -2.79176\n",
      "trainer/Policy mu Mean                                  -0.0132112\n",
      "trainer/Policy mu Std                                    0.383031\n",
      "trainer/Policy mu Max                                    1.79938\n",
      "trainer/Policy mu Min                                   -2.89915\n",
      "trainer/Policy log std Mean                             -2.2894\n",
      "trainer/Policy log std Std                               0.589832\n",
      "trainer/Policy log std Max                              -0.452754\n",
      "trainer/Policy log std Min                              -3.24981\n",
      "trainer/Alpha                                            0.0240853\n",
      "trainer/Alpha Loss                                      -0.142589\n",
      "exploration/num steps total                           7400\n",
      "exploration/num paths total                            370\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0371843\n",
      "exploration/Rewards Std                                  0.0711484\n",
      "exploration/Rewards Max                                  0.131016\n",
      "exploration/Rewards Min                                 -0.192008\n",
      "exploration/Returns Mean                                -0.743686\n",
      "exploration/Returns Std                                  1.24351\n",
      "exploration/Returns Max                                  1.5444\n",
      "exploration/Returns Min                                 -2.19717\n",
      "exploration/Actions Mean                                -0.0063401\n",
      "exploration/Actions Std                                  0.209809\n",
      "exploration/Actions Max                                  0.98555\n",
      "exploration/Actions Min                                 -0.705801\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.743686\n",
      "exploration/env_infos/final/reward_dist Mean             0.0838808\n",
      "exploration/env_infos/final/reward_dist Std              0.0743689\n",
      "exploration/env_infos/final/reward_dist Max              0.208822\n",
      "exploration/env_infos/final/reward_dist Min              0.00101741\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0281441\n",
      "exploration/env_infos/initial/reward_dist Std            0.0339494\n",
      "exploration/env_infos/initial/reward_dist Max            0.0891144\n",
      "exploration/env_infos/initial/reward_dist Min            2.60649e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.227107\n",
      "exploration/env_infos/reward_dist Std                    0.300365\n",
      "exploration/env_infos/reward_dist Max                    0.989678\n",
      "exploration/env_infos/reward_dist Min                    2.60649e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.200139\n",
      "exploration/env_infos/final/reward_energy Std            0.191664\n",
      "exploration/env_infos/final/reward_energy Max           -0.0524516\n",
      "exploration/env_infos/final/reward_energy Min           -0.573338\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.339986\n",
      "exploration/env_infos/initial/reward_energy Std          0.243998\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0584991\n",
      "exploration/env_infos/initial/reward_energy Min         -0.721844\n",
      "exploration/env_infos/reward_energy Mean                -0.218542\n",
      "exploration/env_infos/reward_energy Std                  0.200896\n",
      "exploration/env_infos/reward_energy Max                 -0.00744577\n",
      "exploration/env_infos/reward_energy Min                 -1.2303\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0257641\n",
      "exploration/env_infos/final/end_effector_loc Std         0.155103\n",
      "exploration/env_infos/final/end_effector_loc Max         0.186757\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.315281\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00408129\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0142215\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0277512\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0187135\n",
      "exploration/env_infos/end_effector_loc Mean              0.0136356\n",
      "exploration/env_infos/end_effector_loc Std               0.100509\n",
      "exploration/env_infos/end_effector_loc Max               0.222272\n",
      "exploration/env_infos/end_effector_loc Min              -0.315281\n",
      "evaluation/num steps total                           64000\n",
      "evaluation/num paths total                            3200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0337932\n",
      "evaluation/Rewards Std                                   0.0734834\n",
      "evaluation/Rewards Max                                   0.16716\n",
      "evaluation/Rewards Min                                  -0.479548\n",
      "evaluation/Returns Mean                                 -0.675865\n",
      "evaluation/Returns Std                                   1.05375\n",
      "evaluation/Returns Max                                   2.0205\n",
      "evaluation/Returns Min                                  -3.33007\n",
      "evaluation/Actions Mean                                 -0.00344928\n",
      "evaluation/Actions Std                                   0.0677512\n",
      "evaluation/Actions Max                                   0.517866\n",
      "evaluation/Actions Min                                  -0.620179\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.675865\n",
      "evaluation/env_infos/final/reward_dist Mean              0.289341\n",
      "evaluation/env_infos/final/reward_dist Std               0.297997\n",
      "evaluation/env_infos/final/reward_dist Max               0.972476\n",
      "evaluation/env_infos/final/reward_dist Min               3.59881e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00822387\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0144679\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0608943\n",
      "evaluation/env_infos/initial/reward_dist Min             1.70515e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.245142\n",
      "evaluation/env_infos/reward_dist Std                     0.296029\n",
      "evaluation/env_infos/reward_dist Max                     0.999616\n",
      "evaluation/env_infos/reward_dist Min                     3.59881e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0295788\n",
      "evaluation/env_infos/final/reward_energy Std             0.0187964\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00403183\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0933137\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.275649\n",
      "evaluation/env_infos/initial/reward_energy Std           0.190934\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.021071\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.67586\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0568866\n",
      "evaluation/env_infos/reward_energy Std                   0.0772538\n",
      "evaluation/env_infos/reward_energy Max                  -0.00133965\n",
      "evaluation/env_infos/reward_energy Min                  -0.67586\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0303545\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.219674\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.456076\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.657724\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00154087\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0117547\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0258933\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0310089\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0133605\n",
      "evaluation/env_infos/end_effector_loc Std                0.147809\n",
      "evaluation/env_infos/end_effector_loc Max                0.456076\n",
      "evaluation/env_infos/end_effector_loc Min               -0.657724\n",
      "time/data storing (s)                                    0.00628287\n",
      "time/evaluation sampling (s)                             0.997198\n",
      "time/exploration sampling (s)                            0.137963\n",
      "time/logging (s)                                         0.0193612\n",
      "time/saving (s)                                          0.0269921\n",
      "time/training (s)                                       48.7519\n",
      "time/epoch (s)                                          49.9397\n",
      "time/total (s)                                        3148.54\n",
      "Epoch                                                   63\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:41:58.820954 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 64 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00112701\n",
      "trainer/QF2 Loss                                         0.000757776\n",
      "trainer/Policy Loss                                      3.24952\n",
      "trainer/Q1 Predictions Mean                             -1.21636\n",
      "trainer/Q1 Predictions Std                               0.916525\n",
      "trainer/Q1 Predictions Max                               0.592496\n",
      "trainer/Q1 Predictions Min                              -3.5166\n",
      "trainer/Q2 Predictions Mean                             -1.21197\n",
      "trainer/Q2 Predictions Std                               0.909658\n",
      "trainer/Q2 Predictions Max                               0.639571\n",
      "trainer/Q2 Predictions Min                              -3.49487\n",
      "trainer/Q Targets Mean                                  -1.21771\n",
      "trainer/Q Targets Std                                    0.907131\n",
      "trainer/Q Targets Max                                    0.625402\n",
      "trainer/Q Targets Min                                   -3.49704\n",
      "trainer/Log Pis Mean                                     2.06436\n",
      "trainer/Log Pis Std                                      1.4302\n",
      "trainer/Log Pis Max                                      4.58906\n",
      "trainer/Log Pis Min                                     -5.30592\n",
      "trainer/Policy mu Mean                                  -0.014308\n",
      "trainer/Policy mu Std                                    0.374812\n",
      "trainer/Policy mu Max                                    2.01824\n",
      "trainer/Policy mu Min                                   -2.01189\n",
      "trainer/Policy log std Mean                             -2.32367\n",
      "trainer/Policy log std Std                               0.614063\n",
      "trainer/Policy log std Max                              -0.404828\n",
      "trainer/Policy log std Min                              -3.34685\n",
      "trainer/Alpha                                            0.0235163\n",
      "trainer/Alpha Loss                                       0.241341\n",
      "exploration/num steps total                           7500\n",
      "exploration/num paths total                            375\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0594562\n",
      "exploration/Rewards Std                                  0.0498837\n",
      "exploration/Rewards Max                                  0.0535381\n",
      "exploration/Rewards Min                                 -0.213198\n",
      "exploration/Returns Mean                                -1.18912\n",
      "exploration/Returns Std                                  0.49504\n",
      "exploration/Returns Max                                 -0.388363\n",
      "exploration/Returns Min                                 -1.92628\n",
      "exploration/Actions Mean                                -0.00371189\n",
      "exploration/Actions Std                                  0.0897506\n",
      "exploration/Actions Max                                  0.297097\n",
      "exploration/Actions Min                                 -0.364814\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.18912\n",
      "exploration/env_infos/final/reward_dist Mean             0.0259994\n",
      "exploration/env_infos/final/reward_dist Std              0.0477813\n",
      "exploration/env_infos/final/reward_dist Max              0.121481\n",
      "exploration/env_infos/final/reward_dist Min              3.20869e-13\n",
      "exploration/env_infos/initial/reward_dist Mean           0.011129\n",
      "exploration/env_infos/initial/reward_dist Std            0.0073949\n",
      "exploration/env_infos/initial/reward_dist Max            0.022474\n",
      "exploration/env_infos/initial/reward_dist Min            0.000193838\n",
      "exploration/env_infos/reward_dist Mean                   0.0607261\n",
      "exploration/env_infos/reward_dist Std                    0.0972742\n",
      "exploration/env_infos/reward_dist Max                    0.552679\n",
      "exploration/env_infos/reward_dist Min                    3.20869e-13\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0727122\n",
      "exploration/env_infos/final/reward_energy Std            0.0203647\n",
      "exploration/env_infos/final/reward_energy Max           -0.0486831\n",
      "exploration/env_infos/final/reward_energy Min           -0.101659\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.119941\n",
      "exploration/env_infos/initial/reward_energy Std          0.0663326\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0156859\n",
      "exploration/env_infos/initial/reward_energy Min         -0.21802\n",
      "exploration/env_infos/reward_energy Mean                -0.10143\n",
      "exploration/env_infos/reward_energy Std                  0.0764847\n",
      "exploration/env_infos/reward_energy Max                 -0.00231725\n",
      "exploration/env_infos/reward_energy Min                 -0.365442\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0362968\n",
      "exploration/env_infos/final/end_effector_loc Std         0.262768\n",
      "exploration/env_infos/final/end_effector_loc Max         0.2521\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.557117\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000126361\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00484419\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00861546\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00628627\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0143187\n",
      "exploration/env_infos/end_effector_loc Std               0.152962\n",
      "exploration/env_infos/end_effector_loc Max               0.2521\n",
      "exploration/env_infos/end_effector_loc Min              -0.557117\n",
      "evaluation/num steps total                           65000\n",
      "evaluation/num paths total                            3250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0495191\n",
      "evaluation/Rewards Std                                   0.0651038\n",
      "evaluation/Rewards Max                                   0.123942\n",
      "evaluation/Rewards Min                                  -0.452616\n",
      "evaluation/Returns Mean                                 -0.990382\n",
      "evaluation/Returns Std                                   0.88828\n",
      "evaluation/Returns Max                                   1.13745\n",
      "evaluation/Returns Min                                  -3.04689\n",
      "evaluation/Actions Mean                                 -0.00258786\n",
      "evaluation/Actions Std                                   0.0590669\n",
      "evaluation/Actions Max                                   0.537628\n",
      "evaluation/Actions Min                                  -0.533727\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.990382\n",
      "evaluation/env_infos/final/reward_dist Mean              0.243938\n",
      "evaluation/env_infos/final/reward_dist Std               0.268659\n",
      "evaluation/env_infos/final/reward_dist Max               0.94236\n",
      "evaluation/env_infos/final/reward_dist Min               8.20793e-10\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00623206\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0121003\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0706875\n",
      "evaluation/env_infos/initial/reward_dist Min             2.07393e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.219161\n",
      "evaluation/env_infos/reward_dist Std                     0.273043\n",
      "evaluation/env_infos/reward_dist Max                     0.996959\n",
      "evaluation/env_infos/reward_dist Min                     8.20793e-10\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0261896\n",
      "evaluation/env_infos/final/reward_energy Std             0.0253068\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00245949\n",
      "evaluation/env_infos/final/reward_energy Min            -0.140367\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.219068\n",
      "evaluation/env_infos/initial/reward_energy Std           0.168972\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0100567\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.569234\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0511746\n",
      "evaluation/env_infos/reward_energy Std                   0.0661238\n",
      "evaluation/env_infos/reward_energy Max                  -0.00151638\n",
      "evaluation/env_infos/reward_energy Min                  -0.569234\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0151702\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.205901\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.397796\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.3982\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       8.0154e-06\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0097815\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0268814\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0266863\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00126738\n",
      "evaluation/env_infos/end_effector_loc Std                0.141582\n",
      "evaluation/env_infos/end_effector_loc Max                0.397796\n",
      "evaluation/env_infos/end_effector_loc Min               -0.3982\n",
      "time/data storing (s)                                    0.00625765\n",
      "time/evaluation sampling (s)                             0.953028\n",
      "time/exploration sampling (s)                            0.122617\n",
      "time/logging (s)                                         0.0192978\n",
      "time/saving (s)                                          0.0404132\n",
      "time/training (s)                                       50.014\n",
      "time/epoch (s)                                          51.1556\n",
      "time/total (s)                                        3200.5\n",
      "Epoch                                                   64\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:42:50.006422 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 65 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000648101\r\n",
      "trainer/QF2 Loss                                         0.00169041\r\n",
      "trainer/Policy Loss                                      3.02899\r\n",
      "trainer/Q1 Predictions Mean                             -1.18135\r\n",
      "trainer/Q1 Predictions Std                               0.892795\r\n",
      "trainer/Q1 Predictions Max                               0.662833\r\n",
      "trainer/Q1 Predictions Min                              -3.4206\r\n",
      "trainer/Q2 Predictions Mean                             -1.18155\r\n",
      "trainer/Q2 Predictions Std                               0.898471\r\n",
      "trainer/Q2 Predictions Max                               0.672452\r\n",
      "trainer/Q2 Predictions Min                              -3.45382\r\n",
      "trainer/Q Targets Mean                                  -1.18666\r\n",
      "trainer/Q Targets Std                                    0.895162\r\n",
      "trainer/Q Targets Max                                    0.650226\r\n",
      "trainer/Q Targets Min                                   -3.46276\r\n",
      "trainer/Log Pis Mean                                     1.88205\r\n",
      "trainer/Log Pis Std                                      1.45828\r\n",
      "trainer/Log Pis Max                                      4.30047\r\n",
      "trainer/Log Pis Min                                     -5.88849\r\n",
      "trainer/Policy mu Mean                                  -0.0233983\r\n",
      "trainer/Policy mu Std                                    0.413483\r\n",
      "trainer/Policy mu Max                                    1.98413\r\n",
      "trainer/Policy mu Min                                   -2.14074\r\n",
      "trainer/Policy log std Mean                             -2.22379\r\n",
      "trainer/Policy log std Std                               0.642145\r\n",
      "trainer/Policy log std Max                              -0.324001\r\n",
      "trainer/Policy log std Min                              -3.22432\r\n",
      "trainer/Alpha                                            0.0244798\r\n",
      "trainer/Alpha Loss                                      -0.437554\r\n",
      "exploration/num steps total                           7600\r\n",
      "exploration/num paths total                            380\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0475422\r\n",
      "exploration/Rewards Std                                  0.0796578\r\n",
      "exploration/Rewards Max                                  0.100015\r\n",
      "exploration/Rewards Min                                 -0.231651\r\n",
      "exploration/Returns Mean                                -0.950843\r\n",
      "exploration/Returns Std                                  1.18458\r\n",
      "exploration/Returns Max                                  0.994647\r\n",
      "exploration/Returns Min                                 -2.52016\r\n",
      "exploration/Actions Mean                                 0.000729205\r\n",
      "exploration/Actions Std                                  0.145918\r\n",
      "exploration/Actions Max                                  0.457938\r\n",
      "exploration/Actions Min                                 -0.597549\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.950843\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.320502\r\n",
      "exploration/env_infos/final/reward_dist Std              0.301855\r\n",
      "exploration/env_infos/final/reward_dist Max              0.837569\r\n",
      "exploration/env_infos/final/reward_dist Min              0.0001067\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00693304\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00958976\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.024833\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.59125e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.412851\r\n",
      "exploration/env_infos/reward_dist Std                    0.331301\r\n",
      "exploration/env_infos/reward_dist Max                    0.984953\r\n",
      "exploration/env_infos/reward_dist Min                    5.59125e-05\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.150018\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0753236\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0564143\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.268537\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.372499\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.16677\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.179539\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.603427\r\n",
      "exploration/env_infos/reward_energy Mean                -0.168888\r\n",
      "exploration/env_infos/reward_energy Std                  0.118584\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0170631\r\n",
      "exploration/env_infos/reward_energy Min                 -0.603427\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0154362\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.180421\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.313196\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.232659\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0005715\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0144182\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0228969\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0298774\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.000524272\r\n",
      "exploration/env_infos/end_effector_loc Std               0.139888\r\n",
      "exploration/env_infos/end_effector_loc Max               0.313196\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.243132\r\n",
      "evaluation/num steps total                           66000\r\n",
      "evaluation/num paths total                            3300\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0299933\r\n",
      "evaluation/Rewards Std                                   0.0754878\r\n",
      "evaluation/Rewards Max                                   0.171565\r\n",
      "evaluation/Rewards Min                                  -0.482801\r\n",
      "evaluation/Returns Mean                                 -0.599866\r\n",
      "evaluation/Returns Std                                   1.10414\r\n",
      "evaluation/Returns Max                                   2.42025\r\n",
      "evaluation/Returns Min                                  -3.02132\r\n",
      "evaluation/Actions Mean                                 -0.00389032\r\n",
      "evaluation/Actions Std                                   0.0772714\r\n",
      "evaluation/Actions Max                                   0.699701\r\n",
      "evaluation/Actions Min                                  -0.782878\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.599866\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.208672\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.263601\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.927081\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.09769e-08\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00405309\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00780151\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0346105\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.80407e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.195918\r\n",
      "evaluation/env_infos/reward_dist Std                     0.259791\r\n",
      "evaluation/env_infos/reward_dist Max                     0.991079\r\n",
      "evaluation/env_infos/reward_dist Min                     8.09769e-08\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0308077\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0261926\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00388651\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.160158\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.304569\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.243232\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0238988\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.870067\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0602614\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0913266\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000976836\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.870067\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0523563\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.209482\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.361782\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.5487\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00139061\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0137103\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0349851\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0391439\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0219369\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.151546\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.413374\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.5487\r\n",
      "time/data storing (s)                                    0.00621225\r\n",
      "time/evaluation sampling (s)                             1.00932\r\n",
      "time/exploration sampling (s)                            0.131759\r\n",
      "time/logging (s)                                         0.0204011\r\n",
      "time/saving (s)                                          0.0718837\r\n",
      "time/training (s)                                       49.0604\r\n",
      "time/epoch (s)                                          50.3\r\n",
      "time/total (s)                                        3251.68\r\n",
      "Epoch                                                   65\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:43:40.347736 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 66 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000855934\n",
      "trainer/QF2 Loss                                         0.00124961\n",
      "trainer/Policy Loss                                      3.14899\n",
      "trainer/Q1 Predictions Mean                             -1.19832\n",
      "trainer/Q1 Predictions Std                               0.894354\n",
      "trainer/Q1 Predictions Max                               0.384688\n",
      "trainer/Q1 Predictions Min                              -3.1853\n",
      "trainer/Q2 Predictions Mean                             -1.19282\n",
      "trainer/Q2 Predictions Std                               0.891185\n",
      "trainer/Q2 Predictions Max                               0.401995\n",
      "trainer/Q2 Predictions Min                              -3.16843\n",
      "trainer/Q Targets Mean                                  -1.19722\n",
      "trainer/Q Targets Std                                    0.895412\n",
      "trainer/Q Targets Max                                    0.399836\n",
      "trainer/Q Targets Min                                   -3.17558\n",
      "trainer/Log Pis Mean                                     1.97612\n",
      "trainer/Log Pis Std                                      1.41783\n",
      "trainer/Log Pis Max                                      4.40991\n",
      "trainer/Log Pis Min                                     -6.51051\n",
      "trainer/Policy mu Mean                                  -0.0211367\n",
      "trainer/Policy mu Std                                    0.347615\n",
      "trainer/Policy mu Max                                    1.87751\n",
      "trainer/Policy mu Min                                   -2.25394\n",
      "trainer/Policy log std Mean                             -2.29746\n",
      "trainer/Policy log std Std                               0.576919\n",
      "trainer/Policy log std Max                              -0.31567\n",
      "trainer/Policy log std Min                              -3.22645\n",
      "trainer/Alpha                                            0.0251636\n",
      "trainer/Alpha Loss                                      -0.0879829\n",
      "exploration/num steps total                           7700\n",
      "exploration/num paths total                            385\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.132713\n",
      "exploration/Rewards Std                                  0.0999738\n",
      "exploration/Rewards Max                                  0.0734355\n",
      "exploration/Rewards Min                                 -0.571278\n",
      "exploration/Returns Mean                                -2.65427\n",
      "exploration/Returns Std                                  1.29148\n",
      "exploration/Returns Max                                 -0.846789\n",
      "exploration/Returns Min                                 -4.78243\n",
      "exploration/Actions Mean                                -0.00486689\n",
      "exploration/Actions Std                                  0.106677\n",
      "exploration/Actions Max                                  0.753166\n",
      "exploration/Actions Min                                 -0.283076\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.65427\n",
      "exploration/env_infos/final/reward_dist Mean             0.203374\n",
      "exploration/env_infos/final/reward_dist Std              0.204803\n",
      "exploration/env_infos/final/reward_dist Max              0.569228\n",
      "exploration/env_infos/final/reward_dist Min              5.68186e-16\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000983536\n",
      "exploration/env_infos/initial/reward_dist Std            0.00176785\n",
      "exploration/env_infos/initial/reward_dist Max            0.00451645\n",
      "exploration/env_infos/initial/reward_dist Min            4.60139e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.106176\n",
      "exploration/env_infos/reward_dist Std                    0.173328\n",
      "exploration/env_infos/reward_dist Max                    0.643627\n",
      "exploration/env_infos/reward_dist Min                    2.7158e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.100492\n",
      "exploration/env_infos/final/reward_energy Std            0.0652611\n",
      "exploration/env_infos/final/reward_energy Max           -0.0375425\n",
      "exploration/env_infos/final/reward_energy Min           -0.223123\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.287176\n",
      "exploration/env_infos/initial/reward_energy Std          0.262698\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0459533\n",
      "exploration/env_infos/initial/reward_energy Min         -0.76013\n",
      "exploration/env_infos/reward_energy Mean                -0.115906\n",
      "exploration/env_infos/reward_energy Std                  0.0968152\n",
      "exploration/env_infos/reward_energy Max                 -0.00877634\n",
      "exploration/env_infos/reward_energy Min                 -0.76013\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0792425\n",
      "exploration/env_infos/final/end_effector_loc Std         0.190777\n",
      "exploration/env_infos/final/end_effector_loc Max         0.483355\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.1684\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00621469\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0122771\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0376583\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00513287\n",
      "exploration/env_infos/end_effector_loc Mean              0.0667561\n",
      "exploration/env_infos/end_effector_loc Std               0.135095\n",
      "exploration/env_infos/end_effector_loc Max               0.516079\n",
      "exploration/env_infos/end_effector_loc Min              -0.1684\n",
      "evaluation/num steps total                           67000\n",
      "evaluation/num paths total                            3350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0340597\n",
      "evaluation/Rewards Std                                   0.0806457\n",
      "evaluation/Rewards Max                                   0.172406\n",
      "evaluation/Rewards Min                                  -0.499849\n",
      "evaluation/Returns Mean                                 -0.681194\n",
      "evaluation/Returns Std                                   1.25488\n",
      "evaluation/Returns Max                                   1.97083\n",
      "evaluation/Returns Min                                  -3.34324\n",
      "evaluation/Actions Mean                                 -0.000174391\n",
      "evaluation/Actions Std                                   0.0793134\n",
      "evaluation/Actions Max                                   0.605503\n",
      "evaluation/Actions Min                                  -0.79918\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.681194\n",
      "evaluation/env_infos/final/reward_dist Mean              0.189768\n",
      "evaluation/env_infos/final/reward_dist Std               0.286768\n",
      "evaluation/env_infos/final/reward_dist Max               0.92536\n",
      "evaluation/env_infos/final/reward_dist Min               9.55792e-14\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00929873\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0203855\n",
      "evaluation/env_infos/initial/reward_dist Max             0.108773\n",
      "evaluation/env_infos/initial/reward_dist Min             1.69884e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.177469\n",
      "evaluation/env_infos/reward_dist Std                     0.261715\n",
      "evaluation/env_infos/reward_dist Max                     0.989603\n",
      "evaluation/env_infos/reward_dist Min                     9.55792e-14\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0366713\n",
      "evaluation/env_infos/final/reward_energy Std             0.0275638\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00486182\n",
      "evaluation/env_infos/final/reward_energy Min            -0.125001\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.31426\n",
      "evaluation/env_infos/initial/reward_energy Std           0.216202\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0125632\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.802159\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0654413\n",
      "evaluation/env_infos/reward_energy Std                   0.0910974\n",
      "evaluation/env_infos/reward_energy Max                  -0.000341131\n",
      "evaluation/env_infos/reward_energy Min                  -0.802159\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0099878\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.237221\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.617628\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.443127\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00119045\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134336\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0302751\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.039959\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00732146\n",
      "evaluation/env_infos/end_effector_loc Std                0.164609\n",
      "evaluation/env_infos/end_effector_loc Max                0.617628\n",
      "evaluation/env_infos/end_effector_loc Min               -0.443127\n",
      "time/data storing (s)                                    0.00711906\n",
      "time/evaluation sampling (s)                             0.989428\n",
      "time/exploration sampling (s)                            0.125825\n",
      "time/logging (s)                                         0.0195654\n",
      "time/saving (s)                                          0.0284077\n",
      "time/training (s)                                       48.3144\n",
      "time/epoch (s)                                          49.4847\n",
      "time/total (s)                                        3302.02\n",
      "Epoch                                                   66\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:44:31.973275 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 67 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00075922\n",
      "trainer/QF2 Loss                                         0.000949998\n",
      "trainer/Policy Loss                                      3.3201\n",
      "trainer/Q1 Predictions Mean                             -1.21289\n",
      "trainer/Q1 Predictions Std                               0.924484\n",
      "trainer/Q1 Predictions Max                               0.485234\n",
      "trainer/Q1 Predictions Min                              -3.23115\n",
      "trainer/Q2 Predictions Mean                             -1.21808\n",
      "trainer/Q2 Predictions Std                               0.923907\n",
      "trainer/Q2 Predictions Max                               0.487811\n",
      "trainer/Q2 Predictions Min                              -3.20986\n",
      "trainer/Q Targets Mean                                  -1.22089\n",
      "trainer/Q Targets Std                                    0.926782\n",
      "trainer/Q Targets Max                                    0.465817\n",
      "trainer/Q Targets Min                                   -3.23826\n",
      "trainer/Log Pis Mean                                     2.13844\n",
      "trainer/Log Pis Std                                      1.18806\n",
      "trainer/Log Pis Max                                      4.29328\n",
      "trainer/Log Pis Min                                     -1.22385\n",
      "trainer/Policy mu Mean                                  -0.0259944\n",
      "trainer/Policy mu Std                                    0.349733\n",
      "trainer/Policy mu Max                                    1.518\n",
      "trainer/Policy mu Min                                   -2.12073\n",
      "trainer/Policy log std Mean                             -2.30102\n",
      "trainer/Policy log std Std                               0.597576\n",
      "trainer/Policy log std Max                              -0.605386\n",
      "trainer/Policy log std Min                              -3.2379\n",
      "trainer/Alpha                                            0.0249499\n",
      "trainer/Alpha Loss                                       0.511169\n",
      "exploration/num steps total                           7800\n",
      "exploration/num paths total                            390\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0657425\n",
      "exploration/Rewards Std                                  0.0757542\n",
      "exploration/Rewards Max                                  0.104427\n",
      "exploration/Rewards Min                                 -0.290518\n",
      "exploration/Returns Mean                                -1.31485\n",
      "exploration/Returns Std                                  1.25449\n",
      "exploration/Returns Max                                  0.794102\n",
      "exploration/Returns Min                                 -3.05106\n",
      "exploration/Actions Mean                                 0.0116453\n",
      "exploration/Actions Std                                  0.176091\n",
      "exploration/Actions Max                                  0.673549\n",
      "exploration/Actions Min                                 -0.569984\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.31485\n",
      "exploration/env_infos/final/reward_dist Mean             0.132009\n",
      "exploration/env_infos/final/reward_dist Std              0.230968\n",
      "exploration/env_infos/final/reward_dist Max              0.591202\n",
      "exploration/env_infos/final/reward_dist Min              2.89476e-13\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000289791\n",
      "exploration/env_infos/initial/reward_dist Std            0.000285776\n",
      "exploration/env_infos/initial/reward_dist Max            0.000809926\n",
      "exploration/env_infos/initial/reward_dist Min            1.42164e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0838754\n",
      "exploration/env_infos/reward_dist Std                    0.126254\n",
      "exploration/env_infos/reward_dist Max                    0.595019\n",
      "exploration/env_infos/reward_dist Min                    2.89476e-13\n",
      "exploration/env_infos/final/reward_energy Mean          -0.165765\n",
      "exploration/env_infos/final/reward_energy Std            0.0719715\n",
      "exploration/env_infos/final/reward_energy Max           -0.10009\n",
      "exploration/env_infos/final/reward_energy Min           -0.304557\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.405639\n",
      "exploration/env_infos/initial/reward_energy Std          0.245798\n",
      "exploration/env_infos/initial/reward_energy Max         -0.069331\n",
      "exploration/env_infos/initial/reward_energy Min         -0.691237\n",
      "exploration/env_infos/reward_energy Mean                -0.191103\n",
      "exploration/env_infos/reward_energy Std                  0.160521\n",
      "exploration/env_infos/reward_energy Max                 -0.0149823\n",
      "exploration/env_infos/reward_energy Min                 -0.691237\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0756313\n",
      "exploration/env_infos/final/end_effector_loc Std         0.281515\n",
      "exploration/env_infos/final/end_effector_loc Max         0.489768\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.465968\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00380092\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0163326\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0332064\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0225607\n",
      "exploration/env_infos/end_effector_loc Mean              0.0344955\n",
      "exploration/env_infos/end_effector_loc Std               0.181424\n",
      "exploration/env_infos/end_effector_loc Max               0.489768\n",
      "exploration/env_infos/end_effector_loc Min              -0.465968\n",
      "evaluation/num steps total                           68000\n",
      "evaluation/num paths total                            3400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0321907\n",
      "evaluation/Rewards Std                                   0.0770213\n",
      "evaluation/Rewards Max                                   0.142828\n",
      "evaluation/Rewards Min                                  -0.36089\n",
      "evaluation/Returns Mean                                 -0.643815\n",
      "evaluation/Returns Std                                   1.17002\n",
      "evaluation/Returns Max                                   1.97297\n",
      "evaluation/Returns Min                                  -2.99402\n",
      "evaluation/Actions Mean                                 -0.000633986\n",
      "evaluation/Actions Std                                   0.071544\n",
      "evaluation/Actions Max                                   0.47078\n",
      "evaluation/Actions Min                                  -0.894235\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.643815\n",
      "evaluation/env_infos/final/reward_dist Mean              0.251694\n",
      "evaluation/env_infos/final/reward_dist Std               0.288206\n",
      "evaluation/env_infos/final/reward_dist Max               0.915688\n",
      "evaluation/env_infos/final/reward_dist Min               4.635e-24\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00696117\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0105953\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0392162\n",
      "evaluation/env_infos/initial/reward_dist Min             8.54638e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.240685\n",
      "evaluation/env_infos/reward_dist Std                     0.2977\n",
      "evaluation/env_infos/reward_dist Max                     0.999534\n",
      "evaluation/env_infos/reward_dist Min                     4.635e-24\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0343013\n",
      "evaluation/env_infos/final/reward_energy Std             0.0286176\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00169119\n",
      "evaluation/env_infos/final/reward_energy Min            -0.120874\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.263778\n",
      "evaluation/env_infos/initial/reward_energy Std           0.216962\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00179167\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.971309\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0587379\n",
      "evaluation/env_infos/reward_energy Std                   0.0823879\n",
      "evaluation/env_infos/reward_energy Max                  -0.00065436\n",
      "evaluation/env_infos/reward_energy Min                  -0.971309\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0397012\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.202691\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.350242\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.742064\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00239876\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0118347\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.023539\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0447118\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0261946\n",
      "evaluation/env_infos/end_effector_loc Std                0.139885\n",
      "evaluation/env_infos/end_effector_loc Max                0.350242\n",
      "evaluation/env_infos/end_effector_loc Min               -0.742064\n",
      "time/data storing (s)                                    0.00635146\n",
      "time/evaluation sampling (s)                             0.982102\n",
      "time/exploration sampling (s)                            0.12431\n",
      "time/logging (s)                                         0.0202161\n",
      "time/saving (s)                                          0.0284238\n",
      "time/training (s)                                       49.6215\n",
      "time/epoch (s)                                          50.7829\n",
      "time/total (s)                                        3353.64\n",
      "Epoch                                                   67\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:45:23.798288 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 68 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000888035\n",
      "trainer/QF2 Loss                                         0.00151346\n",
      "trainer/Policy Loss                                      3.04064\n",
      "trainer/Q1 Predictions Mean                             -1.20245\n",
      "trainer/Q1 Predictions Std                               0.835866\n",
      "trainer/Q1 Predictions Max                               0.370069\n",
      "trainer/Q1 Predictions Min                              -3.42912\n",
      "trainer/Q2 Predictions Mean                             -1.20539\n",
      "trainer/Q2 Predictions Std                               0.841336\n",
      "trainer/Q2 Predictions Max                               0.371297\n",
      "trainer/Q2 Predictions Min                              -3.47163\n",
      "trainer/Q Targets Mean                                  -1.20046\n",
      "trainer/Q Targets Std                                    0.832977\n",
      "trainer/Q Targets Max                                    0.353883\n",
      "trainer/Q Targets Min                                   -3.41238\n",
      "trainer/Log Pis Mean                                     1.86922\n",
      "trainer/Log Pis Std                                      1.43422\n",
      "trainer/Log Pis Max                                      4.60232\n",
      "trainer/Log Pis Min                                     -4.13714\n",
      "trainer/Policy mu Mean                                  -0.063984\n",
      "trainer/Policy mu Std                                    0.365088\n",
      "trainer/Policy mu Max                                    1.47946\n",
      "trainer/Policy mu Min                                   -2.17666\n",
      "trainer/Policy log std Mean                             -2.19922\n",
      "trainer/Policy log std Std                               0.618955\n",
      "trainer/Policy log std Max                              -0.410267\n",
      "trainer/Policy log std Min                              -3.24423\n",
      "trainer/Alpha                                            0.0240515\n",
      "trainer/Alpha Loss                                      -0.487281\n",
      "exploration/num steps total                           7900\n",
      "exploration/num paths total                            395\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0706893\n",
      "exploration/Rewards Std                                  0.0545477\n",
      "exploration/Rewards Max                                  0.0353145\n",
      "exploration/Rewards Min                                 -0.247806\n",
      "exploration/Returns Mean                                -1.41379\n",
      "exploration/Returns Std                                  0.522147\n",
      "exploration/Returns Max                                 -0.67925\n",
      "exploration/Returns Min                                 -1.93599\n",
      "exploration/Actions Mean                                -0.00250477\n",
      "exploration/Actions Std                                  0.156093\n",
      "exploration/Actions Max                                  0.652319\n",
      "exploration/Actions Min                                 -0.61264\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.41379\n",
      "exploration/env_infos/final/reward_dist Mean             0.125434\n",
      "exploration/env_infos/final/reward_dist Std              0.0973046\n",
      "exploration/env_infos/final/reward_dist Max              0.25041\n",
      "exploration/env_infos/final/reward_dist Min              0.000141741\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0122318\n",
      "exploration/env_infos/initial/reward_dist Std            0.015943\n",
      "exploration/env_infos/initial/reward_dist Max            0.0407408\n",
      "exploration/env_infos/initial/reward_dist Min            0.000129805\n",
      "exploration/env_infos/reward_dist Mean                   0.159576\n",
      "exploration/env_infos/reward_dist Std                    0.173991\n",
      "exploration/env_infos/reward_dist Max                    0.630665\n",
      "exploration/env_infos/reward_dist Min                    0.000129805\n",
      "exploration/env_infos/final/reward_energy Mean          -0.12543\n",
      "exploration/env_infos/final/reward_energy Std            0.0146607\n",
      "exploration/env_infos/final/reward_energy Max           -0.102752\n",
      "exploration/env_infos/final/reward_energy Min           -0.143966\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.404132\n",
      "exploration/env_infos/initial/reward_energy Std          0.244043\n",
      "exploration/env_infos/initial/reward_energy Max         -0.162386\n",
      "exploration/env_infos/initial/reward_energy Min         -0.765848\n",
      "exploration/env_infos/reward_energy Mean                -0.163626\n",
      "exploration/env_infos/reward_energy Std                  0.148221\n",
      "exploration/env_infos/reward_energy Max                 -0.0149933\n",
      "exploration/env_infos/reward_energy Min                 -0.765848\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0960747\n",
      "exploration/env_infos/final/end_effector_loc Std         0.160555\n",
      "exploration/env_infos/final/end_effector_loc Max         0.373366\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.157553\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00448275\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0160781\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0326159\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0200627\n",
      "exploration/env_infos/end_effector_loc Mean              0.0697581\n",
      "exploration/env_infos/end_effector_loc Std               0.124124\n",
      "exploration/env_infos/end_effector_loc Max               0.373366\n",
      "exploration/env_infos/end_effector_loc Min              -0.157553\n",
      "evaluation/num steps total                           69000\n",
      "evaluation/num paths total                            3450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.043497\n",
      "evaluation/Rewards Std                                   0.0704057\n",
      "evaluation/Rewards Max                                   0.137576\n",
      "evaluation/Rewards Min                                  -0.385436\n",
      "evaluation/Returns Mean                                 -0.86994\n",
      "evaluation/Returns Std                                   1.14933\n",
      "evaluation/Returns Max                                   2.21989\n",
      "evaluation/Returns Min                                  -2.48968\n",
      "evaluation/Actions Mean                                 -0.00305311\n",
      "evaluation/Actions Std                                   0.0627783\n",
      "evaluation/Actions Max                                   0.46985\n",
      "evaluation/Actions Min                                  -0.859427\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.86994\n",
      "evaluation/env_infos/final/reward_dist Mean              0.253489\n",
      "evaluation/env_infos/final/reward_dist Std               0.295864\n",
      "evaluation/env_infos/final/reward_dist Max               0.95718\n",
      "evaluation/env_infos/final/reward_dist Min               2.40782e-10\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00620161\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116465\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0670635\n",
      "evaluation/env_infos/initial/reward_dist Min             5.96689e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.196669\n",
      "evaluation/env_infos/reward_dist Std                     0.281902\n",
      "evaluation/env_infos/reward_dist Max                     0.999892\n",
      "evaluation/env_infos/reward_dist Min                     2.40782e-10\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0301773\n",
      "evaluation/env_infos/final/reward_energy Std             0.029502\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00508925\n",
      "evaluation/env_infos/final/reward_energy Min            -0.171254\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.227451\n",
      "evaluation/env_infos/initial/reward_energy Std           0.228838\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00589152\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.965555\n",
      "evaluation/env_infos/reward_energy Mean                 -0.047678\n",
      "evaluation/env_infos/reward_energy Std                   0.0750179\n",
      "evaluation/env_infos/reward_energy Max                  -0.000642842\n",
      "evaluation/env_infos/reward_energy Min                  -0.965555\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0517957\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.202866\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.349172\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.591506\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00189136\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0112494\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0234925\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0429714\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0249174\n",
      "evaluation/env_infos/end_effector_loc Std                0.132511\n",
      "evaluation/env_infos/end_effector_loc Max                0.349172\n",
      "evaluation/env_infos/end_effector_loc Min               -0.591506\n",
      "time/data storing (s)                                    0.0090329\n",
      "time/evaluation sampling (s)                             0.968899\n",
      "time/exploration sampling (s)                            0.127854\n",
      "time/logging (s)                                         0.0227924\n",
      "time/saving (s)                                          0.0270063\n",
      "time/training (s)                                       49.7492\n",
      "time/epoch (s)                                          50.9047\n",
      "time/total (s)                                        3405.47\n",
      "Epoch                                                   68\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:46:15.444026 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 69 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00124092\n",
      "trainer/QF2 Loss                                         0.000827176\n",
      "trainer/Policy Loss                                      3.1592\n",
      "trainer/Q1 Predictions Mean                             -1.2101\n",
      "trainer/Q1 Predictions Std                               0.904185\n",
      "trainer/Q1 Predictions Max                               0.51518\n",
      "trainer/Q1 Predictions Min                              -3.40901\n",
      "trainer/Q2 Predictions Mean                             -1.21317\n",
      "trainer/Q2 Predictions Std                               0.910472\n",
      "trainer/Q2 Predictions Max                               0.519179\n",
      "trainer/Q2 Predictions Min                              -3.43796\n",
      "trainer/Q Targets Mean                                  -1.20706\n",
      "trainer/Q Targets Std                                    0.912801\n",
      "trainer/Q Targets Max                                    0.526384\n",
      "trainer/Q Targets Min                                   -3.41644\n",
      "trainer/Log Pis Mean                                     1.97866\n",
      "trainer/Log Pis Std                                      1.41837\n",
      "trainer/Log Pis Max                                      4.55097\n",
      "trainer/Log Pis Min                                     -6.9796\n",
      "trainer/Policy mu Mean                                  -0.0469033\n",
      "trainer/Policy mu Std                                    0.308508\n",
      "trainer/Policy mu Max                                    1.25643\n",
      "trainer/Policy mu Min                                   -2.29944\n",
      "trainer/Policy log std Mean                             -2.31462\n",
      "trainer/Policy log std Std                               0.589691\n",
      "trainer/Policy log std Max                              -0.467705\n",
      "trainer/Policy log std Min                              -3.396\n",
      "trainer/Alpha                                            0.0233802\n",
      "trainer/Alpha Loss                                      -0.0801387\n",
      "exploration/num steps total                           8000\n",
      "exploration/num paths total                            400\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.075139\n",
      "exploration/Rewards Std                                  0.0921055\n",
      "exploration/Rewards Max                                  0.0884348\n",
      "exploration/Rewards Min                                 -0.322604\n",
      "exploration/Returns Mean                                -1.50278\n",
      "exploration/Returns Std                                  1.10371\n",
      "exploration/Returns Max                                  0.00732886\n",
      "exploration/Returns Min                                 -3.23859\n",
      "exploration/Actions Mean                                 0.0105203\n",
      "exploration/Actions Std                                  0.103957\n",
      "exploration/Actions Max                                  0.247927\n",
      "exploration/Actions Min                                 -0.785389\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.50278\n",
      "exploration/env_infos/final/reward_dist Mean             0.0526106\n",
      "exploration/env_infos/final/reward_dist Std              0.0989551\n",
      "exploration/env_infos/final/reward_dist Max              0.250405\n",
      "exploration/env_infos/final/reward_dist Min              3.56775e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.013693\n",
      "exploration/env_infos/initial/reward_dist Std            0.0212503\n",
      "exploration/env_infos/initial/reward_dist Max            0.0558517\n",
      "exploration/env_infos/initial/reward_dist Min            6.37671e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.165863\n",
      "exploration/env_infos/reward_dist Std                    0.262373\n",
      "exploration/env_infos/reward_dist Max                    0.97247\n",
      "exploration/env_infos/reward_dist Min                    3.56775e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.13881\n",
      "exploration/env_infos/final/reward_energy Std            0.114942\n",
      "exploration/env_infos/final/reward_energy Max           -0.0200862\n",
      "exploration/env_infos/final/reward_energy Min           -0.345058\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.28653\n",
      "exploration/env_infos/initial/reward_energy Std          0.325721\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0199006\n",
      "exploration/env_infos/initial/reward_energy Min         -0.922706\n",
      "exploration/env_infos/reward_energy Mean                -0.103017\n",
      "exploration/env_infos/reward_energy Std                  0.105938\n",
      "exploration/env_infos/reward_energy Max                 -0.00735794\n",
      "exploration/env_infos/reward_energy Min                 -0.922706\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0231722\n",
      "exploration/env_infos/final/end_effector_loc Std         0.296037\n",
      "exploration/env_infos/final/end_effector_loc Max         0.542097\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.440915\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00669399\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0137997\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0109158\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0392695\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0472769\n",
      "exploration/env_infos/end_effector_loc Std               0.187665\n",
      "exploration/env_infos/end_effector_loc Max               0.542097\n",
      "exploration/env_infos/end_effector_loc Min              -0.440915\n",
      "evaluation/num steps total                           70000\n",
      "evaluation/num paths total                            3500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0337881\n",
      "evaluation/Rewards Std                                   0.0656875\n",
      "evaluation/Rewards Max                                   0.158172\n",
      "evaluation/Rewards Min                                  -0.300021\n",
      "evaluation/Returns Mean                                 -0.675762\n",
      "evaluation/Returns Std                                   0.913787\n",
      "evaluation/Returns Max                                   1.81159\n",
      "evaluation/Returns Min                                  -2.45363\n",
      "evaluation/Actions Mean                                  0.0021955\n",
      "evaluation/Actions Std                                   0.0593349\n",
      "evaluation/Actions Max                                   0.694803\n",
      "evaluation/Actions Min                                  -0.585303\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.675762\n",
      "evaluation/env_infos/final/reward_dist Mean              0.215267\n",
      "evaluation/env_infos/final/reward_dist Std               0.273699\n",
      "evaluation/env_infos/final/reward_dist Max               0.945441\n",
      "evaluation/env_infos/final/reward_dist Min               1.61114e-10\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00579047\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0108018\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0524274\n",
      "evaluation/env_infos/initial/reward_dist Min             9.28371e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.195164\n",
      "evaluation/env_infos/reward_dist Std                     0.268841\n",
      "evaluation/env_infos/reward_dist Max                     0.99729\n",
      "evaluation/env_infos/reward_dist Min                     1.61114e-10\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0348836\n",
      "evaluation/env_infos/final/reward_energy Std             0.0230238\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00913908\n",
      "evaluation/env_infos/final/reward_energy Min            -0.127271\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.227483\n",
      "evaluation/env_infos/initial/reward_energy Std           0.190157\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00603766\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.757191\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0494121\n",
      "evaluation/env_infos/reward_energy Std                   0.0678923\n",
      "evaluation/env_infos/reward_energy Max                  -0.000767588\n",
      "evaluation/env_infos/reward_energy Min                  -0.757191\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0251863\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.210133\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.460985\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.442473\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00085296\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0104479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0347401\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0292652\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0199829\n",
      "evaluation/env_infos/end_effector_loc Std                0.138323\n",
      "evaluation/env_infos/end_effector_loc Max                0.460985\n",
      "evaluation/env_infos/end_effector_loc Min               -0.482229\n",
      "time/data storing (s)                                    0.00628913\n",
      "time/evaluation sampling (s)                             0.962012\n",
      "time/exploration sampling (s)                            0.120787\n",
      "time/logging (s)                                         0.0197162\n",
      "time/saving (s)                                          0.0270479\n",
      "time/training (s)                                       49.6442\n",
      "time/epoch (s)                                          50.7801\n",
      "time/total (s)                                        3457.11\n",
      "Epoch                                                   69\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:47:06.692120 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 70 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000740445\n",
      "trainer/QF2 Loss                                         0.00124124\n",
      "trainer/Policy Loss                                      3.2248\n",
      "trainer/Q1 Predictions Mean                             -1.1905\n",
      "trainer/Q1 Predictions Std                               0.937817\n",
      "trainer/Q1 Predictions Max                               0.648472\n",
      "trainer/Q1 Predictions Min                              -3.62208\n",
      "trainer/Q2 Predictions Mean                             -1.1949\n",
      "trainer/Q2 Predictions Std                               0.933977\n",
      "trainer/Q2 Predictions Max                               0.644928\n",
      "trainer/Q2 Predictions Min                              -3.62417\n",
      "trainer/Q Targets Mean                                  -1.19769\n",
      "trainer/Q Targets Std                                    0.937346\n",
      "trainer/Q Targets Max                                    0.632598\n",
      "trainer/Q Targets Min                                   -3.62652\n",
      "trainer/Log Pis Mean                                     2.06681\n",
      "trainer/Log Pis Std                                      1.39605\n",
      "trainer/Log Pis Max                                      4.61627\n",
      "trainer/Log Pis Min                                     -4.63062\n",
      "trainer/Policy mu Mean                                  -0.0232026\n",
      "trainer/Policy mu Std                                    0.288238\n",
      "trainer/Policy mu Max                                    1.69177\n",
      "trainer/Policy mu Min                                   -1.48109\n",
      "trainer/Policy log std Mean                             -2.37854\n",
      "trainer/Policy log std Std                               0.567448\n",
      "trainer/Policy log std Max                              -0.606399\n",
      "trainer/Policy log std Min                              -3.36688\n",
      "trainer/Alpha                                            0.0227153\n",
      "trainer/Alpha Loss                                       0.252937\n",
      "exploration/num steps total                           8100\n",
      "exploration/num paths total                            405\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0814054\n",
      "exploration/Rewards Std                                  0.0853408\n",
      "exploration/Rewards Max                                  0.0646248\n",
      "exploration/Rewards Min                                 -0.386321\n",
      "exploration/Returns Mean                                -1.62811\n",
      "exploration/Returns Std                                  0.979542\n",
      "exploration/Returns Max                                 -0.620122\n",
      "exploration/Returns Min                                 -3.2492\n",
      "exploration/Actions Mean                                -0.00653808\n",
      "exploration/Actions Std                                  0.129825\n",
      "exploration/Actions Max                                  0.322096\n",
      "exploration/Actions Min                                 -0.585202\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.62811\n",
      "exploration/env_infos/final/reward_dist Mean             0.0215539\n",
      "exploration/env_infos/final/reward_dist Std              0.0389861\n",
      "exploration/env_infos/final/reward_dist Max              0.0994407\n",
      "exploration/env_infos/final/reward_dist Min              1.09433e-21\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00265131\n",
      "exploration/env_infos/initial/reward_dist Std            0.00217704\n",
      "exploration/env_infos/initial/reward_dist Max            0.0065618\n",
      "exploration/env_infos/initial/reward_dist Min            0.00012041\n",
      "exploration/env_infos/reward_dist Mean                   0.0930214\n",
      "exploration/env_infos/reward_dist Std                    0.189052\n",
      "exploration/env_infos/reward_dist Max                    0.894516\n",
      "exploration/env_infos/reward_dist Min                    1.09433e-21\n",
      "exploration/env_infos/final/reward_energy Mean          -0.15889\n",
      "exploration/env_infos/final/reward_energy Std            0.0875147\n",
      "exploration/env_infos/final/reward_energy Max           -0.0544388\n",
      "exploration/env_infos/final/reward_energy Min           -0.278897\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.293352\n",
      "exploration/env_infos/initial/reward_energy Std          0.22578\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0454059\n",
      "exploration/env_infos/initial/reward_energy Min         -0.627839\n",
      "exploration/env_infos/reward_energy Mean                -0.132403\n",
      "exploration/env_infos/reward_energy Std                  0.127529\n",
      "exploration/env_infos/reward_energy Max                 -0.00528715\n",
      "exploration/env_infos/reward_energy Min                 -0.627839\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.169658\n",
      "exploration/env_infos/final/end_effector_loc Std         0.269109\n",
      "exploration/env_infos/final/end_effector_loc Max         0.138746\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.750845\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00577273\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0117459\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.013316\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0284278\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0917222\n",
      "exploration/env_infos/end_effector_loc Std               0.194528\n",
      "exploration/env_infos/end_effector_loc Max               0.24826\n",
      "exploration/env_infos/end_effector_loc Min              -0.750845\n",
      "evaluation/num steps total                           71000\n",
      "evaluation/num paths total                            3550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.037265\n",
      "evaluation/Rewards Std                                   0.0711602\n",
      "evaluation/Rewards Max                                   0.131332\n",
      "evaluation/Rewards Min                                  -0.406731\n",
      "evaluation/Returns Mean                                 -0.745301\n",
      "evaluation/Returns Std                                   1.09448\n",
      "evaluation/Returns Max                                   1.8221\n",
      "evaluation/Returns Min                                  -3.4152\n",
      "evaluation/Actions Mean                                  8.08568e-05\n",
      "evaluation/Actions Std                                   0.0670808\n",
      "evaluation/Actions Max                                   0.749085\n",
      "evaluation/Actions Min                                  -0.686547\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.745301\n",
      "evaluation/env_infos/final/reward_dist Mean              0.239921\n",
      "evaluation/env_infos/final/reward_dist Std               0.305884\n",
      "evaluation/env_infos/final/reward_dist Max               0.999096\n",
      "evaluation/env_infos/final/reward_dist Min               3.9552e-14\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00501537\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00780752\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0328681\n",
      "evaluation/env_infos/initial/reward_dist Min             3.45055e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.175894\n",
      "evaluation/env_infos/reward_dist Std                     0.253281\n",
      "evaluation/env_infos/reward_dist Max                     0.999233\n",
      "evaluation/env_infos/reward_dist Min                     3.9552e-14\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0276986\n",
      "evaluation/env_infos/final/reward_energy Std             0.021391\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00415611\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0892554\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.244925\n",
      "evaluation/env_infos/initial/reward_energy Std           0.228119\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0109452\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.806575\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0530278\n",
      "evaluation/env_infos/reward_energy Std                   0.0786623\n",
      "evaluation/env_infos/reward_energy Max                  -0.000603251\n",
      "evaluation/env_infos/reward_energy Min                  -0.806575\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0189222\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.204948\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.421139\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.488112\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00158072\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0117275\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0374542\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0343273\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0153829\n",
      "evaluation/env_infos/end_effector_loc Std                0.138587\n",
      "evaluation/env_infos/end_effector_loc Max                0.434081\n",
      "evaluation/env_infos/end_effector_loc Min               -0.488112\n",
      "time/data storing (s)                                    0.00591415\n",
      "time/evaluation sampling (s)                             0.942027\n",
      "time/exploration sampling (s)                            0.12481\n",
      "time/logging (s)                                         0.0189514\n",
      "time/saving (s)                                          0.0276381\n",
      "time/training (s)                                       49.2554\n",
      "time/epoch (s)                                          50.3748\n",
      "time/total (s)                                        3508.35\n",
      "Epoch                                                   70\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:47:59.672968 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 71 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00105194\n",
      "trainer/QF2 Loss                                         0.00118813\n",
      "trainer/Policy Loss                                      3.29066\n",
      "trainer/Q1 Predictions Mean                             -1.22228\n",
      "trainer/Q1 Predictions Std                               0.918679\n",
      "trainer/Q1 Predictions Max                               0.673504\n",
      "trainer/Q1 Predictions Min                              -3.56036\n",
      "trainer/Q2 Predictions Mean                             -1.23882\n",
      "trainer/Q2 Predictions Std                               0.924491\n",
      "trainer/Q2 Predictions Max                               0.673888\n",
      "trainer/Q2 Predictions Min                              -3.58614\n",
      "trainer/Q Targets Mean                                  -1.22133\n",
      "trainer/Q Targets Std                                    0.916095\n",
      "trainer/Q Targets Max                                    0.656484\n",
      "trainer/Q Targets Min                                   -3.56641\n",
      "trainer/Log Pis Mean                                     2.08877\n",
      "trainer/Log Pis Std                                      1.25761\n",
      "trainer/Log Pis Max                                      4.36889\n",
      "trainer/Log Pis Min                                     -2.41424\n",
      "trainer/Policy mu Mean                                  -0.0126979\n",
      "trainer/Policy mu Std                                    0.350848\n",
      "trainer/Policy mu Max                                    1.92226\n",
      "trainer/Policy mu Min                                   -2.31034\n",
      "trainer/Policy log std Mean                             -2.33852\n",
      "trainer/Policy log std Std                               0.62682\n",
      "trainer/Policy log std Max                              -0.209682\n",
      "trainer/Policy log std Min                              -3.29411\n",
      "trainer/Alpha                                            0.0244248\n",
      "trainer/Alpha Loss                                       0.329553\n",
      "exploration/num steps total                           8200\n",
      "exploration/num paths total                            410\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0477145\n",
      "exploration/Rewards Std                                  0.0661977\n",
      "exploration/Rewards Max                                  0.12666\n",
      "exploration/Rewards Min                                 -0.210022\n",
      "exploration/Returns Mean                                -0.95429\n",
      "exploration/Returns Std                                  1.07273\n",
      "exploration/Returns Max                                  0.640646\n",
      "exploration/Returns Min                                 -2.37701\n",
      "exploration/Actions Mean                                 0.0158785\n",
      "exploration/Actions Std                                  0.144265\n",
      "exploration/Actions Max                                  0.457846\n",
      "exploration/Actions Min                                 -0.870302\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.95429\n",
      "exploration/env_infos/final/reward_dist Mean             0.154384\n",
      "exploration/env_infos/final/reward_dist Std              0.144835\n",
      "exploration/env_infos/final/reward_dist Max              0.372057\n",
      "exploration/env_infos/final/reward_dist Min              5.45249e-23\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0264992\n",
      "exploration/env_infos/initial/reward_dist Std            0.0315581\n",
      "exploration/env_infos/initial/reward_dist Max            0.089135\n",
      "exploration/env_infos/initial/reward_dist Min            0.00477981\n",
      "exploration/env_infos/reward_dist Mean                   0.145667\n",
      "exploration/env_infos/reward_dist Std                    0.213958\n",
      "exploration/env_infos/reward_dist Max                    0.904871\n",
      "exploration/env_infos/reward_dist Min                    5.45249e-23\n",
      "exploration/env_infos/final/reward_energy Mean          -0.140016\n",
      "exploration/env_infos/final/reward_energy Std            0.0365297\n",
      "exploration/env_infos/final/reward_energy Max           -0.108621\n",
      "exploration/env_infos/final/reward_energy Min           -0.189611\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.342095\n",
      "exploration/env_infos/initial/reward_energy Std          0.289502\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0875669\n",
      "exploration/env_infos/initial/reward_energy Min         -0.899218\n",
      "exploration/env_infos/reward_energy Mean                -0.161832\n",
      "exploration/env_infos/reward_energy Std                  0.126251\n",
      "exploration/env_infos/reward_energy Max                 -0.0108688\n",
      "exploration/env_infos/reward_energy Min                 -0.899218\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.146976\n",
      "exploration/env_infos/final/end_effector_loc Std         0.212811\n",
      "exploration/env_infos/final/end_effector_loc Max         0.596162\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.150922\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00107815\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0158079\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0162154\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0435151\n",
      "exploration/env_infos/end_effector_loc Mean              0.0484854\n",
      "exploration/env_infos/end_effector_loc Std               0.151175\n",
      "exploration/env_infos/end_effector_loc Max               0.596162\n",
      "exploration/env_infos/end_effector_loc Min              -0.318036\n",
      "evaluation/num steps total                           72000\n",
      "evaluation/num paths total                            3600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0429823\n",
      "evaluation/Rewards Std                                   0.0758717\n",
      "evaluation/Rewards Max                                   0.134593\n",
      "evaluation/Rewards Min                                  -0.362035\n",
      "evaluation/Returns Mean                                 -0.859645\n",
      "evaluation/Returns Std                                   1.16623\n",
      "evaluation/Returns Max                                   1.92213\n",
      "evaluation/Returns Min                                  -3.14564\n",
      "evaluation/Actions Mean                                  0.0052421\n",
      "evaluation/Actions Std                                   0.0640518\n",
      "evaluation/Actions Max                                   0.504432\n",
      "evaluation/Actions Min                                  -0.745015\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.859645\n",
      "evaluation/env_infos/final/reward_dist Mean              0.163925\n",
      "evaluation/env_infos/final/reward_dist Std               0.241661\n",
      "evaluation/env_infos/final/reward_dist Max               0.898138\n",
      "evaluation/env_infos/final/reward_dist Min               7.71711e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00565724\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0095964\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0356544\n",
      "evaluation/env_infos/initial/reward_dist Min             9.52489e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.196571\n",
      "evaluation/env_infos/reward_dist Std                     0.273584\n",
      "evaluation/env_infos/reward_dist Max                     0.995964\n",
      "evaluation/env_infos/reward_dist Min                     7.71711e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0326406\n",
      "evaluation/env_infos/final/reward_energy Std             0.0317023\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00127948\n",
      "evaluation/env_infos/final/reward_energy Min            -0.207209\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.241362\n",
      "evaluation/env_infos/initial/reward_energy Std           0.203121\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0148206\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.761402\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0524787\n",
      "evaluation/env_infos/reward_energy Std                   0.0742039\n",
      "evaluation/env_infos/reward_energy Max                  -0.000579488\n",
      "evaluation/env_infos/reward_energy Min                  -0.761402\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0377808\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.230286\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.524489\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.469461\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00128018\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0110794\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0252216\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0372507\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00773606\n",
      "evaluation/env_infos/end_effector_loc Std                0.148162\n",
      "evaluation/env_infos/end_effector_loc Max                0.524489\n",
      "evaluation/env_infos/end_effector_loc Min               -0.469461\n",
      "time/data storing (s)                                    0.00615142\n",
      "time/evaluation sampling (s)                             0.968624\n",
      "time/exploration sampling (s)                            0.132655\n",
      "time/logging (s)                                         0.0202637\n",
      "time/saving (s)                                          0.0285706\n",
      "time/training (s)                                       50.8811\n",
      "time/epoch (s)                                          52.0374\n",
      "time/total (s)                                        3561.33\n",
      "Epoch                                                   71\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:48:52.952009 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 72 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00203907\n",
      "trainer/QF2 Loss                                         0.00110651\n",
      "trainer/Policy Loss                                      3.58704\n",
      "trainer/Q1 Predictions Mean                             -1.35396\n",
      "trainer/Q1 Predictions Std                               0.901364\n",
      "trainer/Q1 Predictions Max                               0.613782\n",
      "trainer/Q1 Predictions Min                              -3.6494\n",
      "trainer/Q2 Predictions Mean                             -1.35574\n",
      "trainer/Q2 Predictions Std                               0.8957\n",
      "trainer/Q2 Predictions Max                               0.604466\n",
      "trainer/Q2 Predictions Min                              -3.61608\n",
      "trainer/Q Targets Mean                                  -1.35517\n",
      "trainer/Q Targets Std                                    0.903811\n",
      "trainer/Q Targets Max                                    0.593633\n",
      "trainer/Q Targets Min                                   -3.63126\n",
      "trainer/Log Pis Mean                                     2.2694\n",
      "trainer/Log Pis Std                                      1.46753\n",
      "trainer/Log Pis Max                                      4.68375\n",
      "trainer/Log Pis Min                                     -3.89124\n",
      "trainer/Policy mu Mean                                  -0.0040709\n",
      "trainer/Policy mu Std                                    0.357316\n",
      "trainer/Policy mu Max                                    1.54054\n",
      "trainer/Policy mu Min                                   -1.50731\n",
      "trainer/Policy log std Mean                             -2.38166\n",
      "trainer/Policy log std Std                               0.684374\n",
      "trainer/Policy log std Max                              -0.373534\n",
      "trainer/Policy log std Min                              -3.3968\n",
      "trainer/Alpha                                            0.023129\n",
      "trainer/Alpha Loss                                       1.01499\n",
      "exploration/num steps total                           8300\n",
      "exploration/num paths total                            415\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0797587\n",
      "exploration/Rewards Std                                  0.0663624\n",
      "exploration/Rewards Max                                  0.0972\n",
      "exploration/Rewards Min                                 -0.308914\n",
      "exploration/Returns Mean                                -1.59517\n",
      "exploration/Returns Std                                  0.669224\n",
      "exploration/Returns Max                                 -0.41733\n",
      "exploration/Returns Min                                 -2.35501\n",
      "exploration/Actions Mean                                 0.00278791\n",
      "exploration/Actions Std                                  0.164992\n",
      "exploration/Actions Max                                  0.651813\n",
      "exploration/Actions Min                                 -0.626012\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.59517\n",
      "exploration/env_infos/final/reward_dist Mean             0.0157672\n",
      "exploration/env_infos/final/reward_dist Std              0.0219765\n",
      "exploration/env_infos/final/reward_dist Max              0.0593413\n",
      "exploration/env_infos/final/reward_dist Min              0.000543812\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000569659\n",
      "exploration/env_infos/initial/reward_dist Std            0.000938269\n",
      "exploration/env_infos/initial/reward_dist Max            0.00244454\n",
      "exploration/env_infos/initial/reward_dist Min            6.03688e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.159405\n",
      "exploration/env_infos/reward_dist Std                    0.230158\n",
      "exploration/env_infos/reward_dist Max                    0.968974\n",
      "exploration/env_infos/reward_dist Min                    6.03688e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.1575\n",
      "exploration/env_infos/final/reward_energy Std            0.0726524\n",
      "exploration/env_infos/final/reward_energy Max           -0.0678079\n",
      "exploration/env_infos/final/reward_energy Min           -0.288895\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.435846\n",
      "exploration/env_infos/initial/reward_energy Std          0.16088\n",
      "exploration/env_infos/initial/reward_energy Max         -0.188419\n",
      "exploration/env_infos/initial/reward_energy Min         -0.630787\n",
      "exploration/env_infos/reward_energy Mean                -0.187976\n",
      "exploration/env_infos/reward_energy Std                  0.138295\n",
      "exploration/env_infos/reward_energy Max                 -0.0143908\n",
      "exploration/env_infos/reward_energy Min                 -0.662879\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0427086\n",
      "exploration/env_infos/final/end_effector_loc Std         0.240704\n",
      "exploration/env_infos/final/end_effector_loc Max         0.432875\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.391139\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00777065\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0144714\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0161474\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0313006\n",
      "exploration/env_infos/end_effector_loc Mean             -0.042238\n",
      "exploration/env_infos/end_effector_loc Std               0.16094\n",
      "exploration/env_infos/end_effector_loc Max               0.432875\n",
      "exploration/env_infos/end_effector_loc Min              -0.391139\n",
      "evaluation/num steps total                           73000\n",
      "evaluation/num paths total                            3650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0434725\n",
      "evaluation/Rewards Std                                   0.0714693\n",
      "evaluation/Rewards Max                                   0.148178\n",
      "evaluation/Rewards Min                                  -0.376771\n",
      "evaluation/Returns Mean                                 -0.869449\n",
      "evaluation/Returns Std                                   1.04659\n",
      "evaluation/Returns Max                                   1.66349\n",
      "evaluation/Returns Min                                  -4.0235\n",
      "evaluation/Actions Mean                                  0.00258983\n",
      "evaluation/Actions Std                                   0.0627585\n",
      "evaluation/Actions Max                                   0.71219\n",
      "evaluation/Actions Min                                  -0.671002\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.869449\n",
      "evaluation/env_infos/final/reward_dist Mean              0.263271\n",
      "evaluation/env_infos/final/reward_dist Std               0.289422\n",
      "evaluation/env_infos/final/reward_dist Max               0.945987\n",
      "evaluation/env_infos/final/reward_dist Min               6.12488e-08\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00433166\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00882008\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0335203\n",
      "evaluation/env_infos/initial/reward_dist Min             3.06859e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.228942\n",
      "evaluation/env_infos/reward_dist Std                     0.281406\n",
      "evaluation/env_infos/reward_dist Max                     0.999293\n",
      "evaluation/env_infos/reward_dist Min                     6.12488e-08\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0356146\n",
      "evaluation/env_infos/final/reward_energy Std             0.0174326\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00150945\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0713217\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.231501\n",
      "evaluation/env_infos/initial/reward_energy Std           0.193734\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0237598\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.783714\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0524723\n",
      "evaluation/env_infos/reward_energy Std                   0.0716752\n",
      "evaluation/env_infos/reward_energy Max                  -0.000616586\n",
      "evaluation/env_infos/reward_energy Min                  -0.783714\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.018818\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23007\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.463597\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.427347\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000252688\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106697\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0356095\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0335501\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00553084\n",
      "evaluation/env_infos/end_effector_loc Std                0.150948\n",
      "evaluation/env_infos/end_effector_loc Max                0.463597\n",
      "evaluation/env_infos/end_effector_loc Min               -0.427347\n",
      "time/data storing (s)                                    0.0060765\n",
      "time/evaluation sampling (s)                             1.04938\n",
      "time/exploration sampling (s)                            0.119755\n",
      "time/logging (s)                                         0.0189535\n",
      "time/saving (s)                                          0.0273363\n",
      "time/training (s)                                       51.0993\n",
      "time/epoch (s)                                          52.3208\n",
      "time/total (s)                                        3614.61\n",
      "Epoch                                                   72\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:49:43.695251 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 73 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000806133\n",
      "trainer/QF2 Loss                                         0.000783424\n",
      "trainer/Policy Loss                                      3.22335\n",
      "trainer/Q1 Predictions Mean                             -1.23189\n",
      "trainer/Q1 Predictions Std                               0.828286\n",
      "trainer/Q1 Predictions Max                               0.405773\n",
      "trainer/Q1 Predictions Min                              -3.72635\n",
      "trainer/Q2 Predictions Mean                             -1.23478\n",
      "trainer/Q2 Predictions Std                               0.83192\n",
      "trainer/Q2 Predictions Max                               0.389598\n",
      "trainer/Q2 Predictions Min                              -3.71164\n",
      "trainer/Q Targets Mean                                  -1.24163\n",
      "trainer/Q Targets Std                                    0.833362\n",
      "trainer/Q Targets Max                                    0.393086\n",
      "trainer/Q Targets Min                                   -3.6925\n",
      "trainer/Log Pis Mean                                     2.0306\n",
      "trainer/Log Pis Std                                      1.50186\n",
      "trainer/Log Pis Max                                      4.40393\n",
      "trainer/Log Pis Min                                     -5.86245\n",
      "trainer/Policy mu Mean                                   0.0211938\n",
      "trainer/Policy mu Std                                    0.402302\n",
      "trainer/Policy mu Max                                    2.5229\n",
      "trainer/Policy mu Min                                   -1.8859\n",
      "trainer/Policy log std Mean                             -2.29462\n",
      "trainer/Policy log std Std                               0.673298\n",
      "trainer/Policy log std Max                              -0.17862\n",
      "trainer/Policy log std Min                              -3.31398\n",
      "trainer/Alpha                                            0.022204\n",
      "trainer/Alpha Loss                                       0.116514\n",
      "exploration/num steps total                           8400\n",
      "exploration/num paths total                            420\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.12934\n",
      "exploration/Rewards Std                                  0.0622869\n",
      "exploration/Rewards Max                                  0.0109897\n",
      "exploration/Rewards Min                                 -0.280624\n",
      "exploration/Returns Mean                                -2.5868\n",
      "exploration/Returns Std                                  0.972337\n",
      "exploration/Returns Max                                 -1.22575\n",
      "exploration/Returns Min                                 -4.03137\n",
      "exploration/Actions Mean                                 0.00952321\n",
      "exploration/Actions Std                                  0.123871\n",
      "exploration/Actions Max                                  0.369705\n",
      "exploration/Actions Min                                 -0.453797\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.5868\n",
      "exploration/env_infos/final/reward_dist Mean             0.0613293\n",
      "exploration/env_infos/final/reward_dist Std              0.119207\n",
      "exploration/env_infos/final/reward_dist Max              0.299712\n",
      "exploration/env_infos/final/reward_dist Min              1.51892e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0123453\n",
      "exploration/env_infos/initial/reward_dist Std            0.0243654\n",
      "exploration/env_infos/initial/reward_dist Max            0.0610753\n",
      "exploration/env_infos/initial/reward_dist Min            4.43703e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0883533\n",
      "exploration/env_infos/reward_dist Std                    0.153165\n",
      "exploration/env_infos/reward_dist Max                    0.624476\n",
      "exploration/env_infos/reward_dist Min                    1.51892e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135736\n",
      "exploration/env_infos/final/reward_energy Std            0.0862212\n",
      "exploration/env_infos/final/reward_energy Max           -0.075431\n",
      "exploration/env_infos/final/reward_energy Min           -0.305169\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.237579\n",
      "exploration/env_infos/initial/reward_energy Std          0.157601\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0288708\n",
      "exploration/env_infos/initial/reward_energy Min         -0.466267\n",
      "exploration/env_infos/reward_energy Mean                -0.143165\n",
      "exploration/env_infos/reward_energy Std                  0.101848\n",
      "exploration/env_infos/reward_energy Max                 -0.0177816\n",
      "exploration/env_infos/reward_energy Min                 -0.466267\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.106811\n",
      "exploration/env_infos/final/end_effector_loc Std         0.225585\n",
      "exploration/env_infos/final/end_effector_loc Max         0.446613\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.317589\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00243841\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0097804\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0105241\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0226899\n",
      "exploration/env_infos/end_effector_loc Mean              0.0331094\n",
      "exploration/env_infos/end_effector_loc Std               0.160288\n",
      "exploration/env_infos/end_effector_loc Max               0.446613\n",
      "exploration/env_infos/end_effector_loc Min              -0.379721\n",
      "evaluation/num steps total                           74000\n",
      "evaluation/num paths total                            3700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0292891\n",
      "evaluation/Rewards Std                                   0.0767467\n",
      "evaluation/Rewards Max                                   0.150599\n",
      "evaluation/Rewards Min                                  -0.403618\n",
      "evaluation/Returns Mean                                 -0.585782\n",
      "evaluation/Returns Std                                   1.12425\n",
      "evaluation/Returns Max                                   2.0678\n",
      "evaluation/Returns Min                                  -3.33476\n",
      "evaluation/Actions Mean                                  0.00571409\n",
      "evaluation/Actions Std                                   0.0770095\n",
      "evaluation/Actions Max                                   0.70886\n",
      "evaluation/Actions Min                                  -0.620107\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.585782\n",
      "evaluation/env_infos/final/reward_dist Mean              0.332346\n",
      "evaluation/env_infos/final/reward_dist Std               0.302437\n",
      "evaluation/env_infos/final/reward_dist Max               0.912312\n",
      "evaluation/env_infos/final/reward_dist Min               2.4528e-10\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00570299\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0115424\n",
      "evaluation/env_infos/initial/reward_dist Max             0.06256\n",
      "evaluation/env_infos/initial/reward_dist Min             1.71665e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.271971\n",
      "evaluation/env_infos/reward_dist Std                     0.305125\n",
      "evaluation/env_infos/reward_dist Max                     0.995979\n",
      "evaluation/env_infos/reward_dist Min                     2.4528e-10\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0656914\n",
      "evaluation/env_infos/final/reward_energy Std             0.0751436\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00703423\n",
      "evaluation/env_infos/final/reward_energy Min            -0.412681\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.29501\n",
      "evaluation/env_infos/initial/reward_energy Std           0.215771\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0242172\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.715126\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0653831\n",
      "evaluation/env_infos/reward_energy Std                   0.0874716\n",
      "evaluation/env_infos/reward_energy Max                  -0.00065501\n",
      "evaluation/env_infos/reward_energy Min                  -0.715126\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0533879\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.191203\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.519116\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.355925\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00119322\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0128671\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.035443\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0310054\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0241777\n",
      "evaluation/env_infos/end_effector_loc Std                0.140459\n",
      "evaluation/env_infos/end_effector_loc Max                0.519116\n",
      "evaluation/env_infos/end_effector_loc Min               -0.355925\n",
      "time/data storing (s)                                    0.00614967\n",
      "time/evaluation sampling (s)                             0.965363\n",
      "time/exploration sampling (s)                            0.123398\n",
      "time/logging (s)                                         0.0196946\n",
      "time/saving (s)                                          0.0274436\n",
      "time/training (s)                                       48.6433\n",
      "time/epoch (s)                                          49.7854\n",
      "time/total (s)                                        3665.35\n",
      "Epoch                                                   73\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:50:35.550715 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 74 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00249525\n",
      "trainer/QF2 Loss                                         0.00115505\n",
      "trainer/Policy Loss                                      3.23272\n",
      "trainer/Q1 Predictions Mean                             -1.21638\n",
      "trainer/Q1 Predictions Std                               0.778044\n",
      "trainer/Q1 Predictions Max                               0.297575\n",
      "trainer/Q1 Predictions Min                              -3.47568\n",
      "trainer/Q2 Predictions Mean                             -1.22741\n",
      "trainer/Q2 Predictions Std                               0.781095\n",
      "trainer/Q2 Predictions Max                               0.277683\n",
      "trainer/Q2 Predictions Min                              -3.49661\n",
      "trainer/Q Targets Mean                                  -1.22874\n",
      "trainer/Q Targets Std                                    0.78023\n",
      "trainer/Q Targets Max                                    0.274389\n",
      "trainer/Q Targets Min                                   -3.48379\n",
      "trainer/Log Pis Mean                                     2.03653\n",
      "trainer/Log Pis Std                                      1.3374\n",
      "trainer/Log Pis Max                                      4.81666\n",
      "trainer/Log Pis Min                                     -1.99818\n",
      "trainer/Policy mu Mean                                   0.0440313\n",
      "trainer/Policy mu Std                                    0.346454\n",
      "trainer/Policy mu Max                                    2.36416\n",
      "trainer/Policy mu Min                                   -1.44244\n",
      "trainer/Policy log std Mean                             -2.30928\n",
      "trainer/Policy log std Std                               0.568384\n",
      "trainer/Policy log std Max                              -0.56864\n",
      "trainer/Policy log std Min                              -3.35732\n",
      "trainer/Alpha                                            0.0230341\n",
      "trainer/Alpha Loss                                       0.137766\n",
      "exploration/num steps total                           8500\n",
      "exploration/num paths total                            425\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0569911\n",
      "exploration/Rewards Std                                  0.0806438\n",
      "exploration/Rewards Max                                  0.107956\n",
      "exploration/Rewards Min                                 -0.238819\n",
      "exploration/Returns Mean                                -1.13982\n",
      "exploration/Returns Std                                  1.21551\n",
      "exploration/Returns Max                                  0.460324\n",
      "exploration/Returns Min                                 -2.519\n",
      "exploration/Actions Mean                                -0.00525505\n",
      "exploration/Actions Std                                  0.199953\n",
      "exploration/Actions Max                                  0.596637\n",
      "exploration/Actions Min                                 -0.768763\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.13982\n",
      "exploration/env_infos/final/reward_dist Mean             0.329095\n",
      "exploration/env_infos/final/reward_dist Std              0.322878\n",
      "exploration/env_infos/final/reward_dist Max              0.861204\n",
      "exploration/env_infos/final/reward_dist Min              0.000495677\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000522519\n",
      "exploration/env_infos/initial/reward_dist Std            0.000770475\n",
      "exploration/env_infos/initial/reward_dist Max            0.00204766\n",
      "exploration/env_infos/initial/reward_dist Min            1.54387e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.237039\n",
      "exploration/env_infos/reward_dist Std                    0.32146\n",
      "exploration/env_infos/reward_dist Max                    0.957666\n",
      "exploration/env_infos/reward_dist Min                    1.21342e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.156382\n",
      "exploration/env_infos/final/reward_energy Std            0.0744943\n",
      "exploration/env_infos/final/reward_energy Max           -0.0557408\n",
      "exploration/env_infos/final/reward_energy Min           -0.263611\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.395558\n",
      "exploration/env_infos/initial/reward_energy Std          0.127964\n",
      "exploration/env_infos/initial/reward_energy Max         -0.192435\n",
      "exploration/env_infos/initial/reward_energy Min         -0.570217\n",
      "exploration/env_infos/reward_energy Mean                -0.235806\n",
      "exploration/env_infos/reward_energy Std                  0.156246\n",
      "exploration/env_infos/reward_energy Max                 -0.0207172\n",
      "exploration/env_infos/reward_energy Min                 -0.768836\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.082481\n",
      "exploration/env_infos/final/end_effector_loc Std         0.167306\n",
      "exploration/env_infos/final/end_effector_loc Max         0.322817\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.160154\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0099899\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0107821\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0279198\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00961659\n",
      "exploration/env_infos/end_effector_loc Mean              0.0761052\n",
      "exploration/env_infos/end_effector_loc Std               0.155549\n",
      "exploration/env_infos/end_effector_loc Max               0.444431\n",
      "exploration/env_infos/end_effector_loc Min              -0.167071\n",
      "evaluation/num steps total                           75000\n",
      "evaluation/num paths total                            3750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0423616\n",
      "evaluation/Rewards Std                                   0.0761716\n",
      "evaluation/Rewards Max                                   0.146965\n",
      "evaluation/Rewards Min                                  -0.513107\n",
      "evaluation/Returns Mean                                 -0.847232\n",
      "evaluation/Returns Std                                   1.15661\n",
      "evaluation/Returns Max                                   1.5479\n",
      "evaluation/Returns Min                                  -3.16723\n",
      "evaluation/Actions Mean                                  0.000129341\n",
      "evaluation/Actions Std                                   0.0708608\n",
      "evaluation/Actions Max                                   0.580463\n",
      "evaluation/Actions Min                                  -0.648864\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.847232\n",
      "evaluation/env_infos/final/reward_dist Mean              0.260324\n",
      "evaluation/env_infos/final/reward_dist Std               0.291887\n",
      "evaluation/env_infos/final/reward_dist Max               0.925933\n",
      "evaluation/env_infos/final/reward_dist Min               2.4377e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0080491\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0149861\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0625899\n",
      "evaluation/env_infos/initial/reward_dist Min             2.13698e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.223204\n",
      "evaluation/env_infos/reward_dist Std                     0.268199\n",
      "evaluation/env_infos/reward_dist Max                     0.990803\n",
      "evaluation/env_infos/reward_dist Min                     2.4377e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0311617\n",
      "evaluation/env_infos/final/reward_energy Std             0.0249822\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00198978\n",
      "evaluation/env_infos/final/reward_energy Min            -0.130694\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.277526\n",
      "evaluation/env_infos/initial/reward_energy Std           0.233553\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0119592\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.726338\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0531135\n",
      "evaluation/env_infos/reward_energy Std                   0.0849794\n",
      "evaluation/env_infos/reward_energy Max                  -0.000609295\n",
      "evaluation/env_infos/reward_energy Min                  -0.726338\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.030844\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.214154\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.567595\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.559294\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00132592\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127554\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0290232\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0324432\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.018532\n",
      "evaluation/env_infos/end_effector_loc Std                0.148326\n",
      "evaluation/env_infos/end_effector_loc Max                0.567595\n",
      "evaluation/env_infos/end_effector_loc Min               -0.559294\n",
      "time/data storing (s)                                    0.00612156\n",
      "time/evaluation sampling (s)                             0.975156\n",
      "time/exploration sampling (s)                            0.120781\n",
      "time/logging (s)                                         0.0201419\n",
      "time/saving (s)                                          0.0273629\n",
      "time/training (s)                                       49.7781\n",
      "time/epoch (s)                                          50.9277\n",
      "time/total (s)                                        3717.2\n",
      "Epoch                                                   74\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:51:26.834709 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 75 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000801156\n",
      "trainer/QF2 Loss                                         0.000679667\n",
      "trainer/Policy Loss                                      3.07374\n",
      "trainer/Q1 Predictions Mean                             -1.16483\n",
      "trainer/Q1 Predictions Std                               0.846426\n",
      "trainer/Q1 Predictions Max                               0.312031\n",
      "trainer/Q1 Predictions Min                              -3.57877\n",
      "trainer/Q2 Predictions Mean                             -1.15518\n",
      "trainer/Q2 Predictions Std                               0.843713\n",
      "trainer/Q2 Predictions Max                               0.319115\n",
      "trainer/Q2 Predictions Min                              -3.54158\n",
      "trainer/Q Targets Mean                                  -1.15649\n",
      "trainer/Q Targets Std                                    0.8429\n",
      "trainer/Q Targets Max                                    0.285059\n",
      "trainer/Q Targets Min                                   -3.5804\n",
      "trainer/Log Pis Mean                                     1.93247\n",
      "trainer/Log Pis Std                                      1.34371\n",
      "trainer/Log Pis Max                                      4.50373\n",
      "trainer/Log Pis Min                                     -2.90817\n",
      "trainer/Policy mu Mean                                   0.0390822\n",
      "trainer/Policy mu Std                                    0.311858\n",
      "trainer/Policy mu Max                                    2.27378\n",
      "trainer/Policy mu Min                                   -1.5767\n",
      "trainer/Policy log std Mean                             -2.32712\n",
      "trainer/Policy log std Std                               0.594704\n",
      "trainer/Policy log std Max                              -0.355377\n",
      "trainer/Policy log std Min                              -3.27906\n",
      "trainer/Alpha                                            0.0218766\n",
      "trainer/Alpha Loss                                      -0.258117\n",
      "exploration/num steps total                           8600\n",
      "exploration/num paths total                            430\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0359862\n",
      "exploration/Rewards Std                                  0.0751418\n",
      "exploration/Rewards Max                                  0.1234\n",
      "exploration/Rewards Min                                 -0.19682\n",
      "exploration/Returns Mean                                -0.719725\n",
      "exploration/Returns Std                                  1.24743\n",
      "exploration/Returns Max                                  1.3253\n",
      "exploration/Returns Min                                 -1.99298\n",
      "exploration/Actions Mean                                -0.00281195\n",
      "exploration/Actions Std                                  0.0981107\n",
      "exploration/Actions Max                                  0.360846\n",
      "exploration/Actions Min                                 -0.485509\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.719725\n",
      "exploration/env_infos/final/reward_dist Mean             0.0285022\n",
      "exploration/env_infos/final/reward_dist Std              0.0463461\n",
      "exploration/env_infos/final/reward_dist Max              0.120618\n",
      "exploration/env_infos/final/reward_dist Min              0.000348466\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00764244\n",
      "exploration/env_infos/initial/reward_dist Std            0.0062837\n",
      "exploration/env_infos/initial/reward_dist Max            0.0150581\n",
      "exploration/env_infos/initial/reward_dist Min            6.71052e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.210208\n",
      "exploration/env_infos/reward_dist Std                    0.253692\n",
      "exploration/env_infos/reward_dist Max                    0.904017\n",
      "exploration/env_infos/reward_dist Min                    6.71052e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.134822\n",
      "exploration/env_infos/final/reward_energy Std            0.079286\n",
      "exploration/env_infos/final/reward_energy Max           -0.0399667\n",
      "exploration/env_infos/final/reward_energy Min           -0.274274\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.242363\n",
      "exploration/env_infos/initial/reward_energy Std          0.160608\n",
      "exploration/env_infos/initial/reward_energy Max         -0.115736\n",
      "exploration/env_infos/initial/reward_energy Min         -0.54433\n",
      "exploration/env_infos/reward_energy Mean                -0.110381\n",
      "exploration/env_infos/reward_energy Std                  0.0841616\n",
      "exploration/env_infos/reward_energy Max                 -0.0027177\n",
      "exploration/env_infos/reward_energy Min                 -0.54433\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0883973\n",
      "exploration/env_infos/final/end_effector_loc Std         0.250657\n",
      "exploration/env_infos/final/end_effector_loc Max         0.293097\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.414758\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00444171\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00927035\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0132402\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0242755\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0417394\n",
      "exploration/env_infos/end_effector_loc Std               0.153266\n",
      "exploration/env_infos/end_effector_loc Max               0.303538\n",
      "exploration/env_infos/end_effector_loc Min              -0.414758\n",
      "evaluation/num steps total                           76000\n",
      "evaluation/num paths total                            3800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0283473\n",
      "evaluation/Rewards Std                                   0.0744621\n",
      "evaluation/Rewards Max                                   0.173437\n",
      "evaluation/Rewards Min                                  -0.449283\n",
      "evaluation/Returns Mean                                 -0.566946\n",
      "evaluation/Returns Std                                   1.06053\n",
      "evaluation/Returns Max                                   2.1372\n",
      "evaluation/Returns Min                                  -2.62536\n",
      "evaluation/Actions Mean                                  0.00387675\n",
      "evaluation/Actions Std                                   0.0744823\n",
      "evaluation/Actions Max                                   0.645614\n",
      "evaluation/Actions Min                                  -0.721518\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.566946\n",
      "evaluation/env_infos/final/reward_dist Mean              0.177107\n",
      "evaluation/env_infos/final/reward_dist Std               0.247138\n",
      "evaluation/env_infos/final/reward_dist Max               0.970728\n",
      "evaluation/env_infos/final/reward_dist Min               1.8316e-14\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00802419\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0151859\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0752677\n",
      "evaluation/env_infos/initial/reward_dist Min             1.90323e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.256436\n",
      "evaluation/env_infos/reward_dist Std                     0.301822\n",
      "evaluation/env_infos/reward_dist Max                     0.997697\n",
      "evaluation/env_infos/reward_dist Min                     1.8316e-14\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0436717\n",
      "evaluation/env_infos/final/reward_energy Std             0.0374292\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00319084\n",
      "evaluation/env_infos/final/reward_energy Min            -0.176448\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.285161\n",
      "evaluation/env_infos/initial/reward_energy Std           0.207109\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0195039\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.743322\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0623513\n",
      "evaluation/env_infos/reward_energy Std                   0.0850741\n",
      "evaluation/env_infos/reward_energy Max                  -0.00159326\n",
      "evaluation/env_infos/reward_energy Min                  -0.743322\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.010475\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.249449\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.638322\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.62457\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00188941\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123164\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0322807\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0360759\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00225352\n",
      "evaluation/env_infos/end_effector_loc Std                0.168793\n",
      "evaluation/env_infos/end_effector_loc Max                0.638322\n",
      "evaluation/env_infos/end_effector_loc Min               -0.62457\n",
      "time/data storing (s)                                    0.00648339\n",
      "time/evaluation sampling (s)                             0.962633\n",
      "time/exploration sampling (s)                            0.125045\n",
      "time/logging (s)                                         0.01887\n",
      "time/saving (s)                                          0.0315847\n",
      "time/training (s)                                       49.216\n",
      "time/epoch (s)                                          50.3606\n",
      "time/total (s)                                        3768.48\n",
      "Epoch                                                   75\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:52:18.307401 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 76 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00158635\n",
      "trainer/QF2 Loss                                         0.00254859\n",
      "trainer/Policy Loss                                      3.39167\n",
      "trainer/Q1 Predictions Mean                             -1.2074\n",
      "trainer/Q1 Predictions Std                               0.881948\n",
      "trainer/Q1 Predictions Max                               0.392986\n",
      "trainer/Q1 Predictions Min                              -3.47955\n",
      "trainer/Q2 Predictions Mean                             -1.21272\n",
      "trainer/Q2 Predictions Std                               0.885254\n",
      "trainer/Q2 Predictions Max                               0.364869\n",
      "trainer/Q2 Predictions Min                              -3.49519\n",
      "trainer/Q Targets Mean                                  -1.21095\n",
      "trainer/Q Targets Std                                    0.88516\n",
      "trainer/Q Targets Max                                    0.409387\n",
      "trainer/Q Targets Min                                   -3.49782\n",
      "trainer/Log Pis Mean                                     2.20797\n",
      "trainer/Log Pis Std                                      1.42314\n",
      "trainer/Log Pis Max                                      4.67562\n",
      "trainer/Log Pis Min                                     -3.15711\n",
      "trainer/Policy mu Mean                                   0.0537672\n",
      "trainer/Policy mu Std                                    0.389417\n",
      "trainer/Policy mu Max                                    2.4902\n",
      "trainer/Policy mu Min                                   -2.30952\n",
      "trainer/Policy log std Mean                             -2.36123\n",
      "trainer/Policy log std Std                               0.690802\n",
      "trainer/Policy log std Max                              -0.272028\n",
      "trainer/Policy log std Min                              -3.42283\n",
      "trainer/Alpha                                            0.0210984\n",
      "trainer/Alpha Loss                                       0.802562\n",
      "exploration/num steps total                           8700\n",
      "exploration/num paths total                            435\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.135896\n",
      "exploration/Rewards Std                                  0.0704115\n",
      "exploration/Rewards Max                                  0.0362389\n",
      "exploration/Rewards Min                                 -0.336301\n",
      "exploration/Returns Mean                                -2.71791\n",
      "exploration/Returns Std                                  0.942178\n",
      "exploration/Returns Max                                 -1.51146\n",
      "exploration/Returns Min                                 -3.95158\n",
      "exploration/Actions Mean                                 0.00829086\n",
      "exploration/Actions Std                                  0.167428\n",
      "exploration/Actions Max                                  0.911515\n",
      "exploration/Actions Min                                 -0.653444\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.71791\n",
      "exploration/env_infos/final/reward_dist Mean             0.0210574\n",
      "exploration/env_infos/final/reward_dist Std              0.0415459\n",
      "exploration/env_infos/final/reward_dist Max              0.104148\n",
      "exploration/env_infos/final/reward_dist Min              2.07124e-24\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0111181\n",
      "exploration/env_infos/initial/reward_dist Std            0.0150822\n",
      "exploration/env_infos/initial/reward_dist Max            0.0385141\n",
      "exploration/env_infos/initial/reward_dist Min            1.79304e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.141074\n",
      "exploration/env_infos/reward_dist Std                    0.25088\n",
      "exploration/env_infos/reward_dist Max                    0.991473\n",
      "exploration/env_infos/reward_dist Min                    7.50755e-25\n",
      "exploration/env_infos/final/reward_energy Mean          -0.236443\n",
      "exploration/env_infos/final/reward_energy Std            0.0752964\n",
      "exploration/env_infos/final/reward_energy Max           -0.119441\n",
      "exploration/env_infos/final/reward_energy Min           -0.351643\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.507234\n",
      "exploration/env_infos/initial/reward_energy Std          0.383537\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0731667\n",
      "exploration/env_infos/initial/reward_energy Min         -1.0532\n",
      "exploration/env_infos/reward_energy Mean                -0.175533\n",
      "exploration/env_infos/reward_energy Std                  0.159342\n",
      "exploration/env_infos/reward_energy Max                 -0.00983688\n",
      "exploration/env_infos/reward_energy Min                 -1.0532\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.064628\n",
      "exploration/env_infos/final/end_effector_loc Std         0.362832\n",
      "exploration/env_infos/final/end_effector_loc Max         0.822217\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.341633\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00219232\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0223758\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0455757\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0326722\n",
      "exploration/env_infos/end_effector_loc Mean              0.0140184\n",
      "exploration/env_infos/end_effector_loc Std               0.261301\n",
      "exploration/env_infos/end_effector_loc Max               0.825958\n",
      "exploration/env_infos/end_effector_loc Min              -0.44637\n",
      "evaluation/num steps total                           77000\n",
      "evaluation/num paths total                            3850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0388937\n",
      "evaluation/Rewards Std                                   0.0746453\n",
      "evaluation/Rewards Max                                   0.163021\n",
      "evaluation/Rewards Min                                  -0.523614\n",
      "evaluation/Returns Mean                                 -0.777875\n",
      "evaluation/Returns Std                                   1.08866\n",
      "evaluation/Returns Max                                   1.66865\n",
      "evaluation/Returns Min                                  -3.39181\n",
      "evaluation/Actions Mean                                  0.00263502\n",
      "evaluation/Actions Std                                   0.0658765\n",
      "evaluation/Actions Max                                   0.567238\n",
      "evaluation/Actions Min                                  -0.457359\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.777875\n",
      "evaluation/env_infos/final/reward_dist Mean              0.195894\n",
      "evaluation/env_infos/final/reward_dist Std               0.25966\n",
      "evaluation/env_infos/final/reward_dist Max               0.857648\n",
      "evaluation/env_infos/final/reward_dist Min               2.21638e-23\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00758687\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0125806\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0541804\n",
      "evaluation/env_infos/initial/reward_dist Min             2.21215e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.218387\n",
      "evaluation/env_infos/reward_dist Std                     0.275327\n",
      "evaluation/env_infos/reward_dist Max                     0.995904\n",
      "evaluation/env_infos/reward_dist Min                     2.21638e-23\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0465709\n",
      "evaluation/env_infos/final/reward_energy Std             0.0402913\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00349963\n",
      "evaluation/env_infos/final/reward_energy Min            -0.212943\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.252588\n",
      "evaluation/env_infos/initial/reward_energy Std           0.176537\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0212913\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.611768\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0586978\n",
      "evaluation/env_infos/reward_energy Std                   0.0724424\n",
      "evaluation/env_infos/reward_energy Max                  -0.000353679\n",
      "evaluation/env_infos/reward_energy Min                  -0.611768\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0235606\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.244576\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.790215\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.466934\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000774639\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0108677\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0283619\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.022868\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0088492\n",
      "evaluation/env_infos/end_effector_loc Std                0.164537\n",
      "evaluation/env_infos/end_effector_loc Max                0.790215\n",
      "evaluation/env_infos/end_effector_loc Min               -0.466934\n",
      "time/data storing (s)                                    0.00642746\n",
      "time/evaluation sampling (s)                             0.960538\n",
      "time/exploration sampling (s)                            0.129806\n",
      "time/logging (s)                                         0.0195395\n",
      "time/saving (s)                                          0.0304285\n",
      "time/training (s)                                       49.3392\n",
      "time/epoch (s)                                          50.4859\n",
      "time/total (s)                                        3819.96\n",
      "Epoch                                                   76\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:53:09.760689 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 77 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00115289\n",
      "trainer/QF2 Loss                                         0.00200745\n",
      "trainer/Policy Loss                                      3.20738\n",
      "trainer/Q1 Predictions Mean                             -1.28385\n",
      "trainer/Q1 Predictions Std                               0.819276\n",
      "trainer/Q1 Predictions Max                               0.326233\n",
      "trainer/Q1 Predictions Min                              -3.64869\n",
      "trainer/Q2 Predictions Mean                             -1.28508\n",
      "trainer/Q2 Predictions Std                               0.817298\n",
      "trainer/Q2 Predictions Max                               0.30126\n",
      "trainer/Q2 Predictions Min                              -3.64083\n",
      "trainer/Q Targets Mean                                  -1.28221\n",
      "trainer/Q Targets Std                                    0.823347\n",
      "trainer/Q Targets Max                                    0.333774\n",
      "trainer/Q Targets Min                                   -3.63985\n",
      "trainer/Log Pis Mean                                     1.95149\n",
      "trainer/Log Pis Std                                      1.40749\n",
      "trainer/Log Pis Max                                      4.80908\n",
      "trainer/Log Pis Min                                     -3.38247\n",
      "trainer/Policy mu Mean                                   0.0284362\n",
      "trainer/Policy mu Std                                    0.388496\n",
      "trainer/Policy mu Max                                    1.96102\n",
      "trainer/Policy mu Min                                   -1.4916\n",
      "trainer/Policy log std Mean                             -2.25909\n",
      "trainer/Policy log std Std                               0.630938\n",
      "trainer/Policy log std Max                              -0.36999\n",
      "trainer/Policy log std Min                              -3.38758\n",
      "trainer/Alpha                                            0.0200385\n",
      "trainer/Alpha Loss                                      -0.189686\n",
      "exploration/num steps total                           8800\n",
      "exploration/num paths total                            440\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0188355\n",
      "exploration/Rewards Std                                  0.0763507\n",
      "exploration/Rewards Max                                  0.10285\n",
      "exploration/Rewards Min                                 -0.38708\n",
      "exploration/Returns Mean                                -0.37671\n",
      "exploration/Returns Std                                  0.613506\n",
      "exploration/Returns Max                                  0.715167\n",
      "exploration/Returns Min                                 -1.1182\n",
      "exploration/Actions Mean                                 0.00566794\n",
      "exploration/Actions Std                                  0.0972567\n",
      "exploration/Actions Max                                  0.327327\n",
      "exploration/Actions Min                                 -0.447285\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.37671\n",
      "exploration/env_infos/final/reward_dist Mean             0.121282\n",
      "exploration/env_infos/final/reward_dist Std              0.239286\n",
      "exploration/env_infos/final/reward_dist Max              0.599847\n",
      "exploration/env_infos/final/reward_dist Min              1.69928e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.007287\n",
      "exploration/env_infos/initial/reward_dist Std            0.00734989\n",
      "exploration/env_infos/initial/reward_dist Max            0.0197745\n",
      "exploration/env_infos/initial/reward_dist Min            9.95028e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.258218\n",
      "exploration/env_infos/reward_dist Std                    0.284331\n",
      "exploration/env_infos/reward_dist Max                    0.919488\n",
      "exploration/env_infos/reward_dist Min                    9.95028e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.130184\n",
      "exploration/env_infos/final/reward_energy Std            0.0287176\n",
      "exploration/env_infos/final/reward_energy Max           -0.0841441\n",
      "exploration/env_infos/final/reward_energy Min           -0.162095\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.270444\n",
      "exploration/env_infos/initial/reward_energy Std          0.14\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0515584\n",
      "exploration/env_infos/initial/reward_energy Min         -0.480843\n",
      "exploration/env_infos/reward_energy Mean                -0.105892\n",
      "exploration/env_infos/reward_energy Std                  0.0881408\n",
      "exploration/env_infos/reward_energy Max                 -0.00580219\n",
      "exploration/env_infos/reward_energy Min                 -0.480843\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0402834\n",
      "exploration/env_infos/final/end_effector_loc Std         0.290418\n",
      "exploration/env_infos/final/end_effector_loc Max         0.441504\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.412144\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000227859\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0107644\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0123837\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0223642\n",
      "exploration/env_infos/end_effector_loc Mean              0.0216795\n",
      "exploration/env_infos/end_effector_loc Std               0.187341\n",
      "exploration/env_infos/end_effector_loc Max               0.441504\n",
      "exploration/env_infos/end_effector_loc Min              -0.412144\n",
      "evaluation/num steps total                           78000\n",
      "evaluation/num paths total                            3900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0553315\n",
      "evaluation/Rewards Std                                   0.0714524\n",
      "evaluation/Rewards Max                                   0.116271\n",
      "evaluation/Rewards Min                                  -0.396638\n",
      "evaluation/Returns Mean                                 -1.10663\n",
      "evaluation/Returns Std                                   0.928168\n",
      "evaluation/Returns Max                                   0.961958\n",
      "evaluation/Returns Min                                  -3.27174\n",
      "evaluation/Actions Mean                                  0.00286731\n",
      "evaluation/Actions Std                                   0.078204\n",
      "evaluation/Actions Max                                   0.508293\n",
      "evaluation/Actions Min                                  -0.774223\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.10663\n",
      "evaluation/env_infos/final/reward_dist Mean              0.122445\n",
      "evaluation/env_infos/final/reward_dist Std               0.211365\n",
      "evaluation/env_infos/final/reward_dist Max               0.926485\n",
      "evaluation/env_infos/final/reward_dist Min               1.33392e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0103561\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0120886\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0496234\n",
      "evaluation/env_infos/initial/reward_dist Min             1.40896e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.201157\n",
      "evaluation/env_infos/reward_dist Std                     0.25238\n",
      "evaluation/env_infos/reward_dist Max                     0.996598\n",
      "evaluation/env_infos/reward_dist Min                     1.33392e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0383613\n",
      "evaluation/env_infos/final/reward_energy Std             0.0281116\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00727415\n",
      "evaluation/env_infos/final/reward_energy Min            -0.126658\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.301877\n",
      "evaluation/env_infos/initial/reward_energy Std           0.246128\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0173254\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.896623\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0611916\n",
      "evaluation/env_infos/reward_energy Std                   0.0922158\n",
      "evaluation/env_infos/reward_energy Max                  -0.0015499\n",
      "evaluation/env_infos/reward_energy Min                  -0.896623\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0103918\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.269145\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.58969\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.653243\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00422437\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0131069\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0254146\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0387112\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0185867\n",
      "evaluation/env_infos/end_effector_loc Std                0.17482\n",
      "evaluation/env_infos/end_effector_loc Max                0.58969\n",
      "evaluation/env_infos/end_effector_loc Min               -0.653243\n",
      "time/data storing (s)                                    0.00635227\n",
      "time/evaluation sampling (s)                             1.05053\n",
      "time/exploration sampling (s)                            0.122257\n",
      "time/logging (s)                                         0.0197882\n",
      "time/saving (s)                                          0.0298267\n",
      "time/training (s)                                       49.2371\n",
      "time/epoch (s)                                          50.4658\n",
      "time/total (s)                                        3871.41\n",
      "Epoch                                                   77\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:54:01.305961 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 78 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00112282\n",
      "trainer/QF2 Loss                                         0.00102611\n",
      "trainer/Policy Loss                                      3.10641\n",
      "trainer/Q1 Predictions Mean                             -1.23407\n",
      "trainer/Q1 Predictions Std                               0.934416\n",
      "trainer/Q1 Predictions Max                               0.584456\n",
      "trainer/Q1 Predictions Min                              -4.15263\n",
      "trainer/Q2 Predictions Mean                             -1.22129\n",
      "trainer/Q2 Predictions Std                               0.927774\n",
      "trainer/Q2 Predictions Max                               0.701715\n",
      "trainer/Q2 Predictions Min                              -4.12529\n",
      "trainer/Q Targets Mean                                  -1.22801\n",
      "trainer/Q Targets Std                                    0.920335\n",
      "trainer/Q Targets Max                                    0.517513\n",
      "trainer/Q Targets Min                                   -4.10349\n",
      "trainer/Log Pis Mean                                     1.92057\n",
      "trainer/Log Pis Std                                      1.48581\n",
      "trainer/Log Pis Max                                      4.82259\n",
      "trainer/Log Pis Min                                     -3.93736\n",
      "trainer/Policy mu Mean                                   0.0280391\n",
      "trainer/Policy mu Std                                    0.4038\n",
      "trainer/Policy mu Max                                    1.99695\n",
      "trainer/Policy mu Min                                   -2.12258\n",
      "trainer/Policy log std Mean                             -2.22829\n",
      "trainer/Policy log std Std                               0.670325\n",
      "trainer/Policy log std Max                              -0.571095\n",
      "trainer/Policy log std Min                              -3.4102\n",
      "trainer/Alpha                                            0.0210596\n",
      "trainer/Alpha Loss                                      -0.306638\n",
      "exploration/num steps total                           8900\n",
      "exploration/num paths total                            445\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.007041\n",
      "exploration/Rewards Std                                  0.0925463\n",
      "exploration/Rewards Max                                  0.158804\n",
      "exploration/Rewards Min                                 -0.227081\n",
      "exploration/Returns Mean                                -0.14082\n",
      "exploration/Returns Std                                  1.49489\n",
      "exploration/Returns Max                                  1.64165\n",
      "exploration/Returns Min                                 -2.08335\n",
      "exploration/Actions Mean                                 0.00321455\n",
      "exploration/Actions Std                                  0.158992\n",
      "exploration/Actions Max                                  0.876447\n",
      "exploration/Actions Min                                 -0.582274\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.14082\n",
      "exploration/env_infos/final/reward_dist Mean             0.062752\n",
      "exploration/env_infos/final/reward_dist Std              0.096094\n",
      "exploration/env_infos/final/reward_dist Max              0.252441\n",
      "exploration/env_infos/final/reward_dist Min              7.84959e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00734006\n",
      "exploration/env_infos/initial/reward_dist Std            0.00758424\n",
      "exploration/env_infos/initial/reward_dist Max            0.0167297\n",
      "exploration/env_infos/initial/reward_dist Min            7.83941e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.297249\n",
      "exploration/env_infos/reward_dist Std                    0.314557\n",
      "exploration/env_infos/reward_dist Max                    0.998196\n",
      "exploration/env_infos/reward_dist Min                    7.83941e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.219606\n",
      "exploration/env_infos/final/reward_energy Std            0.185214\n",
      "exploration/env_infos/final/reward_energy Max           -0.0211485\n",
      "exploration/env_infos/final/reward_energy Min           -0.538784\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.293736\n",
      "exploration/env_infos/initial/reward_energy Std          0.182159\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0912179\n",
      "exploration/env_infos/initial/reward_energy Min         -0.628851\n",
      "exploration/env_infos/reward_energy Mean                -0.166147\n",
      "exploration/env_infos/reward_energy Std                  0.151568\n",
      "exploration/env_infos/reward_energy Max                 -0.00294685\n",
      "exploration/env_infos/reward_energy Min                 -0.881474\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0556377\n",
      "exploration/env_infos/final/end_effector_loc Std         0.248346\n",
      "exploration/env_infos/final/end_effector_loc Max         0.255628\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.419011\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00349619\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0117092\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0123665\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0291137\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0332648\n",
      "exploration/env_infos/end_effector_loc Std               0.165968\n",
      "exploration/env_infos/end_effector_loc Max               0.255628\n",
      "exploration/env_infos/end_effector_loc Min              -0.419011\n",
      "evaluation/num steps total                           79000\n",
      "evaluation/num paths total                            3950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0448703\n",
      "evaluation/Rewards Std                                   0.0715749\n",
      "evaluation/Rewards Max                                   0.124368\n",
      "evaluation/Rewards Min                                  -0.308353\n",
      "evaluation/Returns Mean                                 -0.897405\n",
      "evaluation/Returns Std                                   0.998748\n",
      "evaluation/Returns Max                                   1.27352\n",
      "evaluation/Returns Min                                  -2.76986\n",
      "evaluation/Actions Mean                                  0.0014345\n",
      "evaluation/Actions Std                                   0.0641545\n",
      "evaluation/Actions Max                                   0.764892\n",
      "evaluation/Actions Min                                  -0.555003\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.897405\n",
      "evaluation/env_infos/final/reward_dist Mean              0.179576\n",
      "evaluation/env_infos/final/reward_dist Std               0.238684\n",
      "evaluation/env_infos/final/reward_dist Max               0.87216\n",
      "evaluation/env_infos/final/reward_dist Min               1.3409e-13\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00650019\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0123906\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0563801\n",
      "evaluation/env_infos/initial/reward_dist Min             1.32567e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.198536\n",
      "evaluation/env_infos/reward_dist Std                     0.265776\n",
      "evaluation/env_infos/reward_dist Max                     0.997769\n",
      "evaluation/env_infos/reward_dist Min                     1.3409e-13\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0329318\n",
      "evaluation/env_infos/final/reward_energy Std             0.0201254\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00359043\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0740779\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.239874\n",
      "evaluation/env_infos/initial/reward_energy Std           0.201862\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0187697\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.838223\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0532128\n",
      "evaluation/env_infos/reward_energy Std                   0.0735126\n",
      "evaluation/env_infos/reward_energy Max                  -0.000403096\n",
      "evaluation/env_infos/reward_energy Min                  -0.838223\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0121983\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.239932\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.466572\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.54997\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000641858\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0110656\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0382446\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0277502\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00544337\n",
      "evaluation/env_infos/end_effector_loc Std                0.157971\n",
      "evaluation/env_infos/end_effector_loc Max                0.466572\n",
      "evaluation/env_infos/end_effector_loc Min               -0.54997\n",
      "time/data storing (s)                                    0.00614756\n",
      "time/evaluation sampling (s)                             0.958045\n",
      "time/exploration sampling (s)                            0.122614\n",
      "time/logging (s)                                         0.0188266\n",
      "time/saving (s)                                          0.0273797\n",
      "time/training (s)                                       49.3651\n",
      "time/epoch (s)                                          50.4981\n",
      "time/total (s)                                        3922.95\n",
      "Epoch                                                   78\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:54:52.788493 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 79 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00108862\n",
      "trainer/QF2 Loss                                         0.00132941\n",
      "trainer/Policy Loss                                      3.12971\n",
      "trainer/Q1 Predictions Mean                             -1.15366\n",
      "trainer/Q1 Predictions Std                               0.836161\n",
      "trainer/Q1 Predictions Max                               0.707018\n",
      "trainer/Q1 Predictions Min                              -3.53441\n",
      "trainer/Q2 Predictions Mean                             -1.15915\n",
      "trainer/Q2 Predictions Std                               0.837803\n",
      "trainer/Q2 Predictions Max                               0.700341\n",
      "trainer/Q2 Predictions Min                              -3.57062\n",
      "trainer/Q Targets Mean                                  -1.16347\n",
      "trainer/Q Targets Std                                    0.839902\n",
      "trainer/Q Targets Max                                    0.683447\n",
      "trainer/Q Targets Min                                   -3.61218\n",
      "trainer/Log Pis Mean                                     2.01827\n",
      "trainer/Log Pis Std                                      1.28416\n",
      "trainer/Log Pis Max                                      4.43253\n",
      "trainer/Log Pis Min                                     -2.30807\n",
      "trainer/Policy mu Mean                                   0.0929125\n",
      "trainer/Policy mu Std                                    0.438768\n",
      "trainer/Policy mu Max                                    2.00255\n",
      "trainer/Policy mu Min                                   -2.17196\n",
      "trainer/Policy log std Mean                             -2.2406\n",
      "trainer/Policy log std Std                               0.657478\n",
      "trainer/Policy log std Max                              -0.400571\n",
      "trainer/Policy log std Min                              -3.32607\n",
      "trainer/Alpha                                            0.021302\n",
      "trainer/Alpha Loss                                       0.0703174\n",
      "exploration/num steps total                           9000\n",
      "exploration/num paths total                            450\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0894132\n",
      "exploration/Rewards Std                                  0.0464199\n",
      "exploration/Rewards Max                                  0.0138742\n",
      "exploration/Rewards Min                                 -0.226194\n",
      "exploration/Returns Mean                                -1.78826\n",
      "exploration/Returns Std                                  0.450297\n",
      "exploration/Returns Max                                 -1.08859\n",
      "exploration/Returns Min                                 -2.3649\n",
      "exploration/Actions Mean                                -0.000422304\n",
      "exploration/Actions Std                                  0.0786785\n",
      "exploration/Actions Max                                  0.346699\n",
      "exploration/Actions Min                                 -0.321792\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.78826\n",
      "exploration/env_infos/final/reward_dist Mean             0.248766\n",
      "exploration/env_infos/final/reward_dist Std              0.325618\n",
      "exploration/env_infos/final/reward_dist Max              0.809971\n",
      "exploration/env_infos/final/reward_dist Min              1.37719e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00114747\n",
      "exploration/env_infos/initial/reward_dist Std            0.000860833\n",
      "exploration/env_infos/initial/reward_dist Max            0.00228366\n",
      "exploration/env_infos/initial/reward_dist Min            2.94566e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.229676\n",
      "exploration/env_infos/reward_dist Std                    0.314539\n",
      "exploration/env_infos/reward_dist Max                    0.964968\n",
      "exploration/env_infos/reward_dist Min                    1.37719e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0996253\n",
      "exploration/env_infos/final/reward_energy Std            0.0671409\n",
      "exploration/env_infos/final/reward_energy Max           -0.0100884\n",
      "exploration/env_infos/final/reward_energy Min           -0.193603\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.168563\n",
      "exploration/env_infos/initial/reward_energy Std          0.123227\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0296055\n",
      "exploration/env_infos/initial/reward_energy Min         -0.361976\n",
      "exploration/env_infos/reward_energy Mean                -0.0913038\n",
      "exploration/env_infos/reward_energy Std                  0.0635972\n",
      "exploration/env_infos/reward_energy Max                 -0.00596792\n",
      "exploration/env_infos/reward_energy Min                 -0.361976\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0501825\n",
      "exploration/env_infos/final/end_effector_loc Std         0.264837\n",
      "exploration/env_infos/final/end_effector_loc Max         0.513534\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.378301\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00250194\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00694538\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0173349\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0110001\n",
      "exploration/env_infos/end_effector_loc Mean              0.0364171\n",
      "exploration/env_infos/end_effector_loc Std               0.153491\n",
      "exploration/env_infos/end_effector_loc Max               0.513534\n",
      "exploration/env_infos/end_effector_loc Min              -0.378301\n",
      "evaluation/num steps total                           80000\n",
      "evaluation/num paths total                            4000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0457434\n",
      "evaluation/Rewards Std                                   0.0698893\n",
      "evaluation/Rewards Max                                   0.172115\n",
      "evaluation/Rewards Min                                  -0.436871\n",
      "evaluation/Returns Mean                                 -0.914867\n",
      "evaluation/Returns Std                                   0.995065\n",
      "evaluation/Returns Max                                   1.79228\n",
      "evaluation/Returns Min                                  -3.40989\n",
      "evaluation/Actions Mean                                 -0.00441688\n",
      "evaluation/Actions Std                                   0.0751083\n",
      "evaluation/Actions Max                                   0.853372\n",
      "evaluation/Actions Min                                  -0.857846\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.914867\n",
      "evaluation/env_infos/final/reward_dist Mean              0.13984\n",
      "evaluation/env_infos/final/reward_dist Std               0.218677\n",
      "evaluation/env_infos/final/reward_dist Max               0.872702\n",
      "evaluation/env_infos/final/reward_dist Min               1.02766e-13\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00929417\n",
      "evaluation/env_infos/initial/reward_dist Std             0.015164\n",
      "evaluation/env_infos/initial/reward_dist Max             0.059225\n",
      "evaluation/env_infos/initial/reward_dist Min             1.06797e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.208254\n",
      "evaluation/env_infos/reward_dist Std                     0.275031\n",
      "evaluation/env_infos/reward_dist Max                     0.99519\n",
      "evaluation/env_infos/reward_dist Min                     1.02766e-13\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0311672\n",
      "evaluation/env_infos/final/reward_energy Std             0.022733\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0055508\n",
      "evaluation/env_infos/final/reward_energy Min            -0.102023\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.289939\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255276\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0127539\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.886644\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0570468\n",
      "evaluation/env_infos/reward_energy Std                   0.0898176\n",
      "evaluation/env_infos/reward_energy Max                  -0.00200308\n",
      "evaluation/env_infos/reward_energy Min                  -0.886644\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0658771\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.22932\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.408648\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.583764\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000638074\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.013643\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0426686\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0428923\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.028706\n",
      "evaluation/env_infos/end_effector_loc Std                0.153348\n",
      "evaluation/env_infos/end_effector_loc Max                0.408648\n",
      "evaluation/env_infos/end_effector_loc Min               -0.583764\n",
      "time/data storing (s)                                    0.00647429\n",
      "time/evaluation sampling (s)                             0.958433\n",
      "time/exploration sampling (s)                            0.120616\n",
      "time/logging (s)                                         0.0197746\n",
      "time/saving (s)                                          0.0276202\n",
      "time/training (s)                                       49.38\n",
      "time/epoch (s)                                          50.5129\n",
      "time/total (s)                                        3974.43\n",
      "Epoch                                                   79\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:55:44.640218 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 80 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00118497\n",
      "trainer/QF2 Loss                                         0.000769386\n",
      "trainer/Policy Loss                                      3.16145\n",
      "trainer/Q1 Predictions Mean                             -1.25917\n",
      "trainer/Q1 Predictions Std                               0.88522\n",
      "trainer/Q1 Predictions Max                               0.482569\n",
      "trainer/Q1 Predictions Min                              -3.70019\n",
      "trainer/Q2 Predictions Mean                             -1.25554\n",
      "trainer/Q2 Predictions Std                               0.88542\n",
      "trainer/Q2 Predictions Max                               0.482087\n",
      "trainer/Q2 Predictions Min                              -3.67869\n",
      "trainer/Q Targets Mean                                  -1.24843\n",
      "trainer/Q Targets Std                                    0.881595\n",
      "trainer/Q Targets Max                                    0.450391\n",
      "trainer/Q Targets Min                                   -3.70701\n",
      "trainer/Log Pis Mean                                     1.95027\n",
      "trainer/Log Pis Std                                      1.34122\n",
      "trainer/Log Pis Max                                      4.51569\n",
      "trainer/Log Pis Min                                     -2.40281\n",
      "trainer/Policy mu Mean                                   0.0384644\n",
      "trainer/Policy mu Std                                    0.379984\n",
      "trainer/Policy mu Max                                    1.96366\n",
      "trainer/Policy mu Min                                   -2.03092\n",
      "trainer/Policy log std Mean                             -2.27422\n",
      "trainer/Policy log std Std                               0.62086\n",
      "trainer/Policy log std Max                              -0.279904\n",
      "trainer/Policy log std Min                              -3.25952\n",
      "trainer/Alpha                                            0.0221582\n",
      "trainer/Alpha Loss                                      -0.18943\n",
      "exploration/num steps total                           9100\n",
      "exploration/num paths total                            455\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0987029\n",
      "exploration/Rewards Std                                  0.0712281\n",
      "exploration/Rewards Max                                  0.0572412\n",
      "exploration/Rewards Min                                 -0.329928\n",
      "exploration/Returns Mean                                -1.97406\n",
      "exploration/Returns Std                                  0.883105\n",
      "exploration/Returns Max                                 -1.07695\n",
      "exploration/Returns Min                                 -3.17607\n",
      "exploration/Actions Mean                                 0.00105555\n",
      "exploration/Actions Std                                  0.0811975\n",
      "exploration/Actions Max                                  0.312887\n",
      "exploration/Actions Min                                 -0.303607\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.97406\n",
      "exploration/env_infos/final/reward_dist Mean             0.0806738\n",
      "exploration/env_infos/final/reward_dist Std              0.102967\n",
      "exploration/env_infos/final/reward_dist Max              0.27226\n",
      "exploration/env_infos/final/reward_dist Min              1.45682e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000883761\n",
      "exploration/env_infos/initial/reward_dist Std            0.000982118\n",
      "exploration/env_infos/initial/reward_dist Max            0.00231436\n",
      "exploration/env_infos/initial/reward_dist Min            1.07781e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.107962\n",
      "exploration/env_infos/reward_dist Std                    0.188743\n",
      "exploration/env_infos/reward_dist Max                    0.848193\n",
      "exploration/env_infos/reward_dist Min                    1.07781e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0909446\n",
      "exploration/env_infos/final/reward_energy Std            0.0365556\n",
      "exploration/env_infos/final/reward_energy Max           -0.0362712\n",
      "exploration/env_infos/final/reward_energy Min           -0.123689\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.178512\n",
      "exploration/env_infos/initial/reward_energy Std          0.122834\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0515218\n",
      "exploration/env_infos/initial/reward_energy Min         -0.336479\n",
      "exploration/env_infos/reward_energy Mean                -0.0953092\n",
      "exploration/env_infos/reward_energy Std                  0.0640659\n",
      "exploration/env_infos/reward_energy Max                 -0.00773428\n",
      "exploration/env_infos/reward_energy Min                 -0.336479\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0133222\n",
      "exploration/env_infos/final/end_effector_loc Std         0.22525\n",
      "exploration/env_infos/final/end_effector_loc Max         0.330169\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.359379\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00104777\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00758919\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0156444\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0151803\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00122511\n",
      "exploration/env_infos/end_effector_loc Std               0.142129\n",
      "exploration/env_infos/end_effector_loc Max               0.330169\n",
      "exploration/env_infos/end_effector_loc Min              -0.359379\n",
      "evaluation/num steps total                           81000\n",
      "evaluation/num paths total                            4050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0399715\n",
      "evaluation/Rewards Std                                   0.0843433\n",
      "evaluation/Rewards Max                                   0.162101\n",
      "evaluation/Rewards Min                                  -0.416127\n",
      "evaluation/Returns Mean                                 -0.79943\n",
      "evaluation/Returns Std                                   1.3376\n",
      "evaluation/Returns Max                                   1.72343\n",
      "evaluation/Returns Min                                  -3.43488\n",
      "evaluation/Actions Mean                                  0.0011736\n",
      "evaluation/Actions Std                                   0.078501\n",
      "evaluation/Actions Max                                   0.788552\n",
      "evaluation/Actions Min                                  -0.734922\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.79943\n",
      "evaluation/env_infos/final/reward_dist Mean              0.278374\n",
      "evaluation/env_infos/final/reward_dist Std               0.291154\n",
      "evaluation/env_infos/final/reward_dist Max               0.895931\n",
      "evaluation/env_infos/final/reward_dist Min               1.84815e-31\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00794191\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0138255\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0740953\n",
      "evaluation/env_infos/initial/reward_dist Min             4.55178e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.306087\n",
      "evaluation/env_infos/reward_dist Std                     0.319191\n",
      "evaluation/env_infos/reward_dist Max                     0.999754\n",
      "evaluation/env_infos/reward_dist Min                     1.84815e-31\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.04376\n",
      "evaluation/env_infos/final/reward_energy Std             0.0416648\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00431031\n",
      "evaluation/env_infos/final/reward_energy Min            -0.228632\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.300562\n",
      "evaluation/env_infos/initial/reward_energy Std           0.252266\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00605428\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.997225\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0623022\n",
      "evaluation/env_infos/reward_energy Std                   0.0919022\n",
      "evaluation/env_infos/reward_energy Max                  -0.000925155\n",
      "evaluation/env_infos/reward_energy Min                  -0.997225\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00109292\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.223106\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.781436\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.507554\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000961411\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.01384\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0394276\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0367461\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00458159\n",
      "evaluation/env_infos/end_effector_loc Std                0.154348\n",
      "evaluation/env_infos/end_effector_loc Max                0.781436\n",
      "evaluation/env_infos/end_effector_loc Min               -0.507554\n",
      "time/data storing (s)                                    0.00637663\n",
      "time/evaluation sampling (s)                             0.944802\n",
      "time/exploration sampling (s)                            0.121846\n",
      "time/logging (s)                                         0.0193146\n",
      "time/saving (s)                                          0.0263919\n",
      "time/training (s)                                       49.6716\n",
      "time/epoch (s)                                          50.7903\n",
      "time/total (s)                                        4026.28\n",
      "Epoch                                                   80\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:56:36.451613 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 81 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00123253\n",
      "trainer/QF2 Loss                                         0.000957539\n",
      "trainer/Policy Loss                                      3.0359\n",
      "trainer/Q1 Predictions Mean                             -1.12634\n",
      "trainer/Q1 Predictions Std                               0.844112\n",
      "trainer/Q1 Predictions Max                               0.885433\n",
      "trainer/Q1 Predictions Min                              -3.59039\n",
      "trainer/Q2 Predictions Mean                             -1.12369\n",
      "trainer/Q2 Predictions Std                               0.84137\n",
      "trainer/Q2 Predictions Max                               0.85952\n",
      "trainer/Q2 Predictions Min                              -3.61817\n",
      "trainer/Q Targets Mean                                  -1.11833\n",
      "trainer/Q Targets Std                                    0.844478\n",
      "trainer/Q Targets Max                                    0.905784\n",
      "trainer/Q Targets Min                                   -3.63388\n",
      "trainer/Log Pis Mean                                     1.94734\n",
      "trainer/Log Pis Std                                      1.44072\n",
      "trainer/Log Pis Max                                      4.61727\n",
      "trainer/Log Pis Min                                     -4.56838\n",
      "trainer/Policy mu Mean                                   0.0421179\n",
      "trainer/Policy mu Std                                    0.415029\n",
      "trainer/Policy mu Max                                    1.81251\n",
      "trainer/Policy mu Min                                   -1.88923\n",
      "trainer/Policy log std Mean                             -2.23508\n",
      "trainer/Policy log std Std                               0.665985\n",
      "trainer/Policy log std Max                              -0.399036\n",
      "trainer/Policy log std Min                              -3.28378\n",
      "trainer/Alpha                                            0.0223862\n",
      "trainer/Alpha Loss                                      -0.199996\n",
      "exploration/num steps total                           9200\n",
      "exploration/num paths total                            460\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0789217\n",
      "exploration/Rewards Std                                  0.077375\n",
      "exploration/Rewards Max                                  0.0986343\n",
      "exploration/Rewards Min                                 -0.249873\n",
      "exploration/Returns Mean                                -1.57843\n",
      "exploration/Returns Std                                  1.00553\n",
      "exploration/Returns Max                                  0.101055\n",
      "exploration/Returns Min                                 -2.80316\n",
      "exploration/Actions Mean                                -0.00478884\n",
      "exploration/Actions Std                                  0.163611\n",
      "exploration/Actions Max                                  0.642568\n",
      "exploration/Actions Min                                 -0.833816\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.57843\n",
      "exploration/env_infos/final/reward_dist Mean             0.224277\n",
      "exploration/env_infos/final/reward_dist Std              0.370558\n",
      "exploration/env_infos/final/reward_dist Max              0.961577\n",
      "exploration/env_infos/final/reward_dist Min              9.60199e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00360291\n",
      "exploration/env_infos/initial/reward_dist Std            0.00526173\n",
      "exploration/env_infos/initial/reward_dist Max            0.013552\n",
      "exploration/env_infos/initial/reward_dist Min            3.59217e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.233519\n",
      "exploration/env_infos/reward_dist Std                    0.291032\n",
      "exploration/env_infos/reward_dist Max                    0.961577\n",
      "exploration/env_infos/reward_dist Min                    9.60199e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.143987\n",
      "exploration/env_infos/final/reward_energy Std            0.0706867\n",
      "exploration/env_infos/final/reward_energy Max           -0.0273083\n",
      "exploration/env_infos/final/reward_energy Min           -0.24092\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.268023\n",
      "exploration/env_infos/initial/reward_energy Std          0.180559\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0890067\n",
      "exploration/env_infos/initial/reward_energy Min         -0.521311\n",
      "exploration/env_infos/reward_energy Mean                -0.174561\n",
      "exploration/env_infos/reward_energy Std                  0.152026\n",
      "exploration/env_infos/reward_energy Max                 -0.00705695\n",
      "exploration/env_infos/reward_energy Min                 -0.890501\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.101455\n",
      "exploration/env_infos/final/end_effector_loc Std         0.263439\n",
      "exploration/env_infos/final/end_effector_loc Max         0.377548\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.484904\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00241432\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0111677\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0252183\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00669702\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0427475\n",
      "exploration/env_infos/end_effector_loc Std               0.167546\n",
      "exploration/env_infos/end_effector_loc Max               0.377548\n",
      "exploration/env_infos/end_effector_loc Min              -0.484904\n",
      "evaluation/num steps total                           82000\n",
      "evaluation/num paths total                            4100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0401291\n",
      "evaluation/Rewards Std                                   0.0812656\n",
      "evaluation/Rewards Max                                   0.149735\n",
      "evaluation/Rewards Min                                  -0.347338\n",
      "evaluation/Returns Mean                                 -0.802582\n",
      "evaluation/Returns Std                                   1.27938\n",
      "evaluation/Returns Max                                   1.65276\n",
      "evaluation/Returns Min                                  -3.10534\n",
      "evaluation/Actions Mean                                  0.00477686\n",
      "evaluation/Actions Std                                   0.0724492\n",
      "evaluation/Actions Max                                   0.516293\n",
      "evaluation/Actions Min                                  -0.59724\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.802582\n",
      "evaluation/env_infos/final/reward_dist Mean              0.20278\n",
      "evaluation/env_infos/final/reward_dist Std               0.235835\n",
      "evaluation/env_infos/final/reward_dist Max               0.832151\n",
      "evaluation/env_infos/final/reward_dist Min               4.20862e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0085333\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0173206\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0734066\n",
      "evaluation/env_infos/initial/reward_dist Min             1.95351e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.244884\n",
      "evaluation/env_infos/reward_dist Std                     0.29365\n",
      "evaluation/env_infos/reward_dist Max                     0.995843\n",
      "evaluation/env_infos/reward_dist Min                     4.20862e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.031667\n",
      "evaluation/env_infos/final/reward_energy Std             0.027656\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00283989\n",
      "evaluation/env_infos/final/reward_energy Min            -0.130073\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.275938\n",
      "evaluation/env_infos/initial/reward_energy Std           0.181151\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0152479\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.72073\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0630416\n",
      "evaluation/env_infos/reward_energy Std                   0.0810503\n",
      "evaluation/env_infos/reward_energy Max                  -0.000760239\n",
      "evaluation/env_infos/reward_energy Min                  -0.72073\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0434003\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.231944\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.652607\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.556796\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000207206\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116685\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0258147\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.029862\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0153983\n",
      "evaluation/env_infos/end_effector_loc Std                0.159602\n",
      "evaluation/env_infos/end_effector_loc Max                0.652607\n",
      "evaluation/env_infos/end_effector_loc Min               -0.556796\n",
      "time/data storing (s)                                    0.00613975\n",
      "time/evaluation sampling (s)                             1.19732\n",
      "time/exploration sampling (s)                            0.142037\n",
      "time/logging (s)                                         0.0190983\n",
      "time/saving (s)                                          0.0294669\n",
      "time/training (s)                                       49.5602\n",
      "time/epoch (s)                                          50.9543\n",
      "time/total (s)                                        4078.09\n",
      "Epoch                                                   81\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:57:28.226832 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 82 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00217281\n",
      "trainer/QF2 Loss                                         0.0011669\n",
      "trainer/Policy Loss                                      3.12834\n",
      "trainer/Q1 Predictions Mean                             -1.09499\n",
      "trainer/Q1 Predictions Std                               0.80314\n",
      "trainer/Q1 Predictions Max                               0.880711\n",
      "trainer/Q1 Predictions Min                              -3.27177\n",
      "trainer/Q2 Predictions Mean                             -1.10839\n",
      "trainer/Q2 Predictions Std                               0.808321\n",
      "trainer/Q2 Predictions Max                               0.855497\n",
      "trainer/Q2 Predictions Min                              -3.25295\n",
      "trainer/Q Targets Mean                                  -1.11752\n",
      "trainer/Q Targets Std                                    0.814465\n",
      "trainer/Q Targets Max                                    0.838658\n",
      "trainer/Q Targets Min                                   -3.27814\n",
      "trainer/Log Pis Mean                                     2.06036\n",
      "trainer/Log Pis Std                                      1.4178\n",
      "trainer/Log Pis Max                                      4.92373\n",
      "trainer/Log Pis Min                                     -4.40481\n",
      "trainer/Policy mu Mean                                   0.0471574\n",
      "trainer/Policy mu Std                                    0.3763\n",
      "trainer/Policy mu Max                                    1.79985\n",
      "trainer/Policy mu Min                                   -1.88676\n",
      "trainer/Policy log std Mean                             -2.31071\n",
      "trainer/Policy log std Std                               0.629466\n",
      "trainer/Policy log std Max                              -0.400236\n",
      "trainer/Policy log std Min                              -3.64712\n",
      "trainer/Alpha                                            0.023415\n",
      "trainer/Alpha Loss                                       0.226656\n",
      "exploration/num steps total                           9300\n",
      "exploration/num paths total                            465\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0655059\n",
      "exploration/Rewards Std                                  0.081509\n",
      "exploration/Rewards Max                                  0.104851\n",
      "exploration/Rewards Min                                 -0.315271\n",
      "exploration/Returns Mean                                -1.31012\n",
      "exploration/Returns Std                                  0.70526\n",
      "exploration/Returns Max                                 -0.202074\n",
      "exploration/Returns Min                                 -2.38131\n",
      "exploration/Actions Mean                                -0.00341782\n",
      "exploration/Actions Std                                  0.106784\n",
      "exploration/Actions Max                                  0.405353\n",
      "exploration/Actions Min                                 -0.357317\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.31012\n",
      "exploration/env_infos/final/reward_dist Mean             0.32335\n",
      "exploration/env_infos/final/reward_dist Std              0.39695\n",
      "exploration/env_infos/final/reward_dist Max              0.979553\n",
      "exploration/env_infos/final/reward_dist Min              0.000115061\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00609586\n",
      "exploration/env_infos/initial/reward_dist Std            0.00809048\n",
      "exploration/env_infos/initial/reward_dist Max            0.0204337\n",
      "exploration/env_infos/initial/reward_dist Min            4.87792e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.248117\n",
      "exploration/env_infos/reward_dist Std                    0.289736\n",
      "exploration/env_infos/reward_dist Max                    0.979553\n",
      "exploration/env_infos/reward_dist Min                    4.87792e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.153359\n",
      "exploration/env_infos/final/reward_energy Std            0.0867256\n",
      "exploration/env_infos/final/reward_energy Max           -0.0359569\n",
      "exploration/env_infos/final/reward_energy Min           -0.27051\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.252088\n",
      "exploration/env_infos/initial/reward_energy Std          0.152824\n",
      "exploration/env_infos/initial/reward_energy Max         -0.034966\n",
      "exploration/env_infos/initial/reward_energy Min         -0.423958\n",
      "exploration/env_infos/reward_energy Mean                -0.128158\n",
      "exploration/env_infos/reward_energy Std                  0.0800282\n",
      "exploration/env_infos/reward_energy Max                 -0.0160949\n",
      "exploration/env_infos/reward_energy Min                 -0.423958\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.03295\n",
      "exploration/env_infos/final/end_effector_loc Std         0.219714\n",
      "exploration/env_infos/final/end_effector_loc Max         0.325088\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.332476\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000798558\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0103919\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0202676\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0140608\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0114794\n",
      "exploration/env_infos/end_effector_loc Std               0.14373\n",
      "exploration/env_infos/end_effector_loc Max               0.325088\n",
      "exploration/env_infos/end_effector_loc Min              -0.332476\n",
      "evaluation/num steps total                           83000\n",
      "evaluation/num paths total                            4150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0452157\n",
      "evaluation/Rewards Std                                   0.0698681\n",
      "evaluation/Rewards Max                                   0.154154\n",
      "evaluation/Rewards Min                                  -0.30587\n",
      "evaluation/Returns Mean                                 -0.904314\n",
      "evaluation/Returns Std                                   1.11094\n",
      "evaluation/Returns Max                                   1.28962\n",
      "evaluation/Returns Min                                  -3.3344\n",
      "evaluation/Actions Mean                                 -0.00221289\n",
      "evaluation/Actions Std                                   0.0742632\n",
      "evaluation/Actions Max                                   0.874106\n",
      "evaluation/Actions Min                                  -0.575372\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.904314\n",
      "evaluation/env_infos/final/reward_dist Mean              0.154511\n",
      "evaluation/env_infos/final/reward_dist Std               0.211195\n",
      "evaluation/env_infos/final/reward_dist Max               0.755779\n",
      "evaluation/env_infos/final/reward_dist Min               2.49186e-09\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00709559\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116293\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0552604\n",
      "evaluation/env_infos/initial/reward_dist Min             3.30054e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.196677\n",
      "evaluation/env_infos/reward_dist Std                     0.252106\n",
      "evaluation/env_infos/reward_dist Max                     0.999584\n",
      "evaluation/env_infos/reward_dist Min                     2.49186e-09\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0394181\n",
      "evaluation/env_infos/final/reward_energy Std             0.0505049\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00343804\n",
      "evaluation/env_infos/final/reward_energy Min            -0.256471\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.275609\n",
      "evaluation/env_infos/initial/reward_energy Std           0.214373\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0100217\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.874141\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0624997\n",
      "evaluation/env_infos/reward_energy Std                   0.0844608\n",
      "evaluation/env_infos/reward_energy Max                  -0.000178518\n",
      "evaluation/env_infos/reward_energy Min                  -0.874141\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0193739\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.226817\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.372034\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.431169\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0018283\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122087\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0437053\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0287686\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0039534\n",
      "evaluation/env_infos/end_effector_loc Std                0.158036\n",
      "evaluation/env_infos/end_effector_loc Max                0.372212\n",
      "evaluation/env_infos/end_effector_loc Min               -0.431169\n",
      "time/data storing (s)                                    0.00644248\n",
      "time/evaluation sampling (s)                             0.976377\n",
      "time/exploration sampling (s)                            0.121944\n",
      "time/logging (s)                                         0.0200988\n",
      "time/saving (s)                                          0.0313416\n",
      "time/training (s)                                       49.5401\n",
      "time/epoch (s)                                          50.6963\n",
      "time/total (s)                                        4129.86\n",
      "Epoch                                                   82\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:58:20.405550 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 83 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000911529\r\n",
      "trainer/QF2 Loss                                         0.0011679\r\n",
      "trainer/Policy Loss                                      3.12911\r\n",
      "trainer/Q1 Predictions Mean                             -1.24019\r\n",
      "trainer/Q1 Predictions Std                               0.932339\r\n",
      "trainer/Q1 Predictions Max                               1.01708\r\n",
      "trainer/Q1 Predictions Min                              -4.32937\r\n",
      "trainer/Q2 Predictions Mean                             -1.24759\r\n",
      "trainer/Q2 Predictions Std                               0.928601\r\n",
      "trainer/Q2 Predictions Max                               1.03655\r\n",
      "trainer/Q2 Predictions Min                              -4.26155\r\n",
      "trainer/Q Targets Mean                                  -1.23755\r\n",
      "trainer/Q Targets Std                                    0.933028\r\n",
      "trainer/Q Targets Max                                    1.00757\r\n",
      "trainer/Q Targets Min                                   -4.32167\r\n",
      "trainer/Log Pis Mean                                     1.9192\r\n",
      "trainer/Log Pis Std                                      1.4301\r\n",
      "trainer/Log Pis Max                                      4.36626\r\n",
      "trainer/Log Pis Min                                     -4.21741\r\n",
      "trainer/Policy mu Mean                                   0.0572984\r\n",
      "trainer/Policy mu Std                                    0.370872\r\n",
      "trainer/Policy mu Max                                    2.1014\r\n",
      "trainer/Policy mu Min                                   -1.75823\r\n",
      "trainer/Policy log std Mean                             -2.23451\r\n",
      "trainer/Policy log std Std                               0.599021\r\n",
      "trainer/Policy log std Max                              -0.112739\r\n",
      "trainer/Policy log std Min                              -3.2008\r\n",
      "trainer/Alpha                                            0.0249358\r\n",
      "trainer/Alpha Loss                                      -0.298245\r\n",
      "exploration/num steps total                           9400\r\n",
      "exploration/num paths total                            470\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0694048\r\n",
      "exploration/Rewards Std                                  0.0744803\r\n",
      "exploration/Rewards Max                                  0.118009\r\n",
      "exploration/Rewards Min                                 -0.220312\r\n",
      "exploration/Returns Mean                                -1.3881\r\n",
      "exploration/Returns Std                                  1.03031\r\n",
      "exploration/Returns Max                                  0.0800738\r\n",
      "exploration/Returns Min                                 -2.74623\r\n",
      "exploration/Actions Mean                                -0.00347294\r\n",
      "exploration/Actions Std                                  0.0875973\r\n",
      "exploration/Actions Max                                  0.230634\r\n",
      "exploration/Actions Min                                 -0.351315\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.3881\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0422057\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0802872\r\n",
      "exploration/env_infos/final/reward_dist Max              0.202656\r\n",
      "exploration/env_infos/final/reward_dist Min              5.89505e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00430395\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00481702\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0112033\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.30108e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.171535\r\n",
      "exploration/env_infos/reward_dist Std                    0.281551\r\n",
      "exploration/env_infos/reward_dist Max                    0.964229\r\n",
      "exploration/env_infos/reward_dist Min                    3.30108e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0753729\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0345326\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0305626\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.118375\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.133817\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.097009\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0130986\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.270711\r\n",
      "exploration/env_infos/reward_energy Mean                -0.10566\r\n",
      "exploration/env_infos/reward_energy Std                  0.0648593\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0102124\r\n",
      "exploration/env_infos/reward_energy Min                 -0.351704\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00637979\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.263977\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.357213\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.399817\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00253812\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00526356\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0115122\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00655201\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0224531\r\n",
      "exploration/env_infos/end_effector_loc Std               0.16462\r\n",
      "exploration/env_infos/end_effector_loc Max               0.357213\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.399817\r\n",
      "evaluation/num steps total                           84000\r\n",
      "evaluation/num paths total                            4200\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.027867\r\n",
      "evaluation/Rewards Std                                   0.0820352\r\n",
      "evaluation/Rewards Max                                   0.161523\r\n",
      "evaluation/Rewards Min                                  -0.322608\r\n",
      "evaluation/Returns Mean                                 -0.557339\r\n",
      "evaluation/Returns Std                                   1.22083\r\n",
      "evaluation/Returns Max                                   2.12155\r\n",
      "evaluation/Returns Min                                  -3.18693\r\n",
      "evaluation/Actions Mean                                  0.000829077\r\n",
      "evaluation/Actions Std                                   0.0769622\r\n",
      "evaluation/Actions Max                                   0.777106\r\n",
      "evaluation/Actions Min                                  -0.83075\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.557339\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.215732\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.266981\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.921039\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.43763e-16\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0059576\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.010744\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0511102\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.62016e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.241769\r\n",
      "evaluation/env_infos/reward_dist Std                     0.277137\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997119\r\n",
      "evaluation/env_infos/reward_dist Min                     8.43763e-16\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0364781\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0387664\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00626635\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.152302\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.305227\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.227328\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0216277\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.910939\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0650487\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0872721\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00110328\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.910939\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00477948\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.224692\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.477948\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.524181\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00019439\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134542\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0388553\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0415375\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00388124\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.154471\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.477948\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.524181\r\n",
      "time/data storing (s)                                    0.0064251\r\n",
      "time/evaluation sampling (s)                             0.96226\r\n",
      "time/exploration sampling (s)                            0.12405\r\n",
      "time/logging (s)                                         0.0197449\r\n",
      "time/saving (s)                                          0.028533\r\n",
      "time/training (s)                                       50.0137\r\n",
      "time/epoch (s)                                          51.1547\r\n",
      "time/total (s)                                        4182.04\r\n",
      "Epoch                                                   83\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:59:12.520727 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 84 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00129902\n",
      "trainer/QF2 Loss                                         0.00125048\n",
      "trainer/Policy Loss                                      3.12399\n",
      "trainer/Q1 Predictions Mean                             -1.2584\n",
      "trainer/Q1 Predictions Std                               0.885303\n",
      "trainer/Q1 Predictions Max                               1.01572\n",
      "trainer/Q1 Predictions Min                              -3.56972\n",
      "trainer/Q2 Predictions Mean                             -1.26884\n",
      "trainer/Q2 Predictions Std                               0.890924\n",
      "trainer/Q2 Predictions Max                               0.993801\n",
      "trainer/Q2 Predictions Min                              -3.60556\n",
      "trainer/Q Targets Mean                                  -1.2737\n",
      "trainer/Q Targets Std                                    0.889635\n",
      "trainer/Q Targets Max                                    0.965952\n",
      "trainer/Q Targets Min                                   -3.59767\n",
      "trainer/Log Pis Mean                                     1.89549\n",
      "trainer/Log Pis Std                                      1.28161\n",
      "trainer/Log Pis Max                                      4.14695\n",
      "trainer/Log Pis Min                                     -3.19218\n",
      "trainer/Policy mu Mean                                   0.068161\n",
      "trainer/Policy mu Std                                    0.432444\n",
      "trainer/Policy mu Max                                    2.27172\n",
      "trainer/Policy mu Min                                   -2.05889\n",
      "trainer/Policy log std Mean                             -2.18306\n",
      "trainer/Policy log std Std                               0.598999\n",
      "trainer/Policy log std Max                              -0.182956\n",
      "trainer/Policy log std Min                              -3.17785\n",
      "trainer/Alpha                                            0.0249745\n",
      "trainer/Alpha Loss                                      -0.385455\n",
      "exploration/num steps total                           9500\n",
      "exploration/num paths total                            475\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0640535\n",
      "exploration/Rewards Std                                  0.0870408\n",
      "exploration/Rewards Max                                  0.118514\n",
      "exploration/Rewards Min                                 -0.318441\n",
      "exploration/Returns Mean                                -1.28107\n",
      "exploration/Returns Std                                  1.3414\n",
      "exploration/Returns Max                                  0.397207\n",
      "exploration/Returns Min                                 -3.42722\n",
      "exploration/Actions Mean                                 0.00786422\n",
      "exploration/Actions Std                                  0.132458\n",
      "exploration/Actions Max                                  0.331396\n",
      "exploration/Actions Min                                 -0.533591\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.28107\n",
      "exploration/env_infos/final/reward_dist Mean             0.187001\n",
      "exploration/env_infos/final/reward_dist Std              0.186125\n",
      "exploration/env_infos/final/reward_dist Max              0.437891\n",
      "exploration/env_infos/final/reward_dist Min              1.21996e-20\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00656486\n",
      "exploration/env_infos/initial/reward_dist Std            0.00888525\n",
      "exploration/env_infos/initial/reward_dist Max            0.0224756\n",
      "exploration/env_infos/initial/reward_dist Min            2.75233e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.249377\n",
      "exploration/env_infos/reward_dist Std                    0.321277\n",
      "exploration/env_infos/reward_dist Max                    0.999782\n",
      "exploration/env_infos/reward_dist Min                    1.21996e-20\n",
      "exploration/env_infos/final/reward_energy Mean          -0.130072\n",
      "exploration/env_infos/final/reward_energy Std            0.0854345\n",
      "exploration/env_infos/final/reward_energy Max           -0.0326181\n",
      "exploration/env_infos/final/reward_energy Min           -0.251404\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.252079\n",
      "exploration/env_infos/initial/reward_energy Std          0.18185\n",
      "exploration/env_infos/initial/reward_energy Max         -0.049225\n",
      "exploration/env_infos/initial/reward_energy Min         -0.534219\n",
      "exploration/env_infos/reward_energy Mean                -0.148691\n",
      "exploration/env_infos/reward_energy Std                  0.114476\n",
      "exploration/env_infos/reward_energy Max                 -0.00616094\n",
      "exploration/env_infos/reward_energy Min                 -0.534219\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.116804\n",
      "exploration/env_infos/final/end_effector_loc Std         0.253858\n",
      "exploration/env_infos/final/end_effector_loc Max         0.556489\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.157156\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00260378\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0106764\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0124076\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0266796\n",
      "exploration/env_infos/end_effector_loc Mean              0.0408645\n",
      "exploration/env_infos/end_effector_loc Std               0.176379\n",
      "exploration/env_infos/end_effector_loc Max               0.556489\n",
      "exploration/env_infos/end_effector_loc Min              -0.269997\n",
      "evaluation/num steps total                           85000\n",
      "evaluation/num paths total                            4250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0538219\n",
      "evaluation/Rewards Std                                   0.0750689\n",
      "evaluation/Rewards Max                                   0.142585\n",
      "evaluation/Rewards Min                                  -0.405209\n",
      "evaluation/Returns Mean                                 -1.07644\n",
      "evaluation/Returns Std                                   1.16781\n",
      "evaluation/Returns Max                                   2.00544\n",
      "evaluation/Returns Min                                  -4.0354\n",
      "evaluation/Actions Mean                                  0.00566819\n",
      "evaluation/Actions Std                                   0.0893464\n",
      "evaluation/Actions Max                                   0.849898\n",
      "evaluation/Actions Min                                  -0.79042\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.07644\n",
      "evaluation/env_infos/final/reward_dist Mean              0.170578\n",
      "evaluation/env_infos/final/reward_dist Std               0.283726\n",
      "evaluation/env_infos/final/reward_dist Max               0.96184\n",
      "evaluation/env_infos/final/reward_dist Min               5.23923e-20\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00724402\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0131716\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0751983\n",
      "evaluation/env_infos/initial/reward_dist Min             2.45823e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.205084\n",
      "evaluation/env_infos/reward_dist Std                     0.271259\n",
      "evaluation/env_infos/reward_dist Max                     0.999616\n",
      "evaluation/env_infos/reward_dist Min                     5.23923e-20\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0479697\n",
      "evaluation/env_infos/final/reward_energy Std             0.0376954\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00800594\n",
      "evaluation/env_infos/final/reward_energy Min            -0.174316\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.344791\n",
      "evaluation/env_infos/initial/reward_energy Std           0.238269\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0149446\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.912386\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0763209\n",
      "evaluation/env_infos/reward_energy Std                   0.101019\n",
      "evaluation/env_infos/reward_energy Max                  -0.00173586\n",
      "evaluation/env_infos/reward_energy Min                  -0.912386\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0984018\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.234496\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.571399\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.462117\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00377589\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0143286\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0424949\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.039521\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0539995\n",
      "evaluation/env_infos/end_effector_loc Std                0.165336\n",
      "evaluation/env_infos/end_effector_loc Max                0.571399\n",
      "evaluation/env_infos/end_effector_loc Min               -0.462117\n",
      "time/data storing (s)                                    0.00756658\n",
      "time/evaluation sampling (s)                             0.981662\n",
      "time/exploration sampling (s)                            0.123709\n",
      "time/logging (s)                                         0.0195073\n",
      "time/saving (s)                                          0.0286405\n",
      "time/training (s)                                       49.8644\n",
      "time/epoch (s)                                          51.0255\n",
      "time/total (s)                                        4234.15\n",
      "Epoch                                                   84\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:00:04.446012 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 85 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00165457\n",
      "trainer/QF2 Loss                                         0.00144343\n",
      "trainer/Policy Loss                                      3.22674\n",
      "trainer/Q1 Predictions Mean                             -1.1773\n",
      "trainer/Q1 Predictions Std                               0.888574\n",
      "trainer/Q1 Predictions Max                               0.592964\n",
      "trainer/Q1 Predictions Min                              -3.51586\n",
      "trainer/Q2 Predictions Mean                             -1.16572\n",
      "trainer/Q2 Predictions Std                               0.877363\n",
      "trainer/Q2 Predictions Max                               0.596741\n",
      "trainer/Q2 Predictions Min                              -3.52227\n",
      "trainer/Q Targets Mean                                  -1.17816\n",
      "trainer/Q Targets Std                                    0.87805\n",
      "trainer/Q Targets Max                                    0.569674\n",
      "trainer/Q Targets Min                                   -3.49734\n",
      "trainer/Log Pis Mean                                     2.12091\n",
      "trainer/Log Pis Std                                      1.21158\n",
      "trainer/Log Pis Max                                      4.60932\n",
      "trainer/Log Pis Min                                     -3.45735\n",
      "trainer/Policy mu Mean                                  -0.00433461\n",
      "trainer/Policy mu Std                                    0.508591\n",
      "trainer/Policy mu Max                                    2.10657\n",
      "trainer/Policy mu Min                                   -2.02141\n",
      "trainer/Policy log std Mean                             -2.19434\n",
      "trainer/Policy log std Std                               0.645687\n",
      "trainer/Policy log std Max                              -0.292423\n",
      "trainer/Policy log std Min                              -3.28252\n",
      "trainer/Alpha                                            0.0248108\n",
      "trainer/Alpha Loss                                       0.446991\n",
      "exploration/num steps total                           9600\n",
      "exploration/num paths total                            480\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0882633\n",
      "exploration/Rewards Std                                  0.0694509\n",
      "exploration/Rewards Max                                  0.027882\n",
      "exploration/Rewards Min                                 -0.323987\n",
      "exploration/Returns Mean                                -1.76527\n",
      "exploration/Returns Std                                  0.709696\n",
      "exploration/Returns Max                                 -0.628432\n",
      "exploration/Returns Min                                 -2.56523\n",
      "exploration/Actions Mean                                 0.00817267\n",
      "exploration/Actions Std                                  0.144853\n",
      "exploration/Actions Max                                  0.80931\n",
      "exploration/Actions Min                                 -0.508049\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.76527\n",
      "exploration/env_infos/final/reward_dist Mean             0.341214\n",
      "exploration/env_infos/final/reward_dist Std              0.381646\n",
      "exploration/env_infos/final/reward_dist Max              0.853187\n",
      "exploration/env_infos/final/reward_dist Min              3.65797e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000385915\n",
      "exploration/env_infos/initial/reward_dist Std            0.000534258\n",
      "exploration/env_infos/initial/reward_dist Max            0.00140722\n",
      "exploration/env_infos/initial/reward_dist Min            4.84452e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.202589\n",
      "exploration/env_infos/reward_dist Std                    0.249388\n",
      "exploration/env_infos/reward_dist Max                    0.853187\n",
      "exploration/env_infos/reward_dist Min                    4.84452e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.192559\n",
      "exploration/env_infos/final/reward_energy Std            0.0743445\n",
      "exploration/env_infos/final/reward_energy Max           -0.0753322\n",
      "exploration/env_infos/final/reward_energy Min           -0.275846\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.409411\n",
      "exploration/env_infos/initial/reward_energy Std          0.232496\n",
      "exploration/env_infos/initial/reward_energy Max         -0.123857\n",
      "exploration/env_infos/initial/reward_energy Min         -0.810147\n",
      "exploration/env_infos/reward_energy Mean                -0.152061\n",
      "exploration/env_infos/reward_energy Std                  0.137752\n",
      "exploration/env_infos/reward_energy Max                 -0.00849166\n",
      "exploration/env_infos/reward_energy Min                 -0.810147\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.164468\n",
      "exploration/env_infos/final/end_effector_loc Std         0.208719\n",
      "exploration/env_infos/final/end_effector_loc Max         0.421959\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.2747\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00920182\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0138714\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0404655\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0107686\n",
      "exploration/env_infos/end_effector_loc Mean              0.102684\n",
      "exploration/env_infos/end_effector_loc Std               0.161117\n",
      "exploration/env_infos/end_effector_loc Max               0.421959\n",
      "exploration/env_infos/end_effector_loc Min              -0.2747\n",
      "evaluation/num steps total                           86000\n",
      "evaluation/num paths total                            4300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0405534\n",
      "evaluation/Rewards Std                                   0.0693112\n",
      "evaluation/Rewards Max                                   0.172365\n",
      "evaluation/Rewards Min                                  -0.371512\n",
      "evaluation/Returns Mean                                 -0.811069\n",
      "evaluation/Returns Std                                   1.07402\n",
      "evaluation/Returns Max                                   2.14383\n",
      "evaluation/Returns Min                                  -2.79068\n",
      "evaluation/Actions Mean                                 -0.00204364\n",
      "evaluation/Actions Std                                   0.0623805\n",
      "evaluation/Actions Max                                   0.749641\n",
      "evaluation/Actions Min                                  -0.537112\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.811069\n",
      "evaluation/env_infos/final/reward_dist Mean              0.177852\n",
      "evaluation/env_infos/final/reward_dist Std               0.274192\n",
      "evaluation/env_infos/final/reward_dist Max               0.967453\n",
      "evaluation/env_infos/final/reward_dist Min               3.62339e-12\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00669412\n",
      "evaluation/env_infos/initial/reward_dist Std             0.010592\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0465162\n",
      "evaluation/env_infos/initial/reward_dist Min             1.17394e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.226026\n",
      "evaluation/env_infos/reward_dist Std                     0.295598\n",
      "evaluation/env_infos/reward_dist Max                     0.997835\n",
      "evaluation/env_infos/reward_dist Min                     3.62339e-12\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0260265\n",
      "evaluation/env_infos/final/reward_energy Std             0.0177282\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00129057\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0764897\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.224911\n",
      "evaluation/env_infos/initial/reward_energy Std           0.209315\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00533578\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.840692\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0512236\n",
      "evaluation/env_infos/reward_energy Std                   0.0718828\n",
      "evaluation/env_infos/reward_energy Max                  -0.00129057\n",
      "evaluation/env_infos/reward_energy Min                  -0.840692\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00354496\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.239449\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.530748\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.579958\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00256439\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0105556\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0374821\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0268556\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00818991\n",
      "evaluation/env_infos/end_effector_loc Std                0.153127\n",
      "evaluation/env_infos/end_effector_loc Max                0.530748\n",
      "evaluation/env_infos/end_effector_loc Min               -0.579958\n",
      "time/data storing (s)                                    0.00636503\n",
      "time/evaluation sampling (s)                             0.922978\n",
      "time/exploration sampling (s)                            0.121103\n",
      "time/logging (s)                                         0.0196777\n",
      "time/saving (s)                                          0.0272638\n",
      "time/training (s)                                       49.9606\n",
      "time/epoch (s)                                          51.058\n",
      "time/total (s)                                        4286.07\n",
      "Epoch                                                   85\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:00:57.744364 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 86 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00129748\n",
      "trainer/QF2 Loss                                         0.00159807\n",
      "trainer/Policy Loss                                      3.20363\n",
      "trainer/Q1 Predictions Mean                             -1.24969\n",
      "trainer/Q1 Predictions Std                               0.864312\n",
      "trainer/Q1 Predictions Max                               0.967666\n",
      "trainer/Q1 Predictions Min                              -3.50322\n",
      "trainer/Q2 Predictions Mean                             -1.25656\n",
      "trainer/Q2 Predictions Std                               0.871129\n",
      "trainer/Q2 Predictions Max                               1.0587\n",
      "trainer/Q2 Predictions Min                              -3.52259\n",
      "trainer/Q Targets Mean                                  -1.2476\n",
      "trainer/Q Targets Std                                    0.869855\n",
      "trainer/Q Targets Max                                    0.990498\n",
      "trainer/Q Targets Min                                   -3.55776\n",
      "trainer/Log Pis Mean                                     1.99011\n",
      "trainer/Log Pis Std                                      1.45835\n",
      "trainer/Log Pis Max                                      5.22183\n",
      "trainer/Log Pis Min                                     -4.64913\n",
      "trainer/Policy mu Mean                                   0.0225547\n",
      "trainer/Policy mu Std                                    0.487364\n",
      "trainer/Policy mu Max                                    1.75962\n",
      "trainer/Policy mu Min                                   -2.003\n",
      "trainer/Policy log std Mean                             -2.22356\n",
      "trainer/Policy log std Std                               0.655712\n",
      "trainer/Policy log std Max                              -0.358768\n",
      "trainer/Policy log std Min                              -3.31403\n",
      "trainer/Alpha                                            0.025159\n",
      "trainer/Alpha Loss                                      -0.0364305\n",
      "exploration/num steps total                           9700\n",
      "exploration/num paths total                            485\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0405601\n",
      "exploration/Rewards Std                                  0.0845126\n",
      "exploration/Rewards Max                                  0.146788\n",
      "exploration/Rewards Min                                 -0.255262\n",
      "exploration/Returns Mean                                -0.811201\n",
      "exploration/Returns Std                                  1.32701\n",
      "exploration/Returns Max                                  1.20583\n",
      "exploration/Returns Min                                 -2.23302\n",
      "exploration/Actions Mean                                 0.00724008\n",
      "exploration/Actions Std                                  0.166817\n",
      "exploration/Actions Max                                  0.67266\n",
      "exploration/Actions Min                                 -0.502044\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.811201\n",
      "exploration/env_infos/final/reward_dist Mean             0.141286\n",
      "exploration/env_infos/final/reward_dist Std              0.174101\n",
      "exploration/env_infos/final/reward_dist Max              0.394104\n",
      "exploration/env_infos/final/reward_dist Min              6.17836e-09\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00517923\n",
      "exploration/env_infos/initial/reward_dist Std            0.00950721\n",
      "exploration/env_infos/initial/reward_dist Max            0.0241849\n",
      "exploration/env_infos/initial/reward_dist Min            0.000132978\n",
      "exploration/env_infos/reward_dist Mean                   0.249773\n",
      "exploration/env_infos/reward_dist Std                    0.32766\n",
      "exploration/env_infos/reward_dist Max                    0.992131\n",
      "exploration/env_infos/reward_dist Min                    6.17836e-09\n",
      "exploration/env_infos/final/reward_energy Mean          -0.181015\n",
      "exploration/env_infos/final/reward_energy Std            0.0879557\n",
      "exploration/env_infos/final/reward_energy Max           -0.0502244\n",
      "exploration/env_infos/final/reward_energy Min           -0.326092\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.394081\n",
      "exploration/env_infos/initial/reward_energy Std          0.287435\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0240513\n",
      "exploration/env_infos/initial/reward_energy Min         -0.774144\n",
      "exploration/env_infos/reward_energy Mean                -0.181276\n",
      "exploration/env_infos/reward_energy Std                  0.151325\n",
      "exploration/env_infos/reward_energy Max                 -0.00523701\n",
      "exploration/env_infos/reward_energy Min                 -0.774852\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.159881\n",
      "exploration/env_infos/final/end_effector_loc Std         0.222916\n",
      "exploration/env_infos/final/end_effector_loc Max         0.465826\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.255565\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00628158\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0160605\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.033633\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0177935\n",
      "exploration/env_infos/end_effector_loc Mean              0.095798\n",
      "exploration/env_infos/end_effector_loc Std               0.162419\n",
      "exploration/env_infos/end_effector_loc Max               0.465826\n",
      "exploration/env_infos/end_effector_loc Min              -0.255565\n",
      "evaluation/num steps total                           87000\n",
      "evaluation/num paths total                            4350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0376271\n",
      "evaluation/Rewards Std                                   0.077562\n",
      "evaluation/Rewards Max                                   0.153759\n",
      "evaluation/Rewards Min                                  -0.397413\n",
      "evaluation/Returns Mean                                 -0.752541\n",
      "evaluation/Returns Std                                   1.15447\n",
      "evaluation/Returns Max                                   2.09004\n",
      "evaluation/Returns Min                                  -3.12594\n",
      "evaluation/Actions Mean                                  0.00233796\n",
      "evaluation/Actions Std                                   0.0694317\n",
      "evaluation/Actions Max                                   0.772927\n",
      "evaluation/Actions Min                                  -0.696229\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.752541\n",
      "evaluation/env_infos/final/reward_dist Mean              0.181363\n",
      "evaluation/env_infos/final/reward_dist Std               0.296049\n",
      "evaluation/env_infos/final/reward_dist Max               0.845088\n",
      "evaluation/env_infos/final/reward_dist Min               1.25359e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00830589\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0164791\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0799672\n",
      "evaluation/env_infos/initial/reward_dist Min             2.58153e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.191612\n",
      "evaluation/env_infos/reward_dist Std                     0.282457\n",
      "evaluation/env_infos/reward_dist Max                     0.995149\n",
      "evaluation/env_infos/reward_dist Min                     1.25359e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0299075\n",
      "evaluation/env_infos/final/reward_energy Std             0.0246576\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00319455\n",
      "evaluation/env_infos/final/reward_energy Min            -0.129137\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.273203\n",
      "evaluation/env_infos/initial/reward_energy Std           0.219737\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0119349\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.780251\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0553102\n",
      "evaluation/env_infos/reward_energy Std                   0.0811987\n",
      "evaluation/env_infos/reward_energy Max                  -0.000955415\n",
      "evaluation/env_infos/reward_energy Min                  -0.780251\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0448512\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.254939\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.615278\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.508895\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00132691\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123246\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0386464\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0348114\n",
      "evaluation/env_infos/end_effector_loc Mean               0.021614\n",
      "evaluation/env_infos/end_effector_loc Std                0.168504\n",
      "evaluation/env_infos/end_effector_loc Max                0.615278\n",
      "evaluation/env_infos/end_effector_loc Min               -0.508895\n",
      "time/data storing (s)                                    0.00622189\n",
      "time/evaluation sampling (s)                             0.96321\n",
      "time/exploration sampling (s)                            0.121596\n",
      "time/logging (s)                                         0.0231536\n",
      "time/saving (s)                                          0.0318708\n",
      "time/training (s)                                       51.0946\n",
      "time/epoch (s)                                          52.2407\n",
      "time/total (s)                                        4339.37\n",
      "Epoch                                                   86\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:01:51.369377 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 87 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000818184\n",
      "trainer/QF2 Loss                                         0.00104762\n",
      "trainer/Policy Loss                                      3.13268\n",
      "trainer/Q1 Predictions Mean                             -1.19569\n",
      "trainer/Q1 Predictions Std                               0.890925\n",
      "trainer/Q1 Predictions Max                               0.706173\n",
      "trainer/Q1 Predictions Min                              -3.71725\n",
      "trainer/Q2 Predictions Mean                             -1.19835\n",
      "trainer/Q2 Predictions Std                               0.892363\n",
      "trainer/Q2 Predictions Max                               0.729951\n",
      "trainer/Q2 Predictions Min                              -3.70518\n",
      "trainer/Q Targets Mean                                  -1.19247\n",
      "trainer/Q Targets Std                                    0.888742\n",
      "trainer/Q Targets Max                                    0.716619\n",
      "trainer/Q Targets Min                                   -3.6725\n",
      "trainer/Log Pis Mean                                     1.98188\n",
      "trainer/Log Pis Std                                      1.23737\n",
      "trainer/Log Pis Max                                      4.50345\n",
      "trainer/Log Pis Min                                     -2.47065\n",
      "trainer/Policy mu Mean                                  -0.00104988\n",
      "trainer/Policy mu Std                                    0.462169\n",
      "trainer/Policy mu Max                                    1.88489\n",
      "trainer/Policy mu Min                                   -1.94046\n",
      "trainer/Policy log std Mean                             -2.15749\n",
      "trainer/Policy log std Std                               0.623412\n",
      "trainer/Policy log std Max                              -0.264871\n",
      "trainer/Policy log std Min                              -3.31156\n",
      "trainer/Alpha                                            0.0239479\n",
      "trainer/Alpha Loss                                      -0.0676332\n",
      "exploration/num steps total                           9800\n",
      "exploration/num paths total                            490\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0552733\n",
      "exploration/Rewards Std                                  0.0691529\n",
      "exploration/Rewards Max                                  0.116912\n",
      "exploration/Rewards Min                                 -0.303359\n",
      "exploration/Returns Mean                                -1.10547\n",
      "exploration/Returns Std                                  0.944261\n",
      "exploration/Returns Max                                  0.652969\n",
      "exploration/Returns Min                                 -2.15984\n",
      "exploration/Actions Mean                                -0.00406479\n",
      "exploration/Actions Std                                  0.135518\n",
      "exploration/Actions Max                                  0.490985\n",
      "exploration/Actions Min                                 -0.394579\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.10547\n",
      "exploration/env_infos/final/reward_dist Mean             0.126666\n",
      "exploration/env_infos/final/reward_dist Std              0.232508\n",
      "exploration/env_infos/final/reward_dist Max              0.591314\n",
      "exploration/env_infos/final/reward_dist Min              1.94211e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00636688\n",
      "exploration/env_infos/initial/reward_dist Std            0.0077805\n",
      "exploration/env_infos/initial/reward_dist Max            0.0192861\n",
      "exploration/env_infos/initial/reward_dist Min            2.392e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.173535\n",
      "exploration/env_infos/reward_dist Std                    0.254259\n",
      "exploration/env_infos/reward_dist Max                    0.940545\n",
      "exploration/env_infos/reward_dist Min                    1.94211e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.111802\n",
      "exploration/env_infos/final/reward_energy Std            0.0499984\n",
      "exploration/env_infos/final/reward_energy Max           -0.0451931\n",
      "exploration/env_infos/final/reward_energy Min           -0.197351\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.216029\n",
      "exploration/env_infos/initial/reward_energy Std          0.11834\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0383808\n",
      "exploration/env_infos/initial/reward_energy Min         -0.382375\n",
      "exploration/env_infos/reward_energy Mean                -0.147128\n",
      "exploration/env_infos/reward_energy Std                  0.122949\n",
      "exploration/env_infos/reward_energy Max                 -0.00441384\n",
      "exploration/env_infos/reward_energy Min                 -0.609851\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0605702\n",
      "exploration/env_infos/final/end_effector_loc Std         0.224757\n",
      "exploration/env_infos/final/end_effector_loc Max         0.348284\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.412666\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000603048\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00868779\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0156214\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0146443\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0260649\n",
      "exploration/env_infos/end_effector_loc Std               0.148381\n",
      "exploration/env_infos/end_effector_loc Max               0.348284\n",
      "exploration/env_infos/end_effector_loc Min              -0.412666\n",
      "evaluation/num steps total                           88000\n",
      "evaluation/num paths total                            4400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0415772\n",
      "evaluation/Rewards Std                                   0.0669372\n",
      "evaluation/Rewards Max                                   0.161025\n",
      "evaluation/Rewards Min                                  -0.301447\n",
      "evaluation/Returns Mean                                 -0.831544\n",
      "evaluation/Returns Std                                   1.01158\n",
      "evaluation/Returns Max                                   1.97968\n",
      "evaluation/Returns Min                                  -2.54669\n",
      "evaluation/Actions Mean                                 -0.000162246\n",
      "evaluation/Actions Std                                   0.0634118\n",
      "evaluation/Actions Max                                   0.524722\n",
      "evaluation/Actions Min                                  -0.797972\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.831544\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0895304\n",
      "evaluation/env_infos/final/reward_dist Std               0.202131\n",
      "evaluation/env_infos/final/reward_dist Max               0.863403\n",
      "evaluation/env_infos/final/reward_dist Min               5.36645e-80\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00681301\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0104155\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0376638\n",
      "evaluation/env_infos/initial/reward_dist Min             1.82287e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.151494\n",
      "evaluation/env_infos/reward_dist Std                     0.247932\n",
      "evaluation/env_infos/reward_dist Max                     0.99859\n",
      "evaluation/env_infos/reward_dist Min                     5.36645e-80\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0291649\n",
      "evaluation/env_infos/final/reward_energy Std             0.0160018\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00541957\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0774534\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.216959\n",
      "evaluation/env_infos/initial/reward_energy Std           0.207305\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0143931\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.851791\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0536683\n",
      "evaluation/env_infos/reward_energy Std                   0.0718462\n",
      "evaluation/env_infos/reward_energy Max                  -0.000183159\n",
      "evaluation/env_infos/reward_energy Min                  -0.851791\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0212593\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.266932\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.598552\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00152293\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0104995\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0262361\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0398986\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0140362\n",
      "evaluation/env_infos/end_effector_loc Std                0.166177\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.598552\n",
      "time/data storing (s)                                    0.00614191\n",
      "time/evaluation sampling (s)                             1.06943\n",
      "time/exploration sampling (s)                            0.124807\n",
      "time/logging (s)                                         0.0226338\n",
      "time/saving (s)                                          0.0278119\n",
      "time/training (s)                                       50.9642\n",
      "time/epoch (s)                                          52.215\n",
      "time/total (s)                                        4392.99\n",
      "Epoch                                                   87\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:02:43.554605 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 88 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000981648\n",
      "trainer/QF2 Loss                                         0.00130634\n",
      "trainer/Policy Loss                                      2.98707\n",
      "trainer/Q1 Predictions Mean                             -1.21616\n",
      "trainer/Q1 Predictions Std                               0.929423\n",
      "trainer/Q1 Predictions Max                               0.731953\n",
      "trainer/Q1 Predictions Min                              -4.15235\n",
      "trainer/Q2 Predictions Mean                             -1.22002\n",
      "trainer/Q2 Predictions Std                               0.934365\n",
      "trainer/Q2 Predictions Max                               0.739354\n",
      "trainer/Q2 Predictions Min                              -4.18078\n",
      "trainer/Q Targets Mean                                  -1.21584\n",
      "trainer/Q Targets Std                                    0.932287\n",
      "trainer/Q Targets Max                                    0.716339\n",
      "trainer/Q Targets Min                                   -4.18247\n",
      "trainer/Log Pis Mean                                     1.82446\n",
      "trainer/Log Pis Std                                      1.31431\n",
      "trainer/Log Pis Max                                      4.24314\n",
      "trainer/Log Pis Min                                     -4.2052\n",
      "trainer/Policy mu Mean                                   0.0358812\n",
      "trainer/Policy mu Std                                    0.458929\n",
      "trainer/Policy mu Max                                    1.98868\n",
      "trainer/Policy mu Min                                   -2.28705\n",
      "trainer/Policy log std Mean                             -2.1645\n",
      "trainer/Policy log std Std                               0.597663\n",
      "trainer/Policy log std Max                              -0.245055\n",
      "trainer/Policy log std Min                              -3.22957\n",
      "trainer/Alpha                                            0.0242162\n",
      "trainer/Alpha Loss                                      -0.653221\n",
      "exploration/num steps total                           9900\n",
      "exploration/num paths total                            495\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0802414\n",
      "exploration/Rewards Std                                  0.0746953\n",
      "exploration/Rewards Max                                  0.091021\n",
      "exploration/Rewards Min                                 -0.438809\n",
      "exploration/Returns Mean                                -1.60483\n",
      "exploration/Returns Std                                  0.691684\n",
      "exploration/Returns Max                                 -0.889954\n",
      "exploration/Returns Min                                 -2.47496\n",
      "exploration/Actions Mean                                -0.0115487\n",
      "exploration/Actions Std                                  0.132635\n",
      "exploration/Actions Max                                  0.571721\n",
      "exploration/Actions Min                                 -0.718002\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.60483\n",
      "exploration/env_infos/final/reward_dist Mean             0.0880904\n",
      "exploration/env_infos/final/reward_dist Std              0.172051\n",
      "exploration/env_infos/final/reward_dist Max              0.432133\n",
      "exploration/env_infos/final/reward_dist Min              1.71257e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00205509\n",
      "exploration/env_infos/initial/reward_dist Std            0.00236799\n",
      "exploration/env_infos/initial/reward_dist Max            0.006636\n",
      "exploration/env_infos/initial/reward_dist Min            1.48642e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.140591\n",
      "exploration/env_infos/reward_dist Std                    0.205212\n",
      "exploration/env_infos/reward_dist Max                    0.829545\n",
      "exploration/env_infos/reward_dist Min                    1.71257e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.143857\n",
      "exploration/env_infos/final/reward_energy Std            0.0352667\n",
      "exploration/env_infos/final/reward_energy Max           -0.0765655\n",
      "exploration/env_infos/final/reward_energy Min           -0.179163\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.391644\n",
      "exploration/env_infos/initial/reward_energy Std          0.340021\n",
      "exploration/env_infos/initial/reward_energy Max         -0.102578\n",
      "exploration/env_infos/initial/reward_energy Min         -0.917819\n",
      "exploration/env_infos/reward_energy Mean                -0.13998\n",
      "exploration/env_infos/reward_energy Std                  0.125922\n",
      "exploration/env_infos/reward_energy Max                 -0.00792356\n",
      "exploration/env_infos/reward_energy Min                 -0.917819\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.140932\n",
      "exploration/env_infos/final/end_effector_loc Std         0.285457\n",
      "exploration/env_infos/final/end_effector_loc Max         0.404155\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.467046\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0042077\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0178478\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0285861\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0359001\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0689383\n",
      "exploration/env_infos/end_effector_loc Std               0.179785\n",
      "exploration/env_infos/end_effector_loc Max               0.404155\n",
      "exploration/env_infos/end_effector_loc Min              -0.467046\n",
      "evaluation/num steps total                           89000\n",
      "evaluation/num paths total                            4450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0317249\n",
      "evaluation/Rewards Std                                   0.0748911\n",
      "evaluation/Rewards Max                                   0.151566\n",
      "evaluation/Rewards Min                                  -0.375015\n",
      "evaluation/Returns Mean                                 -0.634498\n",
      "evaluation/Returns Std                                   1.1088\n",
      "evaluation/Returns Max                                   2.06022\n",
      "evaluation/Returns Min                                  -3.09119\n",
      "evaluation/Actions Mean                                  0.00190528\n",
      "evaluation/Actions Std                                   0.0854064\n",
      "evaluation/Actions Max                                   0.881849\n",
      "evaluation/Actions Min                                  -0.691339\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.634498\n",
      "evaluation/env_infos/final/reward_dist Mean              0.218766\n",
      "evaluation/env_infos/final/reward_dist Std               0.286927\n",
      "evaluation/env_infos/final/reward_dist Max               0.870831\n",
      "evaluation/env_infos/final/reward_dist Min               2.58234e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0127695\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0244131\n",
      "evaluation/env_infos/initial/reward_dist Max             0.148787\n",
      "evaluation/env_infos/initial/reward_dist Min             3.57733e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.232995\n",
      "evaluation/env_infos/reward_dist Std                     0.282536\n",
      "evaluation/env_infos/reward_dist Max                     0.993777\n",
      "evaluation/env_infos/reward_dist Min                     2.58234e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0389151\n",
      "evaluation/env_infos/final/reward_energy Std             0.0413194\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00488238\n",
      "evaluation/env_infos/final/reward_energy Min            -0.255086\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.345574\n",
      "evaluation/env_infos/initial/reward_energy Std           0.250843\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0138063\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.956398\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0686445\n",
      "evaluation/env_infos/reward_energy Std                   0.0994167\n",
      "evaluation/env_infos/reward_energy Max                  -0.00154439\n",
      "evaluation/env_infos/reward_energy Min                  -0.956398\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.055626\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.242859\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.545713\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.481961\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00366971\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0146445\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0440925\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.034567\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0339314\n",
      "evaluation/env_infos/end_effector_loc Std                0.167813\n",
      "evaluation/env_infos/end_effector_loc Max                0.545713\n",
      "evaluation/env_infos/end_effector_loc Min               -0.481961\n",
      "time/data storing (s)                                    0.00643455\n",
      "time/evaluation sampling (s)                             0.97336\n",
      "time/exploration sampling (s)                            0.122809\n",
      "time/logging (s)                                         0.0201219\n",
      "time/saving (s)                                          0.0284136\n",
      "time/training (s)                                       49.7682\n",
      "time/epoch (s)                                          50.9193\n",
      "time/total (s)                                        4445.17\n",
      "Epoch                                                   88\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:03:36.286924 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 89 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000918292\n",
      "trainer/QF2 Loss                                         0.000930786\n",
      "trainer/Policy Loss                                      3.27761\n",
      "trainer/Q1 Predictions Mean                             -1.28066\n",
      "trainer/Q1 Predictions Std                               0.934483\n",
      "trainer/Q1 Predictions Max                               0.255915\n",
      "trainer/Q1 Predictions Min                              -4.15804\n",
      "trainer/Q2 Predictions Mean                             -1.28648\n",
      "trainer/Q2 Predictions Std                               0.936971\n",
      "trainer/Q2 Predictions Max                               0.273252\n",
      "trainer/Q2 Predictions Min                              -4.17828\n",
      "trainer/Q Targets Mean                                  -1.29196\n",
      "trainer/Q Targets Std                                    0.936759\n",
      "trainer/Q Targets Max                                    0.244243\n",
      "trainer/Q Targets Min                                   -4.20185\n",
      "trainer/Log Pis Mean                                     2.06003\n",
      "trainer/Log Pis Std                                      1.37973\n",
      "trainer/Log Pis Max                                      4.45503\n",
      "trainer/Log Pis Min                                     -2.3337\n",
      "trainer/Policy mu Mean                                  -0.00739481\n",
      "trainer/Policy mu Std                                    0.493236\n",
      "trainer/Policy mu Max                                    1.81448\n",
      "trainer/Policy mu Min                                   -2.23327\n",
      "trainer/Policy log std Mean                             -2.25411\n",
      "trainer/Policy log std Std                               0.686981\n",
      "trainer/Policy log std Max                              -0.390846\n",
      "trainer/Policy log std Min                              -3.32819\n",
      "trainer/Alpha                                            0.0241882\n",
      "trainer/Alpha Loss                                       0.223396\n",
      "exploration/num steps total                          10000\n",
      "exploration/num paths total                            500\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0612039\n",
      "exploration/Rewards Std                                  0.0807869\n",
      "exploration/Rewards Max                                  0.104559\n",
      "exploration/Rewards Min                                 -0.19295\n",
      "exploration/Returns Mean                                -1.22408\n",
      "exploration/Returns Std                                  1.40793\n",
      "exploration/Returns Max                                  0.900986\n",
      "exploration/Returns Min                                 -2.90271\n",
      "exploration/Actions Mean                                -0.00122823\n",
      "exploration/Actions Std                                  0.1856\n",
      "exploration/Actions Max                                  0.767092\n",
      "exploration/Actions Min                                 -0.541122\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.22408\n",
      "exploration/env_infos/final/reward_dist Mean             0.240831\n",
      "exploration/env_infos/final/reward_dist Std              0.243973\n",
      "exploration/env_infos/final/reward_dist Max              0.680926\n",
      "exploration/env_infos/final/reward_dist Min              7.95252e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0126996\n",
      "exploration/env_infos/initial/reward_dist Std            0.0212316\n",
      "exploration/env_infos/initial/reward_dist Max            0.0549511\n",
      "exploration/env_infos/initial/reward_dist Min            4.75962e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.282793\n",
      "exploration/env_infos/reward_dist Std                    0.332394\n",
      "exploration/env_infos/reward_dist Max                    0.989661\n",
      "exploration/env_infos/reward_dist Min                    7.95252e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.125719\n",
      "exploration/env_infos/final/reward_energy Std            0.0525482\n",
      "exploration/env_infos/final/reward_energy Max           -0.0723622\n",
      "exploration/env_infos/final/reward_energy Min           -0.224129\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.483852\n",
      "exploration/env_infos/initial/reward_energy Std          0.295779\n",
      "exploration/env_infos/initial/reward_energy Max         -0.115577\n",
      "exploration/env_infos/initial/reward_energy Min         -0.767423\n",
      "exploration/env_infos/reward_energy Mean                -0.197203\n",
      "exploration/env_infos/reward_energy Std                  0.173231\n",
      "exploration/env_infos/reward_energy Max                 -0.0182971\n",
      "exploration/env_infos/reward_energy Min                 -0.767423\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0196198\n",
      "exploration/env_infos/final/end_effector_loc Std         0.168958\n",
      "exploration/env_infos/final/end_effector_loc Max         0.228291\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.282634\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00305943\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0198151\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0383546\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0270561\n",
      "exploration/env_infos/end_effector_loc Mean              0.0282793\n",
      "exploration/env_infos/end_effector_loc Std               0.135157\n",
      "exploration/env_infos/end_effector_loc Max               0.268655\n",
      "exploration/env_infos/end_effector_loc Min              -0.284885\n",
      "evaluation/num steps total                           90000\n",
      "evaluation/num paths total                            4500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0631126\n",
      "evaluation/Rewards Std                                   0.0670304\n",
      "evaluation/Rewards Max                                   0.174123\n",
      "evaluation/Rewards Min                                  -0.409353\n",
      "evaluation/Returns Mean                                 -1.26225\n",
      "evaluation/Returns Std                                   1.02516\n",
      "evaluation/Returns Max                                   1.97936\n",
      "evaluation/Returns Min                                  -3.4802\n",
      "evaluation/Actions Mean                                  0.00145968\n",
      "evaluation/Actions Std                                   0.0842775\n",
      "evaluation/Actions Max                                   0.912068\n",
      "evaluation/Actions Min                                  -0.605884\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.26225\n",
      "evaluation/env_infos/final/reward_dist Mean              0.14718\n",
      "evaluation/env_infos/final/reward_dist Std               0.246658\n",
      "evaluation/env_infos/final/reward_dist Max               0.867079\n",
      "evaluation/env_infos/final/reward_dist Min               2.47159e-28\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00724697\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0161419\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0783672\n",
      "evaluation/env_infos/initial/reward_dist Min             5.52658e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.16354\n",
      "evaluation/env_infos/reward_dist Std                     0.254053\n",
      "evaluation/env_infos/reward_dist Max                     0.997438\n",
      "evaluation/env_infos/reward_dist Min                     2.47159e-28\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0316948\n",
      "evaluation/env_infos/final/reward_energy Std             0.0206433\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00442239\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0995702\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.309555\n",
      "evaluation/env_infos/initial/reward_energy Std           0.261282\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0227814\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.932711\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0650379\n",
      "evaluation/env_infos/reward_energy Std                   0.0998985\n",
      "evaluation/env_infos/reward_energy Max                  -0.00131532\n",
      "evaluation/env_infos/reward_energy Min                  -0.932711\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0236309\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.235447\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.496211\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.476883\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00242129\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0141157\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0456034\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0302942\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0162439\n",
      "evaluation/env_infos/end_effector_loc Std                0.160844\n",
      "evaluation/env_infos/end_effector_loc Max                0.496211\n",
      "evaluation/env_infos/end_effector_loc Min               -0.476883\n",
      "time/data storing (s)                                    0.00623988\n",
      "time/evaluation sampling (s)                             1.11098\n",
      "time/exploration sampling (s)                            0.122857\n",
      "time/logging (s)                                         0.0197739\n",
      "time/saving (s)                                          0.0278975\n",
      "time/training (s)                                       50.2628\n",
      "time/epoch (s)                                          51.5505\n",
      "time/total (s)                                        4497.9\n",
      "Epoch                                                   89\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:04:29.242062 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 90 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000895234\n",
      "trainer/QF2 Loss                                         0.00122647\n",
      "trainer/Policy Loss                                      3.14839\n",
      "trainer/Q1 Predictions Mean                             -1.28321\n",
      "trainer/Q1 Predictions Std                               0.933062\n",
      "trainer/Q1 Predictions Max                               0.49515\n",
      "trainer/Q1 Predictions Min                              -3.99135\n",
      "trainer/Q2 Predictions Mean                             -1.2917\n",
      "trainer/Q2 Predictions Std                               0.931587\n",
      "trainer/Q2 Predictions Max                               0.482504\n",
      "trainer/Q2 Predictions Min                              -3.96505\n",
      "trainer/Q Targets Mean                                  -1.28641\n",
      "trainer/Q Targets Std                                    0.932607\n",
      "trainer/Q Targets Max                                    0.495866\n",
      "trainer/Q Targets Min                                   -4.03918\n",
      "trainer/Log Pis Mean                                     1.91503\n",
      "trainer/Log Pis Std                                      1.44992\n",
      "trainer/Log Pis Max                                      4.61978\n",
      "trainer/Log Pis Min                                     -3.88537\n",
      "trainer/Policy mu Mean                                  -0.0115212\n",
      "trainer/Policy mu Std                                    0.538949\n",
      "trainer/Policy mu Max                                    1.7615\n",
      "trainer/Policy mu Min                                   -2.30573\n",
      "trainer/Policy log std Mean                             -2.15391\n",
      "trainer/Policy log std Std                               0.724783\n",
      "trainer/Policy log std Max                              -0.35327\n",
      "trainer/Policy log std Min                              -3.25747\n",
      "trainer/Alpha                                            0.0222315\n",
      "trainer/Alpha Loss                                      -0.323414\n",
      "exploration/num steps total                          10100\n",
      "exploration/num paths total                            505\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0640445\n",
      "exploration/Rewards Std                                  0.0539819\n",
      "exploration/Rewards Max                                  0.0604245\n",
      "exploration/Rewards Min                                 -0.247535\n",
      "exploration/Returns Mean                                -1.28089\n",
      "exploration/Returns Std                                  0.576319\n",
      "exploration/Returns Max                                 -0.482723\n",
      "exploration/Returns Min                                 -2.28409\n",
      "exploration/Actions Mean                                -0.00248075\n",
      "exploration/Actions Std                                  0.151822\n",
      "exploration/Actions Max                                  0.827062\n",
      "exploration/Actions Min                                 -0.524844\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.28089\n",
      "exploration/env_infos/final/reward_dist Mean             0.155504\n",
      "exploration/env_infos/final/reward_dist Std              0.268438\n",
      "exploration/env_infos/final/reward_dist Max              0.688465\n",
      "exploration/env_infos/final/reward_dist Min              0.000484602\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0231321\n",
      "exploration/env_infos/initial/reward_dist Std            0.0307698\n",
      "exploration/env_infos/initial/reward_dist Max            0.0808322\n",
      "exploration/env_infos/initial/reward_dist Min            2.56369e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.209764\n",
      "exploration/env_infos/reward_dist Std                    0.262251\n",
      "exploration/env_infos/reward_dist Max                    0.952438\n",
      "exploration/env_infos/reward_dist Min                    2.56369e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0736341\n",
      "exploration/env_infos/final/reward_energy Std            0.0260017\n",
      "exploration/env_infos/final/reward_energy Max           -0.0319291\n",
      "exploration/env_infos/final/reward_energy Min           -0.107295\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.347094\n",
      "exploration/env_infos/initial/reward_energy Std          0.298752\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0418956\n",
      "exploration/env_infos/initial/reward_energy Min         -0.827783\n",
      "exploration/env_infos/reward_energy Mean                -0.15456\n",
      "exploration/env_infos/reward_energy Std                  0.149076\n",
      "exploration/env_infos/reward_energy Max                 -0.00599167\n",
      "exploration/env_infos/reward_energy Min                 -0.827783\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.006534\n",
      "exploration/env_infos/final/end_effector_loc Std         0.228605\n",
      "exploration/env_infos/final/end_effector_loc Max         0.359282\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.383096\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00373933\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0157536\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0413531\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0262422\n",
      "exploration/env_infos/end_effector_loc Mean              0.0143589\n",
      "exploration/env_infos/end_effector_loc Std               0.159816\n",
      "exploration/env_infos/end_effector_loc Max               0.359282\n",
      "exploration/env_infos/end_effector_loc Min              -0.383096\n",
      "evaluation/num steps total                           91000\n",
      "evaluation/num paths total                            4550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0296138\n",
      "evaluation/Rewards Std                                   0.0782509\n",
      "evaluation/Rewards Max                                   0.145895\n",
      "evaluation/Rewards Min                                  -0.544654\n",
      "evaluation/Returns Mean                                 -0.592276\n",
      "evaluation/Returns Std                                   1.20783\n",
      "evaluation/Returns Max                                   1.80973\n",
      "evaluation/Returns Min                                  -3.3443\n",
      "evaluation/Actions Mean                                 -0.00101399\n",
      "evaluation/Actions Std                                   0.0816152\n",
      "evaluation/Actions Max                                   0.748629\n",
      "evaluation/Actions Min                                  -0.836749\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.592276\n",
      "evaluation/env_infos/final/reward_dist Mean              0.187155\n",
      "evaluation/env_infos/final/reward_dist Std               0.249522\n",
      "evaluation/env_infos/final/reward_dist Max               0.986245\n",
      "evaluation/env_infos/final/reward_dist Min               2.82653e-15\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0118386\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0185637\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0666917\n",
      "evaluation/env_infos/initial/reward_dist Min             1.84977e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.250391\n",
      "evaluation/env_infos/reward_dist Std                     0.287731\n",
      "evaluation/env_infos/reward_dist Max                     0.99436\n",
      "evaluation/env_infos/reward_dist Min                     2.82653e-15\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0311094\n",
      "evaluation/env_infos/final/reward_energy Std             0.0402332\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00170903\n",
      "evaluation/env_infos/final/reward_energy Min            -0.205377\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.340163\n",
      "evaluation/env_infos/initial/reward_energy Std           0.236537\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0236464\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.916235\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0639605\n",
      "evaluation/env_infos/reward_energy Std                   0.0960895\n",
      "evaluation/env_infos/reward_energy Max                  -0.000380631\n",
      "evaluation/env_infos/reward_energy Min                  -0.916235\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0296881\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.234018\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.564402\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.480756\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00106831\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0146094\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0374315\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0418375\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0128439\n",
      "evaluation/env_infos/end_effector_loc Std                0.160212\n",
      "evaluation/env_infos/end_effector_loc Max                0.564402\n",
      "evaluation/env_infos/end_effector_loc Min               -0.480756\n",
      "time/data storing (s)                                    0.00613816\n",
      "time/evaluation sampling (s)                             0.969765\n",
      "time/exploration sampling (s)                            0.125667\n",
      "time/logging (s)                                         0.0217116\n",
      "time/saving (s)                                          0.0349665\n",
      "time/training (s)                                       50.6326\n",
      "time/epoch (s)                                          51.7908\n",
      "time/total (s)                                        4550.85\n",
      "Epoch                                                   90\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:05:25.217702 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 91 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0013359\n",
      "trainer/QF2 Loss                                         0.00124171\n",
      "trainer/Policy Loss                                      3.18944\n",
      "trainer/Q1 Predictions Mean                             -1.19046\n",
      "trainer/Q1 Predictions Std                               0.826393\n",
      "trainer/Q1 Predictions Max                               0.324618\n",
      "trainer/Q1 Predictions Min                              -3.6053\n",
      "trainer/Q2 Predictions Mean                             -1.19093\n",
      "trainer/Q2 Predictions Std                               0.822938\n",
      "trainer/Q2 Predictions Max                               0.299039\n",
      "trainer/Q2 Predictions Min                              -3.5694\n",
      "trainer/Q Targets Mean                                  -1.17859\n",
      "trainer/Q Targets Std                                    0.820569\n",
      "trainer/Q Targets Max                                    0.308083\n",
      "trainer/Q Targets Min                                   -3.50365\n",
      "trainer/Log Pis Mean                                     2.03805\n",
      "trainer/Log Pis Std                                      1.6628\n",
      "trainer/Log Pis Max                                      5.27781\n",
      "trainer/Log Pis Min                                     -5.89721\n",
      "trainer/Policy mu Mean                                   0.0354141\n",
      "trainer/Policy mu Std                                    0.462657\n",
      "trainer/Policy mu Max                                    1.9775\n",
      "trainer/Policy mu Min                                   -1.87699\n",
      "trainer/Policy log std Mean                             -2.26635\n",
      "trainer/Policy log std Std                               0.695037\n",
      "trainer/Policy log std Max                              -0.387627\n",
      "trainer/Policy log std Min                              -3.35686\n",
      "trainer/Alpha                                            0.0210638\n",
      "trainer/Alpha Loss                                       0.146857\n",
      "exploration/num steps total                          10200\n",
      "exploration/num paths total                            510\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.111756\n",
      "exploration/Rewards Std                                  0.0928162\n",
      "exploration/Rewards Max                                  0.0802499\n",
      "exploration/Rewards Min                                 -0.507052\n",
      "exploration/Returns Mean                                -2.23512\n",
      "exploration/Returns Std                                  1.12716\n",
      "exploration/Returns Max                                 -0.823016\n",
      "exploration/Returns Min                                 -3.69764\n",
      "exploration/Actions Mean                                 0.00135658\n",
      "exploration/Actions Std                                  0.177566\n",
      "exploration/Actions Max                                  0.766834\n",
      "exploration/Actions Min                                 -0.850952\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.23512\n",
      "exploration/env_infos/final/reward_dist Mean             0.106889\n",
      "exploration/env_infos/final/reward_dist Std              0.213748\n",
      "exploration/env_infos/final/reward_dist Max              0.534385\n",
      "exploration/env_infos/final/reward_dist Min              4.3892e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0087379\n",
      "exploration/env_infos/initial/reward_dist Std            0.0114393\n",
      "exploration/env_infos/initial/reward_dist Max            0.0285592\n",
      "exploration/env_infos/initial/reward_dist Min            9.88944e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.104828\n",
      "exploration/env_infos/reward_dist Std                    0.219769\n",
      "exploration/env_infos/reward_dist Max                    0.82933\n",
      "exploration/env_infos/reward_dist Min                    3.4765e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0985401\n",
      "exploration/env_infos/final/reward_energy Std            0.0433066\n",
      "exploration/env_infos/final/reward_energy Max           -0.0641348\n",
      "exploration/env_infos/final/reward_energy Min           -0.179693\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.461212\n",
      "exploration/env_infos/initial/reward_energy Std          0.355651\n",
      "exploration/env_infos/initial/reward_energy Max         -0.030878\n",
      "exploration/env_infos/initial/reward_energy Min         -0.922342\n",
      "exploration/env_infos/reward_energy Mean                -0.170439\n",
      "exploration/env_infos/reward_energy Std                  0.184428\n",
      "exploration/env_infos/reward_energy Max                 -0.011102\n",
      "exploration/env_infos/reward_energy Min                 -0.922342\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0848391\n",
      "exploration/env_infos/final/end_effector_loc Std         0.259044\n",
      "exploration/env_infos/final/end_effector_loc Max         0.397434\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.386804\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00742208\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0192072\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0330802\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0425476\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0717348\n",
      "exploration/env_infos/end_effector_loc Std               0.181818\n",
      "exploration/env_infos/end_effector_loc Max               0.397434\n",
      "exploration/env_infos/end_effector_loc Min              -0.420192\n",
      "evaluation/num steps total                           92000\n",
      "evaluation/num paths total                            4600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0344288\n",
      "evaluation/Rewards Std                                   0.075688\n",
      "evaluation/Rewards Max                                   0.140705\n",
      "evaluation/Rewards Min                                  -0.406303\n",
      "evaluation/Returns Mean                                 -0.688575\n",
      "evaluation/Returns Std                                   1.20596\n",
      "evaluation/Returns Max                                   2.16156\n",
      "evaluation/Returns Min                                  -3.54785\n",
      "evaluation/Actions Mean                                  0.00464167\n",
      "evaluation/Actions Std                                   0.0805526\n",
      "evaluation/Actions Max                                   0.827204\n",
      "evaluation/Actions Min                                  -0.594616\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.688575\n",
      "evaluation/env_infos/final/reward_dist Mean              0.222165\n",
      "evaluation/env_infos/final/reward_dist Std               0.249258\n",
      "evaluation/env_infos/final/reward_dist Max               0.796109\n",
      "evaluation/env_infos/final/reward_dist Min               5.14051e-40\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0109302\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0197742\n",
      "evaluation/env_infos/initial/reward_dist Max             0.11732\n",
      "evaluation/env_infos/initial/reward_dist Min             9.22454e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.252931\n",
      "evaluation/env_infos/reward_dist Std                     0.284102\n",
      "evaluation/env_infos/reward_dist Max                     0.998528\n",
      "evaluation/env_infos/reward_dist Min                     5.14051e-40\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0392162\n",
      "evaluation/env_infos/final/reward_energy Std             0.0449696\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0103304\n",
      "evaluation/env_infos/final/reward_energy Min            -0.287313\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.304706\n",
      "evaluation/env_infos/initial/reward_energy Std           0.25966\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.032112\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.874032\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0623752\n",
      "evaluation/env_infos/reward_energy Std                   0.0955503\n",
      "evaluation/env_infos/reward_energy Max                  -0.00112869\n",
      "evaluation/env_infos/reward_energy Min                  -0.874032\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0328608\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.226617\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.605849\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.344753\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00229877\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0139661\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0413602\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0297308\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0172818\n",
      "evaluation/env_infos/end_effector_loc Std                0.149407\n",
      "evaluation/env_infos/end_effector_loc Max                0.605849\n",
      "evaluation/env_infos/end_effector_loc Min               -0.344753\n",
      "time/data storing (s)                                    0.00621119\n",
      "time/evaluation sampling (s)                             1.15265\n",
      "time/exploration sampling (s)                            0.14741\n",
      "time/logging (s)                                         0.0204036\n",
      "time/saving (s)                                          0.0288289\n",
      "time/training (s)                                       53.3987\n",
      "time/epoch (s)                                          54.7542\n",
      "time/total (s)                                        4606.83\n",
      "Epoch                                                   91\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:06:19.382040 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 92 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00201842\n",
      "trainer/QF2 Loss                                         0.0019482\n",
      "trainer/Policy Loss                                      3.15806\n",
      "trainer/Q1 Predictions Mean                             -1.28496\n",
      "trainer/Q1 Predictions Std                               0.973809\n",
      "trainer/Q1 Predictions Max                               0.646414\n",
      "trainer/Q1 Predictions Min                              -4.17533\n",
      "trainer/Q2 Predictions Mean                             -1.28514\n",
      "trainer/Q2 Predictions Std                               0.971535\n",
      "trainer/Q2 Predictions Max                               0.639607\n",
      "trainer/Q2 Predictions Min                              -4.19981\n",
      "trainer/Q Targets Mean                                  -1.30298\n",
      "trainer/Q Targets Std                                    0.978052\n",
      "trainer/Q Targets Max                                    0.585661\n",
      "trainer/Q Targets Min                                   -4.27923\n",
      "trainer/Log Pis Mean                                     1.93776\n",
      "trainer/Log Pis Std                                      1.54795\n",
      "trainer/Log Pis Max                                      7.19185\n",
      "trainer/Log Pis Min                                     -2.61064\n",
      "trainer/Policy mu Mean                                  -0.0770262\n",
      "trainer/Policy mu Std                                    0.624622\n",
      "trainer/Policy mu Max                                    2.38489\n",
      "trainer/Policy mu Min                                   -2.90399\n",
      "trainer/Policy log std Mean                             -2.05641\n",
      "trainer/Policy log std Std                               0.723417\n",
      "trainer/Policy log std Max                              -0.0430002\n",
      "trainer/Policy log std Min                              -3.45056\n",
      "trainer/Alpha                                            0.0213343\n",
      "trainer/Alpha Loss                                      -0.239476\n",
      "exploration/num steps total                          10300\n",
      "exploration/num paths total                            515\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0351474\n",
      "exploration/Rewards Std                                  0.0857104\n",
      "exploration/Rewards Max                                  0.124257\n",
      "exploration/Rewards Min                                 -0.325015\n",
      "exploration/Returns Mean                                -0.702948\n",
      "exploration/Returns Std                                  1.07739\n",
      "exploration/Returns Max                                  0.889091\n",
      "exploration/Returns Min                                 -1.78588\n",
      "exploration/Actions Mean                                 0.0100113\n",
      "exploration/Actions Std                                  0.140976\n",
      "exploration/Actions Max                                  0.48463\n",
      "exploration/Actions Min                                 -0.759252\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.702948\n",
      "exploration/env_infos/final/reward_dist Mean             0.312823\n",
      "exploration/env_infos/final/reward_dist Std              0.393657\n",
      "exploration/env_infos/final/reward_dist Max              0.939643\n",
      "exploration/env_infos/final/reward_dist Min              6.46452e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0045168\n",
      "exploration/env_infos/initial/reward_dist Std            0.00683804\n",
      "exploration/env_infos/initial/reward_dist Max            0.0179175\n",
      "exploration/env_infos/initial/reward_dist Min            3.79492e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.26871\n",
      "exploration/env_infos/reward_dist Std                    0.353844\n",
      "exploration/env_infos/reward_dist Max                    0.998031\n",
      "exploration/env_infos/reward_dist Min                    6.46452e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.183503\n",
      "exploration/env_infos/final/reward_energy Std            0.12635\n",
      "exploration/env_infos/final/reward_energy Max           -0.0276726\n",
      "exploration/env_infos/final/reward_energy Min           -0.377441\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.45649\n",
      "exploration/env_infos/initial/reward_energy Std          0.253298\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0402386\n",
      "exploration/env_infos/initial/reward_energy Min         -0.833935\n",
      "exploration/env_infos/reward_energy Mean                -0.157365\n",
      "exploration/env_infos/reward_energy Std                  0.123227\n",
      "exploration/env_infos/reward_energy Max                 -0.0168796\n",
      "exploration/env_infos/reward_energy Min                 -0.833935\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00415793\n",
      "exploration/env_infos/final/end_effector_loc Std         0.214327\n",
      "exploration/env_infos/final/end_effector_loc Max         0.438653\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.228936\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00525244\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0176944\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0242315\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0379626\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0233486\n",
      "exploration/env_infos/end_effector_loc Std               0.166144\n",
      "exploration/env_infos/end_effector_loc Max               0.438653\n",
      "exploration/env_infos/end_effector_loc Min              -0.320126\n",
      "evaluation/num steps total                           93000\n",
      "evaluation/num paths total                            4650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.045148\n",
      "evaluation/Rewards Std                                   0.0760671\n",
      "evaluation/Rewards Max                                   0.121493\n",
      "evaluation/Rewards Min                                  -0.523611\n",
      "evaluation/Returns Mean                                 -0.90296\n",
      "evaluation/Returns Std                                   1.09819\n",
      "evaluation/Returns Max                                   1.32445\n",
      "evaluation/Returns Min                                  -3.52642\n",
      "evaluation/Actions Mean                                  0.0049153\n",
      "evaluation/Actions Std                                   0.073647\n",
      "evaluation/Actions Max                                   0.695832\n",
      "evaluation/Actions Min                                  -0.751403\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.90296\n",
      "evaluation/env_infos/final/reward_dist Mean              0.286862\n",
      "evaluation/env_infos/final/reward_dist Std               0.305922\n",
      "evaluation/env_infos/final/reward_dist Max               0.965275\n",
      "evaluation/env_infos/final/reward_dist Min               1.024e-40\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00871863\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0159236\n",
      "evaluation/env_infos/initial/reward_dist Max             0.064876\n",
      "evaluation/env_infos/initial/reward_dist Min             1.09472e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.228392\n",
      "evaluation/env_infos/reward_dist Std                     0.287444\n",
      "evaluation/env_infos/reward_dist Max                     0.999514\n",
      "evaluation/env_infos/reward_dist Min                     1.024e-40\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0304411\n",
      "evaluation/env_infos/final/reward_energy Std             0.0262543\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00216157\n",
      "evaluation/env_infos/final/reward_energy Min            -0.108308\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.276411\n",
      "evaluation/env_infos/initial/reward_energy Std           0.236804\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00465378\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.835442\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0563545\n",
      "evaluation/env_infos/reward_energy Std                   0.087865\n",
      "evaluation/env_infos/reward_energy Max                  -0.000495699\n",
      "evaluation/env_infos/reward_energy Min                  -0.835442\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0312519\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.213039\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.648235\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.415151\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000871627\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.012839\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0347916\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0375701\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00683468\n",
      "evaluation/env_infos/end_effector_loc Std                0.149621\n",
      "evaluation/env_infos/end_effector_loc Max                0.648235\n",
      "evaluation/env_infos/end_effector_loc Min               -0.415886\n",
      "time/data storing (s)                                    0.00640034\n",
      "time/evaluation sampling (s)                             0.970173\n",
      "time/exploration sampling (s)                            0.166264\n",
      "time/logging (s)                                         0.0202447\n",
      "time/saving (s)                                          0.0281876\n",
      "time/training (s)                                       51.7825\n",
      "time/epoch (s)                                          52.9737\n",
      "time/total (s)                                        4660.99\n",
      "Epoch                                                   92\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:07:12.171132 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 93 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0014338\n",
      "trainer/QF2 Loss                                         0.00112451\n",
      "trainer/Policy Loss                                      3.01086\n",
      "trainer/Q1 Predictions Mean                             -1.13184\n",
      "trainer/Q1 Predictions Std                               0.876876\n",
      "trainer/Q1 Predictions Max                               0.561032\n",
      "trainer/Q1 Predictions Min                              -3.39829\n",
      "trainer/Q2 Predictions Mean                             -1.12656\n",
      "trainer/Q2 Predictions Std                               0.88286\n",
      "trainer/Q2 Predictions Max                               0.57183\n",
      "trainer/Q2 Predictions Min                              -3.41693\n",
      "trainer/Q Targets Mean                                  -1.13012\n",
      "trainer/Q Targets Std                                    0.88325\n",
      "trainer/Q Targets Max                                    0.587334\n",
      "trainer/Q Targets Min                                   -3.39888\n",
      "trainer/Log Pis Mean                                     1.92171\n",
      "trainer/Log Pis Std                                      1.49466\n",
      "trainer/Log Pis Max                                      6.45196\n",
      "trainer/Log Pis Min                                     -3.35057\n",
      "trainer/Policy mu Mean                                   0.0318728\n",
      "trainer/Policy mu Std                                    0.499652\n",
      "trainer/Policy mu Max                                    2.09104\n",
      "trainer/Policy mu Min                                   -2.47841\n",
      "trainer/Policy log std Mean                             -2.15308\n",
      "trainer/Policy log std Std                               0.726284\n",
      "trainer/Policy log std Max                              -0.255523\n",
      "trainer/Policy log std Min                              -3.42699\n",
      "trainer/Alpha                                            0.0199785\n",
      "trainer/Alpha Loss                                      -0.306377\n",
      "exploration/num steps total                          10400\n",
      "exploration/num paths total                            520\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0818316\n",
      "exploration/Rewards Std                                  0.0690585\n",
      "exploration/Rewards Max                                  0.0427293\n",
      "exploration/Rewards Min                                 -0.286246\n",
      "exploration/Returns Mean                                -1.63663\n",
      "exploration/Returns Std                                  0.867621\n",
      "exploration/Returns Max                                 -0.425124\n",
      "exploration/Returns Min                                 -2.74545\n",
      "exploration/Actions Mean                                 0.00204908\n",
      "exploration/Actions Std                                  0.195256\n",
      "exploration/Actions Max                                  0.659228\n",
      "exploration/Actions Min                                 -0.789074\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.63663\n",
      "exploration/env_infos/final/reward_dist Mean             0.152231\n",
      "exploration/env_infos/final/reward_dist Std              0.159121\n",
      "exploration/env_infos/final/reward_dist Max              0.415779\n",
      "exploration/env_infos/final/reward_dist Min              1.40171e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0085615\n",
      "exploration/env_infos/initial/reward_dist Std            0.0153773\n",
      "exploration/env_infos/initial/reward_dist Max            0.0392272\n",
      "exploration/env_infos/initial/reward_dist Min            3.56095e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.236366\n",
      "exploration/env_infos/reward_dist Std                    0.276688\n",
      "exploration/env_infos/reward_dist Max                    0.957929\n",
      "exploration/env_infos/reward_dist Min                    1.40171e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.181601\n",
      "exploration/env_infos/final/reward_energy Std            0.119576\n",
      "exploration/env_infos/final/reward_energy Max           -0.0562047\n",
      "exploration/env_infos/final/reward_energy Min           -0.409284\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.471291\n",
      "exploration/env_infos/initial/reward_energy Std          0.33233\n",
      "exploration/env_infos/initial/reward_energy Max         -0.137056\n",
      "exploration/env_infos/initial/reward_energy Min         -0.969636\n",
      "exploration/env_infos/reward_energy Mean                -0.205306\n",
      "exploration/env_infos/reward_energy Std                  0.184681\n",
      "exploration/env_infos/reward_energy Max                 -0.0122159\n",
      "exploration/env_infos/reward_energy Min                 -0.969636\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00852849\n",
      "exploration/env_infos/final/end_effector_loc Std         0.228825\n",
      "exploration/env_infos/final/end_effector_loc Max         0.436804\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.267153\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0046731\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0198459\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0281761\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0394537\n",
      "exploration/env_infos/end_effector_loc Mean              0.0151867\n",
      "exploration/env_infos/end_effector_loc Std               0.171908\n",
      "exploration/env_infos/end_effector_loc Max               0.436804\n",
      "exploration/env_infos/end_effector_loc Min              -0.267153\n",
      "evaluation/num steps total                           94000\n",
      "evaluation/num paths total                            4700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.037927\n",
      "evaluation/Rewards Std                                   0.0698449\n",
      "evaluation/Rewards Max                                   0.138208\n",
      "evaluation/Rewards Min                                  -0.41867\n",
      "evaluation/Returns Mean                                 -0.75854\n",
      "evaluation/Returns Std                                   1.0095\n",
      "evaluation/Returns Max                                   1.63549\n",
      "evaluation/Returns Min                                  -3.27317\n",
      "evaluation/Actions Mean                                  0.00359706\n",
      "evaluation/Actions Std                                   0.0828737\n",
      "evaluation/Actions Max                                   0.933374\n",
      "evaluation/Actions Min                                  -0.621186\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.75854\n",
      "evaluation/env_infos/final/reward_dist Mean              0.155648\n",
      "evaluation/env_infos/final/reward_dist Std               0.228578\n",
      "evaluation/env_infos/final/reward_dist Max               0.857586\n",
      "evaluation/env_infos/final/reward_dist Min               3.37008e-24\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00775459\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0113553\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0485993\n",
      "evaluation/env_infos/initial/reward_dist Min             1.78501e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.201788\n",
      "evaluation/env_infos/reward_dist Std                     0.263917\n",
      "evaluation/env_infos/reward_dist Max                     0.991164\n",
      "evaluation/env_infos/reward_dist Min                     3.37008e-24\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0467003\n",
      "evaluation/env_infos/final/reward_energy Std             0.0511996\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00287057\n",
      "evaluation/env_infos/final/reward_energy Min            -0.269855\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.331864\n",
      "evaluation/env_infos/initial/reward_energy Std           0.249472\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0123212\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.01084\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0653904\n",
      "evaluation/env_infos/reward_energy Std                   0.0973965\n",
      "evaluation/env_infos/reward_energy Max                  -0.000810198\n",
      "evaluation/env_infos/reward_energy Min                  -1.01084\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0669496\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.2236\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.697174\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.371247\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00293552\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0143821\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0466687\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0310593\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0374862\n",
      "evaluation/env_infos/end_effector_loc Std                0.15652\n",
      "evaluation/env_infos/end_effector_loc Max                0.697174\n",
      "evaluation/env_infos/end_effector_loc Min               -0.371247\n",
      "time/data storing (s)                                    0.00662253\n",
      "time/evaluation sampling (s)                             0.978208\n",
      "time/exploration sampling (s)                            0.123657\n",
      "time/logging (s)                                         0.022938\n",
      "time/saving (s)                                          0.028302\n",
      "time/training (s)                                       50.4988\n",
      "time/epoch (s)                                          51.6585\n",
      "time/total (s)                                        4713.78\n",
      "Epoch                                                   93\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:08:04.380005 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 94 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000575166\r\n",
      "trainer/QF2 Loss                                         0.00141247\r\n",
      "trainer/Policy Loss                                      2.90604\r\n",
      "trainer/Q1 Predictions Mean                             -1.06254\r\n",
      "trainer/Q1 Predictions Std                               0.869546\r\n",
      "trainer/Q1 Predictions Max                               0.88482\r\n",
      "trainer/Q1 Predictions Min                              -4.19677\r\n",
      "trainer/Q2 Predictions Mean                             -1.05963\r\n",
      "trainer/Q2 Predictions Std                               0.868019\r\n",
      "trainer/Q2 Predictions Max                               0.843896\r\n",
      "trainer/Q2 Predictions Min                              -4.16778\r\n",
      "trainer/Q Targets Mean                                  -1.06553\r\n",
      "trainer/Q Targets Std                                    0.866299\r\n",
      "trainer/Q Targets Max                                    0.872233\r\n",
      "trainer/Q Targets Min                                   -4.21349\r\n",
      "trainer/Log Pis Mean                                     1.88531\r\n",
      "trainer/Log Pis Std                                      1.38779\r\n",
      "trainer/Log Pis Max                                      4.79531\r\n",
      "trainer/Log Pis Min                                     -2.008\r\n",
      "trainer/Policy mu Mean                                   0.0300278\r\n",
      "trainer/Policy mu Std                                    0.53769\r\n",
      "trainer/Policy mu Max                                    2.46504\r\n",
      "trainer/Policy mu Min                                   -3.38\r\n",
      "trainer/Policy log std Mean                             -2.1591\r\n",
      "trainer/Policy log std Std                               0.761353\r\n",
      "trainer/Policy log std Max                               0.350627\r\n",
      "trainer/Policy log std Min                              -3.52934\r\n",
      "trainer/Alpha                                            0.0184449\r\n",
      "trainer/Alpha Loss                                      -0.457943\r\n",
      "exploration/num steps total                          10500\r\n",
      "exploration/num paths total                            525\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0172575\r\n",
      "exploration/Rewards Std                                  0.0946582\r\n",
      "exploration/Rewards Max                                  0.135774\r\n",
      "exploration/Rewards Min                                 -0.351568\r\n",
      "exploration/Returns Mean                                -0.345149\r\n",
      "exploration/Returns Std                                  1.4798\r\n",
      "exploration/Returns Max                                  1.61737\r\n",
      "exploration/Returns Min                                 -2.95785\r\n",
      "exploration/Actions Mean                                 0.00239823\r\n",
      "exploration/Actions Std                                  0.165584\r\n",
      "exploration/Actions Max                                  0.913645\r\n",
      "exploration/Actions Min                                 -0.555215\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.345149\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.362632\r\n",
      "exploration/env_infos/final/reward_dist Std              0.309477\r\n",
      "exploration/env_infos/final/reward_dist Max              0.862452\r\n",
      "exploration/env_infos/final/reward_dist Min              1.2122e-11\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00562342\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00402658\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0102902\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.9579e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.413632\r\n",
      "exploration/env_infos/reward_dist Std                    0.357234\r\n",
      "exploration/env_infos/reward_dist Max                    0.999367\r\n",
      "exploration/env_infos/reward_dist Min                    1.2122e-11\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.136633\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0902252\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0414426\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.288889\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.377781\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.41088\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0268315\r\n",
      "exploration/env_infos/initial/reward_energy Min         -1.14064\r\n",
      "exploration/env_infos/reward_energy Mean                -0.171077\r\n",
      "exploration/env_infos/reward_energy Std                  0.159938\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0146434\r\n",
      "exploration/env_infos/reward_energy Min                 -1.14064\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.121866\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.18381\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.478936\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.205619\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0109828\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0163953\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0456822\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00591066\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0761319\r\n",
      "exploration/env_infos/end_effector_loc Std               0.133831\r\n",
      "exploration/env_infos/end_effector_loc Max               0.478936\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.205619\r\n",
      "evaluation/num steps total                           95000\r\n",
      "evaluation/num paths total                            4750\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0383952\r\n",
      "evaluation/Rewards Std                                   0.0730915\r\n",
      "evaluation/Rewards Max                                   0.163157\r\n",
      "evaluation/Rewards Min                                  -0.305742\r\n",
      "evaluation/Returns Mean                                 -0.767904\r\n",
      "evaluation/Returns Std                                   1.11961\r\n",
      "evaluation/Returns Max                                   1.63791\r\n",
      "evaluation/Returns Min                                  -3.29782\r\n",
      "evaluation/Actions Mean                                  0.00531959\r\n",
      "evaluation/Actions Std                                   0.0718794\r\n",
      "evaluation/Actions Max                                   0.940012\r\n",
      "evaluation/Actions Min                                  -0.614148\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.767904\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.244458\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.330343\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.999125\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.5093e-30\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0112611\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0201979\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.089424\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.82273e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.220571\r\n",
      "evaluation/env_infos/reward_dist Std                     0.290503\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999125\r\n",
      "evaluation/env_infos/reward_dist Min                     2.5093e-30\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.032574\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0324394\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00354249\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.193749\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.264577\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.236459\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00811426\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.955299\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0581937\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0836862\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000311058\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.955299\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0693796\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.216363\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.685141\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.338513\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00126381\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0124818\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0470006\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0307074\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0346644\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.144093\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.685141\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.338513\r\n",
      "time/data storing (s)                                    0.00655697\r\n",
      "time/evaluation sampling (s)                             0.898263\r\n",
      "time/exploration sampling (s)                            0.122426\r\n",
      "time/logging (s)                                         0.0209988\r\n",
      "time/saving (s)                                          0.0279227\r\n",
      "time/training (s)                                       50.0855\r\n",
      "time/epoch (s)                                          51.1617\r\n",
      "time/total (s)                                        4765.98\r\n",
      "Epoch                                                   94\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:09:02.808405 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 95 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00150455\r\n",
      "trainer/QF2 Loss                                         0.00171364\r\n",
      "trainer/Policy Loss                                      3.1628\r\n",
      "trainer/Q1 Predictions Mean                             -1.12038\r\n",
      "trainer/Q1 Predictions Std                               0.905319\r\n",
      "trainer/Q1 Predictions Max                               0.617779\r\n",
      "trainer/Q1 Predictions Min                              -3.98002\r\n",
      "trainer/Q2 Predictions Mean                             -1.12686\r\n",
      "trainer/Q2 Predictions Std                               0.911765\r\n",
      "trainer/Q2 Predictions Max                               0.611186\r\n",
      "trainer/Q2 Predictions Min                              -3.89522\r\n",
      "trainer/Q Targets Mean                                  -1.11254\r\n",
      "trainer/Q Targets Std                                    0.903717\r\n",
      "trainer/Q Targets Max                                    0.661685\r\n",
      "trainer/Q Targets Min                                   -3.8289\r\n",
      "trainer/Log Pis Mean                                     2.07918\r\n",
      "trainer/Log Pis Std                                      1.48106\r\n",
      "trainer/Log Pis Max                                      5.83534\r\n",
      "trainer/Log Pis Min                                     -5.82418\r\n",
      "trainer/Policy mu Mean                                   0.0225377\r\n",
      "trainer/Policy mu Std                                    0.509012\r\n",
      "trainer/Policy mu Max                                    1.72659\r\n",
      "trainer/Policy mu Min                                   -2.65173\r\n",
      "trainer/Policy log std Mean                             -2.23232\r\n",
      "trainer/Policy log std Std                               0.730752\r\n",
      "trainer/Policy log std Max                              -0.344891\r\n",
      "trainer/Policy log std Min                              -3.51284\r\n",
      "trainer/Alpha                                            0.0183444\r\n",
      "trainer/Alpha Loss                                       0.316729\r\n",
      "exploration/num steps total                          10600\r\n",
      "exploration/num paths total                            530\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0567882\r\n",
      "exploration/Rewards Std                                  0.0724923\r\n",
      "exploration/Rewards Max                                  0.0563545\r\n",
      "exploration/Rewards Min                                 -0.397578\r\n",
      "exploration/Returns Mean                                -1.13576\r\n",
      "exploration/Returns Std                                  0.799289\r\n",
      "exploration/Returns Max                                  0.0465602\r\n",
      "exploration/Returns Min                                 -1.89012\r\n",
      "exploration/Actions Mean                                 0.00396645\r\n",
      "exploration/Actions Std                                  0.166481\r\n",
      "exploration/Actions Max                                  0.793514\r\n",
      "exploration/Actions Min                                 -0.525175\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.13576\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.25648\r\n",
      "exploration/env_infos/final/reward_dist Std              0.275651\r\n",
      "exploration/env_infos/final/reward_dist Max              0.698508\r\n",
      "exploration/env_infos/final/reward_dist Min              0.000211249\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00572162\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00856935\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0221821\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.73739e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.234048\r\n",
      "exploration/env_infos/reward_dist Std                    0.229953\r\n",
      "exploration/env_infos/reward_dist Max                    0.909768\r\n",
      "exploration/env_infos/reward_dist Min                    3.73739e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.114499\r\n",
      "exploration/env_infos/final/reward_energy Std            0.067706\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.029226\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.228797\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.534307\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.235819\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.320975\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.927839\r\n",
      "exploration/env_infos/reward_energy Mean                -0.181105\r\n",
      "exploration/env_infos/reward_energy Std                  0.150548\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0190372\r\n",
      "exploration/env_infos/reward_energy Min                 -0.927839\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0892881\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.198012\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.209681\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.384362\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00040517\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0206447\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0396757\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0220526\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0476102\r\n",
      "exploration/env_infos/end_effector_loc Std               0.157341\r\n",
      "exploration/env_infos/end_effector_loc Max               0.248043\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.384362\r\n",
      "evaluation/num steps total                           96000\r\n",
      "evaluation/num paths total                            4800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0373031\r\n",
      "evaluation/Rewards Std                                   0.0763979\r\n",
      "evaluation/Rewards Max                                   0.142775\r\n",
      "evaluation/Rewards Min                                  -0.390081\r\n",
      "evaluation/Returns Mean                                 -0.746062\r\n",
      "evaluation/Returns Std                                   1.10995\r\n",
      "evaluation/Returns Max                                   2.01122\r\n",
      "evaluation/Returns Min                                  -3.67219\r\n",
      "evaluation/Actions Mean                                  0.00326278\r\n",
      "evaluation/Actions Std                                   0.0818462\r\n",
      "evaluation/Actions Max                                   0.843851\r\n",
      "evaluation/Actions Min                                  -0.745022\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.746062\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.197377\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.272315\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.878838\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.41335e-20\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00691069\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117136\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0479428\r\n",
      "evaluation/env_infos/initial/reward_dist Min             4.30124e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.217883\r\n",
      "evaluation/env_infos/reward_dist Std                     0.271134\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999407\r\n",
      "evaluation/env_infos/reward_dist Min                     6.41335e-20\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0296439\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0247428\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00256603\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.115229\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.351727\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.253632\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0168715\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.972023\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0621754\r\n",
      "evaluation/env_infos/reward_energy Std                   0.09774\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000525087\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.972023\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0124072\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.232332\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.517792\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.58924\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000714163\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0153147\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0421926\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0372511\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0130597\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.160262\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.517792\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.58924\r\n",
      "time/data storing (s)                                    0.00629272\r\n",
      "time/evaluation sampling (s)                             0.974577\r\n",
      "time/exploration sampling (s)                            0.145661\r\n",
      "time/logging (s)                                         0.0195828\r\n",
      "time/saving (s)                                          0.026886\r\n",
      "time/training (s)                                       56.0145\r\n",
      "time/epoch (s)                                          57.1875\r\n",
      "time/total (s)                                        4824.41\r\n",
      "Epoch                                                   95\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:09:54.155729 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 96 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000883497\n",
      "trainer/QF2 Loss                                         0.000841947\n",
      "trainer/Policy Loss                                      3.37279\n",
      "trainer/Q1 Predictions Mean                             -1.19661\n",
      "trainer/Q1 Predictions Std                               0.899509\n",
      "trainer/Q1 Predictions Max                               0.407772\n",
      "trainer/Q1 Predictions Min                              -4.45912\n",
      "trainer/Q2 Predictions Mean                             -1.19789\n",
      "trainer/Q2 Predictions Std                               0.899174\n",
      "trainer/Q2 Predictions Max                               0.440796\n",
      "trainer/Q2 Predictions Min                              -4.47851\n",
      "trainer/Q Targets Mean                                  -1.1978\n",
      "trainer/Q Targets Std                                    0.900722\n",
      "trainer/Q Targets Max                                    0.438111\n",
      "trainer/Q Targets Min                                   -4.40732\n",
      "trainer/Log Pis Mean                                     2.23425\n",
      "trainer/Log Pis Std                                      1.32238\n",
      "trainer/Log Pis Max                                      4.7751\n",
      "trainer/Log Pis Min                                     -3.32216\n",
      "trainer/Policy mu Mean                                  -0.00471572\n",
      "trainer/Policy mu Std                                    0.479822\n",
      "trainer/Policy mu Max                                    1.89065\n",
      "trainer/Policy mu Min                                   -2.31487\n",
      "trainer/Policy log std Mean                             -2.30655\n",
      "trainer/Policy log std Std                               0.731435\n",
      "trainer/Policy log std Max                              -0.315712\n",
      "trainer/Policy log std Min                              -3.66109\n",
      "trainer/Alpha                                            0.0185629\n",
      "trainer/Alpha Loss                                       0.934254\n",
      "exploration/num steps total                          10700\n",
      "exploration/num paths total                            535\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.115735\n",
      "exploration/Rewards Std                                  0.0810013\n",
      "exploration/Rewards Max                                  0.0281302\n",
      "exploration/Rewards Min                                 -0.290902\n",
      "exploration/Returns Mean                                -2.31471\n",
      "exploration/Returns Std                                  1.3228\n",
      "exploration/Returns Max                                 -0.692794\n",
      "exploration/Returns Min                                 -3.88908\n",
      "exploration/Actions Mean                                 0.0046468\n",
      "exploration/Actions Std                                  0.109443\n",
      "exploration/Actions Max                                  0.396446\n",
      "exploration/Actions Min                                 -0.390406\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.31471\n",
      "exploration/env_infos/final/reward_dist Mean             0.0984841\n",
      "exploration/env_infos/final/reward_dist Std              0.157248\n",
      "exploration/env_infos/final/reward_dist Max              0.410846\n",
      "exploration/env_infos/final/reward_dist Min              3.45943e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000909593\n",
      "exploration/env_infos/initial/reward_dist Std            0.00106688\n",
      "exploration/env_infos/initial/reward_dist Max            0.00286898\n",
      "exploration/env_infos/initial/reward_dist Min            9.06045e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.125656\n",
      "exploration/env_infos/reward_dist Std                    0.200791\n",
      "exploration/env_infos/reward_dist Max                    0.930638\n",
      "exploration/env_infos/reward_dist Min                    3.45943e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.102377\n",
      "exploration/env_infos/final/reward_energy Std            0.0966614\n",
      "exploration/env_infos/final/reward_energy Max           -0.0111978\n",
      "exploration/env_infos/final/reward_energy Min           -0.290041\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.250458\n",
      "exploration/env_infos/initial/reward_energy Std          0.197926\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0256638\n",
      "exploration/env_infos/initial/reward_energy Min         -0.517304\n",
      "exploration/env_infos/reward_energy Mean                -0.117453\n",
      "exploration/env_infos/reward_energy Std                  0.101013\n",
      "exploration/env_infos/reward_energy Max                 -0.00456853\n",
      "exploration/env_infos/reward_energy Min                 -0.517304\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0657797\n",
      "exploration/env_infos/final/end_effector_loc Std         0.26195\n",
      "exploration/env_infos/final/end_effector_loc Max         0.429042\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.615995\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00428533\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0104411\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0198223\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0114497\n",
      "exploration/env_infos/end_effector_loc Mean              0.0390457\n",
      "exploration/env_infos/end_effector_loc Std               0.170559\n",
      "exploration/env_infos/end_effector_loc Max               0.429042\n",
      "exploration/env_infos/end_effector_loc Min              -0.615995\n",
      "evaluation/num steps total                           97000\n",
      "evaluation/num paths total                            4850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0443671\n",
      "evaluation/Rewards Std                                   0.0725628\n",
      "evaluation/Rewards Max                                   0.16074\n",
      "evaluation/Rewards Min                                  -0.528982\n",
      "evaluation/Returns Mean                                 -0.887343\n",
      "evaluation/Returns Std                                   1.12241\n",
      "evaluation/Returns Max                                   1.92782\n",
      "evaluation/Returns Min                                  -3.04686\n",
      "evaluation/Actions Mean                                  0.000858705\n",
      "evaluation/Actions Std                                   0.074789\n",
      "evaluation/Actions Max                                   0.920749\n",
      "evaluation/Actions Min                                  -0.750138\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.887343\n",
      "evaluation/env_infos/final/reward_dist Mean              0.228429\n",
      "evaluation/env_infos/final/reward_dist Std               0.29801\n",
      "evaluation/env_infos/final/reward_dist Max               0.993791\n",
      "evaluation/env_infos/final/reward_dist Min               1.76441e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00518622\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00780548\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0355283\n",
      "evaluation/env_infos/initial/reward_dist Min             1.82504e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.210347\n",
      "evaluation/env_infos/reward_dist Std                     0.272978\n",
      "evaluation/env_infos/reward_dist Max                     0.993791\n",
      "evaluation/env_infos/reward_dist Min                     1.76441e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0303496\n",
      "evaluation/env_infos/final/reward_energy Std             0.0273852\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00283179\n",
      "evaluation/env_infos/final/reward_energy Min            -0.131541\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.287943\n",
      "evaluation/env_infos/initial/reward_energy Std           0.248825\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0303032\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.03495\n",
      "evaluation/env_infos/reward_energy Mean                 -0.057992\n",
      "evaluation/env_infos/reward_energy Std                   0.0884601\n",
      "evaluation/env_infos/reward_energy Max                  -0.000460406\n",
      "evaluation/env_infos/reward_energy Min                  -1.03495\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0129032\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23056\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.512697\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.508846\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000316935\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134511\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0460375\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0375069\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00767605\n",
      "evaluation/env_infos/end_effector_loc Std                0.157321\n",
      "evaluation/env_infos/end_effector_loc Max                0.512697\n",
      "evaluation/env_infos/end_effector_loc Min               -0.508846\n",
      "time/data storing (s)                                    0.0060773\n",
      "time/evaluation sampling (s)                             1.04454\n",
      "time/exploration sampling (s)                            0.124235\n",
      "time/logging (s)                                         0.019913\n",
      "time/saving (s)                                          0.0266915\n",
      "time/training (s)                                       48.8824\n",
      "time/epoch (s)                                          50.1039\n",
      "time/total (s)                                        4875.75\n",
      "Epoch                                                   96\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:10:44.981476 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 97 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00243217\r\n",
      "trainer/QF2 Loss                                         0.00107363\r\n",
      "trainer/Policy Loss                                      3.1433\r\n",
      "trainer/Q1 Predictions Mean                             -1.1494\r\n",
      "trainer/Q1 Predictions Std                               0.899417\r\n",
      "trainer/Q1 Predictions Max                               0.498959\r\n",
      "trainer/Q1 Predictions Min                              -3.47488\r\n",
      "trainer/Q2 Predictions Mean                             -1.15529\r\n",
      "trainer/Q2 Predictions Std                               0.894786\r\n",
      "trainer/Q2 Predictions Max                               0.51023\r\n",
      "trainer/Q2 Predictions Min                              -3.49772\r\n",
      "trainer/Q Targets Mean                                  -1.14702\r\n",
      "trainer/Q Targets Std                                    0.894284\r\n",
      "trainer/Q Targets Max                                    0.499111\r\n",
      "trainer/Q Targets Min                                   -3.47578\r\n",
      "trainer/Log Pis Mean                                     2.02343\r\n",
      "trainer/Log Pis Std                                      1.41223\r\n",
      "trainer/Log Pis Max                                      5.02056\r\n",
      "trainer/Log Pis Min                                     -2.8398\r\n",
      "trainer/Policy mu Mean                                   0.0378364\r\n",
      "trainer/Policy mu Std                                    0.463256\r\n",
      "trainer/Policy mu Max                                    1.99209\r\n",
      "trainer/Policy mu Min                                   -1.8005\r\n",
      "trainer/Policy log std Mean                             -2.23187\r\n",
      "trainer/Policy log std Std                               0.726524\r\n",
      "trainer/Policy log std Max                              -0.210703\r\n",
      "trainer/Policy log std Min                              -3.55341\r\n",
      "trainer/Alpha                                            0.0200249\r\n",
      "trainer/Alpha Loss                                       0.0915984\r\n",
      "exploration/num steps total                          10800\r\n",
      "exploration/num paths total                            540\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0818915\r\n",
      "exploration/Rewards Std                                  0.057921\r\n",
      "exploration/Rewards Max                                  0.00999299\r\n",
      "exploration/Rewards Min                                 -0.489532\r\n",
      "exploration/Returns Mean                                -1.63783\r\n",
      "exploration/Returns Std                                  0.57\r\n",
      "exploration/Returns Max                                 -0.918101\r\n",
      "exploration/Returns Min                                 -2.19039\r\n",
      "exploration/Actions Mean                                -0.00106122\r\n",
      "exploration/Actions Std                                  0.0925317\r\n",
      "exploration/Actions Max                                  0.636148\r\n",
      "exploration/Actions Min                                 -0.377055\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.63783\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0238632\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0247944\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0677638\r\n",
      "exploration/env_infos/final/reward_dist Min              3.89459e-13\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00709544\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0066611\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0169295\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000703665\r\n",
      "exploration/env_infos/reward_dist Mean                   0.17143\r\n",
      "exploration/env_infos/reward_dist Std                    0.27067\r\n",
      "exploration/env_infos/reward_dist Max                    0.953414\r\n",
      "exploration/env_infos/reward_dist Min                    3.89459e-13\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0635346\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0066821\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0544741\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.0694498\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.241331\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.230022\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0630128\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.666893\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0947653\r\n",
      "exploration/env_infos/reward_energy Std                  0.0902552\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00208586\r\n",
      "exploration/env_infos/reward_energy Min                 -0.666893\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.033052\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.254081\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.392388\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.530112\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00317792\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0113507\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0318074\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0100079\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0155677\r\n",
      "exploration/env_infos/end_effector_loc Std               0.166183\r\n",
      "exploration/env_infos/end_effector_loc Max               0.392388\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.530112\r\n",
      "evaluation/num steps total                           98000\r\n",
      "evaluation/num paths total                            4900\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0454009\r\n",
      "evaluation/Rewards Std                                   0.0754837\r\n",
      "evaluation/Rewards Max                                   0.167728\r\n",
      "evaluation/Rewards Min                                  -0.352284\r\n",
      "evaluation/Returns Mean                                 -0.908019\r\n",
      "evaluation/Returns Std                                   1.21583\r\n",
      "evaluation/Returns Max                                   2.26575\r\n",
      "evaluation/Returns Min                                  -3.81297\r\n",
      "evaluation/Actions Mean                                  0.00136363\r\n",
      "evaluation/Actions Std                                   0.0766206\r\n",
      "evaluation/Actions Max                                   0.792495\r\n",
      "evaluation/Actions Min                                  -0.779216\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.908019\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.286075\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.313702\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.993061\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.60753e-14\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00707156\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0122914\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0536082\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.4641e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.221913\r\n",
      "evaluation/env_infos/reward_dist Std                     0.286642\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993061\r\n",
      "evaluation/env_infos/reward_dist Min                     1.22722e-15\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0324604\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0399525\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000605197\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.222632\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.26703\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.256598\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00738408\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.960393\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0597438\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0904202\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000418231\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.960393\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0135372\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.202409\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.472529\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.432194\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000944507\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0130592\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0396248\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0389608\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0101462\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.142761\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.472529\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.432194\r\n",
      "time/data storing (s)                                    0.0061194\r\n",
      "time/evaluation sampling (s)                             0.990104\r\n",
      "time/exploration sampling (s)                            0.117125\r\n",
      "time/logging (s)                                         0.0196509\r\n",
      "time/saving (s)                                          0.0269158\r\n",
      "time/training (s)                                       48.4003\r\n",
      "time/epoch (s)                                          49.5602\r\n",
      "time/total (s)                                        4926.57\r\n",
      "Epoch                                                   97\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:11:35.887616 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 98 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00144975\n",
      "trainer/QF2 Loss                                         0.00172845\n",
      "trainer/Policy Loss                                      3.07361\n",
      "trainer/Q1 Predictions Mean                             -1.22053\n",
      "trainer/Q1 Predictions Std                               0.900333\n",
      "trainer/Q1 Predictions Max                               0.617642\n",
      "trainer/Q1 Predictions Min                              -4.47952\n",
      "trainer/Q2 Predictions Mean                             -1.20952\n",
      "trainer/Q2 Predictions Std                               0.897617\n",
      "trainer/Q2 Predictions Max                               0.636625\n",
      "trainer/Q2 Predictions Min                              -4.46961\n",
      "trainer/Q Targets Mean                                  -1.21595\n",
      "trainer/Q Targets Std                                    0.900843\n",
      "trainer/Q Targets Max                                    0.640196\n",
      "trainer/Q Targets Min                                   -4.45638\n",
      "trainer/Log Pis Mean                                     1.913\n",
      "trainer/Log Pis Std                                      1.42197\n",
      "trainer/Log Pis Max                                      4.4241\n",
      "trainer/Log Pis Min                                     -5.29478\n",
      "trainer/Policy mu Mean                                   0.0733059\n",
      "trainer/Policy mu Std                                    0.485325\n",
      "trainer/Policy mu Max                                    1.98306\n",
      "trainer/Policy mu Min                                   -1.9008\n",
      "trainer/Policy log std Mean                             -2.21569\n",
      "trainer/Policy log std Std                               0.699978\n",
      "trainer/Policy log std Max                              -0.423967\n",
      "trainer/Policy log std Min                              -3.32254\n",
      "trainer/Alpha                                            0.0217272\n",
      "trainer/Alpha Loss                                      -0.333201\n",
      "exploration/num steps total                          10900\n",
      "exploration/num paths total                            545\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0969706\n",
      "exploration/Rewards Std                                  0.067214\n",
      "exploration/Rewards Max                                  0.018034\n",
      "exploration/Rewards Min                                 -0.371506\n",
      "exploration/Returns Mean                                -1.93941\n",
      "exploration/Returns Std                                  0.421089\n",
      "exploration/Returns Max                                 -1.24701\n",
      "exploration/Returns Min                                 -2.29355\n",
      "exploration/Actions Mean                                 0.0032202\n",
      "exploration/Actions Std                                  0.120697\n",
      "exploration/Actions Max                                  0.726185\n",
      "exploration/Actions Min                                 -0.460558\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.93941\n",
      "exploration/env_infos/final/reward_dist Mean             0.0837498\n",
      "exploration/env_infos/final/reward_dist Std              0.115326\n",
      "exploration/env_infos/final/reward_dist Max              0.293262\n",
      "exploration/env_infos/final/reward_dist Min              1.15959e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0110637\n",
      "exploration/env_infos/initial/reward_dist Std            0.0170339\n",
      "exploration/env_infos/initial/reward_dist Max            0.0447221\n",
      "exploration/env_infos/initial/reward_dist Min            4.72197e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.179506\n",
      "exploration/env_infos/reward_dist Std                    0.287915\n",
      "exploration/env_infos/reward_dist Max                    0.997792\n",
      "exploration/env_infos/reward_dist Min                    1.15959e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.187537\n",
      "exploration/env_infos/final/reward_energy Std            0.0668322\n",
      "exploration/env_infos/final/reward_energy Max           -0.0671042\n",
      "exploration/env_infos/final/reward_energy Min           -0.250447\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.402983\n",
      "exploration/env_infos/initial/reward_energy Std          0.227197\n",
      "exploration/env_infos/initial/reward_energy Max         -0.101441\n",
      "exploration/env_infos/initial/reward_energy Min         -0.732578\n",
      "exploration/env_infos/reward_energy Mean                -0.120822\n",
      "exploration/env_infos/reward_energy Std                  0.120657\n",
      "exploration/env_infos/reward_energy Max                 -0.00788197\n",
      "exploration/env_infos/reward_energy Min                 -0.732578\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.19879\n",
      "exploration/env_infos/final/end_effector_loc Std         0.244863\n",
      "exploration/env_infos/final/end_effector_loc Max         0.517868\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.269374\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0095715\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0132629\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0363093\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00682692\n",
      "exploration/env_infos/end_effector_loc Mean              0.11517\n",
      "exploration/env_infos/end_effector_loc Std               0.169987\n",
      "exploration/env_infos/end_effector_loc Max               0.517868\n",
      "exploration/env_infos/end_effector_loc Min              -0.269374\n",
      "evaluation/num steps total                           99000\n",
      "evaluation/num paths total                            4950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0426496\n",
      "evaluation/Rewards Std                                   0.0714846\n",
      "evaluation/Rewards Max                                   0.122247\n",
      "evaluation/Rewards Min                                  -0.34042\n",
      "evaluation/Returns Mean                                 -0.852991\n",
      "evaluation/Returns Std                                   1.05748\n",
      "evaluation/Returns Max                                   1.04957\n",
      "evaluation/Returns Min                                  -2.99592\n",
      "evaluation/Actions Mean                                  0.00467506\n",
      "evaluation/Actions Std                                   0.0609315\n",
      "evaluation/Actions Max                                   0.826764\n",
      "evaluation/Actions Min                                  -0.556945\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.852991\n",
      "evaluation/env_infos/final/reward_dist Mean              0.242709\n",
      "evaluation/env_infos/final/reward_dist Std               0.323032\n",
      "evaluation/env_infos/final/reward_dist Max               0.892938\n",
      "evaluation/env_infos/final/reward_dist Min               7.33519e-45\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00477655\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00866049\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0349819\n",
      "evaluation/env_infos/initial/reward_dist Min             1.05462e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.219155\n",
      "evaluation/env_infos/reward_dist Std                     0.301018\n",
      "evaluation/env_infos/reward_dist Max                     0.997626\n",
      "evaluation/env_infos/reward_dist Min                     7.33519e-45\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0310761\n",
      "evaluation/env_infos/final/reward_energy Std             0.0256573\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00387819\n",
      "evaluation/env_infos/final/reward_energy Min            -0.112876\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.231871\n",
      "evaluation/env_infos/initial/reward_energy Std           0.173976\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0265547\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.883941\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0506253\n",
      "evaluation/env_infos/reward_energy Std                   0.0700434\n",
      "evaluation/env_infos/reward_energy Max                  -0.00163092\n",
      "evaluation/env_infos/reward_energy Min                  -0.883941\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0422702\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.227213\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.588666\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.387873\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000637238\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0102291\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0413382\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0278473\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0145428\n",
      "evaluation/env_infos/end_effector_loc Std                0.148714\n",
      "evaluation/env_infos/end_effector_loc Max                0.588666\n",
      "evaluation/env_infos/end_effector_loc Min               -0.387873\n",
      "time/data storing (s)                                    0.00616116\n",
      "time/evaluation sampling (s)                             1.04534\n",
      "time/exploration sampling (s)                            0.119852\n",
      "time/logging (s)                                         0.0200074\n",
      "time/saving (s)                                          0.0270994\n",
      "time/training (s)                                       48.433\n",
      "time/epoch (s)                                          49.6515\n",
      "time/total (s)                                        4977.48\n",
      "Epoch                                                   98\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:12:26.112880 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10] Epoch 99 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.00177027\n",
      "trainer/QF2 Loss                                          0.0015849\n",
      "trainer/Policy Loss                                       3.21083\n",
      "trainer/Q1 Predictions Mean                              -1.14515\n",
      "trainer/Q1 Predictions Std                                0.921126\n",
      "trainer/Q1 Predictions Max                                0.798337\n",
      "trainer/Q1 Predictions Min                               -3.2957\n",
      "trainer/Q2 Predictions Mean                              -1.14952\n",
      "trainer/Q2 Predictions Std                                0.921889\n",
      "trainer/Q2 Predictions Max                                0.78482\n",
      "trainer/Q2 Predictions Min                               -3.31543\n",
      "trainer/Q Targets Mean                                   -1.16268\n",
      "trainer/Q Targets Std                                     0.918443\n",
      "trainer/Q Targets Max                                     0.758549\n",
      "trainer/Q Targets Min                                    -3.33608\n",
      "trainer/Log Pis Mean                                      2.11084\n",
      "trainer/Log Pis Std                                       1.45191\n",
      "trainer/Log Pis Max                                       4.66329\n",
      "trainer/Log Pis Min                                      -2.42879\n",
      "trainer/Policy mu Mean                                   -0.00858343\n",
      "trainer/Policy mu Std                                     0.469079\n",
      "trainer/Policy mu Max                                     1.8055\n",
      "trainer/Policy mu Min                                    -1.96514\n",
      "trainer/Policy log std Mean                              -2.26178\n",
      "trainer/Policy log std Std                                0.735147\n",
      "trainer/Policy log std Max                               -0.390337\n",
      "trainer/Policy log std Min                               -3.48512\n",
      "trainer/Alpha                                             0.0209694\n",
      "trainer/Alpha Loss                                        0.428439\n",
      "exploration/num steps total                           11000\n",
      "exploration/num paths total                             550\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0762588\n",
      "exploration/Rewards Std                                   0.0612162\n",
      "exploration/Rewards Max                                   0.0530134\n",
      "exploration/Rewards Min                                  -0.226374\n",
      "exploration/Returns Mean                                 -1.52518\n",
      "exploration/Returns Std                                   0.875395\n",
      "exploration/Returns Max                                  -0.610133\n",
      "exploration/Returns Min                                  -3.01908\n",
      "exploration/Actions Mean                                  0.00486841\n",
      "exploration/Actions Std                                   0.0970521\n",
      "exploration/Actions Max                                   0.317694\n",
      "exploration/Actions Min                                  -0.259397\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.52518\n",
      "exploration/env_infos/final/reward_dist Mean              0.171622\n",
      "exploration/env_infos/final/reward_dist Std               0.28904\n",
      "exploration/env_infos/final/reward_dist Max               0.746786\n",
      "exploration/env_infos/final/reward_dist Min               1.78777e-05\n",
      "exploration/env_infos/initial/reward_dist Mean            0.0056444\n",
      "exploration/env_infos/initial/reward_dist Std             0.00569794\n",
      "exploration/env_infos/initial/reward_dist Max             0.0127317\n",
      "exploration/env_infos/initial/reward_dist Min             2.51299e-05\n",
      "exploration/env_infos/reward_dist Mean                    0.21271\n",
      "exploration/env_infos/reward_dist Std                     0.274826\n",
      "exploration/env_infos/reward_dist Max                     0.982735\n",
      "exploration/env_infos/reward_dist Min                     1.58086e-06\n",
      "exploration/env_infos/final/reward_energy Mean           -0.127158\n",
      "exploration/env_infos/final/reward_energy Std             0.0940162\n",
      "exploration/env_infos/final/reward_energy Max            -0.0178786\n",
      "exploration/env_infos/final/reward_energy Min            -0.286261\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.201827\n",
      "exploration/env_infos/initial/reward_energy Std           0.0873548\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0946978\n",
      "exploration/env_infos/initial/reward_energy Min          -0.339247\n",
      "exploration/env_infos/reward_energy Mean                 -0.112314\n",
      "exploration/env_infos/reward_energy Std                   0.0791908\n",
      "exploration/env_infos/reward_energy Max                  -0.00653273\n",
      "exploration/env_infos/reward_energy Min                  -0.339247\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.0429079\n",
      "exploration/env_infos/final/end_effector_loc Std          0.205713\n",
      "exploration/env_infos/final/end_effector_loc Max          0.329409\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.286246\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.000790367\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00773508\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0153058\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0101638\n",
      "exploration/env_infos/end_effector_loc Mean               0.0191813\n",
      "exploration/env_infos/end_effector_loc Std                0.133763\n",
      "exploration/env_infos/end_effector_loc Max                0.329409\n",
      "exploration/env_infos/end_effector_loc Min               -0.286246\n",
      "evaluation/num steps total                           100000\n",
      "evaluation/num paths total                             5000\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0368138\n",
      "evaluation/Rewards Std                                    0.0791709\n",
      "evaluation/Rewards Max                                    0.165293\n",
      "evaluation/Rewards Min                                   -0.46463\n",
      "evaluation/Returns Mean                                  -0.736276\n",
      "evaluation/Returns Std                                    1.08921\n",
      "evaluation/Returns Max                                    2.73977\n",
      "evaluation/Returns Min                                   -2.90433\n",
      "evaluation/Actions Mean                                  -0.00278925\n",
      "evaluation/Actions Std                                    0.0810519\n",
      "evaluation/Actions Max                                    0.954338\n",
      "evaluation/Actions Min                                   -0.521498\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.736276\n",
      "evaluation/env_infos/final/reward_dist Mean               0.233475\n",
      "evaluation/env_infos/final/reward_dist Std                0.298353\n",
      "evaluation/env_infos/final/reward_dist Max                0.998135\n",
      "evaluation/env_infos/final/reward_dist Min                5.12212e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00749168\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0136073\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0582328\n",
      "evaluation/env_infos/initial/reward_dist Min              1.19151e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.251221\n",
      "evaluation/env_infos/reward_dist Std                      0.295792\n",
      "evaluation/env_infos/reward_dist Max                      0.998135\n",
      "evaluation/env_infos/reward_dist Min                      5.12212e-18\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0324157\n",
      "evaluation/env_infos/final/reward_energy Std              0.0330843\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00641784\n",
      "evaluation/env_infos/final/reward_energy Min             -0.192526\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.3341\n",
      "evaluation/env_infos/initial/reward_energy Std            0.227867\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0284777\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.956471\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0658236\n",
      "evaluation/env_infos/reward_energy Std                    0.0939235\n",
      "evaluation/env_infos/reward_energy Max                   -0.000473759\n",
      "evaluation/env_infos/reward_energy Min                   -0.956471\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0650979\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.257978\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.577137\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.766615\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000134525\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0142974\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0477169\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0260749\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.0305365\n",
      "evaluation/env_infos/end_effector_loc Std                 0.175891\n",
      "evaluation/env_infos/end_effector_loc Max                 0.577137\n",
      "evaluation/env_infos/end_effector_loc Min                -0.766615\n",
      "time/data storing (s)                                     0.0062566\n",
      "time/evaluation sampling (s)                              0.845254\n",
      "time/exploration sampling (s)                             0.125193\n",
      "time/logging (s)                                          0.0196255\n",
      "time/saving (s)                                           0.0292258\n",
      "time/training (s)                                        48.3167\n",
      "time/epoch (s)                                           49.3422\n",
      "time/total (s)                                         5027.7\n",
      "Epoch                                                    99\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[21081]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21c94778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21d15740). One of the two will be used. Which one is undefined.\n",
      "objc[21081]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21c94700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21d15768). One of the two will be used. Which one is undefined.\n",
      "objc[21081]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21c947a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21d157b8). One of the two will be used. Which one is undefined.\n",
      "objc[21081]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21c94818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21d15830). One of the two will be used. Which one is undefined.\n",
      "using sparse reward if specified\n",
      "2021-05-25 13:12:33.181742 PDT | Variant:\n",
      "2021-05-25 13:12:33.182406 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"pointreacherobs\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 20,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 20,\n",
      "    \"num_train_loops_per_epoch\": 5\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.97,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 2000,\n",
      "    \"latent_dim\": 6,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": true,\n",
      "    \"n_sampled_latents\": 100,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"sparse_reward\": null,\n",
      "    \"fixed_ratio\": null\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": true,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"horizon\": 20\n",
      "  }\n",
      "}\n",
      "pointreacher\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "2021-05-25 13:13:06.531896 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 0 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        1.02318\n",
      "trainer/QF2 Loss                                        1.0213\n",
      "trainer/Policy Loss                                    -1.37885\n",
      "trainer/Q1 Predictions Mean                            -0.00123041\n",
      "trainer/Q1 Predictions Std                              0.000628694\n",
      "trainer/Q1 Predictions Max                              0.00059533\n",
      "trainer/Q1 Predictions Min                             -0.00226844\n",
      "trainer/Q2 Predictions Mean                            -5.11984e-05\n",
      "trainer/Q2 Predictions Std                              0.00069396\n",
      "trainer/Q2 Predictions Max                              0.00126331\n",
      "trainer/Q2 Predictions Min                             -0.0017172\n",
      "trainer/Q Targets Mean                                  0.819907\n",
      "trainer/Q Targets Std                                   0.590703\n",
      "trainer/Q Targets Max                                   1.69958\n",
      "trainer/Q Targets Min                                  -1.35869\n",
      "trainer/Log Pis Mean                                   -1.3802\n",
      "trainer/Log Pis Std                                     0.289152\n",
      "trainer/Log Pis Max                                    -0.577523\n",
      "trainer/Log Pis Min                                    -2.60808\n",
      "trainer/Policy mu Mean                                  0.000991679\n",
      "trainer/Policy mu Std                                   0.000561408\n",
      "trainer/Policy mu Max                                   0.00220575\n",
      "trainer/Policy mu Min                                   5.95292e-05\n",
      "trainer/Policy log std Mean                             5.1369e-05\n",
      "trainer/Policy log std Std                              0.000675256\n",
      "trainer/Policy log std Max                              0.000954493\n",
      "trainer/Policy log std Min                             -0.00125532\n",
      "trainer/Alpha                                           0.997005\n",
      "trainer/Alpha Loss                                     -0\n",
      "exploration/num steps total                          1100\n",
      "exploration/num paths total                            55\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.782749\n",
      "exploration/Rewards Std                                 0.347327\n",
      "exploration/Rewards Max                                -0.144178\n",
      "exploration/Rewards Min                                -1.48172\n",
      "exploration/Returns Mean                              -15.655\n",
      "exploration/Returns Std                                 4.11611\n",
      "exploration/Returns Max                               -10.2897\n",
      "exploration/Returns Min                               -21.0432\n",
      "exploration/Actions Mean                                0.0664757\n",
      "exploration/Actions Std                                 0.591084\n",
      "exploration/Actions Max                                 0.98148\n",
      "exploration/Actions Min                                -0.95017\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -15.655\n",
      "exploration/env_infos/final/reward_dist Mean            9.06924e-46\n",
      "exploration/env_infos/final/reward_dist Std             1.81385e-45\n",
      "exploration/env_infos/final/reward_dist Max             4.53462e-45\n",
      "exploration/env_infos/final/reward_dist Min             5.62953e-138\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00583009\n",
      "exploration/env_infos/initial/reward_dist Std           0.0115506\n",
      "exploration/env_infos/initial/reward_dist Max           0.0289309\n",
      "exploration/env_infos/initial/reward_dist Min           2.88528e-08\n",
      "exploration/env_infos/reward_dist Mean                  0.00653357\n",
      "exploration/env_infos/reward_dist Std                   0.032089\n",
      "exploration/env_infos/reward_dist Max                   0.226219\n",
      "exploration/env_infos/reward_dist Min                   5.62953e-138\n",
      "exploration/env_infos/final/reward_energy Mean         -0.804141\n",
      "exploration/env_infos/final/reward_energy Std           0.313001\n",
      "exploration/env_infos/final/reward_energy Max          -0.183826\n",
      "exploration/env_infos/final/reward_energy Min          -1.03901\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.906971\n",
      "exploration/env_infos/initial/reward_energy Std         0.128033\n",
      "exploration/env_infos/initial/reward_energy Max        -0.743948\n",
      "exploration/env_infos/initial/reward_energy Min        -1.11742\n",
      "exploration/env_infos/reward_energy Mean               -0.792445\n",
      "exploration/env_infos/reward_energy Std                 0.282186\n",
      "exploration/env_infos/reward_energy Max                -0.183826\n",
      "exploration/env_infos/reward_energy Min                -1.25848\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.4564\n",
      "exploration/env_infos/final/end_effector_loc Std        0.64723\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.0224551\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0233347\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0474246\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.036915\n",
      "exploration/env_infos/end_effector_loc Mean             0.292628\n",
      "exploration/env_infos/end_effector_loc Std              0.485147\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           1000\n",
      "evaluation/num paths total                             50\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0712633\n",
      "evaluation/Rewards Std                                  0.0475803\n",
      "evaluation/Rewards Max                                  0.0475514\n",
      "evaluation/Rewards Min                                 -0.173638\n",
      "evaluation/Returns Mean                                -1.42527\n",
      "evaluation/Returns Std                                  0.950319\n",
      "evaluation/Returns Max                                  0.788764\n",
      "evaluation/Returns Min                                 -3.44079\n",
      "evaluation/Actions Mean                                 0.000994572\n",
      "evaluation/Actions Std                                  0.000557625\n",
      "evaluation/Actions Max                                  0.00203314\n",
      "evaluation/Actions Min                                  0.000153224\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.42527\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00403968\n",
      "evaluation/env_infos/final/reward_dist Std              0.00768942\n",
      "evaluation/env_infos/final/reward_dist Max              0.0353948\n",
      "evaluation/env_infos/final/reward_dist Min              3.75807e-07\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00313915\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00574652\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0262178\n",
      "evaluation/env_infos/initial/reward_dist Min            7.84187e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.00336929\n",
      "evaluation/env_infos/reward_dist Std                    0.00617529\n",
      "evaluation/env_infos/reward_dist Max                    0.0353948\n",
      "evaluation/env_infos/reward_dist Min                    3.75807e-07\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.00163784\n",
      "evaluation/env_infos/final/reward_energy Std            0.000254649\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00111555\n",
      "evaluation/env_infos/final/reward_energy Min           -0.00207608\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.00155191\n",
      "evaluation/env_infos/initial/reward_energy Std          0.000214815\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00115512\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.00197441\n",
      "evaluation/env_infos/reward_energy Mean                -0.00159509\n",
      "evaluation/env_infos/reward_energy Std                  0.0002365\n",
      "evaluation/env_infos/reward_energy Max                 -0.00111555\n",
      "evaluation/env_infos/reward_energy Min                 -0.00207608\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.0103731\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.00574269\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.0208968\n",
      "evaluation/env_infos/final/end_effector_loc Min         0.00181135\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      4.87051e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std       2.63823e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Max       9.79645e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Min       7.66118e-06\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00379042\n",
      "evaluation/env_infos/end_effector_loc Std               0.00422653\n",
      "evaluation/env_infos/end_effector_loc Max               0.0208968\n",
      "evaluation/env_infos/end_effector_loc Min               7.66118e-06\n",
      "time/data storing (s)                                   0.171095\n",
      "time/evaluation sampling (s)                            0.924758\n",
      "time/exploration sampling (s)                           0.117184\n",
      "time/logging (s)                                        0.0205708\n",
      "time/saving (s)                                         0.0783875\n",
      "time/training (s)                                      28.9167\n",
      "time/epoch (s)                                         30.2287\n",
      "time/total (s)                                         36.647\n",
      "Epoch                                                   0\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:13:45.634294 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 1 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.021879\n",
      "trainer/QF2 Loss                                        0.0270591\n",
      "trainer/Policy Loss                                    -1.45842\n",
      "trainer/Q1 Predictions Mean                             0.132824\n",
      "trainer/Q1 Predictions Std                              0.726922\n",
      "trainer/Q1 Predictions Max                              0.952736\n",
      "trainer/Q1 Predictions Min                             -2.0146\n",
      "trainer/Q2 Predictions Mean                             0.153286\n",
      "trainer/Q2 Predictions Std                              0.718707\n",
      "trainer/Q2 Predictions Max                              1.00797\n",
      "trainer/Q2 Predictions Min                             -1.91428\n",
      "trainer/Q Targets Mean                                  0.105628\n",
      "trainer/Q Targets Std                                   0.733789\n",
      "trainer/Q Targets Max                                   0.976877\n",
      "trainer/Q Targets Min                                  -2.05525\n",
      "trainer/Log Pis Mean                                   -1.29245\n",
      "trainer/Log Pis Std                                     0.388878\n",
      "trainer/Log Pis Max                                    -0.57036\n",
      "trainer/Log Pis Min                                    -3.75367\n",
      "trainer/Policy mu Mean                                  0.0285586\n",
      "trainer/Policy mu Std                                   0.046973\n",
      "trainer/Policy mu Max                                   0.132546\n",
      "trainer/Policy mu Min                                  -0.0972846\n",
      "trainer/Policy log std Mean                            -0.348636\n",
      "trainer/Policy log std Std                              0.150635\n",
      "trainer/Policy log std Max                             -0.143799\n",
      "trainer/Policy log std Min                             -0.745127\n",
      "trainer/Alpha                                           0.224015\n",
      "trainer/Alpha Loss                                     -4.91595\n",
      "exploration/num steps total                          1200\n",
      "exploration/num paths total                            60\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.437723\n",
      "exploration/Rewards Std                                 0.290415\n",
      "exploration/Rewards Max                                -0.0161116\n",
      "exploration/Rewards Min                                -1.19085\n",
      "exploration/Returns Mean                               -8.75447\n",
      "exploration/Returns Std                                 3.54643\n",
      "exploration/Returns Max                                -4.47049\n",
      "exploration/Returns Min                               -14.1604\n",
      "exploration/Actions Mean                                0.0310878\n",
      "exploration/Actions Std                                 0.468715\n",
      "exploration/Actions Max                                 0.967497\n",
      "exploration/Actions Min                                -0.967218\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -8.75447\n",
      "exploration/env_infos/final/reward_dist Mean            2.57636e-30\n",
      "exploration/env_infos/final/reward_dist Std             5.15272e-30\n",
      "exploration/env_infos/final/reward_dist Max             1.28818e-29\n",
      "exploration/env_infos/final/reward_dist Min             1.51013e-89\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00155307\n",
      "exploration/env_infos/initial/reward_dist Std           0.00288381\n",
      "exploration/env_infos/initial/reward_dist Max           0.00731896\n",
      "exploration/env_infos/initial/reward_dist Min           4.46296e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0550432\n",
      "exploration/env_infos/reward_dist Std                   0.177504\n",
      "exploration/env_infos/reward_dist Max                   0.831432\n",
      "exploration/env_infos/reward_dist Min                   1.51013e-89\n",
      "exploration/env_infos/final/reward_energy Mean         -0.673666\n",
      "exploration/env_infos/final/reward_energy Std           0.179282\n",
      "exploration/env_infos/final/reward_energy Max          -0.380676\n",
      "exploration/env_infos/final/reward_energy Min          -0.906077\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.58767\n",
      "exploration/env_infos/initial/reward_energy Std         0.186861\n",
      "exploration/env_infos/initial/reward_energy Max        -0.379047\n",
      "exploration/env_infos/initial/reward_energy Min        -0.871669\n",
      "exploration/env_infos/reward_energy Mean               -0.603278\n",
      "exploration/env_infos/reward_energy Std                 0.278166\n",
      "exploration/env_infos/reward_energy Max                -0.0943582\n",
      "exploration/env_infos/reward_energy Min                -1.22577\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.208593\n",
      "exploration/env_infos/final/end_effector_loc Std        0.725824\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.000391298\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0217988\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0318921\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0408166\n",
      "exploration/env_infos/end_effector_loc Mean             0.0905952\n",
      "exploration/env_infos/end_effector_loc Std              0.485053\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           2000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.114125\n",
      "evaluation/Rewards Std                                  0.0957268\n",
      "evaluation/Rewards Max                                  0.111657\n",
      "evaluation/Rewards Min                                 -0.724508\n",
      "evaluation/Returns Mean                                -2.2825\n",
      "evaluation/Returns Std                                  1.23398\n",
      "evaluation/Returns Max                                  0.521001\n",
      "evaluation/Returns Min                                 -5.83967\n",
      "evaluation/Actions Mean                                 0.024116\n",
      "evaluation/Actions Std                                  0.0465231\n",
      "evaluation/Actions Max                                  0.113284\n",
      "evaluation/Actions Min                                 -0.0982297\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.2825\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0260026\n",
      "evaluation/env_infos/final/reward_dist Std              0.0849142\n",
      "evaluation/env_infos/final/reward_dist Max              0.37666\n",
      "evaluation/env_infos/final/reward_dist Min              3.44775e-99\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00482087\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00818285\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0315676\n",
      "evaluation/env_infos/initial/reward_dist Min            1.03427e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0390308\n",
      "evaluation/env_infos/reward_dist Std                    0.113685\n",
      "evaluation/env_infos/reward_dist Max                    0.984581\n",
      "evaluation/env_infos/reward_dist Min                    3.44775e-99\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0726062\n",
      "evaluation/env_infos/final/reward_energy Std            0.0341126\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0213347\n",
      "evaluation/env_infos/final/reward_energy Min           -0.131482\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.0542799\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0339393\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00309216\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.135224\n",
      "evaluation/env_infos/reward_energy Mean                -0.0651809\n",
      "evaluation/env_infos/reward_energy Std                  0.0352622\n",
      "evaluation/env_infos/reward_energy Max                 -0.00309216\n",
      "evaluation/env_infos/reward_energy Min                 -0.136826\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.230477\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.458789\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.820571\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000924281\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00206602\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.00558739\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00304279\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0828569\n",
      "evaluation/env_infos/end_effector_loc Std               0.233594\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -0.820571\n",
      "time/data storing (s)                                   0.168608\n",
      "time/evaluation sampling (s)                            1.05364\n",
      "time/exploration sampling (s)                           0.115856\n",
      "time/logging (s)                                        0.0190396\n",
      "time/saving (s)                                         0.0266307\n",
      "time/training (s)                                      37.5326\n",
      "time/epoch (s)                                         38.9163\n",
      "time/total (s)                                         75.7469\n",
      "Epoch                                                   1\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:14:27.384871 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 2 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.0157411\n",
      "trainer/QF2 Loss                                        0.0205905\n",
      "trainer/Policy Loss                                    -0.289263\n",
      "trainer/Q1 Predictions Mean                            -0.341368\n",
      "trainer/Q1 Predictions Std                              1.10569\n",
      "trainer/Q1 Predictions Max                              0.759096\n",
      "trainer/Q1 Predictions Min                             -4.10044\n",
      "trainer/Q2 Predictions Mean                            -0.294782\n",
      "trainer/Q2 Predictions Std                              1.06268\n",
      "trainer/Q2 Predictions Max                              0.780594\n",
      "trainer/Q2 Predictions Min                             -3.87503\n",
      "trainer/Q Targets Mean                                 -0.351638\n",
      "trainer/Q Targets Std                                   1.09681\n",
      "trainer/Q Targets Max                                   0.788732\n",
      "trainer/Q Targets Min                                  -3.90439\n",
      "trainer/Log Pis Mean                                   -0.505664\n",
      "trainer/Log Pis Std                                     1.10721\n",
      "trainer/Log Pis Max                                     2.72575\n",
      "trainer/Log Pis Min                                    -5.33552\n",
      "trainer/Policy mu Mean                                  0.026246\n",
      "trainer/Policy mu Std                                   0.264029\n",
      "trainer/Policy mu Max                                   1.12429\n",
      "trainer/Policy mu Min                                  -1.57186\n",
      "trainer/Policy log std Mean                            -0.894428\n",
      "trainer/Policy log std Std                              0.521966\n",
      "trainer/Policy log std Max                             -0.157303\n",
      "trainer/Policy log std Min                             -2.14909\n",
      "trainer/Alpha                                           0.0564989\n",
      "trainer/Alpha Loss                                     -7.19392\n",
      "exploration/num steps total                          1300\n",
      "exploration/num paths total                            65\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.409046\n",
      "exploration/Rewards Std                                 0.169814\n",
      "exploration/Rewards Max                                -0.101315\n",
      "exploration/Rewards Min                                -0.955184\n",
      "exploration/Returns Mean                               -8.18093\n",
      "exploration/Returns Std                                 1.20494\n",
      "exploration/Returns Max                                -6.04047\n",
      "exploration/Returns Min                                -9.31192\n",
      "exploration/Actions Mean                               -0.0326369\n",
      "exploration/Actions Std                                 0.39914\n",
      "exploration/Actions Max                                 0.83389\n",
      "exploration/Actions Min                                -0.973917\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -8.18093\n",
      "exploration/env_infos/final/reward_dist Mean            7.44243e-10\n",
      "exploration/env_infos/final/reward_dist Std             1.48849e-09\n",
      "exploration/env_infos/final/reward_dist Max             3.72121e-09\n",
      "exploration/env_infos/final/reward_dist Min             9.59298e-91\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00310668\n",
      "exploration/env_infos/initial/reward_dist Std           0.00437623\n",
      "exploration/env_infos/initial/reward_dist Max           0.0113058\n",
      "exploration/env_infos/initial/reward_dist Min           1.25569e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0592213\n",
      "exploration/env_infos/reward_dist Std                   0.18015\n",
      "exploration/env_infos/reward_dist Max                   0.960072\n",
      "exploration/env_infos/reward_dist Min                   9.59298e-91\n",
      "exploration/env_infos/final/reward_energy Mean         -0.484182\n",
      "exploration/env_infos/final/reward_energy Std           0.178075\n",
      "exploration/env_infos/final/reward_energy Max          -0.193465\n",
      "exploration/env_infos/final/reward_energy Min          -0.717357\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.57261\n",
      "exploration/env_infos/initial/reward_energy Std         0.159814\n",
      "exploration/env_infos/initial/reward_energy Max        -0.347217\n",
      "exploration/env_infos/initial/reward_energy Min        -0.77121\n",
      "exploration/env_infos/reward_energy Mean               -0.505865\n",
      "exploration/env_infos/reward_energy Std                 0.254669\n",
      "exploration/env_infos/reward_energy Max                -0.0410072\n",
      "exploration/env_infos/reward_energy Min                -1.1053\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.217693\n",
      "exploration/env_infos/final/end_effector_loc Std        0.638154\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.000459856\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0210135\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0359652\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.03856\n",
      "exploration/env_infos/end_effector_loc Mean             0.222149\n",
      "exploration/env_infos/end_effector_loc Std              0.447824\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           3000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.187198\n",
      "evaluation/Rewards Std                                  0.164423\n",
      "evaluation/Rewards Max                                  0.145996\n",
      "evaluation/Rewards Min                                 -0.824948\n",
      "evaluation/Returns Mean                                -3.74396\n",
      "evaluation/Returns Std                                  1.93615\n",
      "evaluation/Returns Max                                  0.337633\n",
      "evaluation/Returns Min                                 -8.99521\n",
      "evaluation/Actions Mean                                -0.0287675\n",
      "evaluation/Actions Std                                  0.12341\n",
      "evaluation/Actions Max                                  0.295236\n",
      "evaluation/Actions Min                                 -0.481237\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.74396\n",
      "evaluation/env_infos/final/reward_dist Mean             4.02596e-07\n",
      "evaluation/env_infos/final/reward_dist Std              2.11882e-06\n",
      "evaluation/env_infos/final/reward_dist Max              1.46298e-05\n",
      "evaluation/env_infos/final/reward_dist Min              6.58472e-114\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00361696\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0067161\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0254002\n",
      "evaluation/env_infos/initial/reward_dist Min            1.09499e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.036205\n",
      "evaluation/env_infos/reward_dist Std                    0.126671\n",
      "evaluation/env_infos/reward_dist Max                    0.962577\n",
      "evaluation/env_infos/reward_dist Min                    6.58472e-114\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.187755\n",
      "evaluation/env_infos/final/reward_energy Std            0.11449\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0177185\n",
      "evaluation/env_infos/final/reward_energy Min           -0.478053\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.129904\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0787734\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0151984\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.328759\n",
      "evaluation/env_infos/reward_energy Mean                -0.146359\n",
      "evaluation/env_infos/reward_energy Std                  0.103413\n",
      "evaluation/env_infos/reward_energy Max                 -0.00120922\n",
      "evaluation/env_infos/reward_energy Min                 -0.538324\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.070563\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.629397\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00138697\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0051891\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0146213\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00865114\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0245234\n",
      "evaluation/env_infos/end_effector_loc Std               0.371774\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.180525\n",
      "time/evaluation sampling (s)                            1.05973\n",
      "time/exploration sampling (s)                           0.122498\n",
      "time/logging (s)                                        0.0192817\n",
      "time/saving (s)                                         0.0293437\n",
      "time/training (s)                                      40.2787\n",
      "time/epoch (s)                                         41.69\n",
      "time/total (s)                                        117.497\n",
      "Epoch                                                   2\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:15:09.867944 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 3 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.029441\n",
      "trainer/QF2 Loss                                        0.0364579\n",
      "trainer/Policy Loss                                     1.39748\n",
      "trainer/Q1 Predictions Mean                            -1.02111\n",
      "trainer/Q1 Predictions Std                              1.38674\n",
      "trainer/Q1 Predictions Max                              0.462104\n",
      "trainer/Q1 Predictions Min                             -5.96059\n",
      "trainer/Q2 Predictions Mean                            -1.06352\n",
      "trainer/Q2 Predictions Std                              1.3958\n",
      "trainer/Q2 Predictions Max                              0.477835\n",
      "trainer/Q2 Predictions Min                             -5.99442\n",
      "trainer/Q Targets Mean                                 -1.07266\n",
      "trainer/Q Targets Std                                   1.40578\n",
      "trainer/Q Targets Max                                   0.471801\n",
      "trainer/Q Targets Min                                  -5.92077\n",
      "trainer/Log Pis Mean                                    0.466328\n",
      "trainer/Log Pis Std                                     1.38997\n",
      "trainer/Log Pis Max                                     4.41614\n",
      "trainer/Log Pis Min                                    -4.11397\n",
      "trainer/Policy mu Mean                                 -0.00814241\n",
      "trainer/Policy mu Std                                   0.498167\n",
      "trainer/Policy mu Max                                   2.26084\n",
      "trainer/Policy mu Min                                  -3.4149\n",
      "trainer/Policy log std Mean                            -1.37729\n",
      "trainer/Policy log std Std                              0.627543\n",
      "trainer/Policy log std Max                             -0.0286426\n",
      "trainer/Policy log std Min                             -2.63298\n",
      "trainer/Alpha                                           0.0194543\n",
      "trainer/Alpha Loss                                     -6.03961\n",
      "exploration/num steps total                          1400\n",
      "exploration/num paths total                            70\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.422586\n",
      "exploration/Rewards Std                                 0.315085\n",
      "exploration/Rewards Max                                -0.0158758\n",
      "exploration/Rewards Min                                -1.37123\n",
      "exploration/Returns Mean                               -8.45171\n",
      "exploration/Returns Std                                 3.76678\n",
      "exploration/Returns Max                                -3.11728\n",
      "exploration/Returns Min                               -13.2358\n",
      "exploration/Actions Mean                               -0.0182026\n",
      "exploration/Actions Std                                 0.36965\n",
      "exploration/Actions Max                                 0.941873\n",
      "exploration/Actions Min                                -0.914941\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -8.45171\n",
      "exploration/env_infos/final/reward_dist Mean            3.83561e-20\n",
      "exploration/env_infos/final/reward_dist Std             7.67121e-20\n",
      "exploration/env_infos/final/reward_dist Max             1.9178e-19\n",
      "exploration/env_infos/final/reward_dist Min             9.58475e-140\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0146405\n",
      "exploration/env_infos/initial/reward_dist Std           0.0280997\n",
      "exploration/env_infos/initial/reward_dist Max           0.0708168\n",
      "exploration/env_infos/initial/reward_dist Min           1.0325e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0165409\n",
      "exploration/env_infos/reward_dist Std                   0.0758551\n",
      "exploration/env_infos/reward_dist Max                   0.522781\n",
      "exploration/env_infos/reward_dist Min                   1.26194e-146\n",
      "exploration/env_infos/final/reward_energy Mean         -0.58954\n",
      "exploration/env_infos/final/reward_energy Std           0.275046\n",
      "exploration/env_infos/final/reward_energy Max          -0.182032\n",
      "exploration/env_infos/final/reward_energy Min          -1.02011\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.531765\n",
      "exploration/env_infos/initial/reward_energy Std         0.347629\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0381182\n",
      "exploration/env_infos/initial/reward_energy Min        -0.976975\n",
      "exploration/env_infos/reward_energy Mean               -0.431464\n",
      "exploration/env_infos/reward_energy Std                 0.296283\n",
      "exploration/env_infos/reward_energy Max                -0.0189254\n",
      "exploration/env_infos/reward_energy Min                -1.06577\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.240482\n",
      "exploration/env_infos/final/end_effector_loc Std        0.735945\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00466591\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0219717\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0349611\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0454119\n",
      "exploration/env_infos/end_effector_loc Mean            -0.133567\n",
      "exploration/env_infos/end_effector_loc Std              0.535704\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           4000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.228346\n",
      "evaluation/Rewards Std                                  0.207126\n",
      "evaluation/Rewards Max                                  0.125679\n",
      "evaluation/Rewards Min                                 -0.965551\n",
      "evaluation/Returns Mean                                -4.56691\n",
      "evaluation/Returns Std                                  2.774\n",
      "evaluation/Returns Max                                 -0.341915\n",
      "evaluation/Returns Min                                -10.1888\n",
      "evaluation/Actions Mean                                -0.0296313\n",
      "evaluation/Actions Std                                  0.166511\n",
      "evaluation/Actions Max                                  0.873105\n",
      "evaluation/Actions Min                                 -0.624991\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -4.56691\n",
      "evaluation/env_infos/final/reward_dist Mean             0.000818653\n",
      "evaluation/env_infos/final/reward_dist Std              0.00326482\n",
      "evaluation/env_infos/final/reward_dist Max              0.0202191\n",
      "evaluation/env_infos/final/reward_dist Min              2.18825e-100\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00489768\n",
      "evaluation/env_infos/initial/reward_dist Std            0.014264\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0953997\n",
      "evaluation/env_infos/initial/reward_dist Min            1.00454e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0332141\n",
      "evaluation/env_infos/reward_dist Std                    0.122438\n",
      "evaluation/env_infos/reward_dist Max                    0.89894\n",
      "evaluation/env_infos/reward_dist Min                    6.37732e-102\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.227037\n",
      "evaluation/env_infos/final/reward_energy Std            0.168629\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0165859\n",
      "evaluation/env_infos/final/reward_energy Min           -0.711059\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.2753\n",
      "evaluation/env_infos/initial/reward_energy Std          0.244369\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0160728\n",
      "evaluation/env_infos/initial/reward_energy Min         -1.01278\n",
      "evaluation/env_infos/reward_energy Mean                -0.178916\n",
      "evaluation/env_infos/reward_energy Std                  0.158735\n",
      "evaluation/env_infos/reward_energy Max                 -0.00254854\n",
      "evaluation/env_infos/reward_energy Min                 -1.01278\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0241426\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.646796\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00411367\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0123475\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0436553\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0277295\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0225208\n",
      "evaluation/env_infos/end_effector_loc Std               0.423732\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.234775\n",
      "time/evaluation sampling (s)                            0.964641\n",
      "time/exploration sampling (s)                           0.121103\n",
      "time/logging (s)                                        0.0197056\n",
      "time/saving (s)                                         0.0280681\n",
      "time/training (s)                                      41.0409\n",
      "time/epoch (s)                                         42.4092\n",
      "time/total (s)                                        159.98\n",
      "Epoch                                                   3\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:15:53.066740 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 4 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.0113806\n",
      "trainer/QF2 Loss                                        0.0119479\n",
      "trainer/Policy Loss                                     2.63252\n",
      "trainer/Q1 Predictions Mean                            -1.44442\n",
      "trainer/Q1 Predictions Std                              1.49893\n",
      "trainer/Q1 Predictions Max                              0.30087\n",
      "trainer/Q1 Predictions Min                             -7.60727\n",
      "trainer/Q2 Predictions Mean                            -1.46048\n",
      "trainer/Q2 Predictions Std                              1.51141\n",
      "trainer/Q2 Predictions Max                              0.234084\n",
      "trainer/Q2 Predictions Min                             -7.55092\n",
      "trainer/Q Targets Mean                                 -1.46186\n",
      "trainer/Q Targets Std                                   1.52884\n",
      "trainer/Q Targets Max                                   0.311858\n",
      "trainer/Q Targets Min                                  -7.63921\n",
      "trainer/Log Pis Mean                                    1.32945\n",
      "trainer/Log Pis Std                                     1.86026\n",
      "trainer/Log Pis Max                                     9.82923\n",
      "trainer/Log Pis Min                                    -6.1304\n",
      "trainer/Policy mu Mean                                 -0.0812581\n",
      "trainer/Policy mu Std                                   0.730778\n",
      "trainer/Policy mu Max                                   3.43424\n",
      "trainer/Policy mu Min                                  -3.409\n",
      "trainer/Policy log std Mean                            -1.63542\n",
      "trainer/Policy log std Std                              0.739314\n",
      "trainer/Policy log std Max                             -0.0338542\n",
      "trainer/Policy log std Min                             -3.27202\n",
      "trainer/Alpha                                           0.010531\n",
      "trainer/Alpha Loss                                     -3.05271\n",
      "exploration/num steps total                          1500\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.283021\n",
      "exploration/Rewards Std                                 0.169159\n",
      "exploration/Rewards Max                                 0.0269444\n",
      "exploration/Rewards Min                                -0.833807\n",
      "exploration/Returns Mean                               -5.66043\n",
      "exploration/Returns Std                                 2.23631\n",
      "exploration/Returns Max                                -2.1773\n",
      "exploration/Returns Min                                -9.22774\n",
      "exploration/Actions Mean                               -0.070297\n",
      "exploration/Actions Std                                 0.235049\n",
      "exploration/Actions Max                                 0.522793\n",
      "exploration/Actions Min                                -0.942836\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -5.66043\n",
      "exploration/env_infos/final/reward_dist Mean            0.0274051\n",
      "exploration/env_infos/final/reward_dist Std             0.0548102\n",
      "exploration/env_infos/final/reward_dist Max             0.137025\n",
      "exploration/env_infos/final/reward_dist Min             3.655e-107\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00120265\n",
      "exploration/env_infos/initial/reward_dist Std           0.00164493\n",
      "exploration/env_infos/initial/reward_dist Max           0.00443065\n",
      "exploration/env_infos/initial/reward_dist Min           1.3371e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0798176\n",
      "exploration/env_infos/reward_dist Std                   0.191301\n",
      "exploration/env_infos/reward_dist Max                   0.938626\n",
      "exploration/env_infos/reward_dist Min                   3.655e-107\n",
      "exploration/env_infos/final/reward_energy Mean         -0.371038\n",
      "exploration/env_infos/final/reward_energy Std           0.139083\n",
      "exploration/env_infos/final/reward_energy Max          -0.196267\n",
      "exploration/env_infos/final/reward_energy Min          -0.622562\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.453462\n",
      "exploration/env_infos/initial/reward_energy Std         0.428575\n",
      "exploration/env_infos/initial/reward_energy Max        -0.134939\n",
      "exploration/env_infos/initial/reward_energy Min        -1.28795\n",
      "exploration/env_infos/reward_energy Mean               -0.273441\n",
      "exploration/env_infos/reward_energy Std                 0.213564\n",
      "exploration/env_infos/reward_energy Max                -0.0314251\n",
      "exploration/env_infos/reward_energy Min                -1.28795\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.294086\n",
      "exploration/env_infos/final/end_effector_loc Std        0.677109\n",
      "exploration/env_infos/final/end_effector_loc Max        0.847004\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00614153\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0211875\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0210827\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0471418\n",
      "exploration/env_infos/end_effector_loc Mean            -0.178905\n",
      "exploration/env_infos/end_effector_loc Std              0.472608\n",
      "exploration/env_infos/end_effector_loc Max              0.847004\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           5000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.220367\n",
      "evaluation/Rewards Std                                  0.206926\n",
      "evaluation/Rewards Max                                  0.0155983\n",
      "evaluation/Rewards Min                                 -1.02433\n",
      "evaluation/Returns Mean                                -4.40735\n",
      "evaluation/Returns Std                                  2.92968\n",
      "evaluation/Returns Max                                 -0.584877\n",
      "evaluation/Returns Min                                -13.9565\n",
      "evaluation/Actions Mean                                -0.029005\n",
      "evaluation/Actions Std                                  0.190331\n",
      "evaluation/Actions Max                                  0.95058\n",
      "evaluation/Actions Min                                 -0.88377\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -4.40735\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00576363\n",
      "evaluation/env_infos/final/reward_dist Std              0.0235129\n",
      "evaluation/env_infos/final/reward_dist Max              0.135635\n",
      "evaluation/env_infos/final/reward_dist Min              2.31651e-114\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.0124682\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0275005\n",
      "evaluation/env_infos/initial/reward_dist Max            0.1341\n",
      "evaluation/env_infos/initial/reward_dist Min            8.3841e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0393682\n",
      "evaluation/env_infos/reward_dist Std                    0.12006\n",
      "evaluation/env_infos/reward_dist Max                    0.958871\n",
      "evaluation/env_infos/reward_dist Min                    2.31651e-114\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.196888\n",
      "evaluation/env_infos/final/reward_energy Std            0.123868\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0282769\n",
      "evaluation/env_infos/final/reward_energy Min           -0.612013\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.369685\n",
      "evaluation/env_infos/initial/reward_energy Std          0.345921\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0282767\n",
      "evaluation/env_infos/initial/reward_energy Min         -1.19318\n",
      "evaluation/env_infos/reward_energy Mean                -0.198341\n",
      "evaluation/env_infos/reward_energy Std                  0.186535\n",
      "evaluation/env_infos/reward_energy Max                 -0.00313825\n",
      "evaluation/env_infos/reward_energy Min                 -1.19318\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.121158\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.680151\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000415784\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0178952\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.047529\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0430223\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0612721\n",
      "evaluation/env_infos/end_effector_loc Std               0.450396\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.176945\n",
      "time/evaluation sampling (s)                            0.932934\n",
      "time/exploration sampling (s)                           0.115033\n",
      "time/logging (s)                                        0.0196857\n",
      "time/saving (s)                                         0.0275964\n",
      "time/training (s)                                      41.8174\n",
      "time/epoch (s)                                         43.0895\n",
      "time/total (s)                                        203.178\n",
      "Epoch                                                   4\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:16:36.644664 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 5 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00740577\n",
      "trainer/QF2 Loss                                        0.0158123\n",
      "trainer/Policy Loss                                     3.95194\n",
      "trainer/Q1 Predictions Mean                            -1.94086\n",
      "trainer/Q1 Predictions Std                              1.72043\n",
      "trainer/Q1 Predictions Max                              0.153363\n",
      "trainer/Q1 Predictions Min                             -8.36648\n",
      "trainer/Q2 Predictions Mean                            -1.9484\n",
      "trainer/Q2 Predictions Std                              1.71277\n",
      "trainer/Q2 Predictions Max                              0.0451607\n",
      "trainer/Q2 Predictions Min                             -8.36347\n",
      "trainer/Q Targets Mean                                 -1.90147\n",
      "trainer/Q Targets Std                                   1.70703\n",
      "trainer/Q Targets Max                                   0.160402\n",
      "trainer/Q Targets Min                                  -8.21582\n",
      "trainer/Log Pis Mean                                    2.16776\n",
      "trainer/Log Pis Std                                     1.58874\n",
      "trainer/Log Pis Max                                     8.88206\n",
      "trainer/Log Pis Min                                    -1.88978\n",
      "trainer/Policy mu Mean                                 -0.0947256\n",
      "trainer/Policy mu Std                                   0.795725\n",
      "trainer/Policy mu Max                                   3.68586\n",
      "trainer/Policy mu Min                                  -3.43914\n",
      "trainer/Policy log std Mean                            -1.97792\n",
      "trainer/Policy log std Std                              0.765433\n",
      "trainer/Policy log std Max                             -0.184131\n",
      "trainer/Policy log std Min                             -3.28751\n",
      "trainer/Alpha                                           0.00761514\n",
      "trainer/Alpha Loss                                      0.818259\n",
      "exploration/num steps total                          1600\n",
      "exploration/num paths total                            80\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.159276\n",
      "exploration/Rewards Std                                 0.100169\n",
      "exploration/Rewards Max                                -0.0287647\n",
      "exploration/Rewards Min                                -0.726637\n",
      "exploration/Returns Mean                               -3.18553\n",
      "exploration/Returns Std                                 0.487434\n",
      "exploration/Returns Max                                -2.33611\n",
      "exploration/Returns Min                                -3.66985\n",
      "exploration/Actions Mean                               -0.00802018\n",
      "exploration/Actions Std                                 0.11133\n",
      "exploration/Actions Max                                 0.27973\n",
      "exploration/Actions Min                                -0.516436\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -3.18553\n",
      "exploration/env_infos/final/reward_dist Mean            0.00683225\n",
      "exploration/env_infos/final/reward_dist Std             0.00840213\n",
      "exploration/env_infos/final/reward_dist Max             0.018281\n",
      "exploration/env_infos/final/reward_dist Min             8.95204e-55\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00277095\n",
      "exploration/env_infos/initial/reward_dist Std           0.00194424\n",
      "exploration/env_infos/initial/reward_dist Max           0.00528572\n",
      "exploration/env_infos/initial/reward_dist Min           3.1364e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0730961\n",
      "exploration/env_infos/reward_dist Std                   0.188082\n",
      "exploration/env_infos/reward_dist Max                   0.938534\n",
      "exploration/env_infos/reward_dist Min                   8.95204e-55\n",
      "exploration/env_infos/final/reward_energy Mean         -0.222899\n",
      "exploration/env_infos/final/reward_energy Std           0.192655\n",
      "exploration/env_infos/final/reward_energy Max          -0.0565603\n",
      "exploration/env_infos/final/reward_energy Min          -0.586492\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.187092\n",
      "exploration/env_infos/initial/reward_energy Std         0.0834782\n",
      "exploration/env_infos/initial/reward_energy Max        -0.100998\n",
      "exploration/env_infos/initial/reward_energy Min        -0.3357\n",
      "exploration/env_infos/reward_energy Mean               -0.130872\n",
      "exploration/env_infos/reward_energy Std                 0.0882596\n",
      "exploration/env_infos/reward_energy Max                -0.0108835\n",
      "exploration/env_infos/reward_energy Min                -0.586492\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.130124\n",
      "exploration/env_infos/final/end_effector_loc Std        0.382337\n",
      "exploration/env_infos/final/end_effector_loc Max        0.413412\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00404253\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00601026\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00587219\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0129907\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0594147\n",
      "exploration/env_infos/end_effector_loc Std              0.212749\n",
      "exploration/env_infos/end_effector_loc Max              0.413412\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           6000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.173925\n",
      "evaluation/Rewards Std                                  0.180502\n",
      "evaluation/Rewards Max                                  0.0878243\n",
      "evaluation/Rewards Min                                 -0.959218\n",
      "evaluation/Returns Mean                                -3.4785\n",
      "evaluation/Returns Std                                  2.69288\n",
      "evaluation/Returns Max                                  0.260211\n",
      "evaluation/Returns Min                                -12.4609\n",
      "evaluation/Actions Mean                                -0.00409171\n",
      "evaluation/Actions Std                                  0.16468\n",
      "evaluation/Actions Max                                  0.97047\n",
      "evaluation/Actions Min                                 -0.852935\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.4785\n",
      "evaluation/env_infos/final/reward_dist Mean             0.026971\n",
      "evaluation/env_infos/final/reward_dist Std              0.107161\n",
      "evaluation/env_infos/final/reward_dist Max              0.662035\n",
      "evaluation/env_infos/final/reward_dist Min              4.15835e-122\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00487996\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00981165\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0348265\n",
      "evaluation/env_infos/initial/reward_dist Min            6.79745e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0390584\n",
      "evaluation/env_infos/reward_dist Std                    0.129036\n",
      "evaluation/env_infos/reward_dist Max                    0.952229\n",
      "evaluation/env_infos/reward_dist Min                    4.15835e-122\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.193333\n",
      "evaluation/env_infos/final/reward_energy Std            0.181464\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0185494\n",
      "evaluation/env_infos/final/reward_energy Min           -0.812151\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.253024\n",
      "evaluation/env_infos/initial/reward_energy Std          0.265274\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0405654\n",
      "evaluation/env_infos/initial/reward_energy Min         -1.14819\n",
      "evaluation/env_infos/reward_energy Mean                -0.155024\n",
      "evaluation/env_infos/reward_energy Std                  0.173896\n",
      "evaluation/env_infos/reward_energy Max                 -0.00166815\n",
      "evaluation/env_infos/reward_energy Min                 -1.14819\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.108467\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.519356\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean     -0.00132292\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0128934\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0485235\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0426468\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0681857\n",
      "evaluation/env_infos/end_effector_loc Std               0.34392\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.174104\n",
      "time/evaluation sampling (s)                            1.06405\n",
      "time/exploration sampling (s)                           0.124975\n",
      "time/logging (s)                                        0.0207629\n",
      "time/saving (s)                                         0.0272669\n",
      "time/training (s)                                      42.074\n",
      "time/epoch (s)                                         43.4852\n",
      "time/total (s)                                        246.756\n",
      "Epoch                                                   5\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"relabeled_latents\": np.array(self.relabeled_latents),\n",
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:404: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"relabeled_rewards\": np.array(self.relabeled_rewards)}\n",
      "2021-05-25 13:17:19.862811 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 6 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.006187\n",
      "trainer/QF2 Loss                                        0.00921116\n",
      "trainer/Policy Loss                                     3.9465\n",
      "trainer/Q1 Predictions Mean                            -1.94231\n",
      "trainer/Q1 Predictions Std                              1.57308\n",
      "trainer/Q1 Predictions Max                              0.09427\n",
      "trainer/Q1 Predictions Min                             -7.90892\n",
      "trainer/Q2 Predictions Mean                            -1.99057\n",
      "trainer/Q2 Predictions Std                              1.58073\n",
      "trainer/Q2 Predictions Max                             -0.00780889\n",
      "trainer/Q2 Predictions Min                             -7.98233\n",
      "trainer/Q Targets Mean                                 -1.95502\n",
      "trainer/Q Targets Std                                   1.5941\n",
      "trainer/Q Targets Max                                   0.0739021\n",
      "trainer/Q Targets Min                                  -7.97616\n",
      "trainer/Log Pis Mean                                    2.1276\n",
      "trainer/Log Pis Std                                     1.95976\n",
      "trainer/Log Pis Max                                    10.9105\n",
      "trainer/Log Pis Min                                    -3.33338\n",
      "trainer/Policy mu Mean                                 -0.158503\n",
      "trainer/Policy mu Std                                   0.903255\n",
      "trainer/Policy mu Max                                   3.59471\n",
      "trainer/Policy mu Min                                  -4.43751\n",
      "trainer/Policy log std Mean                            -1.95982\n",
      "trainer/Policy log std Std                              0.764829\n",
      "trainer/Policy log std Max                              0.274636\n",
      "trainer/Policy log std Min                             -3.26719\n",
      "trainer/Alpha                                           0.00796779\n",
      "trainer/Alpha Loss                                      0.616607\n",
      "exploration/num steps total                          1700\n",
      "exploration/num paths total                            85\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.19079\n",
      "exploration/Rewards Std                                 0.171507\n",
      "exploration/Rewards Max                                 0.0206439\n",
      "exploration/Rewards Min                                -0.694219\n",
      "exploration/Returns Mean                               -3.8158\n",
      "exploration/Returns Std                                 3.01828\n",
      "exploration/Returns Max                                -1.25859\n",
      "exploration/Returns Min                                -9.56534\n",
      "exploration/Actions Mean                                0.0299564\n",
      "exploration/Actions Std                                 0.262264\n",
      "exploration/Actions Max                                 0.849917\n",
      "exploration/Actions Min                                -0.910619\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -3.8158\n",
      "exploration/env_infos/final/reward_dist Mean            2.69932e-10\n",
      "exploration/env_infos/final/reward_dist Std             5.2362e-10\n",
      "exploration/env_infos/final/reward_dist Max             1.31686e-09\n",
      "exploration/env_infos/final/reward_dist Min             3.68334e-90\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0104042\n",
      "exploration/env_infos/initial/reward_dist Std           0.0136242\n",
      "exploration/env_infos/initial/reward_dist Max           0.0373511\n",
      "exploration/env_infos/initial/reward_dist Min           0.000232047\n",
      "exploration/env_infos/reward_dist Mean                  0.017989\n",
      "exploration/env_infos/reward_dist Std                   0.0895183\n",
      "exploration/env_infos/reward_dist Max                   0.826703\n",
      "exploration/env_infos/reward_dist Min                   3.68334e-90\n",
      "exploration/env_infos/final/reward_energy Mean         -0.208985\n",
      "exploration/env_infos/final/reward_energy Std           0.081818\n",
      "exploration/env_infos/final/reward_energy Max          -0.105604\n",
      "exploration/env_infos/final/reward_energy Min          -0.35299\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.335027\n",
      "exploration/env_infos/initial/reward_energy Std         0.27467\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0507106\n",
      "exploration/env_infos/initial/reward_energy Min        -0.861087\n",
      "exploration/env_infos/reward_energy Mean               -0.276258\n",
      "exploration/env_infos/reward_energy Std                 0.251081\n",
      "exploration/env_infos/reward_energy Max                -0.0166621\n",
      "exploration/env_infos/reward_energy Min                -1.17685\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.149022\n",
      "exploration/env_infos/final/end_effector_loc Std        0.550073\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.840687\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00584302\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0141586\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0424959\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0132118\n",
      "exploration/env_infos/end_effector_loc Mean             0.126093\n",
      "exploration/env_infos/end_effector_loc Std              0.380607\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -0.840687\n",
      "evaluation/num steps total                           7000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.166617\n",
      "evaluation/Rewards Std                                  0.156259\n",
      "evaluation/Rewards Max                                  0.126482\n",
      "evaluation/Rewards Min                                 -1.15441\n",
      "evaluation/Returns Mean                                -3.33234\n",
      "evaluation/Returns Std                                  2.07046\n",
      "evaluation/Returns Max                                 -0.214681\n",
      "evaluation/Returns Min                                -10.2653\n",
      "evaluation/Actions Mean                                -0.00391447\n",
      "evaluation/Actions Std                                  0.150941\n",
      "evaluation/Actions Max                                  0.923869\n",
      "evaluation/Actions Min                                 -0.862876\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.33234\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00751399\n",
      "evaluation/env_infos/final/reward_dist Std              0.0340839\n",
      "evaluation/env_infos/final/reward_dist Max              0.189144\n",
      "evaluation/env_infos/final/reward_dist Min              2.07727e-106\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00980837\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0239308\n",
      "evaluation/env_infos/initial/reward_dist Max            0.119083\n",
      "evaluation/env_infos/initial/reward_dist Min            1.3996e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0346297\n",
      "evaluation/env_infos/reward_dist Std                    0.116307\n",
      "evaluation/env_infos/reward_dist Max                    0.986718\n",
      "evaluation/env_infos/reward_dist Min                    2.07727e-106\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.212059\n",
      "evaluation/env_infos/final/reward_energy Std            0.191596\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00718469\n",
      "evaluation/env_infos/final/reward_energy Min           -0.847242\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.286709\n",
      "evaluation/env_infos/initial/reward_energy Std          0.327569\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0253584\n",
      "evaluation/env_infos/initial/reward_energy Min         -1.19948\n",
      "evaluation/env_infos/reward_energy Mean                -0.148942\n",
      "evaluation/env_infos/reward_energy Std                  0.153014\n",
      "evaluation/env_infos/reward_energy Max                 -0.00322036\n",
      "evaluation/env_infos/reward_energy Min                 -1.19948\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0318403\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.480243\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean     -0.00146598\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0153209\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0461935\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0431438\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0284525\n",
      "evaluation/env_infos/end_effector_loc Std               0.300096\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.16987\n",
      "time/evaluation sampling (s)                            0.921665\n",
      "time/exploration sampling (s)                           0.119099\n",
      "time/logging (s)                                        0.0185884\n",
      "time/saving (s)                                         0.0284867\n",
      "time/training (s)                                      41.85\n",
      "time/epoch (s)                                         43.1077\n",
      "time/total (s)                                        289.971\n",
      "Epoch                                                   6\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:18:03.496133 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 7 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00913595\n",
      "trainer/QF2 Loss                                        0.00714247\n",
      "trainer/Policy Loss                                     4.36166\n",
      "trainer/Q1 Predictions Mean                            -2.27862\n",
      "trainer/Q1 Predictions Std                              1.71949\n",
      "trainer/Q1 Predictions Max                              0.0695316\n",
      "trainer/Q1 Predictions Min                            -10.4014\n",
      "trainer/Q2 Predictions Mean                            -2.27913\n",
      "trainer/Q2 Predictions Std                              1.71884\n",
      "trainer/Q2 Predictions Max                              0.09634\n",
      "trainer/Q2 Predictions Min                            -10.2759\n",
      "trainer/Q Targets Mean                                 -2.24932\n",
      "trainer/Q Targets Std                                   1.7158\n",
      "trainer/Q Targets Max                                   0.171178\n",
      "trainer/Q Targets Min                                 -10.1194\n",
      "trainer/Log Pis Mean                                    2.24571\n",
      "trainer/Log Pis Std                                     1.74899\n",
      "trainer/Log Pis Max                                    11.1401\n",
      "trainer/Log Pis Min                                    -2.94202\n",
      "trainer/Policy mu Mean                                 -0.174283\n",
      "trainer/Policy mu Std                                   0.82556\n",
      "trainer/Policy mu Max                                   3.93356\n",
      "trainer/Policy mu Min                                  -3.36218\n",
      "trainer/Policy log std Mean                            -2.02971\n",
      "trainer/Policy log std Std                              0.828622\n",
      "trainer/Policy log std Max                              0.824064\n",
      "trainer/Policy log std Min                             -3.33344\n",
      "trainer/Alpha                                           0.0085107\n",
      "trainer/Alpha Loss                                      1.1712\n",
      "exploration/num steps total                          1800\n",
      "exploration/num paths total                            90\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.215886\n",
      "exploration/Rewards Std                                 0.163152\n",
      "exploration/Rewards Max                                -0.0275179\n",
      "exploration/Rewards Min                                -0.732701\n",
      "exploration/Returns Mean                               -4.31772\n",
      "exploration/Returns Std                                 2.1857\n",
      "exploration/Returns Max                                -2.06978\n",
      "exploration/Returns Min                                -8.30346\n",
      "exploration/Actions Mean                               -0.00303867\n",
      "exploration/Actions Std                                 0.154861\n",
      "exploration/Actions Max                                 0.449845\n",
      "exploration/Actions Min                                -0.522306\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -4.31772\n",
      "exploration/env_infos/final/reward_dist Mean            4.23815e-09\n",
      "exploration/env_infos/final/reward_dist Std             5.91701e-09\n",
      "exploration/env_infos/final/reward_dist Max             1.50865e-08\n",
      "exploration/env_infos/final/reward_dist Min             1.37007e-45\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000649137\n",
      "exploration/env_infos/initial/reward_dist Std           0.000788081\n",
      "exploration/env_infos/initial/reward_dist Max           0.00211786\n",
      "exploration/env_infos/initial/reward_dist Min           5.00967e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.000610044\n",
      "exploration/env_infos/reward_dist Std                   0.00126525\n",
      "exploration/env_infos/reward_dist Max                   0.00909753\n",
      "exploration/env_infos/reward_dist Min                   5.08668e-60\n",
      "exploration/env_infos/final/reward_energy Mean         -0.239287\n",
      "exploration/env_infos/final/reward_energy Std           0.10099\n",
      "exploration/env_infos/final/reward_energy Max          -0.164393\n",
      "exploration/env_infos/final/reward_energy Min          -0.437705\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.155083\n",
      "exploration/env_infos/initial/reward_energy Std         0.114566\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0507862\n",
      "exploration/env_infos/initial/reward_energy Min        -0.364306\n",
      "exploration/env_infos/reward_energy Mean               -0.174596\n",
      "exploration/env_infos/reward_energy Std                 0.132282\n",
      "exploration/env_infos/reward_energy Max                -0.00810666\n",
      "exploration/env_infos/reward_energy Min                -0.653739\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.116042\n",
      "exploration/env_infos/final/end_effector_loc Std        0.346154\n",
      "exploration/env_infos/final/end_effector_loc Max        0.347506\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.678809\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00379349\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00566388\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0029203\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0182\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0602949\n",
      "exploration/env_infos/end_effector_loc Std              0.231854\n",
      "exploration/env_infos/end_effector_loc Max              0.427477\n",
      "exploration/env_infos/end_effector_loc Min             -0.761812\n",
      "evaluation/num steps total                           8000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.17391\n",
      "evaluation/Rewards Std                                  0.145423\n",
      "evaluation/Rewards Max                                  0.0943324\n",
      "evaluation/Rewards Min                                 -0.985972\n",
      "evaluation/Returns Mean                                -3.47821\n",
      "evaluation/Returns Std                                  2.11806\n",
      "evaluation/Returns Max                                  0.450535\n",
      "evaluation/Returns Min                                 -8.66734\n",
      "evaluation/Actions Mean                                -0.0067924\n",
      "evaluation/Actions Std                                  0.126367\n",
      "evaluation/Actions Max                                  0.685385\n",
      "evaluation/Actions Min                                 -0.751719\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.47821\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0307757\n",
      "evaluation/env_infos/final/reward_dist Std              0.113279\n",
      "evaluation/env_infos/final/reward_dist Max              0.714939\n",
      "evaluation/env_infos/final/reward_dist Min              1.11664e-162\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00703713\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0141177\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0695961\n",
      "evaluation/env_infos/initial/reward_dist Min            8.28789e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0356799\n",
      "evaluation/env_infos/reward_dist Std                    0.123039\n",
      "evaluation/env_infos/reward_dist Max                    0.964844\n",
      "evaluation/env_infos/reward_dist Min                    1.11664e-162\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.154333\n",
      "evaluation/env_infos/final/reward_energy Std            0.0925979\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00771887\n",
      "evaluation/env_infos/final/reward_energy Min           -0.390418\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.254321\n",
      "evaluation/env_infos/initial/reward_energy Std          0.203453\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0171177\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.873863\n",
      "evaluation/env_infos/reward_energy Mean                -0.13361\n",
      "evaluation/env_infos/reward_energy Std                  0.11907\n",
      "evaluation/env_infos/reward_energy Max                 -0.00620593\n",
      "evaluation/env_infos/reward_energy Min                 -0.873863\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.113874\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.429234\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean     -0.000425695\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0115069\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0294609\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0375859\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0472127\n",
      "evaluation/env_infos/end_effector_loc Std               0.271916\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.173876\n",
      "time/evaluation sampling (s)                            0.93702\n",
      "time/exploration sampling (s)                           0.119902\n",
      "time/logging (s)                                        0.0201675\n",
      "time/saving (s)                                         0.0269935\n",
      "time/training (s)                                      42.2053\n",
      "time/epoch (s)                                         43.4833\n",
      "time/total (s)                                        333.605\n",
      "Epoch                                                   7\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:18:46.736000 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 8 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.00533563\n",
      "trainer/QF2 Loss                                        0.00659788\n",
      "trainer/Policy Loss                                     4.42035\n",
      "trainer/Q1 Predictions Mean                            -2.28057\n",
      "trainer/Q1 Predictions Std                              1.62644\n",
      "trainer/Q1 Predictions Max                              0.0824078\n",
      "trainer/Q1 Predictions Min                             -9.12065\n",
      "trainer/Q2 Predictions Mean                            -2.22764\n",
      "trainer/Q2 Predictions Std                              1.60344\n",
      "trainer/Q2 Predictions Max                              0.122725\n",
      "trainer/Q2 Predictions Min                             -8.83307\n",
      "trainer/Q Targets Mean                                 -2.25968\n",
      "trainer/Q Targets Std                                   1.61937\n",
      "trainer/Q Targets Max                                   0.220764\n",
      "trainer/Q Targets Min                                  -9.13877\n",
      "trainer/Log Pis Mean                                    2.34288\n",
      "trainer/Log Pis Std                                     2.18089\n",
      "trainer/Log Pis Max                                    14.7293\n",
      "trainer/Log Pis Min                                    -4.16367\n",
      "trainer/Policy mu Mean                                 -0.157253\n",
      "trainer/Policy mu Std                                   0.953318\n",
      "trainer/Policy mu Max                                   4.30166\n",
      "trainer/Policy mu Min                                  -4.42701\n",
      "trainer/Policy log std Mean                            -2.01362\n",
      "trainer/Policy log std Std                              0.834473\n",
      "trainer/Policy log std Max                             -0.20653\n",
      "trainer/Policy log std Min                             -3.47713\n",
      "trainer/Alpha                                           0.00985431\n",
      "trainer/Alpha Loss                                      1.58431\n",
      "exploration/num steps total                          1900\n",
      "exploration/num paths total                            95\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.223974\n",
      "exploration/Rewards Std                                 0.141288\n",
      "exploration/Rewards Max                                -0.0394922\n",
      "exploration/Rewards Min                                -0.717659\n",
      "exploration/Returns Mean                               -4.47949\n",
      "exploration/Returns Std                                 1.46446\n",
      "exploration/Returns Max                                -2.92307\n",
      "exploration/Returns Min                                -7.16017\n",
      "exploration/Actions Mean                               -0.0112399\n",
      "exploration/Actions Std                                 0.205312\n",
      "exploration/Actions Max                                 0.740161\n",
      "exploration/Actions Min                                -0.87331\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -4.47949\n",
      "exploration/env_infos/final/reward_dist Mean            1.03656e-19\n",
      "exploration/env_infos/final/reward_dist Std             2.07311e-19\n",
      "exploration/env_infos/final/reward_dist Max             5.18279e-19\n",
      "exploration/env_infos/final/reward_dist Min             3.72565e-56\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00744965\n",
      "exploration/env_infos/initial/reward_dist Std           0.00512225\n",
      "exploration/env_infos/initial/reward_dist Max           0.013882\n",
      "exploration/env_infos/initial/reward_dist Min           2.40008e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0111772\n",
      "exploration/env_infos/reward_dist Std                   0.0443288\n",
      "exploration/env_infos/reward_dist Max                   0.320754\n",
      "exploration/env_infos/reward_dist Min                   3.44758e-56\n",
      "exploration/env_infos/final/reward_energy Mean         -0.200055\n",
      "exploration/env_infos/final/reward_energy Std           0.0717528\n",
      "exploration/env_infos/final/reward_energy Max          -0.127019\n",
      "exploration/env_infos/final/reward_energy Min          -0.308255\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.379102\n",
      "exploration/env_infos/initial/reward_energy Std         0.254928\n",
      "exploration/env_infos/initial/reward_energy Max        -0.167003\n",
      "exploration/env_infos/initial/reward_energy Min        -0.790057\n",
      "exploration/env_infos/reward_energy Mean               -0.220564\n",
      "exploration/env_infos/reward_energy Std                 0.189501\n",
      "exploration/env_infos/reward_energy Max                -0.0119804\n",
      "exploration/env_infos/reward_energy Min                -0.884617\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.0410105\n",
      "exploration/env_infos/final/end_effector_loc Std        0.518885\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.771769\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00587854\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0150441\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.037008\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.013816\n",
      "exploration/env_infos/end_effector_loc Mean             0.0577314\n",
      "exploration/env_infos/end_effector_loc Std              0.302228\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -0.771769\n",
      "evaluation/num steps total                           9000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.17266\n",
      "evaluation/Rewards Std                                  0.114783\n",
      "evaluation/Rewards Max                                  0.0694299\n",
      "evaluation/Rewards Min                                 -0.977339\n",
      "evaluation/Returns Mean                                -3.4532\n",
      "evaluation/Returns Std                                  1.63278\n",
      "evaluation/Returns Max                                 -0.609876\n",
      "evaluation/Returns Min                                 -8.09088\n",
      "evaluation/Actions Mean                                -0.0144938\n",
      "evaluation/Actions Std                                  0.135241\n",
      "evaluation/Actions Max                                  0.817857\n",
      "evaluation/Actions Min                                 -0.764278\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.4532\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0341713\n",
      "evaluation/env_infos/final/reward_dist Std              0.13621\n",
      "evaluation/env_infos/final/reward_dist Max              0.882031\n",
      "evaluation/env_infos/final/reward_dist Min              4.91068e-118\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00640933\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0145283\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0885174\n",
      "evaluation/env_infos/initial/reward_dist Min            4.48761e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0270263\n",
      "evaluation/env_infos/reward_dist Std                    0.123674\n",
      "evaluation/env_infos/reward_dist Max                    0.959137\n",
      "evaluation/env_infos/reward_dist Min                    2.58579e-118\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.136292\n",
      "evaluation/env_infos/final/reward_energy Std            0.0963622\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0205446\n",
      "evaluation/env_infos/final/reward_energy Min           -0.521737\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.252065\n",
      "evaluation/env_infos/initial/reward_energy Std          0.218089\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0197162\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.832727\n",
      "evaluation/env_infos/reward_energy Mean                -0.135048\n",
      "evaluation/env_infos/reward_energy Std                  0.136976\n",
      "evaluation/env_infos/reward_energy Max                 -0.00143426\n",
      "evaluation/env_infos/reward_energy Min                 -0.878671\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0312723\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.4489\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000623246\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.011768\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0408152\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0336095\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0123837\n",
      "evaluation/env_infos/end_effector_loc Std               0.282542\n",
      "evaluation/env_infos/end_effector_loc Max               1\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.171167\n",
      "time/evaluation sampling (s)                            0.943539\n",
      "time/exploration sampling (s)                           0.118137\n",
      "time/logging (s)                                        0.0210369\n",
      "time/saving (s)                                         0.0315204\n",
      "time/training (s)                                      41.8207\n",
      "time/epoch (s)                                         43.1061\n",
      "time/total (s)                                        376.846\n",
      "Epoch                                                   8\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:19:30.878877 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 9 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00432969\n",
      "trainer/QF2 Loss                                         0.00949648\n",
      "trainer/Policy Loss                                      3.88852\n",
      "trainer/Q1 Predictions Mean                             -1.95893\n",
      "trainer/Q1 Predictions Std                               1.52931\n",
      "trainer/Q1 Predictions Max                               0.248303\n",
      "trainer/Q1 Predictions Min                              -7.67954\n",
      "trainer/Q2 Predictions Mean                             -1.95263\n",
      "trainer/Q2 Predictions Std                               1.51812\n",
      "trainer/Q2 Predictions Max                               0.200359\n",
      "trainer/Q2 Predictions Min                              -7.72436\n",
      "trainer/Q Targets Mean                                  -1.95022\n",
      "trainer/Q Targets Std                                    1.52448\n",
      "trainer/Q Targets Max                                    0.285175\n",
      "trainer/Q Targets Min                                   -7.60175\n",
      "trainer/Log Pis Mean                                     2.03785\n",
      "trainer/Log Pis Std                                      1.71557\n",
      "trainer/Log Pis Max                                      8.68068\n",
      "trainer/Log Pis Min                                     -5.26658\n",
      "trainer/Policy mu Mean                                  -0.112582\n",
      "trainer/Policy mu Std                                    0.679807\n",
      "trainer/Policy mu Max                                    3.07349\n",
      "trainer/Policy mu Min                                   -3.20926\n",
      "trainer/Policy log std Mean                             -2.10826\n",
      "trainer/Policy log std Std                               0.764556\n",
      "trainer/Policy log std Max                              -0.281164\n",
      "trainer/Policy log std Min                              -3.45967\n",
      "trainer/Alpha                                            0.0111275\n",
      "trainer/Alpha Loss                                       0.170275\n",
      "exploration/num steps total                           2000\n",
      "exploration/num paths total                            100\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.187784\n",
      "exploration/Rewards Std                                  0.114619\n",
      "exploration/Rewards Max                                  0.0296615\n",
      "exploration/Rewards Min                                 -0.576474\n",
      "exploration/Returns Mean                                -3.75567\n",
      "exploration/Returns Std                                  1.1919\n",
      "exploration/Returns Max                                 -1.99091\n",
      "exploration/Returns Min                                 -5.50404\n",
      "exploration/Actions Mean                                -0.00838535\n",
      "exploration/Actions Std                                  0.164762\n",
      "exploration/Actions Max                                  0.466892\n",
      "exploration/Actions Min                                 -0.634303\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.75567\n",
      "exploration/env_infos/final/reward_dist Mean             0.143828\n",
      "exploration/env_infos/final/reward_dist Std              0.287656\n",
      "exploration/env_infos/final/reward_dist Max              0.719141\n",
      "exploration/env_infos/final/reward_dist Min              1.72796e-32\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00133341\n",
      "exploration/env_infos/initial/reward_dist Std            0.00173258\n",
      "exploration/env_infos/initial/reward_dist Max            0.00440372\n",
      "exploration/env_infos/initial/reward_dist Min            1.15914e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0404029\n",
      "exploration/env_infos/reward_dist Std                    0.124964\n",
      "exploration/env_infos/reward_dist Max                    0.719141\n",
      "exploration/env_infos/reward_dist Min                    1.72796e-32\n",
      "exploration/env_infos/final/reward_energy Mean          -0.215428\n",
      "exploration/env_infos/final/reward_energy Std            0.136869\n",
      "exploration/env_infos/final/reward_energy Max           -0.0102378\n",
      "exploration/env_infos/final/reward_energy Min           -0.358182\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.256306\n",
      "exploration/env_infos/initial/reward_energy Std          0.120952\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0407215\n",
      "exploration/env_infos/initial/reward_energy Min         -0.385877\n",
      "exploration/env_infos/reward_energy Mean                -0.195565\n",
      "exploration/env_infos/reward_energy Std                  0.127231\n",
      "exploration/env_infos/reward_energy Max                 -0.00972723\n",
      "exploration/env_infos/reward_energy Min                 -0.696804\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00419778\n",
      "exploration/env_infos/final/end_effector_loc Std         0.277579\n",
      "exploration/env_infos/final/end_effector_loc Max         0.433277\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.658093\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00205885\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00980631\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0181713\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.01456\n",
      "exploration/env_infos/end_effector_loc Mean              0.0136436\n",
      "exploration/env_infos/end_effector_loc Std               0.15759\n",
      "exploration/env_infos/end_effector_loc Max               0.433277\n",
      "exploration/env_infos/end_effector_loc Min              -0.658093\n",
      "evaluation/num steps total                           10000\n",
      "evaluation/num paths total                             500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.172073\n",
      "evaluation/Rewards Std                                   0.16912\n",
      "evaluation/Rewards Max                                   0.133876\n",
      "evaluation/Rewards Min                                  -1.33916\n",
      "evaluation/Returns Mean                                 -3.44147\n",
      "evaluation/Returns Std                                   2.24701\n",
      "evaluation/Returns Max                                   0.635755\n",
      "evaluation/Returns Min                                  -9.63737\n",
      "evaluation/Actions Mean                                 -0.0133422\n",
      "evaluation/Actions Std                                   0.128601\n",
      "evaluation/Actions Max                                   0.827027\n",
      "evaluation/Actions Min                                  -0.745304\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -3.44147\n",
      "evaluation/env_infos/final/reward_dist Mean              0.00376458\n",
      "evaluation/env_infos/final/reward_dist Std               0.0206459\n",
      "evaluation/env_infos/final/reward_dist Max               0.145419\n",
      "evaluation/env_infos/final/reward_dist Min               4.56394e-183\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00546571\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0108334\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0451844\n",
      "evaluation/env_infos/initial/reward_dist Min             6.3523e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0320316\n",
      "evaluation/env_infos/reward_dist Std                     0.127326\n",
      "evaluation/env_infos/reward_dist Max                     0.988918\n",
      "evaluation/env_infos/reward_dist Min                     4.56394e-183\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.161763\n",
      "evaluation/env_infos/final/reward_energy Std             0.171641\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00583886\n",
      "evaluation/env_infos/final/reward_energy Min            -0.649548\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.307693\n",
      "evaluation/env_infos/initial/reward_energy Std           0.203542\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0426279\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.891255\n",
      "evaluation/env_infos/reward_energy Mean                 -0.136088\n",
      "evaluation/env_infos/reward_energy Std                   0.122117\n",
      "evaluation/env_infos/reward_energy Max                  -0.00369965\n",
      "evaluation/env_infos/reward_energy Min                  -0.891255\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.139958\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.482312\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000220228\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0130415\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0413513\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0372652\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.048171\n",
      "evaluation/env_infos/end_effector_loc Std                0.314972\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.177306\n",
      "time/evaluation sampling (s)                             1.21679\n",
      "time/exploration sampling (s)                            0.144775\n",
      "time/logging (s)                                         0.0182229\n",
      "time/saving (s)                                          0.0267277\n",
      "time/training (s)                                       42.2538\n",
      "time/epoch (s)                                          43.8376\n",
      "time/total (s)                                         420.985\n",
      "Epoch                                                    9\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:20:16.563968 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 10 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00375932\n",
      "trainer/QF2 Loss                                         0.00386138\n",
      "trainer/Policy Loss                                      3.6665\n",
      "trainer/Q1 Predictions Mean                             -1.77828\n",
      "trainer/Q1 Predictions Std                               1.34037\n",
      "trainer/Q1 Predictions Max                               0.0250147\n",
      "trainer/Q1 Predictions Min                              -7.70352\n",
      "trainer/Q2 Predictions Mean                             -1.80202\n",
      "trainer/Q2 Predictions Std                               1.35646\n",
      "trainer/Q2 Predictions Max                               0.0587815\n",
      "trainer/Q2 Predictions Min                              -7.73715\n",
      "trainer/Q Targets Mean                                  -1.79382\n",
      "trainer/Q Targets Std                                    1.34041\n",
      "trainer/Q Targets Max                                    0.0277676\n",
      "trainer/Q Targets Min                                   -7.49038\n",
      "trainer/Log Pis Mean                                     2.00101\n",
      "trainer/Log Pis Std                                      1.41792\n",
      "trainer/Log Pis Max                                      7.69955\n",
      "trainer/Log Pis Min                                     -3.86346\n",
      "trainer/Policy mu Mean                                  -0.144252\n",
      "trainer/Policy mu Std                                    0.659271\n",
      "trainer/Policy mu Max                                    3.50321\n",
      "trainer/Policy mu Min                                   -2.99413\n",
      "trainer/Policy log std Mean                             -2.06312\n",
      "trainer/Policy log std Std                               0.756047\n",
      "trainer/Policy log std Max                               0.134629\n",
      "trainer/Policy log std Min                              -3.50487\n",
      "trainer/Alpha                                            0.0126905\n",
      "trainer/Alpha Loss                                       0.00441414\n",
      "exploration/num steps total                           2100\n",
      "exploration/num paths total                            105\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.351919\n",
      "exploration/Rewards Std                                  0.25124\n",
      "exploration/Rewards Max                                 -0.00248589\n",
      "exploration/Rewards Min                                 -0.909028\n",
      "exploration/Returns Mean                                -7.03838\n",
      "exploration/Returns Std                                  3.10261\n",
      "exploration/Returns Max                                 -2.78787\n",
      "exploration/Returns Min                                -10.5875\n",
      "exploration/Actions Mean                                -0.0117833\n",
      "exploration/Actions Std                                  0.242177\n",
      "exploration/Actions Max                                  0.956115\n",
      "exploration/Actions Min                                 -0.64888\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -7.03838\n",
      "exploration/env_infos/final/reward_dist Mean             7.17182e-16\n",
      "exploration/env_infos/final/reward_dist Std              1.36332e-15\n",
      "exploration/env_infos/final/reward_dist Max              3.44152e-15\n",
      "exploration/env_infos/final/reward_dist Min              1.73361e-90\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0104677\n",
      "exploration/env_infos/initial/reward_dist Std            0.0157596\n",
      "exploration/env_infos/initial/reward_dist Max            0.0415014\n",
      "exploration/env_infos/initial/reward_dist Min            3.22839e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.00760994\n",
      "exploration/env_infos/reward_dist Std                    0.0444904\n",
      "exploration/env_infos/reward_dist Max                    0.412049\n",
      "exploration/env_infos/reward_dist Min                    1.73361e-90\n",
      "exploration/env_infos/final/reward_energy Mean          -0.188189\n",
      "exploration/env_infos/final/reward_energy Std            0.0966954\n",
      "exploration/env_infos/final/reward_energy Max           -0.0308467\n",
      "exploration/env_infos/final/reward_energy Min           -0.324004\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.47689\n",
      "exploration/env_infos/initial/reward_energy Std          0.347521\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0883841\n",
      "exploration/env_infos/initial/reward_energy Min         -0.964634\n",
      "exploration/env_infos/reward_energy Mean                -0.259544\n",
      "exploration/env_infos/reward_energy Std                  0.224085\n",
      "exploration/env_infos/reward_energy Max                 -0.00563919\n",
      "exploration/env_infos/reward_energy Min                 -0.964634\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.155729\n",
      "exploration/env_infos/final/end_effector_loc Std         0.631204\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00709865\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0196177\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0478057\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0142489\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0417779\n",
      "exploration/env_infos/end_effector_loc Std               0.452022\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           11000\n",
      "evaluation/num paths total                             550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.123058\n",
      "evaluation/Rewards Std                                   0.118541\n",
      "evaluation/Rewards Max                                   0.178139\n",
      "evaluation/Rewards Min                                  -0.739437\n",
      "evaluation/Returns Mean                                 -2.46116\n",
      "evaluation/Returns Std                                   1.71654\n",
      "evaluation/Returns Max                                   1.60115\n",
      "evaluation/Returns Min                                  -6.64254\n",
      "evaluation/Actions Mean                                 -0.00773353\n",
      "evaluation/Actions Std                                   0.101417\n",
      "evaluation/Actions Max                                   0.778709\n",
      "evaluation/Actions Min                                  -0.546928\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.46116\n",
      "evaluation/env_infos/final/reward_dist Mean              0.040064\n",
      "evaluation/env_infos/final/reward_dist Std               0.14852\n",
      "evaluation/env_infos/final/reward_dist Max               0.84485\n",
      "evaluation/env_infos/final/reward_dist Min               7.04058e-121\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0072069\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00998785\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0436458\n",
      "evaluation/env_infos/initial/reward_dist Min             1.33628e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0466879\n",
      "evaluation/env_infos/reward_dist Std                     0.146497\n",
      "evaluation/env_infos/reward_dist Max                     0.985842\n",
      "evaluation/env_infos/reward_dist Min                     7.04058e-121\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.099192\n",
      "evaluation/env_infos/final/reward_energy Std             0.0951603\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00261687\n",
      "evaluation/env_infos/final/reward_energy Min            -0.454667\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.209966\n",
      "evaluation/env_infos/initial/reward_energy Std           0.200965\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00718199\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.969747\n",
      "evaluation/env_infos/reward_energy Mean                 -0.100144\n",
      "evaluation/env_infos/reward_energy Std                   0.103255\n",
      "evaluation/env_infos/reward_energy Max                  -0.00135115\n",
      "evaluation/env_infos/reward_energy Min                  -0.969747\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0213261\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.382142\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.718012\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00398973\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00946958\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0389355\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0200942\n",
      "evaluation/env_infos/end_effector_loc Mean               0.041182\n",
      "evaluation/env_infos/end_effector_loc Std                0.213378\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.718012\n",
      "time/data storing (s)                                    0.179135\n",
      "time/evaluation sampling (s)                             0.923932\n",
      "time/exploration sampling (s)                            0.120966\n",
      "time/logging (s)                                         0.0197826\n",
      "time/saving (s)                                          0.0284253\n",
      "time/training (s)                                       44.262\n",
      "time/epoch (s)                                          45.5342\n",
      "time/total (s)                                         466.671\n",
      "Epoch                                                   10\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:21:04.258137 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 11 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00508362\r\n",
      "trainer/QF2 Loss                                         0.00336302\r\n",
      "trainer/Policy Loss                                      3.70492\r\n",
      "trainer/Q1 Predictions Mean                             -1.59371\r\n",
      "trainer/Q1 Predictions Std                               1.08342\r\n",
      "trainer/Q1 Predictions Max                               0.635657\r\n",
      "trainer/Q1 Predictions Min                              -6.7742\r\n",
      "trainer/Q2 Predictions Mean                             -1.59499\r\n",
      "trainer/Q2 Predictions Std                               1.10311\r\n",
      "trainer/Q2 Predictions Max                               0.627198\r\n",
      "trainer/Q2 Predictions Min                              -6.87951\r\n",
      "trainer/Q Targets Mean                                  -1.60293\r\n",
      "trainer/Q Targets Std                                    1.1026\r\n",
      "trainer/Q Targets Max                                    0.615727\r\n",
      "trainer/Q Targets Min                                   -6.9573\r\n",
      "trainer/Log Pis Mean                                     2.16442\r\n",
      "trainer/Log Pis Std                                      1.44247\r\n",
      "trainer/Log Pis Max                                      7.09072\r\n",
      "trainer/Log Pis Min                                     -2.70239\r\n",
      "trainer/Policy mu Mean                                  -0.0712673\r\n",
      "trainer/Policy mu Std                                    0.641167\r\n",
      "trainer/Policy mu Max                                    2.88123\r\n",
      "trainer/Policy mu Min                                   -2.85011\r\n",
      "trainer/Policy log std Mean                             -2.16279\r\n",
      "trainer/Policy log std Std                               0.704553\r\n",
      "trainer/Policy log std Max                              -0.185432\r\n",
      "trainer/Policy log std Min                              -3.38647\r\n",
      "trainer/Alpha                                            0.0130276\r\n",
      "trainer/Alpha Loss                                       0.713781\r\n",
      "exploration/num steps total                           2200\r\n",
      "exploration/num paths total                            110\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0773909\r\n",
      "exploration/Rewards Std                                  0.0753963\r\n",
      "exploration/Rewards Max                                  0.0731863\r\n",
      "exploration/Rewards Min                                 -0.373624\r\n",
      "exploration/Returns Mean                                -1.54782\r\n",
      "exploration/Returns Std                                  0.868501\r\n",
      "exploration/Returns Max                                  0.127097\r\n",
      "exploration/Returns Min                                 -2.24173\r\n",
      "exploration/Actions Mean                                -0.0147604\r\n",
      "exploration/Actions Std                                  0.123196\r\n",
      "exploration/Actions Max                                  0.32415\r\n",
      "exploration/Actions Min                                 -0.668935\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.54782\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.000499486\r\n",
      "exploration/env_infos/final/reward_dist Std              0.000829286\r\n",
      "exploration/env_infos/final/reward_dist Max              0.00214961\r\n",
      "exploration/env_infos/final/reward_dist Min              5.14296e-20\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00727142\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00794715\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0181094\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.32066e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.145168\r\n",
      "exploration/env_infos/reward_dist Std                    0.257929\r\n",
      "exploration/env_infos/reward_dist Max                    0.986206\r\n",
      "exploration/env_infos/reward_dist Min                    5.14296e-20\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.126821\r\n",
      "exploration/env_infos/final/reward_energy Std            0.060831\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.063874\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.236822\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.131434\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0772697\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0349751\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.235663\r\n",
      "exploration/env_infos/reward_energy Mean                -0.136786\r\n",
      "exploration/env_infos/reward_energy Std                  0.10991\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0125471\r\n",
      "exploration/env_infos/reward_energy Min                 -0.674851\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0538256\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.241035\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.436396\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.38482\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000280046\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00538316\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00881542\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0108689\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00269004\r\n",
      "exploration/env_infos/end_effector_loc Std               0.131697\r\n",
      "exploration/env_infos/end_effector_loc Max               0.436396\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.38482\r\n",
      "evaluation/num steps total                           12000\r\n",
      "evaluation/num paths total                             600\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.131871\r\n",
      "evaluation/Rewards Std                                   0.118552\r\n",
      "evaluation/Rewards Max                                   0.123927\r\n",
      "evaluation/Rewards Min                                  -0.84059\r\n",
      "evaluation/Returns Mean                                 -2.63742\r\n",
      "evaluation/Returns Std                                   1.81808\r\n",
      "evaluation/Returns Max                                   0.708023\r\n",
      "evaluation/Returns Min                                  -8.97095\r\n",
      "evaluation/Actions Mean                                 -0.00228051\r\n",
      "evaluation/Actions Std                                   0.133697\r\n",
      "evaluation/Actions Max                                   0.858471\r\n",
      "evaluation/Actions Min                                  -0.988873\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.63742\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0448632\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.159926\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.735813\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.50509e-165\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00601648\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00849142\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0292654\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.62967e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0357439\r\n",
      "evaluation/env_infos/reward_dist Std                     0.115876\r\n",
      "evaluation/env_infos/reward_dist Max                     0.98411\r\n",
      "evaluation/env_infos/reward_dist Min                     5.50509e-165\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0781812\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0663491\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00879432\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.346223\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.315906\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.289615\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0192281\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.12601\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.11466\r\n",
      "evaluation/env_infos/reward_energy Std                   0.150377\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00257988\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.12601\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0546761\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.397888\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00331776\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0147846\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0404301\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0494436\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0430164\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.274676\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.178121\r\n",
      "time/evaluation sampling (s)                             1.06612\r\n",
      "time/exploration sampling (s)                            0.142423\r\n",
      "time/logging (s)                                         0.019011\r\n",
      "time/saving (s)                                          0.0270995\r\n",
      "time/training (s)                                       46.0849\r\n",
      "time/epoch (s)                                          47.5177\r\n",
      "time/total (s)                                         514.363\r\n",
      "Epoch                                                   11\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:21:53.736339 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 12 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00436827\r\n",
      "trainer/QF2 Loss                                         0.00471083\r\n",
      "trainer/Policy Loss                                      3.60161\r\n",
      "trainer/Q1 Predictions Mean                             -1.58437\r\n",
      "trainer/Q1 Predictions Std                               1.16664\r\n",
      "trainer/Q1 Predictions Max                               0.489487\r\n",
      "trainer/Q1 Predictions Min                              -6.13129\r\n",
      "trainer/Q2 Predictions Mean                             -1.59585\r\n",
      "trainer/Q2 Predictions Std                               1.17361\r\n",
      "trainer/Q2 Predictions Max                               0.52279\r\n",
      "trainer/Q2 Predictions Min                              -6.20424\r\n",
      "trainer/Q Targets Mean                                  -1.58524\r\n",
      "trainer/Q Targets Std                                    1.18014\r\n",
      "trainer/Q Targets Max                                    0.540905\r\n",
      "trainer/Q Targets Min                                   -6.34138\r\n",
      "trainer/Log Pis Mean                                     2.07089\r\n",
      "trainer/Log Pis Std                                      1.37663\r\n",
      "trainer/Log Pis Max                                      9.6285\r\n",
      "trainer/Log Pis Min                                     -2.96343\r\n",
      "trainer/Policy mu Mean                                   0.0194308\r\n",
      "trainer/Policy mu Std                                    0.6758\r\n",
      "trainer/Policy mu Max                                    3.78291\r\n",
      "trainer/Policy mu Min                                   -2.93252\r\n",
      "trainer/Policy log std Mean                             -2.11089\r\n",
      "trainer/Policy log std Std                               0.696\r\n",
      "trainer/Policy log std Max                               0.365521\r\n",
      "trainer/Policy log std Min                              -3.24961\r\n",
      "trainer/Alpha                                            0.0156643\r\n",
      "trainer/Alpha Loss                                       0.294698\r\n",
      "exploration/num steps total                           2300\r\n",
      "exploration/num paths total                            115\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.173914\r\n",
      "exploration/Rewards Std                                  0.123202\r\n",
      "exploration/Rewards Max                                  0.020627\r\n",
      "exploration/Rewards Min                                 -0.555277\r\n",
      "exploration/Returns Mean                                -3.47829\r\n",
      "exploration/Returns Std                                  1.56991\r\n",
      "exploration/Returns Max                                 -0.958831\r\n",
      "exploration/Returns Min                                 -5.73595\r\n",
      "exploration/Actions Mean                                -0.0160256\r\n",
      "exploration/Actions Std                                  0.112136\r\n",
      "exploration/Actions Max                                  0.486386\r\n",
      "exploration/Actions Min                                 -0.409416\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -3.47829\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00400715\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00801429\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0200357\r\n",
      "exploration/env_infos/final/reward_dist Min              6.46841e-45\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00370872\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00452754\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00930164\r\n",
      "exploration/env_infos/initial/reward_dist Min            8.64227e-07\r\n",
      "exploration/env_infos/reward_dist Mean                   0.00818922\r\n",
      "exploration/env_infos/reward_dist Std                    0.0241299\r\n",
      "exploration/env_infos/reward_dist Max                    0.171947\r\n",
      "exploration/env_infos/reward_dist Min                    6.46841e-45\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0788333\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0435452\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.024686\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.152422\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.198723\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0973748\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0625437\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.362634\r\n",
      "exploration/env_infos/reward_energy Mean                -0.124543\r\n",
      "exploration/env_infos/reward_energy Std                  0.100755\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00578171\r\n",
      "exploration/env_infos/reward_energy Min                 -0.52634\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.219739\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.292281\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.147962\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.891772\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00316972\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00715323\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00652812\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0176643\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.105393\r\n",
      "exploration/env_infos/end_effector_loc Std               0.20296\r\n",
      "exploration/env_infos/end_effector_loc Max               0.147962\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.891772\r\n",
      "evaluation/num steps total                           13000\r\n",
      "evaluation/num paths total                             650\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.125398\r\n",
      "evaluation/Rewards Std                                   0.123925\r\n",
      "evaluation/Rewards Max                                   0.1084\r\n",
      "evaluation/Rewards Min                                  -0.879698\r\n",
      "evaluation/Returns Mean                                 -2.50797\r\n",
      "evaluation/Returns Std                                   2.06089\r\n",
      "evaluation/Returns Max                                   1.6589\r\n",
      "evaluation/Returns Min                                  -9.8894\r\n",
      "evaluation/Actions Mean                                 -0.00298701\r\n",
      "evaluation/Actions Std                                   0.120858\r\n",
      "evaluation/Actions Max                                   0.878175\r\n",
      "evaluation/Actions Min                                  -0.839027\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.50797\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.045579\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.173493\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.866043\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.91876e-185\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00791295\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0182642\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0975853\r\n",
      "evaluation/env_infos/initial/reward_dist Min             5.6482e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0410415\r\n",
      "evaluation/env_infos/reward_dist Std                     0.144235\r\n",
      "evaluation/env_infos/reward_dist Max                     0.95596\r\n",
      "evaluation/env_infos/reward_dist Min                     3.91876e-185\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0921162\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.116084\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00797032\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.509026\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.26269\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.232692\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0269401\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.91267\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.10912\r\n",
      "evaluation/env_infos/reward_energy Std                   0.13162\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000704015\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.91267\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00190371\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.373925\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00216609\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122167\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0439088\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.02704\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0218112\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.238931\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.193832\r\n",
      "time/evaluation sampling (s)                             1.01957\r\n",
      "time/exploration sampling (s)                            0.135077\r\n",
      "time/logging (s)                                         0.0189747\r\n",
      "time/saving (s)                                          0.0278517\r\n",
      "time/training (s)                                       47.9009\r\n",
      "time/epoch (s)                                          49.2962\r\n",
      "time/total (s)                                         563.841\r\n",
      "Epoch                                                   12\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:22:41.134023 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 13 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00372415\n",
      "trainer/QF2 Loss                                         0.00450029\n",
      "trainer/Policy Loss                                      3.39179\n",
      "trainer/Q1 Predictions Mean                             -1.5031\n",
      "trainer/Q1 Predictions Std                               1.07709\n",
      "trainer/Q1 Predictions Max                               0.718206\n",
      "trainer/Q1 Predictions Min                              -4.6791\n",
      "trainer/Q2 Predictions Mean                             -1.54052\n",
      "trainer/Q2 Predictions Std                               1.07596\n",
      "trainer/Q2 Predictions Max                               0.767517\n",
      "trainer/Q2 Predictions Min                              -4.87849\n",
      "trainer/Q Targets Mean                                  -1.51184\n",
      "trainer/Q Targets Std                                    1.07121\n",
      "trainer/Q Targets Max                                    0.77626\n",
      "trainer/Q Targets Min                                   -4.81492\n",
      "trainer/Log Pis Mean                                     1.92119\n",
      "trainer/Log Pis Std                                      1.38772\n",
      "trainer/Log Pis Max                                      6.35731\n",
      "trainer/Log Pis Min                                     -6.00636\n",
      "trainer/Policy mu Mean                                  -0.030332\n",
      "trainer/Policy mu Std                                    0.616604\n",
      "trainer/Policy mu Max                                    2.74742\n",
      "trainer/Policy mu Min                                   -2.48769\n",
      "trainer/Policy log std Mean                             -2.10196\n",
      "trainer/Policy log std Std                               0.656503\n",
      "trainer/Policy log std Max                              -0.0974851\n",
      "trainer/Policy log std Min                              -3.22321\n",
      "trainer/Alpha                                            0.0171183\n",
      "trainer/Alpha Loss                                      -0.320557\n",
      "exploration/num steps total                           2400\n",
      "exploration/num paths total                            120\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.179286\n",
      "exploration/Rewards Std                                  0.106115\n",
      "exploration/Rewards Max                                  0.0314356\n",
      "exploration/Rewards Min                                 -0.513585\n",
      "exploration/Returns Mean                                -3.58573\n",
      "exploration/Returns Std                                  0.910134\n",
      "exploration/Returns Max                                 -2.30097\n",
      "exploration/Returns Min                                 -4.98422\n",
      "exploration/Actions Mean                                 0.000128117\n",
      "exploration/Actions Std                                  0.154865\n",
      "exploration/Actions Max                                  0.45417\n",
      "exploration/Actions Min                                 -0.475243\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.58573\n",
      "exploration/env_infos/final/reward_dist Mean             0.000507639\n",
      "exploration/env_infos/final/reward_dist Std              0.00101528\n",
      "exploration/env_infos/final/reward_dist Max              0.0025382\n",
      "exploration/env_infos/final/reward_dist Min              3.48882e-60\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0122829\n",
      "exploration/env_infos/initial/reward_dist Std            0.0207355\n",
      "exploration/env_infos/initial/reward_dist Max            0.0536094\n",
      "exploration/env_infos/initial/reward_dist Min            7.82664e-07\n",
      "exploration/env_infos/reward_dist Mean                   0.0262456\n",
      "exploration/env_infos/reward_dist Std                    0.0886218\n",
      "exploration/env_infos/reward_dist Max                    0.613538\n",
      "exploration/env_infos/reward_dist Min                    3.48882e-60\n",
      "exploration/env_infos/final/reward_energy Mean          -0.128828\n",
      "exploration/env_infos/final/reward_energy Std            0.0797272\n",
      "exploration/env_infos/final/reward_energy Max           -0.0599201\n",
      "exploration/env_infos/final/reward_energy Min           -0.276414\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.258612\n",
      "exploration/env_infos/initial/reward_energy Std          0.148042\n",
      "exploration/env_infos/initial/reward_energy Max         -0.062248\n",
      "exploration/env_infos/initial/reward_energy Min         -0.454312\n",
      "exploration/env_infos/reward_energy Mean                -0.184611\n",
      "exploration/env_infos/reward_energy Std                  0.117835\n",
      "exploration/env_infos/reward_energy Max                 -0.00807313\n",
      "exploration/env_infos/reward_energy Min                 -0.555415\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.160611\n",
      "exploration/env_infos/final/end_effector_loc Std         0.454095\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.836992\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00402873\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00973473\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0227085\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0154693\n",
      "exploration/env_infos/end_effector_loc Mean              0.0982248\n",
      "exploration/env_infos/end_effector_loc Std               0.282059\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -0.836992\n",
      "evaluation/num steps total                           14000\n",
      "evaluation/num paths total                             700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.170656\n",
      "evaluation/Rewards Std                                   0.151066\n",
      "evaluation/Rewards Max                                   0.160317\n",
      "evaluation/Rewards Min                                  -0.948084\n",
      "evaluation/Returns Mean                                 -3.41313\n",
      "evaluation/Returns Std                                   2.37003\n",
      "evaluation/Returns Max                                   0.835648\n",
      "evaluation/Returns Min                                 -10.5219\n",
      "evaluation/Actions Mean                                 -0.00219925\n",
      "evaluation/Actions Std                                   0.146213\n",
      "evaluation/Actions Max                                   0.742587\n",
      "evaluation/Actions Min                                  -0.912639\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -3.41313\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0189754\n",
      "evaluation/env_infos/final/reward_dist Std               0.067522\n",
      "evaluation/env_infos/final/reward_dist Max               0.372218\n",
      "evaluation/env_infos/final/reward_dist Min               2.11609e-172\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00743573\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0119804\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0496238\n",
      "evaluation/env_infos/initial/reward_dist Min             5.37165e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0318105\n",
      "evaluation/env_infos/reward_dist Std                     0.12675\n",
      "evaluation/env_infos/reward_dist Max                     0.980774\n",
      "evaluation/env_infos/reward_dist Min                     2.11609e-172\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0984182\n",
      "evaluation/env_infos/final/reward_energy Std             0.0846333\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00510355\n",
      "evaluation/env_infos/final/reward_energy Min            -0.440591\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.282127\n",
      "evaluation/env_infos/initial/reward_energy Std           0.229451\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0238195\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.00612\n",
      "evaluation/env_infos/reward_energy Mean                 -0.144706\n",
      "evaluation/env_infos/reward_energy Std                   0.147738\n",
      "evaluation/env_infos/reward_energy Max                  -0.00321104\n",
      "evaluation/env_infos/reward_energy Min                  -1.00612\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.11211\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.535749\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00133426\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127877\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0371294\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0456319\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0537763\n",
      "evaluation/env_infos/end_effector_loc Std                0.336184\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.187823\n",
      "time/evaluation sampling (s)                             1.22176\n",
      "time/exploration sampling (s)                            0.127844\n",
      "time/logging (s)                                         0.0193261\n",
      "time/saving (s)                                          0.0277258\n",
      "time/training (s)                                       45.6192\n",
      "time/epoch (s)                                          47.2037\n",
      "time/total (s)                                         611.238\n",
      "Epoch                                                   13\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:23:26.820266 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 14 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00254301\n",
      "trainer/QF2 Loss                                         0.00295065\n",
      "trainer/Policy Loss                                      3.43288\n",
      "trainer/Q1 Predictions Mean                             -1.41961\n",
      "trainer/Q1 Predictions Std                               1.0654\n",
      "trainer/Q1 Predictions Max                               0.614613\n",
      "trainer/Q1 Predictions Min                              -5.14882\n",
      "trainer/Q2 Predictions Mean                             -1.40999\n",
      "trainer/Q2 Predictions Std                               1.06072\n",
      "trainer/Q2 Predictions Max                               0.705597\n",
      "trainer/Q2 Predictions Min                              -5.00753\n",
      "trainer/Q Targets Mean                                  -1.43226\n",
      "trainer/Q Targets Std                                    1.05382\n",
      "trainer/Q Targets Max                                    0.568111\n",
      "trainer/Q Targets Min                                   -5.09475\n",
      "trainer/Log Pis Mean                                     2.07512\n",
      "trainer/Log Pis Std                                      1.37585\n",
      "trainer/Log Pis Max                                      6.37336\n",
      "trainer/Log Pis Min                                     -4.19628\n",
      "trainer/Policy mu Mean                                  -0.0289633\n",
      "trainer/Policy mu Std                                    0.501602\n",
      "trainer/Policy mu Max                                    2.62307\n",
      "trainer/Policy mu Min                                   -2.45589\n",
      "trainer/Policy log std Mean                             -2.26313\n",
      "trainer/Policy log std Std                               0.599945\n",
      "trainer/Policy log std Max                              -0.193658\n",
      "trainer/Policy log std Min                              -3.40329\n",
      "trainer/Alpha                                            0.0188505\n",
      "trainer/Alpha Loss                                       0.298288\n",
      "exploration/num steps total                           2500\n",
      "exploration/num paths total                            125\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.144813\n",
      "exploration/Rewards Std                                  0.0643495\n",
      "exploration/Rewards Max                                 -0.025124\n",
      "exploration/Rewards Min                                 -0.294684\n",
      "exploration/Returns Mean                                -2.89626\n",
      "exploration/Returns Std                                  0.908743\n",
      "exploration/Returns Max                                 -1.85465\n",
      "exploration/Returns Min                                 -4.12178\n",
      "exploration/Actions Mean                                 0.0141318\n",
      "exploration/Actions Std                                  0.174819\n",
      "exploration/Actions Max                                  0.688549\n",
      "exploration/Actions Min                                 -0.540635\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.89626\n",
      "exploration/env_infos/final/reward_dist Mean             0.111085\n",
      "exploration/env_infos/final/reward_dist Std              0.222169\n",
      "exploration/env_infos/final/reward_dist Max              0.555424\n",
      "exploration/env_infos/final/reward_dist Min              2.45151e-100\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00424078\n",
      "exploration/env_infos/initial/reward_dist Std            0.00287944\n",
      "exploration/env_infos/initial/reward_dist Max            0.00705017\n",
      "exploration/env_infos/initial/reward_dist Min            7.04973e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0463842\n",
      "exploration/env_infos/reward_dist Std                    0.122543\n",
      "exploration/env_infos/reward_dist Max                    0.560228\n",
      "exploration/env_infos/reward_dist Min                    2.45151e-100\n",
      "exploration/env_infos/final/reward_energy Mean          -0.146526\n",
      "exploration/env_infos/final/reward_energy Std            0.0936752\n",
      "exploration/env_infos/final/reward_energy Max           -0.0352998\n",
      "exploration/env_infos/final/reward_energy Min           -0.263744\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.32472\n",
      "exploration/env_infos/initial/reward_energy Std          0.19688\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0393303\n",
      "exploration/env_infos/initial/reward_energy Min         -0.582162\n",
      "exploration/env_infos/reward_energy Mean                -0.191438\n",
      "exploration/env_infos/reward_energy Std                  0.157716\n",
      "exploration/env_infos/reward_energy Max                 -0.0259084\n",
      "exploration/env_infos/reward_energy Min                 -0.787305\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.127992\n",
      "exploration/env_infos/final/end_effector_loc Std         0.630872\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00335809\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0129992\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0192169\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0270317\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0562268\n",
      "exploration/env_infos/end_effector_loc Std               0.437086\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           15000\n",
      "evaluation/num paths total                             750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.125188\n",
      "evaluation/Rewards Std                                   0.15293\n",
      "evaluation/Rewards Max                                   0.114742\n",
      "evaluation/Rewards Min                                  -1.338\n",
      "evaluation/Returns Mean                                 -2.50377\n",
      "evaluation/Returns Std                                   2.37564\n",
      "evaluation/Returns Max                                   1.14732\n",
      "evaluation/Returns Min                                 -13.6777\n",
      "evaluation/Actions Mean                                  0.00511224\n",
      "evaluation/Actions Std                                   0.107422\n",
      "evaluation/Actions Max                                   0.597285\n",
      "evaluation/Actions Min                                  -0.970582\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.50377\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0164729\n",
      "evaluation/env_infos/final/reward_dist Std               0.0714971\n",
      "evaluation/env_infos/final/reward_dist Max               0.477969\n",
      "evaluation/env_infos/final/reward_dist Min               5.44516e-173\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00345077\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00513179\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0203038\n",
      "evaluation/env_infos/initial/reward_dist Min             1.06126e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0417693\n",
      "evaluation/env_infos/reward_dist Std                     0.124628\n",
      "evaluation/env_infos/reward_dist Max                     0.950878\n",
      "evaluation/env_infos/reward_dist Min                     5.44516e-173\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0764095\n",
      "evaluation/env_infos/final/reward_energy Std             0.0801973\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00350672\n",
      "evaluation/env_infos/final/reward_energy Min            -0.344498\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.243858\n",
      "evaluation/env_infos/initial/reward_energy Std           0.229635\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0294266\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.33946\n",
      "evaluation/env_infos/reward_energy Mean                 -0.102391\n",
      "evaluation/env_infos/reward_energy Std                   0.112461\n",
      "evaluation/env_infos/reward_energy Max                  -0.00099204\n",
      "evaluation/env_infos/reward_energy Min                  -1.33946\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0314293\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.411794\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000299868\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0118389\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0298643\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0485291\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0068352\n",
      "evaluation/env_infos/end_effector_loc Std                0.265035\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.177008\n",
      "time/evaluation sampling (s)                             0.946792\n",
      "time/exploration sampling (s)                            0.122521\n",
      "time/logging (s)                                         0.0199422\n",
      "time/saving (s)                                          0.0268975\n",
      "time/training (s)                                       44.19\n",
      "time/epoch (s)                                          45.4832\n",
      "time/total (s)                                         656.924\n",
      "Epoch                                                   14\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:24:11.861964 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 15 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00204143\r\n",
      "trainer/QF2 Loss                                         0.0037417\r\n",
      "trainer/Policy Loss                                      3.26351\r\n",
      "trainer/Q1 Predictions Mean                             -1.30752\r\n",
      "trainer/Q1 Predictions Std                               0.936492\r\n",
      "trainer/Q1 Predictions Max                               0.938189\r\n",
      "trainer/Q1 Predictions Min                              -5.4479\r\n",
      "trainer/Q2 Predictions Mean                             -1.31709\r\n",
      "trainer/Q2 Predictions Std                               0.938636\r\n",
      "trainer/Q2 Predictions Max                               0.87707\r\n",
      "trainer/Q2 Predictions Min                              -5.30512\r\n",
      "trainer/Q Targets Mean                                  -1.32235\r\n",
      "trainer/Q Targets Std                                    0.941249\r\n",
      "trainer/Q Targets Max                                    0.918086\r\n",
      "trainer/Q Targets Min                                   -5.39879\r\n",
      "trainer/Log Pis Mean                                     1.99354\r\n",
      "trainer/Log Pis Std                                      1.48063\r\n",
      "trainer/Log Pis Max                                      7.19128\r\n",
      "trainer/Log Pis Min                                     -3.20709\r\n",
      "trainer/Policy mu Mean                                   0.0110164\r\n",
      "trainer/Policy mu Std                                    0.528002\r\n",
      "trainer/Policy mu Max                                    2.48883\r\n",
      "trainer/Policy mu Min                                   -2.60201\r\n",
      "trainer/Policy log std Mean                             -2.22109\r\n",
      "trainer/Policy log std Std                               0.640064\r\n",
      "trainer/Policy log std Max                              -0.245907\r\n",
      "trainer/Policy log std Min                              -3.24543\r\n",
      "trainer/Alpha                                            0.01885\r\n",
      "trainer/Alpha Loss                                      -0.02564\r\n",
      "exploration/num steps total                           2600\r\n",
      "exploration/num paths total                            130\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.114106\r\n",
      "exploration/Rewards Std                                  0.133489\r\n",
      "exploration/Rewards Max                                  0.105709\r\n",
      "exploration/Rewards Min                                 -0.506449\r\n",
      "exploration/Returns Mean                                -2.28211\r\n",
      "exploration/Returns Std                                  2.32447\r\n",
      "exploration/Returns Max                                  0.0944577\r\n",
      "exploration/Returns Min                                 -5.50622\r\n",
      "exploration/Actions Mean                                -0.000321957\r\n",
      "exploration/Actions Std                                  0.15788\r\n",
      "exploration/Actions Max                                  0.432223\r\n",
      "exploration/Actions Min                                 -0.528275\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.28211\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0452917\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0448297\r\n",
      "exploration/env_infos/final/reward_dist Max              0.101794\r\n",
      "exploration/env_infos/final/reward_dist Min              3.34833e-32\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0067975\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00507594\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0137927\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000269622\r\n",
      "exploration/env_infos/reward_dist Mean                   0.191099\r\n",
      "exploration/env_infos/reward_dist Std                    0.28355\r\n",
      "exploration/env_infos/reward_dist Max                    0.966603\r\n",
      "exploration/env_infos/reward_dist Min                    3.34833e-32\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.229445\r\n",
      "exploration/env_infos/final/reward_energy Std            0.124207\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0473251\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.36358\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.32503\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.195128\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.127238\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.657986\r\n",
      "exploration/env_infos/reward_energy Mean                -0.183552\r\n",
      "exploration/env_infos/reward_energy Std                  0.127125\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0170111\r\n",
      "exploration/env_infos/reward_energy Min                 -0.657986\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.130081\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.319891\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.267677\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.721957\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00568814\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0121365\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0128315\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0264137\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0798977\r\n",
      "exploration/env_infos/end_effector_loc Std               0.228348\r\n",
      "exploration/env_infos/end_effector_loc Max               0.274549\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.721957\r\n",
      "evaluation/num steps total                           16000\r\n",
      "evaluation/num paths total                             800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.111621\r\n",
      "evaluation/Rewards Std                                   0.14062\r\n",
      "evaluation/Rewards Max                                   0.0890608\r\n",
      "evaluation/Rewards Min                                  -1.17535\r\n",
      "evaluation/Returns Mean                                 -2.23241\r\n",
      "evaluation/Returns Std                                   2.28366\r\n",
      "evaluation/Returns Max                                   0.27796\r\n",
      "evaluation/Returns Min                                 -15.1626\r\n",
      "evaluation/Actions Mean                                  0.00631068\r\n",
      "evaluation/Actions Std                                   0.0980231\r\n",
      "evaluation/Actions Max                                   0.787459\r\n",
      "evaluation/Actions Min                                  -0.643367\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.23241\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0320664\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.0826978\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.416776\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.91143e-110\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00414451\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00746726\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0313488\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.05081e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0536445\r\n",
      "evaluation/env_infos/reward_dist Std                     0.130155\r\n",
      "evaluation/env_infos/reward_dist Max                     0.953451\r\n",
      "evaluation/env_infos/reward_dist Min                     6.37186e-122\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0541416\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0521945\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00214396\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.271641\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.187844\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.152865\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0288773\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.680958\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0898956\r\n",
      "evaluation/env_infos/reward_energy Std                   0.105903\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.0019103\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.895925\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0978765\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.309703\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000439601\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00855121\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0182038\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0321684\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0398748\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.195221\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.172309\r\n",
      "time/evaluation sampling (s)                             0.95632\r\n",
      "time/exploration sampling (s)                            0.115128\r\n",
      "time/logging (s)                                         0.0207801\r\n",
      "time/saving (s)                                          0.0280409\r\n",
      "time/training (s)                                       43.5353\r\n",
      "time/epoch (s)                                          44.8279\r\n",
      "time/total (s)                                         701.966\r\n",
      "Epoch                                                   15\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:24:57.188842 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 16 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00212623\n",
      "trainer/QF2 Loss                                         0.00200728\n",
      "trainer/Policy Loss                                      3.39075\n",
      "trainer/Q1 Predictions Mean                             -1.35073\n",
      "trainer/Q1 Predictions Std                               0.920606\n",
      "trainer/Q1 Predictions Max                               0.47567\n",
      "trainer/Q1 Predictions Min                              -5.73633\n",
      "trainer/Q2 Predictions Mean                             -1.34965\n",
      "trainer/Q2 Predictions Std                               0.918114\n",
      "trainer/Q2 Predictions Max                               0.489799\n",
      "trainer/Q2 Predictions Min                              -5.76636\n",
      "trainer/Q Targets Mean                                  -1.35135\n",
      "trainer/Q Targets Std                                    0.928576\n",
      "trainer/Q Targets Max                                    0.481841\n",
      "trainer/Q Targets Min                                   -5.71947\n",
      "trainer/Log Pis Mean                                     2.08309\n",
      "trainer/Log Pis Std                                      1.25772\n",
      "trainer/Log Pis Max                                      5.42515\n",
      "trainer/Log Pis Min                                     -2.4925\n",
      "trainer/Policy mu Mean                                  -0.0230118\n",
      "trainer/Policy mu Std                                    0.594884\n",
      "trainer/Policy mu Max                                    2.71452\n",
      "trainer/Policy mu Min                                   -2.48542\n",
      "trainer/Policy log std Mean                             -2.2066\n",
      "trainer/Policy log std Std                               0.72984\n",
      "trainer/Policy log std Max                               0.176347\n",
      "trainer/Policy log std Min                              -3.35188\n",
      "trainer/Alpha                                            0.0188141\n",
      "trainer/Alpha Loss                                       0.330112\n",
      "exploration/num steps total                           2700\n",
      "exploration/num paths total                            135\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0751663\n",
      "exploration/Rewards Std                                  0.11001\n",
      "exploration/Rewards Max                                  0.187596\n",
      "exploration/Rewards Min                                 -0.281657\n",
      "exploration/Returns Mean                                -1.50333\n",
      "exploration/Returns Std                                  1.7575\n",
      "exploration/Returns Max                                  0.735813\n",
      "exploration/Returns Min                                 -3.59799\n",
      "exploration/Actions Mean                                -0.0108538\n",
      "exploration/Actions Std                                  0.143149\n",
      "exploration/Actions Max                                  0.443617\n",
      "exploration/Actions Min                                 -0.581962\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.50333\n",
      "exploration/env_infos/final/reward_dist Mean             0.193918\n",
      "exploration/env_infos/final/reward_dist Std              0.178515\n",
      "exploration/env_infos/final/reward_dist Max              0.457988\n",
      "exploration/env_infos/final/reward_dist Min              1.35293e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00725801\n",
      "exploration/env_infos/initial/reward_dist Std            0.00979641\n",
      "exploration/env_infos/initial/reward_dist Max            0.0254049\n",
      "exploration/env_infos/initial/reward_dist Min            4.32321e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.126464\n",
      "exploration/env_infos/reward_dist Std                    0.261213\n",
      "exploration/env_infos/reward_dist Max                    0.979655\n",
      "exploration/env_infos/reward_dist Min                    1.35293e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0977282\n",
      "exploration/env_infos/final/reward_energy Std            0.0704289\n",
      "exploration/env_infos/final/reward_energy Max           -0.0218261\n",
      "exploration/env_infos/final/reward_energy Min           -0.210842\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.180772\n",
      "exploration/env_infos/initial/reward_energy Std          0.119037\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0668437\n",
      "exploration/env_infos/initial/reward_energy Min         -0.395275\n",
      "exploration/env_infos/reward_energy Mean                -0.160018\n",
      "exploration/env_infos/reward_energy Std                  0.124953\n",
      "exploration/env_infos/reward_energy Max                 -0.0152949\n",
      "exploration/env_infos/reward_energy Min                 -0.688714\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0779129\n",
      "exploration/env_infos/final/end_effector_loc Std         0.17716\n",
      "exploration/env_infos/final/end_effector_loc Max         0.252906\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.398871\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00276232\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00713652\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0196004\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00996285\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00294384\n",
      "exploration/env_infos/end_effector_loc Std               0.13037\n",
      "exploration/env_infos/end_effector_loc Max               0.252906\n",
      "exploration/env_infos/end_effector_loc Min              -0.398871\n",
      "evaluation/num steps total                           17000\n",
      "evaluation/num paths total                             850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.117585\n",
      "evaluation/Rewards Std                                   0.112374\n",
      "evaluation/Rewards Max                                   0.0798498\n",
      "evaluation/Rewards Min                                  -0.735861\n",
      "evaluation/Returns Mean                                 -2.3517\n",
      "evaluation/Returns Std                                   1.86634\n",
      "evaluation/Returns Max                                   0.290168\n",
      "evaluation/Returns Min                                  -7.04644\n",
      "evaluation/Actions Mean                                 -0.00691822\n",
      "evaluation/Actions Std                                   0.0908146\n",
      "evaluation/Actions Max                                   0.575663\n",
      "evaluation/Actions Min                                  -0.617602\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.3517\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0756196\n",
      "evaluation/env_infos/final/reward_dist Std               0.168\n",
      "evaluation/env_infos/final/reward_dist Max               0.728972\n",
      "evaluation/env_infos/final/reward_dist Min               3.79118e-122\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00451658\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00910611\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0370458\n",
      "evaluation/env_infos/initial/reward_dist Min             1.22508e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0649629\n",
      "evaluation/env_infos/reward_dist Std                     0.163055\n",
      "evaluation/env_infos/reward_dist Max                     0.940406\n",
      "evaluation/env_infos/reward_dist Min                     3.79118e-122\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0679383\n",
      "evaluation/env_infos/final/reward_energy Std             0.115117\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00520989\n",
      "evaluation/env_infos/final/reward_energy Min            -0.795176\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.189243\n",
      "evaluation/env_infos/initial/reward_energy Std           0.167945\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00915714\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.665455\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0854027\n",
      "evaluation/env_infos/reward_energy Std                   0.0964193\n",
      "evaluation/env_infos/reward_energy Max                  -0.00314355\n",
      "evaluation/env_infos/reward_energy Min                  -0.795176\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0651188\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.342986\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.865791\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -4.23139e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00894544\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0287832\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0308801\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0273543\n",
      "evaluation/env_infos/end_effector_loc Std                0.213448\n",
      "evaluation/env_infos/end_effector_loc Max                0.865791\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.180284\n",
      "time/evaluation sampling (s)                             0.859825\n",
      "time/exploration sampling (s)                            0.117532\n",
      "time/logging (s)                                         0.0204291\n",
      "time/saving (s)                                          0.0272754\n",
      "time/training (s)                                       43.9136\n",
      "time/epoch (s)                                          45.119\n",
      "time/total (s)                                         747.291\n",
      "Epoch                                                   16\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:25:44.958122 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 17 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00294718\r\n",
      "trainer/QF2 Loss                                         0.0021548\r\n",
      "trainer/Policy Loss                                      3.22449\r\n",
      "trainer/Q1 Predictions Mean                             -1.18397\r\n",
      "trainer/Q1 Predictions Std                               0.898243\r\n",
      "trainer/Q1 Predictions Max                               0.627158\r\n",
      "trainer/Q1 Predictions Min                              -6.03148\r\n",
      "trainer/Q2 Predictions Mean                             -1.18288\r\n",
      "trainer/Q2 Predictions Std                               0.892247\r\n",
      "trainer/Q2 Predictions Max                               0.630633\r\n",
      "trainer/Q2 Predictions Min                              -6.0041\r\n",
      "trainer/Q Targets Mean                                  -1.195\r\n",
      "trainer/Q Targets Std                                    0.896049\r\n",
      "trainer/Q Targets Max                                    0.634519\r\n",
      "trainer/Q Targets Min                                   -6.02068\r\n",
      "trainer/Log Pis Mean                                     2.08311\r\n",
      "trainer/Log Pis Std                                      1.34003\r\n",
      "trainer/Log Pis Max                                      6.41667\r\n",
      "trainer/Log Pis Min                                     -2.14024\r\n",
      "trainer/Policy mu Mean                                   0.026999\r\n",
      "trainer/Policy mu Std                                    0.544246\r\n",
      "trainer/Policy mu Max                                    2.67782\r\n",
      "trainer/Policy mu Min                                   -2.10544\r\n",
      "trainer/Policy log std Mean                             -2.20697\r\n",
      "trainer/Policy log std Std                               0.653433\r\n",
      "trainer/Policy log std Max                              -0.0864687\r\n",
      "trainer/Policy log std Min                              -3.37077\r\n",
      "trainer/Alpha                                            0.0188878\r\n",
      "trainer/Alpha Loss                                       0.329958\r\n",
      "exploration/num steps total                           2800\r\n",
      "exploration/num paths total                            140\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.164194\r\n",
      "exploration/Rewards Std                                  0.190315\r\n",
      "exploration/Rewards Max                                  0.15545\r\n",
      "exploration/Rewards Min                                 -0.693955\r\n",
      "exploration/Returns Mean                                -3.28387\r\n",
      "exploration/Returns Std                                  3.32499\r\n",
      "exploration/Returns Max                                  1.72602\r\n",
      "exploration/Returns Min                                 -8.62785\r\n",
      "exploration/Actions Mean                                 0.00400845\r\n",
      "exploration/Actions Std                                  0.210887\r\n",
      "exploration/Actions Max                                  0.772653\r\n",
      "exploration/Actions Min                                 -0.921331\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -3.28387\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.134611\r\n",
      "exploration/env_infos/final/reward_dist Std              0.263143\r\n",
      "exploration/env_infos/final/reward_dist Max              0.660814\r\n",
      "exploration/env_infos/final/reward_dist Min              1.41162e-162\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00867048\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0143328\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.037149\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000204161\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0721635\r\n",
      "exploration/env_infos/reward_dist Std                    0.186716\r\n",
      "exploration/env_infos/reward_dist Max                    0.857752\r\n",
      "exploration/env_infos/reward_dist Min                    1.41162e-162\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.315499\r\n",
      "exploration/env_infos/final/reward_energy Std            0.127891\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.139029\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.509229\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.47511\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.320884\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.104346\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.969072\r\n",
      "exploration/env_infos/reward_energy Mean                -0.236134\r\n",
      "exploration/env_infos/reward_energy Std                  0.182263\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0149353\r\n",
      "exploration/env_infos/reward_energy Min                 -0.969072\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0240918\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.458444\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.936809\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000293977\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0202678\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0386327\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0361233\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00291545\r\n",
      "exploration/env_infos/end_effector_loc Std               0.307321\r\n",
      "exploration/env_infos/end_effector_loc Max               0.936809\r\n",
      "exploration/env_infos/end_effector_loc Min              -1\r\n",
      "evaluation/num steps total                           18000\r\n",
      "evaluation/num paths total                             900\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0940174\r\n",
      "evaluation/Rewards Std                                   0.0948013\r\n",
      "evaluation/Rewards Max                                   0.126327\r\n",
      "evaluation/Rewards Min                                  -0.555522\r\n",
      "evaluation/Returns Mean                                 -1.88035\r\n",
      "evaluation/Returns Std                                   1.55898\r\n",
      "evaluation/Returns Max                                   0.987353\r\n",
      "evaluation/Returns Min                                  -7.19826\r\n",
      "evaluation/Actions Mean                                  0.00119503\r\n",
      "evaluation/Actions Std                                   0.0962792\r\n",
      "evaluation/Actions Max                                   0.903593\r\n",
      "evaluation/Actions Min                                  -0.99095\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.88035\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0538817\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.115862\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.643297\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.16482e-181\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00487557\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00902705\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0363025\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.8552e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0793066\r\n",
      "evaluation/env_infos/reward_dist Std                     0.172965\r\n",
      "evaluation/env_infos/reward_dist Max                     0.915049\r\n",
      "evaluation/env_infos/reward_dist Min                     5.16482e-181\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0643571\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.179395\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00163509\r\n",
      "evaluation/env_infos/final/reward_energy Min            -1.29483\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.168731\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.19115\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00798566\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.913149\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0769959\r\n",
      "evaluation/env_infos/reward_energy Std                   0.112311\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000235421\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.29483\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0982408\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.353505\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00138645\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00890722\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0451796\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0156934\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0491691\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.218199\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.181004\r\n",
      "time/evaluation sampling (s)                             1.03605\r\n",
      "time/exploration sampling (s)                            0.130869\r\n",
      "time/logging (s)                                         0.0206584\r\n",
      "time/saving (s)                                          0.0303878\r\n",
      "time/training (s)                                       46.1254\r\n",
      "time/epoch (s)                                          47.5243\r\n",
      "time/total (s)                                         795.06\r\n",
      "Epoch                                                   17\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:26:31.134558 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 18 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00417027\n",
      "trainer/QF2 Loss                                         0.00266028\n",
      "trainer/Policy Loss                                      3.21993\n",
      "trainer/Q1 Predictions Mean                             -1.15014\n",
      "trainer/Q1 Predictions Std                               0.855573\n",
      "trainer/Q1 Predictions Max                               0.222141\n",
      "trainer/Q1 Predictions Min                              -5.83673\n",
      "trainer/Q2 Predictions Mean                             -1.15041\n",
      "trainer/Q2 Predictions Std                               0.862839\n",
      "trainer/Q2 Predictions Max                               0.341262\n",
      "trainer/Q2 Predictions Min                              -5.9921\n",
      "trainer/Q Targets Mean                                  -1.14755\n",
      "trainer/Q Targets Std                                    0.877727\n",
      "trainer/Q Targets Max                                    0.281878\n",
      "trainer/Q Targets Min                                   -6.01829\n",
      "trainer/Log Pis Mean                                     2.09703\n",
      "trainer/Log Pis Std                                      1.31346\n",
      "trainer/Log Pis Max                                      4.71608\n",
      "trainer/Log Pis Min                                     -2.89165\n",
      "trainer/Policy mu Mean                                   0.0144778\n",
      "trainer/Policy mu Std                                    0.464325\n",
      "trainer/Policy mu Max                                    2.95913\n",
      "trainer/Policy mu Min                                   -1.74475\n",
      "trainer/Policy log std Mean                             -2.30747\n",
      "trainer/Policy log std Std                               0.682583\n",
      "trainer/Policy log std Max                               0.327854\n",
      "trainer/Policy log std Min                              -3.34253\n",
      "trainer/Alpha                                            0.017392\n",
      "trainer/Alpha Loss                                       0.393059\n",
      "exploration/num steps total                           2900\n",
      "exploration/num paths total                            145\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.116414\n",
      "exploration/Rewards Std                                  0.0785842\n",
      "exploration/Rewards Max                                  0.0512431\n",
      "exploration/Rewards Min                                 -0.290434\n",
      "exploration/Returns Mean                                -2.32828\n",
      "exploration/Returns Std                                  1.2696\n",
      "exploration/Returns Max                                 -0.730678\n",
      "exploration/Returns Min                                 -3.83598\n",
      "exploration/Actions Mean                                -0.00520824\n",
      "exploration/Actions Std                                  0.140135\n",
      "exploration/Actions Max                                  0.421063\n",
      "exploration/Actions Min                                 -0.587988\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.32828\n",
      "exploration/env_infos/final/reward_dist Mean             0.040677\n",
      "exploration/env_infos/final/reward_dist Std              0.0776599\n",
      "exploration/env_infos/final/reward_dist Max              0.195893\n",
      "exploration/env_infos/final/reward_dist Min              2.01084e-15\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00869812\n",
      "exploration/env_infos/initial/reward_dist Std            0.00784665\n",
      "exploration/env_infos/initial/reward_dist Max            0.0212847\n",
      "exploration/env_infos/initial/reward_dist Min            0.000265217\n",
      "exploration/env_infos/reward_dist Mean                   0.11156\n",
      "exploration/env_infos/reward_dist Std                    0.16066\n",
      "exploration/env_infos/reward_dist Max                    0.728515\n",
      "exploration/env_infos/reward_dist Min                    2.01084e-15\n",
      "exploration/env_infos/final/reward_energy Mean          -0.182287\n",
      "exploration/env_infos/final/reward_energy Std            0.093673\n",
      "exploration/env_infos/final/reward_energy Max           -0.07579\n",
      "exploration/env_infos/final/reward_energy Min           -0.326241\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.214366\n",
      "exploration/env_infos/initial/reward_energy Std          0.174415\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0629741\n",
      "exploration/env_infos/initial/reward_energy Min         -0.538301\n",
      "exploration/env_infos/reward_energy Mean                -0.150663\n",
      "exploration/env_infos/reward_energy Std                  0.12896\n",
      "exploration/env_infos/reward_energy Max                 -0.0115508\n",
      "exploration/env_infos/reward_energy Min                 -0.661235\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.121157\n",
      "exploration/env_infos/final/end_effector_loc Std         0.279879\n",
      "exploration/env_infos/final/end_effector_loc Max         0.366307\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.641656\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00244031\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00946105\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0126531\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0265593\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0611485\n",
      "exploration/env_infos/end_effector_loc Std               0.174329\n",
      "exploration/env_infos/end_effector_loc Max               0.366307\n",
      "exploration/env_infos/end_effector_loc Min              -0.641656\n",
      "evaluation/num steps total                           19000\n",
      "evaluation/num paths total                             950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0910313\n",
      "evaluation/Rewards Std                                   0.119522\n",
      "evaluation/Rewards Max                                   0.147902\n",
      "evaluation/Rewards Min                                  -0.740356\n",
      "evaluation/Returns Mean                                 -1.82063\n",
      "evaluation/Returns Std                                   1.97406\n",
      "evaluation/Returns Max                                   1.29367\n",
      "evaluation/Returns Min                                  -9.46101\n",
      "evaluation/Actions Mean                                 -0.0040379\n",
      "evaluation/Actions Std                                   0.0786374\n",
      "evaluation/Actions Max                                   0.837213\n",
      "evaluation/Actions Min                                  -0.699036\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.82063\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0502599\n",
      "evaluation/env_infos/final/reward_dist Std               0.138702\n",
      "evaluation/env_infos/final/reward_dist Max               0.799903\n",
      "evaluation/env_infos/final/reward_dist Min               5.98617e-91\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00603801\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0115532\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0590119\n",
      "evaluation/env_infos/initial/reward_dist Min             2.5553e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.117416\n",
      "evaluation/env_infos/reward_dist Std                     0.222656\n",
      "evaluation/env_infos/reward_dist Max                     0.998681\n",
      "evaluation/env_infos/reward_dist Min                     5.98617e-91\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.072706\n",
      "evaluation/env_infos/final/reward_energy Std             0.0878625\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00590276\n",
      "evaluation/env_infos/final/reward_energy Min            -0.505321\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.195603\n",
      "evaluation/env_infos/initial/reward_energy Std           0.210174\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0114158\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.896995\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0731694\n",
      "evaluation/env_infos/reward_energy Std                   0.0839437\n",
      "evaluation/env_infos/reward_energy Max                  -0.000348674\n",
      "evaluation/env_infos/reward_energy Min                  -0.896995\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00877208\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.346897\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000643919\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0101305\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0418606\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0349518\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0137968\n",
      "evaluation/env_infos/end_effector_loc Std                0.217782\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.17591\n",
      "time/evaluation sampling (s)                             1.04296\n",
      "time/exploration sampling (s)                            0.122059\n",
      "time/logging (s)                                         0.0211504\n",
      "time/saving (s)                                          0.0280109\n",
      "time/training (s)                                       44.5145\n",
      "time/epoch (s)                                          45.9046\n",
      "time/total (s)                                         841.236\n",
      "Epoch                                                   18\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:27:18.033736 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 19 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00361566\r\n",
      "trainer/QF2 Loss                                         0.00425876\r\n",
      "trainer/Policy Loss                                      2.78552\r\n",
      "trainer/Q1 Predictions Mean                             -1.05747\r\n",
      "trainer/Q1 Predictions Std                               0.951688\r\n",
      "trainer/Q1 Predictions Max                               0.791141\r\n",
      "trainer/Q1 Predictions Min                              -5.74365\r\n",
      "trainer/Q2 Predictions Mean                             -1.07618\r\n",
      "trainer/Q2 Predictions Std                               0.95182\r\n",
      "trainer/Q2 Predictions Max                               0.788314\r\n",
      "trainer/Q2 Predictions Min                              -5.55286\r\n",
      "trainer/Q Targets Mean                                  -1.0736\r\n",
      "trainer/Q Targets Std                                    0.959196\r\n",
      "trainer/Q Targets Max                                    0.789384\r\n",
      "trainer/Q Targets Min                                   -5.76723\r\n",
      "trainer/Log Pis Mean                                     1.76544\r\n",
      "trainer/Log Pis Std                                      1.39586\r\n",
      "trainer/Log Pis Max                                      5.72317\r\n",
      "trainer/Log Pis Min                                     -3.47894\r\n",
      "trainer/Policy mu Mean                                  -0.00657688\r\n",
      "trainer/Policy mu Std                                    0.595839\r\n",
      "trainer/Policy mu Max                                    2.54148\r\n",
      "trainer/Policy mu Min                                   -2.52602\r\n",
      "trainer/Policy log std Mean                             -2.05189\r\n",
      "trainer/Policy log std Std                               0.703901\r\n",
      "trainer/Policy log std Max                               1.28065\r\n",
      "trainer/Policy log std Min                              -3.30749\r\n",
      "trainer/Alpha                                            0.0187649\r\n",
      "trainer/Alpha Loss                                      -0.932449\r\n",
      "exploration/num steps total                           3000\r\n",
      "exploration/num paths total                            150\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.105968\r\n",
      "exploration/Rewards Std                                  0.0618281\r\n",
      "exploration/Rewards Max                                  0.0393586\r\n",
      "exploration/Rewards Min                                 -0.248441\r\n",
      "exploration/Returns Mean                                -2.11936\r\n",
      "exploration/Returns Std                                  0.75352\r\n",
      "exploration/Returns Max                                 -0.7712\r\n",
      "exploration/Returns Min                                 -3.02065\r\n",
      "exploration/Actions Mean                                 0.0142718\r\n",
      "exploration/Actions Std                                  0.0937951\r\n",
      "exploration/Actions Max                                  0.312175\r\n",
      "exploration/Actions Min                                 -0.3005\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.11936\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.164678\r\n",
      "exploration/env_infos/final/reward_dist Std              0.328868\r\n",
      "exploration/env_infos/final/reward_dist Max              0.822414\r\n",
      "exploration/env_infos/final/reward_dist Min              2.1564e-18\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00283307\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00289934\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00795435\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.0233e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0728503\r\n",
      "exploration/env_infos/reward_dist Std                    0.163824\r\n",
      "exploration/env_infos/reward_dist Max                    0.822414\r\n",
      "exploration/env_infos/reward_dist Min                    2.1564e-18\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.132058\r\n",
      "exploration/env_infos/final/reward_energy Std            0.039282\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0832685\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.194082\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.153556\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0925766\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0507587\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.312624\r\n",
      "exploration/env_infos/reward_energy Mean                -0.115599\r\n",
      "exploration/env_infos/reward_energy Std                  0.0681117\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0054453\r\n",
      "exploration/env_infos/reward_energy Min                 -0.313043\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.196749\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.168656\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.479736\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.150334\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0030282\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00556931\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0156088\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00451479\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0923198\r\n",
      "exploration/env_infos/end_effector_loc Std               0.1147\r\n",
      "exploration/env_infos/end_effector_loc Max               0.479736\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.150334\r\n",
      "evaluation/num steps total                           20000\r\n",
      "evaluation/num paths total                            1000\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0970253\r\n",
      "evaluation/Rewards Std                                   0.115917\r\n",
      "evaluation/Rewards Max                                   0.132123\r\n",
      "evaluation/Rewards Min                                  -0.715751\r\n",
      "evaluation/Returns Mean                                 -1.94051\r\n",
      "evaluation/Returns Std                                   1.94168\r\n",
      "evaluation/Returns Max                                   1.37245\r\n",
      "evaluation/Returns Min                                  -7.62026\r\n",
      "evaluation/Actions Mean                                  0.00109065\r\n",
      "evaluation/Actions Std                                   0.0844699\r\n",
      "evaluation/Actions Max                                   0.84784\r\n",
      "evaluation/Actions Min                                  -0.60674\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.94051\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.113824\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.23406\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.921232\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.37081e-136\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0037432\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00831257\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0334945\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.09486e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.101601\r\n",
      "evaluation/env_infos/reward_dist Std                     0.210267\r\n",
      "evaluation/env_infos/reward_dist Max                     0.985804\r\n",
      "evaluation/env_infos/reward_dist Min                     6.37081e-136\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0765403\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.115913\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00176414\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.749255\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.156967\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.169436\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00466013\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.02592\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0775046\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0909162\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00136222\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.02592\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00761149\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.358318\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00016745\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00816431\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.042392\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0153315\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0136583\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.213432\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.176256\r\n",
      "time/evaluation sampling (s)                             0.961786\r\n",
      "time/exploration sampling (s)                            0.120926\r\n",
      "time/logging (s)                                         0.0209777\r\n",
      "time/saving (s)                                          0.0280325\r\n",
      "time/training (s)                                       45.3085\r\n",
      "time/epoch (s)                                          46.6165\r\n",
      "time/total (s)                                         888.134\r\n",
      "Epoch                                                   19\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:28:03.694665 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 20 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00143063\r\n",
      "trainer/QF2 Loss                                         0.00127386\r\n",
      "trainer/Policy Loss                                      2.98129\r\n",
      "trainer/Q1 Predictions Mean                             -1.10684\r\n",
      "trainer/Q1 Predictions Std                               0.933454\r\n",
      "trainer/Q1 Predictions Max                               0.787858\r\n",
      "trainer/Q1 Predictions Min                              -5.27475\r\n",
      "trainer/Q2 Predictions Mean                             -1.11047\r\n",
      "trainer/Q2 Predictions Std                               0.933073\r\n",
      "trainer/Q2 Predictions Max                               0.811669\r\n",
      "trainer/Q2 Predictions Min                              -5.23849\r\n",
      "trainer/Q Targets Mean                                  -1.10154\r\n",
      "trainer/Q Targets Std                                    0.92254\r\n",
      "trainer/Q Targets Max                                    0.794168\r\n",
      "trainer/Q Targets Min                                   -5.13028\r\n",
      "trainer/Log Pis Mean                                     1.91064\r\n",
      "trainer/Log Pis Std                                      1.48685\r\n",
      "trainer/Log Pis Max                                      4.55863\r\n",
      "trainer/Log Pis Min                                     -4.73811\r\n",
      "trainer/Policy mu Mean                                   0.0490682\r\n",
      "trainer/Policy mu Std                                    0.533424\r\n",
      "trainer/Policy mu Max                                    2.32921\r\n",
      "trainer/Policy mu Min                                   -2.38966\r\n",
      "trainer/Policy log std Mean                             -2.17712\r\n",
      "trainer/Policy log std Std                               0.679884\r\n",
      "trainer/Policy log std Max                               0.16048\r\n",
      "trainer/Policy log std Min                              -3.23966\r\n",
      "trainer/Alpha                                            0.0192951\r\n",
      "trainer/Alpha Loss                                      -0.352745\r\n",
      "exploration/num steps total                           3100\r\n",
      "exploration/num paths total                            155\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.225955\r\n",
      "exploration/Rewards Std                                  0.236454\r\n",
      "exploration/Rewards Max                                  0.0458165\r\n",
      "exploration/Rewards Min                                 -0.814168\r\n",
      "exploration/Returns Mean                                -4.51909\r\n",
      "exploration/Returns Std                                  4.06694\r\n",
      "exploration/Returns Max                                 -0.142127\r\n",
      "exploration/Returns Min                                -10.9045\r\n",
      "exploration/Actions Mean                                -0.00610589\r\n",
      "exploration/Actions Std                                  0.137936\r\n",
      "exploration/Actions Max                                  0.439568\r\n",
      "exploration/Actions Min                                 -0.453226\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -4.51909\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0203663\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0309609\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0799164\r\n",
      "exploration/env_infos/final/reward_dist Min              1.09838e-175\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00430057\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00638255\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0164787\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.21547e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0727111\r\n",
      "exploration/env_infos/reward_dist Std                    0.117153\r\n",
      "exploration/env_infos/reward_dist Max                    0.431657\r\n",
      "exploration/env_infos/reward_dist Min                    1.09838e-175\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.143007\r\n",
      "exploration/env_infos/final/reward_energy Std            0.188684\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.01604\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.516818\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.17541\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.113436\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0580777\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.315377\r\n",
      "exploration/env_infos/reward_energy Mean                -0.152159\r\n",
      "exploration/env_infos/reward_energy Std                  0.122371\r\n",
      "exploration/env_infos/reward_energy Max                 -0.01604\r\n",
      "exploration/env_infos/reward_energy Min                 -0.527095\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0413618\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.609228\r\n",
      "exploration/env_infos/final/end_effector_loc Max         1\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000145676\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00738406\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0134673\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0132816\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0239282\r\n",
      "exploration/env_infos/end_effector_loc Std               0.401927\r\n",
      "exploration/env_infos/end_effector_loc Max               1\r\n",
      "exploration/env_infos/end_effector_loc Min              -1\r\n",
      "evaluation/num steps total                           21000\r\n",
      "evaluation/num paths total                            1050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.117129\r\n",
      "evaluation/Rewards Std                                   0.153571\r\n",
      "evaluation/Rewards Max                                   0.116993\r\n",
      "evaluation/Rewards Min                                  -0.934133\r\n",
      "evaluation/Returns Mean                                 -2.34258\r\n",
      "evaluation/Returns Std                                   2.51723\r\n",
      "evaluation/Returns Max                                   1.00188\r\n",
      "evaluation/Returns Min                                 -10.5716\r\n",
      "evaluation/Actions Mean                                  0.0102529\r\n",
      "evaluation/Actions Std                                   0.10647\r\n",
      "evaluation/Actions Max                                   0.713409\r\n",
      "evaluation/Actions Min                                  -0.872661\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.34258\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0313028\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.11242\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.747676\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.35715e-159\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00600372\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00971056\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0349707\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.06491e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0665371\r\n",
      "evaluation/env_infos/reward_dist Std                     0.161714\r\n",
      "evaluation/env_infos/reward_dist Max                     0.974098\r\n",
      "evaluation/env_infos/reward_dist Min                     3.35715e-159\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0670086\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0877048\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00280286\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.512078\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.224044\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.247713\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0157909\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.04308\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0866895\r\n",
      "evaluation/env_infos/reward_energy Std                   0.123964\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00166819\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.04308\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.109109\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.419756\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -4.16685e-05\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0118087\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0351411\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0436331\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0479812\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.273297\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.180844\r\n",
      "time/evaluation sampling (s)                             0.99902\r\n",
      "time/exploration sampling (s)                            0.11746\r\n",
      "time/logging (s)                                         0.0193164\r\n",
      "time/saving (s)                                          0.0276602\r\n",
      "time/training (s)                                       44.0255\r\n",
      "time/epoch (s)                                          45.3698\r\n",
      "time/total (s)                                         933.792\r\n",
      "Epoch                                                   20\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:28:51.264144 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 21 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00352827\r\n",
      "trainer/QF2 Loss                                         0.00267879\r\n",
      "trainer/Policy Loss                                      2.86298\r\n",
      "trainer/Q1 Predictions Mean                             -0.836101\r\n",
      "trainer/Q1 Predictions Std                               0.857706\r\n",
      "trainer/Q1 Predictions Max                               0.825573\r\n",
      "trainer/Q1 Predictions Min                              -3.47673\r\n",
      "trainer/Q2 Predictions Mean                             -0.833136\r\n",
      "trainer/Q2 Predictions Std                               0.853113\r\n",
      "trainer/Q2 Predictions Max                               0.85359\r\n",
      "trainer/Q2 Predictions Min                              -3.42872\r\n",
      "trainer/Q Targets Mean                                  -0.860709\r\n",
      "trainer/Q Targets Std                                    0.859452\r\n",
      "trainer/Q Targets Max                                    0.814316\r\n",
      "trainer/Q Targets Min                                   -3.50173\r\n",
      "trainer/Log Pis Mean                                     2.0452\r\n",
      "trainer/Log Pis Std                                      1.45977\r\n",
      "trainer/Log Pis Max                                      5.86779\r\n",
      "trainer/Log Pis Min                                     -3.02729\r\n",
      "trainer/Policy mu Mean                                   0.0435604\r\n",
      "trainer/Policy mu Std                                    0.526576\r\n",
      "trainer/Policy mu Max                                    2.34579\r\n",
      "trainer/Policy mu Min                                   -2.53298\r\n",
      "trainer/Policy log std Mean                             -2.22102\r\n",
      "trainer/Policy log std Std                               0.682262\r\n",
      "trainer/Policy log std Max                              -0.198343\r\n",
      "trainer/Policy log std Min                              -3.52146\r\n",
      "trainer/Alpha                                            0.0189215\r\n",
      "trainer/Alpha Loss                                       0.179328\r\n",
      "exploration/num steps total                           3200\r\n",
      "exploration/num paths total                            160\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0552434\r\n",
      "exploration/Rewards Std                                  0.0714195\r\n",
      "exploration/Rewards Max                                  0.145358\r\n",
      "exploration/Rewards Min                                 -0.234576\r\n",
      "exploration/Returns Mean                                -1.10487\r\n",
      "exploration/Returns Std                                  0.991893\r\n",
      "exploration/Returns Max                                  0.79098\r\n",
      "exploration/Returns Min                                 -1.95213\r\n",
      "exploration/Actions Mean                                -0.00579491\r\n",
      "exploration/Actions Std                                  0.122271\r\n",
      "exploration/Actions Max                                  0.431939\r\n",
      "exploration/Actions Min                                 -0.509075\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.10487\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.181388\r\n",
      "exploration/env_infos/final/reward_dist Std              0.252213\r\n",
      "exploration/env_infos/final/reward_dist Max              0.64576\r\n",
      "exploration/env_infos/final/reward_dist Min              1.60164e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00313536\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00295143\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0070575\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.03045e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.149531\r\n",
      "exploration/env_infos/reward_dist Std                    0.241262\r\n",
      "exploration/env_infos/reward_dist Max                    0.961405\r\n",
      "exploration/env_infos/reward_dist Min                    1.60164e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0850208\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0420347\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0481465\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.158254\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.13844\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0825715\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0315206\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.257878\r\n",
      "exploration/env_infos/reward_energy Mean                -0.135266\r\n",
      "exploration/env_infos/reward_energy Std                  0.108031\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0149132\r\n",
      "exploration/env_infos/reward_energy Min                 -0.558569\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0437434\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.197345\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.303601\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.359563\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00130935\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00554664\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00934553\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0123641\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00556101\r\n",
      "exploration/env_infos/end_effector_loc Std               0.121162\r\n",
      "exploration/env_infos/end_effector_loc Max               0.303601\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.359563\r\n",
      "evaluation/num steps total                           22000\r\n",
      "evaluation/num paths total                            1100\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0927433\r\n",
      "evaluation/Rewards Std                                   0.10782\r\n",
      "evaluation/Rewards Max                                   0.134199\r\n",
      "evaluation/Rewards Min                                  -0.700546\r\n",
      "evaluation/Returns Mean                                 -1.85487\r\n",
      "evaluation/Returns Std                                   1.90902\r\n",
      "evaluation/Returns Max                                   1.45956\r\n",
      "evaluation/Returns Min                                  -7.43322\r\n",
      "evaluation/Actions Mean                                  0.000180101\r\n",
      "evaluation/Actions Std                                   0.109252\r\n",
      "evaluation/Actions Max                                   0.879487\r\n",
      "evaluation/Actions Min                                  -0.655818\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.85487\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0423573\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.116609\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.544384\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.46828e-68\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00429258\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00852198\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0346478\r\n",
      "evaluation/env_infos/initial/reward_dist Min             5.07633e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0784318\r\n",
      "evaluation/env_infos/reward_dist Std                     0.177603\r\n",
      "evaluation/env_infos/reward_dist Max                     0.980279\r\n",
      "evaluation/env_infos/reward_dist Min                     1.46828e-68\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0617376\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0718459\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00296373\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.312213\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.206707\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.269811\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00713126\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.14534\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0897981\r\n",
      "evaluation/env_infos/reward_energy Std                   0.125731\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00116793\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.14534\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0509208\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.314018\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.772985\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00295591\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116478\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0439744\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0303424\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0402173\n",
      "evaluation/env_infos/end_effector_loc Std                0.197197\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.772985\n",
      "time/data storing (s)                                    0.182537\n",
      "time/evaluation sampling (s)                             1.17867\n",
      "time/exploration sampling (s)                            0.127643\n",
      "time/logging (s)                                         0.0196213\n",
      "time/saving (s)                                          0.0278825\n",
      "time/training (s)                                       45.7275\n",
      "time/epoch (s)                                          47.2639\n",
      "time/total (s)                                         981.361\n",
      "Epoch                                                   21\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 13:29:37.361316 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 22 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0035725\n",
      "trainer/QF2 Loss                                         0.00519686\n",
      "trainer/Policy Loss                                      2.84814\n",
      "trainer/Q1 Predictions Mean                             -0.954978\n",
      "trainer/Q1 Predictions Std                               0.874056\n",
      "trainer/Q1 Predictions Max                               0.868976\n",
      "trainer/Q1 Predictions Min                              -3.0576\n",
      "trainer/Q2 Predictions Mean                             -0.943114\n",
      "trainer/Q2 Predictions Std                               0.864875\n",
      "trainer/Q2 Predictions Max                               0.849259\n",
      "trainer/Q2 Predictions Min                              -3.0888\n",
      "trainer/Q Targets Mean                                  -0.951818\n",
      "trainer/Q Targets Std                                    0.86972\n",
      "trainer/Q Targets Max                                    0.880738\n",
      "trainer/Q Targets Min                                   -3.09466\n",
      "trainer/Log Pis Mean                                     1.9163\n",
      "trainer/Log Pis Std                                      1.35271\n",
      "trainer/Log Pis Max                                      6.14289\n",
      "trainer/Log Pis Min                                     -2.0445\n",
      "trainer/Policy mu Mean                                   0.00743124\n",
      "trainer/Policy mu Std                                    0.587918\n",
      "trainer/Policy mu Max                                    2.61897\n",
      "trainer/Policy mu Min                                   -2.67231\n",
      "trainer/Policy log std Mean                             -2.12592\n",
      "trainer/Policy log std Std                               0.678808\n",
      "trainer/Policy log std Max                              -0.181886\n",
      "trainer/Policy log std Min                              -3.30241\n",
      "trainer/Alpha                                            0.0195905\n",
      "trainer/Alpha Loss                                      -0.328994\n",
      "exploration/num steps total                           3300\n",
      "exploration/num paths total                            165\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.194838\n",
      "exploration/Rewards Std                                  0.207625\n",
      "exploration/Rewards Max                                  0.0709144\n",
      "exploration/Rewards Min                                 -0.803336\n",
      "exploration/Returns Mean                                -3.89676\n",
      "exploration/Returns Std                                  3.58863\n",
      "exploration/Returns Max                                 -0.803619\n",
      "exploration/Returns Min                                -10.3674\n",
      "exploration/Actions Mean                                -0.00636294\n",
      "exploration/Actions Std                                  0.211681\n",
      "exploration/Actions Max                                  0.979289\n",
      "exploration/Actions Min                                 -0.751552\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.89676\n",
      "exploration/env_infos/final/reward_dist Mean             1.69751e-06\n",
      "exploration/env_infos/final/reward_dist Std              1.82258e-06\n",
      "exploration/env_infos/final/reward_dist Max              4.44336e-06\n",
      "exploration/env_infos/final/reward_dist Min              1.11971e-80\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00413705\n",
      "exploration/env_infos/initial/reward_dist Std            0.00667108\n",
      "exploration/env_infos/initial/reward_dist Max            0.017413\n",
      "exploration/env_infos/initial/reward_dist Min            7.21447e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0743535\n",
      "exploration/env_infos/reward_dist Std                    0.171021\n",
      "exploration/env_infos/reward_dist Max                    0.849329\n",
      "exploration/env_infos/reward_dist Min                    1.11971e-80\n",
      "exploration/env_infos/final/reward_energy Mean          -0.148629\n",
      "exploration/env_infos/final/reward_energy Std            0.0846118\n",
      "exploration/env_infos/final/reward_energy Max           -0.0176599\n",
      "exploration/env_infos/final/reward_energy Min           -0.242595\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.396912\n",
      "exploration/env_infos/initial/reward_energy Std          0.439789\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0773983\n",
      "exploration/env_infos/initial/reward_energy Min         -1.23697\n",
      "exploration/env_infos/reward_energy Mean                -0.213209\n",
      "exploration/env_infos/reward_energy Std                  0.210336\n",
      "exploration/env_infos/reward_energy Max                 -0.00547559\n",
      "exploration/env_infos/reward_energy Min                 -1.23697\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.243871\n",
      "exploration/env_infos/final/end_effector_loc Std         0.475448\n",
      "exploration/env_infos/final/end_effector_loc Max         0.792358\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.7585\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0108256\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0179304\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0489645\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0141631\n",
      "exploration/env_infos/end_effector_loc Mean              0.145176\n",
      "exploration/env_infos/end_effector_loc Std               0.338856\n",
      "exploration/env_infos/end_effector_loc Max               0.887276\n",
      "exploration/env_infos/end_effector_loc Min              -0.7585\n",
      "evaluation/num steps total                           23000\n",
      "evaluation/num paths total                            1150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.107876\n",
      "evaluation/Rewards Std                                   0.111159\n",
      "evaluation/Rewards Max                                   0.141176\n",
      "evaluation/Rewards Min                                  -0.610248\n",
      "evaluation/Returns Mean                                 -2.15752\n",
      "evaluation/Returns Std                                   1.75503\n",
      "evaluation/Returns Max                                   1.2411\n",
      "evaluation/Returns Min                                  -6.13435\n",
      "evaluation/Actions Mean                                  2.10142e-06\n",
      "evaluation/Actions Std                                   0.123549\n",
      "evaluation/Actions Max                                   0.89983\n",
      "evaluation/Actions Min                                  -0.965554\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.15752\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0575445\n",
      "evaluation/env_infos/final/reward_dist Std               0.159817\n",
      "evaluation/env_infos/final/reward_dist Max               0.768169\n",
      "evaluation/env_infos/final/reward_dist Min               1.93089e-98\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00794123\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0217483\n",
      "evaluation/env_infos/initial/reward_dist Max             0.116986\n",
      "evaluation/env_infos/initial/reward_dist Min             1.68962e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0847913\n",
      "evaluation/env_infos/reward_dist Std                     0.195612\n",
      "evaluation/env_infos/reward_dist Max                     0.994038\n",
      "evaluation/env_infos/reward_dist Min                     1.93089e-98\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.10345\n",
      "evaluation/env_infos/final/reward_energy Std             0.169652\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00424102\n",
      "evaluation/env_infos/final/reward_energy Min            -1.06681\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.316644\n",
      "evaluation/env_infos/initial/reward_energy Std           0.291889\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0282493\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.21652\n",
      "evaluation/env_infos/reward_energy Mean                 -0.106047\n",
      "evaluation/env_infos/reward_energy Std                   0.138862\n",
      "evaluation/env_infos/reward_energy Max                  -0.00101738\n",
      "evaluation/env_infos/reward_energy Min                  -1.21652\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.102133\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.441785\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0015372\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0151481\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0449915\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0482777\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0641771\n",
      "evaluation/env_infos/end_effector_loc Std                0.284467\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.172237\n",
      "time/evaluation sampling (s)                             1.03922\n",
      "time/exploration sampling (s)                            0.120092\n",
      "time/logging (s)                                         0.018714\n",
      "time/saving (s)                                          0.0274242\n",
      "time/training (s)                                       44.3772\n",
      "time/epoch (s)                                          45.7549\n",
      "time/total (s)                                        1027.46\n",
      "Epoch                                                   22\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:30:24.838609 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 23 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00200664\n",
      "trainer/QF2 Loss                                         0.00200688\n",
      "trainer/Policy Loss                                      3.03136\n",
      "trainer/Q1 Predictions Mean                             -0.904632\n",
      "trainer/Q1 Predictions Std                               0.890976\n",
      "trainer/Q1 Predictions Max                               1.1252\n",
      "trainer/Q1 Predictions Min                              -4.12732\n",
      "trainer/Q2 Predictions Mean                             -0.91113\n",
      "trainer/Q2 Predictions Std                               0.893211\n",
      "trainer/Q2 Predictions Max                               1.06374\n",
      "trainer/Q2 Predictions Min                              -4.02634\n",
      "trainer/Q Targets Mean                                  -0.920077\n",
      "trainer/Q Targets Std                                    0.900652\n",
      "trainer/Q Targets Max                                    1.10514\n",
      "trainer/Q Targets Min                                   -4.12512\n",
      "trainer/Log Pis Mean                                     2.13823\n",
      "trainer/Log Pis Std                                      1.42795\n",
      "trainer/Log Pis Max                                      6.35332\n",
      "trainer/Log Pis Min                                     -5.18352\n",
      "trainer/Policy mu Mean                                  -0.00771861\n",
      "trainer/Policy mu Std                                    0.444056\n",
      "trainer/Policy mu Max                                    2.59581\n",
      "trainer/Policy mu Min                                   -2.38021\n",
      "trainer/Policy log std Mean                             -2.35067\n",
      "trainer/Policy log std Std                               0.599586\n",
      "trainer/Policy log std Max                              -0.258306\n",
      "trainer/Policy log std Min                              -3.35645\n",
      "trainer/Alpha                                            0.0178365\n",
      "trainer/Alpha Loss                                       0.556644\n",
      "exploration/num steps total                           3400\n",
      "exploration/num paths total                            170\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.133218\n",
      "exploration/Rewards Std                                  0.0906068\n",
      "exploration/Rewards Max                                  0.00288249\n",
      "exploration/Rewards Min                                 -0.440627\n",
      "exploration/Returns Mean                                -2.66435\n",
      "exploration/Returns Std                                  1.3426\n",
      "exploration/Returns Max                                 -1.809\n",
      "exploration/Returns Min                                 -5.33434\n",
      "exploration/Actions Mean                                -0.0249274\n",
      "exploration/Actions Std                                  0.145358\n",
      "exploration/Actions Max                                  0.350769\n",
      "exploration/Actions Min                                 -0.559069\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.66435\n",
      "exploration/env_infos/final/reward_dist Mean             4.70167e-05\n",
      "exploration/env_infos/final/reward_dist Std              9.37477e-05\n",
      "exploration/env_infos/final/reward_dist Max              0.000234512\n",
      "exploration/env_infos/final/reward_dist Min              1.56749e-41\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00661064\n",
      "exploration/env_infos/initial/reward_dist Std            0.00665122\n",
      "exploration/env_infos/initial/reward_dist Max            0.0165555\n",
      "exploration/env_infos/initial/reward_dist Min            0.000564714\n",
      "exploration/env_infos/reward_dist Mean                   0.0878457\n",
      "exploration/env_infos/reward_dist Std                    0.1769\n",
      "exploration/env_infos/reward_dist Max                    0.831553\n",
      "exploration/env_infos/reward_dist Min                    1.53178e-41\n",
      "exploration/env_infos/final/reward_energy Mean          -0.202566\n",
      "exploration/env_infos/final/reward_energy Std            0.201473\n",
      "exploration/env_infos/final/reward_energy Max           -0.0330949\n",
      "exploration/env_infos/final/reward_energy Min           -0.578077\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.296706\n",
      "exploration/env_infos/initial/reward_energy Std          0.197734\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0769872\n",
      "exploration/env_infos/initial/reward_energy Min         -0.559248\n",
      "exploration/env_infos/reward_energy Mean                -0.167564\n",
      "exploration/env_infos/reward_energy Std                  0.124189\n",
      "exploration/env_infos/reward_energy Max                 -0.0124519\n",
      "exploration/env_infos/reward_energy Min                 -0.578077\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.230096\n",
      "exploration/env_infos/final/end_effector_loc Std         0.425198\n",
      "exploration/env_infos/final/end_effector_loc Max         0.404716\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00439169\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0118165\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0111084\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0279535\n",
      "exploration/env_infos/end_effector_loc Mean             -0.107964\n",
      "exploration/env_infos/end_effector_loc Std               0.278818\n",
      "exploration/env_infos/end_effector_loc Max               0.404716\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           24000\n",
      "evaluation/num paths total                            1200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0710178\n",
      "evaluation/Rewards Std                                   0.0888532\n",
      "evaluation/Rewards Max                                   0.145202\n",
      "evaluation/Rewards Min                                  -0.582481\n",
      "evaluation/Returns Mean                                 -1.42036\n",
      "evaluation/Returns Std                                   1.28338\n",
      "evaluation/Returns Max                                   1.18988\n",
      "evaluation/Returns Min                                  -4.39876\n",
      "evaluation/Actions Mean                                  0.00285575\n",
      "evaluation/Actions Std                                   0.0816794\n",
      "evaluation/Actions Max                                   0.610828\n",
      "evaluation/Actions Min                                  -0.873312\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.42036\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0618234\n",
      "evaluation/env_infos/final/reward_dist Std               0.163337\n",
      "evaluation/env_infos/final/reward_dist Max               0.979669\n",
      "evaluation/env_infos/final/reward_dist Min               8.94366e-103\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00475556\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00777466\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0317718\n",
      "evaluation/env_infos/initial/reward_dist Min             3.20142e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0791843\n",
      "evaluation/env_infos/reward_dist Std                     0.168009\n",
      "evaluation/env_infos/reward_dist Max                     0.979669\n",
      "evaluation/env_infos/reward_dist Min                     8.94366e-103\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0453441\n",
      "evaluation/env_infos/final/reward_energy Std             0.034911\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00618698\n",
      "evaluation/env_infos/final/reward_energy Min            -0.160628\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.180066\n",
      "evaluation/env_infos/initial/reward_energy Std           0.182536\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0109791\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.898199\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0732825\n",
      "evaluation/env_infos/reward_energy Std                   0.0893813\n",
      "evaluation/env_infos/reward_energy Max                  -0.000715874\n",
      "evaluation/env_infos/reward_energy Min                  -0.898199\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0595537\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.345999\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.692497\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00073978\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00903503\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0299562\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0436656\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0307056\n",
      "evaluation/env_infos/end_effector_loc Std                0.21937\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.692497\n",
      "time/data storing (s)                                    0.188514\n",
      "time/evaluation sampling (s)                             0.854249\n",
      "time/exploration sampling (s)                            0.133935\n",
      "time/logging (s)                                         0.0193872\n",
      "time/saving (s)                                          0.0266098\n",
      "time/training (s)                                       45.9873\n",
      "time/epoch (s)                                          47.21\n",
      "time/total (s)                                        1074.93\n",
      "Epoch                                                   23\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:31:10.846008 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 24 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00382583\r\n",
      "trainer/QF2 Loss                                         0.0026784\r\n",
      "trainer/Policy Loss                                      2.83617\r\n",
      "trainer/Q1 Predictions Mean                             -0.908244\r\n",
      "trainer/Q1 Predictions Std                               0.992214\r\n",
      "trainer/Q1 Predictions Max                               1.18426\r\n",
      "trainer/Q1 Predictions Min                              -4.6062\r\n",
      "trainer/Q2 Predictions Mean                             -0.926026\r\n",
      "trainer/Q2 Predictions Std                               0.992034\r\n",
      "trainer/Q2 Predictions Max                               1.14099\r\n",
      "trainer/Q2 Predictions Min                              -4.63642\r\n",
      "trainer/Q Targets Mean                                  -0.917863\r\n",
      "trainer/Q Targets Std                                    0.993363\r\n",
      "trainer/Q Targets Max                                    1.20709\r\n",
      "trainer/Q Targets Min                                   -4.64734\r\n",
      "trainer/Log Pis Mean                                     1.92914\r\n",
      "trainer/Log Pis Std                                      1.47214\r\n",
      "trainer/Log Pis Max                                      7.77822\r\n",
      "trainer/Log Pis Min                                     -5.29729\r\n",
      "trainer/Policy mu Mean                                  -0.0480667\r\n",
      "trainer/Policy mu Std                                    0.55492\r\n",
      "trainer/Policy mu Max                                    2.85691\r\n",
      "trainer/Policy mu Min                                   -2.84633\r\n",
      "trainer/Policy log std Mean                             -2.1728\r\n",
      "trainer/Policy log std Std                               0.666731\r\n",
      "trainer/Policy log std Max                               0.418348\r\n",
      "trainer/Policy log std Min                              -3.3289\r\n",
      "trainer/Alpha                                            0.0195065\r\n",
      "trainer/Alpha Loss                                      -0.279068\r\n",
      "exploration/num steps total                           3500\r\n",
      "exploration/num paths total                            175\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.202137\r\n",
      "exploration/Rewards Std                                  0.126256\r\n",
      "exploration/Rewards Max                                  0.0141058\r\n",
      "exploration/Rewards Min                                 -0.602665\r\n",
      "exploration/Returns Mean                                -4.04275\r\n",
      "exploration/Returns Std                                  1.56058\r\n",
      "exploration/Returns Max                                 -2.56375\r\n",
      "exploration/Returns Min                                 -6.52692\r\n",
      "exploration/Actions Mean                                 0.00610206\r\n",
      "exploration/Actions Std                                  0.296105\r\n",
      "exploration/Actions Max                                  0.983074\r\n",
      "exploration/Actions Min                                 -0.991009\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -4.04275\r\n",
      "exploration/env_infos/final/reward_dist Mean             1.713e-07\r\n",
      "exploration/env_infos/final/reward_dist Std              3.42598e-07\r\n",
      "exploration/env_infos/final/reward_dist Max              8.56496e-07\r\n",
      "exploration/env_infos/final/reward_dist Min              1.42012e-96\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00088186\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00136979\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00359227\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.15744e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.011995\r\n",
      "exploration/env_infos/reward_dist Std                    0.0593049\r\n",
      "exploration/env_infos/reward_dist Max                    0.496146\r\n",
      "exploration/env_infos/reward_dist Min                    1.42012e-96\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.352301\r\n",
      "exploration/env_infos/final/reward_energy Std            0.312101\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0652839\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.933697\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.452141\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.39627\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0247752\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.994137\r\n",
      "exploration/env_infos/reward_energy Mean                -0.291092\r\n",
      "exploration/env_infos/reward_energy Std                  0.301158\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0120265\r\n",
      "exploration/env_infos/reward_energy Min                 -1.18601\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.11859\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.586656\r\n",
      "exploration/env_infos/final/end_effector_loc Max         1\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000282777\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0212543\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0491537\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0411199\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.121219\r\n",
      "exploration/env_infos/end_effector_loc Std               0.430138\r\n",
      "exploration/env_infos/end_effector_loc Max               1\r\n",
      "exploration/env_infos/end_effector_loc Min              -1\r\n",
      "evaluation/num steps total                           25000\r\n",
      "evaluation/num paths total                            1250\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.102533\r\n",
      "evaluation/Rewards Std                                   0.116698\r\n",
      "evaluation/Rewards Max                                   0.0946023\r\n",
      "evaluation/Rewards Min                                  -0.804997\r\n",
      "evaluation/Returns Mean                                 -2.05066\r\n",
      "evaluation/Returns Std                                   2.04725\r\n",
      "evaluation/Returns Max                                   0.885121\r\n",
      "evaluation/Returns Min                                 -11.0948\r\n",
      "evaluation/Actions Mean                                  0.00371771\r\n",
      "evaluation/Actions Std                                   0.113303\r\n",
      "evaluation/Actions Max                                   0.968752\r\n",
      "evaluation/Actions Min                                  -0.991957\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.05066\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0970351\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.215792\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.963973\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.1584e-152\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00492334\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00857156\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0354469\r\n",
      "evaluation/env_infos/initial/reward_dist Min             3.07416e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0680432\r\n",
      "evaluation/env_infos/reward_dist Std                     0.166892\r\n",
      "evaluation/env_infos/reward_dist Max                     0.994744\r\n",
      "evaluation/env_infos/reward_dist Min                     1.1584e-152\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0549057\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0992733\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00189237\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.677854\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.218373\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.250884\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00446773\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.24704\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.084561\r\n",
      "evaluation/env_infos/reward_energy Std                   0.136207\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000545709\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.29407\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0842186\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.369192\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000225816\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0117574\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0475943\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0418931\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0501271\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.25875\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.172115\r\n",
      "time/evaluation sampling (s)                             0.956033\r\n",
      "time/exploration sampling (s)                            0.11793\r\n",
      "time/logging (s)                                         0.0188989\r\n",
      "time/saving (s)                                          0.0285043\r\n",
      "time/training (s)                                       44.4005\r\n",
      "time/epoch (s)                                          45.694\r\n",
      "time/total (s)                                        1120.94\r\n",
      "Epoch                                                   24\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:31:58.903013 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 25 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00119087\n",
      "trainer/QF2 Loss                                         0.00245098\n",
      "trainer/Policy Loss                                      2.77845\n",
      "trainer/Q1 Predictions Mean                             -0.888455\n",
      "trainer/Q1 Predictions Std                               0.970276\n",
      "trainer/Q1 Predictions Max                               1.20557\n",
      "trainer/Q1 Predictions Min                              -3.95636\n",
      "trainer/Q2 Predictions Mean                             -0.890135\n",
      "trainer/Q2 Predictions Std                               0.968071\n",
      "trainer/Q2 Predictions Max                               1.19648\n",
      "trainer/Q2 Predictions Min                              -3.92963\n",
      "trainer/Q Targets Mean                                  -0.893505\n",
      "trainer/Q Targets Std                                    0.967731\n",
      "trainer/Q Targets Max                                    1.18079\n",
      "trainer/Q Targets Min                                   -3.93845\n",
      "trainer/Log Pis Mean                                     1.91835\n",
      "trainer/Log Pis Std                                      1.55221\n",
      "trainer/Log Pis Max                                      8.76781\n",
      "trainer/Log Pis Min                                     -4.40993\n",
      "trainer/Policy mu Mean                                  -0.0848233\n",
      "trainer/Policy mu Std                                    0.518014\n",
      "trainer/Policy mu Max                                    2.48415\n",
      "trainer/Policy mu Min                                   -2.83966\n",
      "trainer/Policy log std Mean                             -2.15521\n",
      "trainer/Policy log std Std                               0.654813\n",
      "trainer/Policy log std Max                               0.509475\n",
      "trainer/Policy log std Min                              -3.19753\n",
      "trainer/Alpha                                            0.0185974\n",
      "trainer/Alpha Loss                                      -0.325325\n",
      "exploration/num steps total                           3600\n",
      "exploration/num paths total                            180\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.119373\n",
      "exploration/Rewards Std                                  0.0611437\n",
      "exploration/Rewards Max                                  0.0339279\n",
      "exploration/Rewards Min                                 -0.30943\n",
      "exploration/Returns Mean                                -2.38746\n",
      "exploration/Returns Std                                  0.91232\n",
      "exploration/Returns Max                                 -0.801385\n",
      "exploration/Returns Min                                 -3.38653\n",
      "exploration/Actions Mean                                -0.00326488\n",
      "exploration/Actions Std                                  0.170628\n",
      "exploration/Actions Max                                  0.542088\n",
      "exploration/Actions Min                                 -0.808978\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.38746\n",
      "exploration/env_infos/final/reward_dist Mean             0.00327319\n",
      "exploration/env_infos/final/reward_dist Std              0.00607581\n",
      "exploration/env_infos/final/reward_dist Max              0.0154019\n",
      "exploration/env_infos/final/reward_dist Min              1.73986e-21\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00559604\n",
      "exploration/env_infos/initial/reward_dist Std            0.00894647\n",
      "exploration/env_infos/initial/reward_dist Max            0.0231467\n",
      "exploration/env_infos/initial/reward_dist Min            3.97885e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0194977\n",
      "exploration/env_infos/reward_dist Std                    0.0759771\n",
      "exploration/env_infos/reward_dist Max                    0.5913\n",
      "exploration/env_infos/reward_dist Min                    1.73986e-21\n",
      "exploration/env_infos/final/reward_energy Mean          -0.148092\n",
      "exploration/env_infos/final/reward_energy Std            0.112168\n",
      "exploration/env_infos/final/reward_energy Max           -0.016465\n",
      "exploration/env_infos/final/reward_energy Min           -0.330546\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.197921\n",
      "exploration/env_infos/initial/reward_energy Std          0.151485\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0249643\n",
      "exploration/env_infos/initial/reward_energy Min         -0.464011\n",
      "exploration/env_infos/reward_energy Mean                -0.168553\n",
      "exploration/env_infos/reward_energy Std                  0.17274\n",
      "exploration/env_infos/reward_energy Max                 -0.0123447\n",
      "exploration/env_infos/reward_energy Min                 -0.874587\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.126324\n",
      "exploration/env_infos/final/end_effector_loc Std         0.2363\n",
      "exploration/env_infos/final/end_effector_loc Max         0.42051\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.350243\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00382441\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00793879\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0193035\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0102769\n",
      "exploration/env_infos/end_effector_loc Mean              0.0885085\n",
      "exploration/env_infos/end_effector_loc Std               0.143187\n",
      "exploration/env_infos/end_effector_loc Max               0.454609\n",
      "exploration/env_infos/end_effector_loc Min              -0.350243\n",
      "evaluation/num steps total                           26000\n",
      "evaluation/num paths total                            1300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0830209\n",
      "evaluation/Rewards Std                                   0.106492\n",
      "evaluation/Rewards Max                                   0.150142\n",
      "evaluation/Rewards Min                                  -0.85654\n",
      "evaluation/Returns Mean                                 -1.66042\n",
      "evaluation/Returns Std                                   1.59325\n",
      "evaluation/Returns Max                                   1.81282\n",
      "evaluation/Returns Min                                  -5.61133\n",
      "evaluation/Actions Mean                                 -0.00162153\n",
      "evaluation/Actions Std                                   0.0932929\n",
      "evaluation/Actions Max                                   0.936739\n",
      "evaluation/Actions Min                                  -0.754209\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.66042\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0621433\n",
      "evaluation/env_infos/final/reward_dist Std               0.162342\n",
      "evaluation/env_infos/final/reward_dist Max               0.953108\n",
      "evaluation/env_infos/final/reward_dist Min               2.66478e-145\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00741236\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0143771\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0735385\n",
      "evaluation/env_infos/initial/reward_dist Min             3.19179e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0870504\n",
      "evaluation/env_infos/reward_dist Std                     0.178279\n",
      "evaluation/env_infos/reward_dist Max                     0.996257\n",
      "evaluation/env_infos/reward_dist Min                     2.66478e-145\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0587113\n",
      "evaluation/env_infos/final/reward_energy Std             0.0794853\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00688111\n",
      "evaluation/env_infos/final/reward_energy Min            -0.541588\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.229673\n",
      "evaluation/env_infos/initial/reward_energy Std           0.229549\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0037665\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.29408\n",
      "evaluation/env_infos/reward_energy Mean                 -0.080752\n",
      "evaluation/env_infos/reward_energy Std                   0.104362\n",
      "evaluation/env_infos/reward_energy Max                  -0.0010233\n",
      "evaluation/env_infos/reward_energy Min                  -1.29408\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0114535\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.387556\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00128849\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.011408\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0468369\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0377105\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00384563\n",
      "evaluation/env_infos/end_effector_loc Std                0.244254\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.180169\n",
      "time/evaluation sampling (s)                             0.915136\n",
      "time/exploration sampling (s)                            0.126066\n",
      "time/logging (s)                                         0.0186509\n",
      "time/saving (s)                                          0.0285996\n",
      "time/training (s)                                       46.4516\n",
      "time/epoch (s)                                          47.7202\n",
      "time/total (s)                                        1168.99\n",
      "Epoch                                                   25\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:32:45.776821 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 26 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00615648\n",
      "trainer/QF2 Loss                                         0.00269244\n",
      "trainer/Policy Loss                                      2.90053\n",
      "trainer/Q1 Predictions Mean                             -0.804892\n",
      "trainer/Q1 Predictions Std                               0.970108\n",
      "trainer/Q1 Predictions Max                               1.08577\n",
      "trainer/Q1 Predictions Min                              -4.53549\n",
      "trainer/Q2 Predictions Mean                             -0.808454\n",
      "trainer/Q2 Predictions Std                               0.96594\n",
      "trainer/Q2 Predictions Max                               1.11185\n",
      "trainer/Q2 Predictions Min                              -4.48438\n",
      "trainer/Q Targets Mean                                  -0.806042\n",
      "trainer/Q Targets Std                                    0.965308\n",
      "trainer/Q Targets Max                                    1.12418\n",
      "trainer/Q Targets Min                                   -4.52394\n",
      "trainer/Log Pis Mean                                     2.12461\n",
      "trainer/Log Pis Std                                      1.41717\n",
      "trainer/Log Pis Max                                      6.31944\n",
      "trainer/Log Pis Min                                     -4.60898\n",
      "trainer/Policy mu Mean                                  -0.0787\n",
      "trainer/Policy mu Std                                    0.555224\n",
      "trainer/Policy mu Max                                    2.462\n",
      "trainer/Policy mu Min                                   -2.44467\n",
      "trainer/Policy log std Mean                             -2.21469\n",
      "trainer/Policy log std Std                               0.703278\n",
      "trainer/Policy log std Max                               0.133418\n",
      "trainer/Policy log std Min                              -3.27861\n",
      "trainer/Alpha                                            0.0189201\n",
      "trainer/Alpha Loss                                       0.494449\n",
      "exploration/num steps total                           3700\n",
      "exploration/num paths total                            185\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.112297\n",
      "exploration/Rewards Std                                  0.0950588\n",
      "exploration/Rewards Max                                  0.0241148\n",
      "exploration/Rewards Min                                 -0.755844\n",
      "exploration/Returns Mean                                -2.24594\n",
      "exploration/Returns Std                                  0.690781\n",
      "exploration/Returns Max                                 -1.08003\n",
      "exploration/Returns Min                                 -3.23672\n",
      "exploration/Actions Mean                                 0.00794984\n",
      "exploration/Actions Std                                  0.122528\n",
      "exploration/Actions Max                                  0.852915\n",
      "exploration/Actions Min                                 -0.399033\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.24594\n",
      "exploration/env_infos/final/reward_dist Mean             0.0181111\n",
      "exploration/env_infos/final/reward_dist Std              0.0362212\n",
      "exploration/env_infos/final/reward_dist Max              0.0905535\n",
      "exploration/env_infos/final/reward_dist Min              1.63854e-94\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103203\n",
      "exploration/env_infos/initial/reward_dist Std            0.0128314\n",
      "exploration/env_infos/initial/reward_dist Max            0.031157\n",
      "exploration/env_infos/initial/reward_dist Min            9.02764e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0486244\n",
      "exploration/env_infos/reward_dist Std                    0.0940278\n",
      "exploration/env_infos/reward_dist Max                    0.379773\n",
      "exploration/env_infos/reward_dist Min                    1.17977e-95\n",
      "exploration/env_infos/final/reward_energy Mean          -0.121416\n",
      "exploration/env_infos/final/reward_energy Std            0.05602\n",
      "exploration/env_infos/final/reward_energy Max           -0.0352942\n",
      "exploration/env_infos/final/reward_energy Min           -0.186967\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.307883\n",
      "exploration/env_infos/initial/reward_energy Std          0.32483\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0308195\n",
      "exploration/env_infos/initial/reward_energy Min         -0.941643\n",
      "exploration/env_infos/reward_energy Mean                -0.130441\n",
      "exploration/env_infos/reward_energy Std                  0.114619\n",
      "exploration/env_infos/reward_energy Max                 -0.00516646\n",
      "exploration/env_infos/reward_energy Min                 -0.941643\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.139723\n",
      "exploration/env_infos/final/end_effector_loc Std         0.4202\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.302881\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00113357\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0157828\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0426457\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0199516\n",
      "exploration/env_infos/end_effector_loc Mean              0.0654249\n",
      "exploration/env_infos/end_effector_loc Std               0.276912\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -0.302881\n",
      "evaluation/num steps total                           27000\n",
      "evaluation/num paths total                            1350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0755507\n",
      "evaluation/Rewards Std                                   0.133688\n",
      "evaluation/Rewards Max                                   0.156044\n",
      "evaluation/Rewards Min                                  -0.949084\n",
      "evaluation/Returns Mean                                 -1.51101\n",
      "evaluation/Returns Std                                   1.91514\n",
      "evaluation/Returns Max                                   1.96317\n",
      "evaluation/Returns Min                                  -7.57802\n",
      "evaluation/Actions Mean                                 -0.00550431\n",
      "evaluation/Actions Std                                   0.11781\n",
      "evaluation/Actions Max                                   0.968373\n",
      "evaluation/Actions Min                                  -0.925585\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.51101\n",
      "evaluation/env_infos/final/reward_dist Mean              0.114894\n",
      "evaluation/env_infos/final/reward_dist Std               0.237961\n",
      "evaluation/env_infos/final/reward_dist Max               0.990244\n",
      "evaluation/env_infos/final/reward_dist Min               6.94128e-152\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00686259\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00997514\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0539005\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0579e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.116475\n",
      "evaluation/env_infos/reward_dist Std                     0.216231\n",
      "evaluation/env_infos/reward_dist Max                     0.999727\n",
      "evaluation/env_infos/reward_dist Min                     6.94128e-152\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0995294\n",
      "evaluation/env_infos/final/reward_energy Std             0.133677\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00379811\n",
      "evaluation/env_infos/final/reward_energy Min            -0.718317\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.257974\n",
      "evaluation/env_infos/initial/reward_energy Std           0.270846\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0302604\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.24984\n",
      "evaluation/env_infos/reward_energy Mean                 -0.102063\n",
      "evaluation/env_infos/reward_energy Std                   0.131917\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178384\n",
      "evaluation/env_infos/reward_energy Min                  -1.24984\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.000925753\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.406934\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000791323\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0132007\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0484187\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0462792\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00510117\n",
      "evaluation/env_infos/end_effector_loc Std                0.260883\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.171644\n",
      "time/evaluation sampling (s)                             0.964326\n",
      "time/exploration sampling (s)                            0.119221\n",
      "time/logging (s)                                         0.0194158\n",
      "time/saving (s)                                          0.0267572\n",
      "time/training (s)                                       45.2271\n",
      "time/epoch (s)                                          46.5285\n",
      "time/total (s)                                        1215.87\n",
      "Epoch                                                   26\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:33:32.488824 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 27 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0036945\n",
      "trainer/QF2 Loss                                         0.00593672\n",
      "trainer/Policy Loss                                      2.81446\n",
      "trainer/Q1 Predictions Mean                             -0.85372\n",
      "trainer/Q1 Predictions Std                               0.961013\n",
      "trainer/Q1 Predictions Max                               1.0863\n",
      "trainer/Q1 Predictions Min                              -3.90536\n",
      "trainer/Q2 Predictions Mean                             -0.853472\n",
      "trainer/Q2 Predictions Std                               0.95949\n",
      "trainer/Q2 Predictions Max                               1.10002\n",
      "trainer/Q2 Predictions Min                              -3.95004\n",
      "trainer/Q Targets Mean                                  -0.866453\n",
      "trainer/Q Targets Std                                    0.963936\n",
      "trainer/Q Targets Max                                    1.07735\n",
      "trainer/Q Targets Min                                   -3.93939\n",
      "trainer/Log Pis Mean                                     1.98523\n",
      "trainer/Log Pis Std                                      1.56954\n",
      "trainer/Log Pis Max                                      7.83599\n",
      "trainer/Log Pis Min                                     -3.39643\n",
      "trainer/Policy mu Mean                                  -0.0569778\n",
      "trainer/Policy mu Std                                    0.606052\n",
      "trainer/Policy mu Max                                    2.48424\n",
      "trainer/Policy mu Min                                   -2.98756\n",
      "trainer/Policy log std Mean                             -2.10774\n",
      "trainer/Policy log std Std                               0.7299\n",
      "trainer/Policy log std Max                              -0.203054\n",
      "trainer/Policy log std Min                              -3.2319\n",
      "trainer/Alpha                                            0.018201\n",
      "trainer/Alpha Loss                                      -0.0591465\n",
      "exploration/num steps total                           3800\n",
      "exploration/num paths total                            190\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.22783\n",
      "exploration/Rewards Std                                  0.156051\n",
      "exploration/Rewards Max                                  0.0132548\n",
      "exploration/Rewards Min                                 -0.693866\n",
      "exploration/Returns Mean                                -4.5566\n",
      "exploration/Returns Std                                  2.60112\n",
      "exploration/Returns Max                                 -0.833673\n",
      "exploration/Returns Min                                 -6.97779\n",
      "exploration/Actions Mean                                 0.00769918\n",
      "exploration/Actions Std                                  0.221659\n",
      "exploration/Actions Max                                  0.771135\n",
      "exploration/Actions Min                                 -0.802931\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -4.5566\n",
      "exploration/env_infos/final/reward_dist Mean             6.79104e-05\n",
      "exploration/env_infos/final/reward_dist Std              0.000113475\n",
      "exploration/env_infos/final/reward_dist Max              0.000291829\n",
      "exploration/env_infos/final/reward_dist Min              1.43364e-109\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0166658\n",
      "exploration/env_infos/initial/reward_dist Std            0.0265674\n",
      "exploration/env_infos/initial/reward_dist Max            0.0689138\n",
      "exploration/env_infos/initial/reward_dist Min            0.000111704\n",
      "exploration/env_infos/reward_dist Mean                   0.0572597\n",
      "exploration/env_infos/reward_dist Std                    0.158029\n",
      "exploration/env_infos/reward_dist Max                    0.760761\n",
      "exploration/env_infos/reward_dist Min                    1.43364e-109\n",
      "exploration/env_infos/final/reward_energy Mean          -0.148872\n",
      "exploration/env_infos/final/reward_energy Std            0.0692086\n",
      "exploration/env_infos/final/reward_energy Max           -0.0729846\n",
      "exploration/env_infos/final/reward_energy Min           -0.262449\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.383863\n",
      "exploration/env_infos/initial/reward_energy Std          0.303811\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0592196\n",
      "exploration/env_infos/initial/reward_energy Min         -0.819888\n",
      "exploration/env_infos/reward_energy Mean                -0.22666\n",
      "exploration/env_infos/reward_energy Std                  0.216816\n",
      "exploration/env_infos/reward_energy Max                 -0.00705719\n",
      "exploration/env_infos/reward_energy Min                 -1.09978\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.297121\n",
      "exploration/env_infos/final/end_effector_loc Std         0.646431\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00722689\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0157269\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0385567\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0147617\n",
      "exploration/env_infos/end_effector_loc Mean              0.155695\n",
      "exploration/env_infos/end_effector_loc Std               0.483503\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           28000\n",
      "evaluation/num paths total                            1400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.11653\n",
      "evaluation/Rewards Std                                   0.130844\n",
      "evaluation/Rewards Max                                   0.141158\n",
      "evaluation/Rewards Min                                  -0.620812\n",
      "evaluation/Returns Mean                                 -2.3306\n",
      "evaluation/Returns Std                                   2.25289\n",
      "evaluation/Returns Max                                   1.79829\n",
      "evaluation/Returns Min                                  -9.42577\n",
      "evaluation/Actions Mean                                 -0.00417927\n",
      "evaluation/Actions Std                                   0.131807\n",
      "evaluation/Actions Max                                   0.7005\n",
      "evaluation/Actions Min                                  -0.928514\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.3306\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0721404\n",
      "evaluation/env_infos/final/reward_dist Std               0.198844\n",
      "evaluation/env_infos/final/reward_dist Max               0.839393\n",
      "evaluation/env_infos/final/reward_dist Min               1.30727e-123\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00680102\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0124371\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0554541\n",
      "evaluation/env_infos/initial/reward_dist Min             1.72748e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0660885\n",
      "evaluation/env_infos/reward_dist Std                     0.167109\n",
      "evaluation/env_infos/reward_dist Max                     0.931325\n",
      "evaluation/env_infos/reward_dist Min                     1.30727e-123\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0911053\n",
      "evaluation/env_infos/final/reward_energy Std             0.11624\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00253611\n",
      "evaluation/env_infos/final/reward_energy Min            -0.452537\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.303352\n",
      "evaluation/env_infos/initial/reward_energy Std           0.266202\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00864417\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.994935\n",
      "evaluation/env_infos/reward_energy Mean                 -0.116083\n",
      "evaluation/env_infos/reward_energy Std                   0.145966\n",
      "evaluation/env_infos/reward_energy Max                  -0.00132851\n",
      "evaluation/env_infos/reward_energy Min                  -0.994935\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0831281\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.477359\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00462203\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134998\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.035025\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0464257\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0521191\n",
      "evaluation/env_infos/end_effector_loc Std                0.324285\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.169026\n",
      "time/evaluation sampling (s)                             0.93127\n",
      "time/exploration sampling (s)                            0.115515\n",
      "time/logging (s)                                         0.0201562\n",
      "time/saving (s)                                          0.0287222\n",
      "time/training (s)                                       45.087\n",
      "time/epoch (s)                                          46.3517\n",
      "time/total (s)                                        1262.58\n",
      "Epoch                                                   27\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:34:20.839837 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 28 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00254942\n",
      "trainer/QF2 Loss                                         0.0029971\n",
      "trainer/Policy Loss                                      3.20494\n",
      "trainer/Q1 Predictions Mean                             -1.02229\n",
      "trainer/Q1 Predictions Std                               0.915038\n",
      "trainer/Q1 Predictions Max                               1.0048\n",
      "trainer/Q1 Predictions Min                              -3.61492\n",
      "trainer/Q2 Predictions Mean                             -1.01457\n",
      "trainer/Q2 Predictions Std                               0.90685\n",
      "trainer/Q2 Predictions Max                               0.992903\n",
      "trainer/Q2 Predictions Min                              -3.56851\n",
      "trainer/Q Targets Mean                                  -1.02903\n",
      "trainer/Q Targets Std                                    0.908753\n",
      "trainer/Q Targets Max                                    1.05054\n",
      "trainer/Q Targets Min                                   -3.44014\n",
      "trainer/Log Pis Mean                                     2.21633\n",
      "trainer/Log Pis Std                                      1.43487\n",
      "trainer/Log Pis Max                                      9.13857\n",
      "trainer/Log Pis Min                                     -4.42116\n",
      "trainer/Policy mu Mean                                  -0.0169071\n",
      "trainer/Policy mu Std                                    0.642957\n",
      "trainer/Policy mu Max                                    2.37437\n",
      "trainer/Policy mu Min                                   -3.13039\n",
      "trainer/Policy log std Mean                             -2.23525\n",
      "trainer/Policy log std Std                               0.689014\n",
      "trainer/Policy log std Max                               0.20891\n",
      "trainer/Policy log std Min                              -3.18413\n",
      "trainer/Alpha                                            0.0201112\n",
      "trainer/Alpha Loss                                       0.845085\n",
      "exploration/num steps total                           3900\n",
      "exploration/num paths total                            195\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.196121\n",
      "exploration/Rewards Std                                  0.0971599\n",
      "exploration/Rewards Max                                 -0.0437076\n",
      "exploration/Rewards Min                                 -0.416186\n",
      "exploration/Returns Mean                                -3.92242\n",
      "exploration/Returns Std                                  1.10268\n",
      "exploration/Returns Max                                 -2.38971\n",
      "exploration/Returns Min                                 -4.87945\n",
      "exploration/Actions Mean                                -0.00158907\n",
      "exploration/Actions Std                                  0.164517\n",
      "exploration/Actions Max                                  0.704998\n",
      "exploration/Actions Min                                 -0.530179\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.92242\n",
      "exploration/env_infos/final/reward_dist Mean             0.00214718\n",
      "exploration/env_infos/final/reward_dist Std              0.00263529\n",
      "exploration/env_infos/final/reward_dist Max              0.00563813\n",
      "exploration/env_infos/final/reward_dist Min              1.18097e-56\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00929294\n",
      "exploration/env_infos/initial/reward_dist Std            0.0182271\n",
      "exploration/env_infos/initial/reward_dist Max            0.0457454\n",
      "exploration/env_infos/initial/reward_dist Min            2.62337e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0105866\n",
      "exploration/env_infos/reward_dist Std                    0.0272223\n",
      "exploration/env_infos/reward_dist Max                    0.14936\n",
      "exploration/env_infos/reward_dist Min                    1.18097e-56\n",
      "exploration/env_infos/final/reward_energy Mean          -0.110858\n",
      "exploration/env_infos/final/reward_energy Std            0.0895148\n",
      "exploration/env_infos/final/reward_energy Max           -0.0163686\n",
      "exploration/env_infos/final/reward_energy Min           -0.27701\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.211453\n",
      "exploration/env_infos/initial/reward_energy Std          0.173103\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0488639\n",
      "exploration/env_infos/initial/reward_energy Min         -0.535476\n",
      "exploration/env_infos/reward_energy Mean                -0.18237\n",
      "exploration/env_infos/reward_energy Std                  0.144492\n",
      "exploration/env_infos/reward_energy Max                 -0.00169676\n",
      "exploration/env_infos/reward_energy Min                 -0.797499\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.04231\n",
      "exploration/env_infos/final/end_effector_loc Std         0.435607\n",
      "exploration/env_infos/final/end_effector_loc Max         0.873305\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.648691\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000134144\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00966065\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0102446\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.025236\n",
      "exploration/env_infos/end_effector_loc Mean              0.0168154\n",
      "exploration/env_infos/end_effector_loc Std               0.229614\n",
      "exploration/env_infos/end_effector_loc Max               0.873305\n",
      "exploration/env_infos/end_effector_loc Min              -0.648691\n",
      "evaluation/num steps total                           29000\n",
      "evaluation/num paths total                            1450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.1056\n",
      "evaluation/Rewards Std                                   0.109352\n",
      "evaluation/Rewards Max                                   0.125343\n",
      "evaluation/Rewards Min                                  -0.880397\n",
      "evaluation/Returns Mean                                 -2.11199\n",
      "evaluation/Returns Std                                   1.50588\n",
      "evaluation/Returns Max                                   0.245622\n",
      "evaluation/Returns Min                                  -6.05898\n",
      "evaluation/Actions Mean                                 -0.00793413\n",
      "evaluation/Actions Std                                   0.119961\n",
      "evaluation/Actions Max                                   0.93126\n",
      "evaluation/Actions Min                                  -0.684286\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.11199\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0724339\n",
      "evaluation/env_infos/final/reward_dist Std               0.184862\n",
      "evaluation/env_infos/final/reward_dist Max               0.818447\n",
      "evaluation/env_infos/final/reward_dist Min               7.72685e-177\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00267465\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00482948\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0224722\n",
      "evaluation/env_infos/initial/reward_dist Min             1.1521e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0654007\n",
      "evaluation/env_infos/reward_dist Std                     0.177888\n",
      "evaluation/env_infos/reward_dist Max                     0.989251\n",
      "evaluation/env_infos/reward_dist Min                     7.72685e-177\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.105473\n",
      "evaluation/env_infos/final/reward_energy Std             0.162047\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00436761\n",
      "evaluation/env_infos/final/reward_energy Min            -0.734882\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.336029\n",
      "evaluation/env_infos/initial/reward_energy Std           0.30026\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00443917\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.22494\n",
      "evaluation/env_infos/reward_energy Mean                 -0.100642\n",
      "evaluation/env_infos/reward_energy Std                   0.137034\n",
      "evaluation/env_infos/reward_energy Max                  -0.000473189\n",
      "evaluation/env_infos/reward_energy Min                  -1.22494\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0730963\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.527965\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000309613\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0159293\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.046563\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0342143\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.020203\n",
      "evaluation/env_infos/end_effector_loc Std                0.345584\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.173663\n",
      "time/evaluation sampling (s)                             0.963394\n",
      "time/exploration sampling (s)                            0.116219\n",
      "time/logging (s)                                         0.0198153\n",
      "time/saving (s)                                          0.0325023\n",
      "time/training (s)                                       46.6647\n",
      "time/epoch (s)                                          47.9703\n",
      "time/total (s)                                        1310.93\n",
      "Epoch                                                   28\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:35:08.076766 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 29 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00467309\r\n",
      "trainer/QF2 Loss                                         0.00925383\r\n",
      "trainer/Policy Loss                                      3.02267\r\n",
      "trainer/Q1 Predictions Mean                             -0.996667\r\n",
      "trainer/Q1 Predictions Std                               0.864926\r\n",
      "trainer/Q1 Predictions Max                               1.10254\r\n",
      "trainer/Q1 Predictions Min                              -3.22839\r\n",
      "trainer/Q2 Predictions Mean                             -0.995803\r\n",
      "trainer/Q2 Predictions Std                               0.860123\r\n",
      "trainer/Q2 Predictions Max                               1.08243\r\n",
      "trainer/Q2 Predictions Min                              -3.32094\r\n",
      "trainer/Q Targets Mean                                  -0.992908\r\n",
      "trainer/Q Targets Std                                    0.871395\r\n",
      "trainer/Q Targets Max                                    1.13602\r\n",
      "trainer/Q Targets Min                                   -3.26242\r\n",
      "trainer/Log Pis Mean                                     2.05817\r\n",
      "trainer/Log Pis Std                                      1.43164\r\n",
      "trainer/Log Pis Max                                      7.81468\r\n",
      "trainer/Log Pis Min                                     -2.77226\r\n",
      "trainer/Policy mu Mean                                  -0.0374752\r\n",
      "trainer/Policy mu Std                                    0.571582\r\n",
      "trainer/Policy mu Max                                    2.45097\r\n",
      "trainer/Policy mu Min                                   -2.71336\r\n",
      "trainer/Policy log std Mean                             -2.16343\r\n",
      "trainer/Policy log std Std                               0.72091\r\n",
      "trainer/Policy log std Max                               0.0329058\r\n",
      "trainer/Policy log std Min                              -3.19728\r\n",
      "trainer/Alpha                                            0.0204351\r\n",
      "trainer/Alpha Loss                                       0.226314\r\n",
      "exploration/num steps total                           4000\r\n",
      "exploration/num paths total                            200\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.218629\r\n",
      "exploration/Rewards Std                                  0.144788\r\n",
      "exploration/Rewards Max                                 -0.0457412\r\n",
      "exploration/Rewards Min                                 -0.659877\r\n",
      "exploration/Returns Mean                                -4.37259\r\n",
      "exploration/Returns Std                                  2.40095\r\n",
      "exploration/Returns Max                                 -2.4296\r\n",
      "exploration/Returns Min                                 -9.07355\r\n",
      "exploration/Actions Mean                                -0.0415983\r\n",
      "exploration/Actions Std                                  0.259913\r\n",
      "exploration/Actions Max                                  0.976563\r\n",
      "exploration/Actions Min                                 -0.962598\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -4.37259\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00047646\r\n",
      "exploration/env_infos/final/reward_dist Std              0.000924501\r\n",
      "exploration/env_infos/final/reward_dist Max              0.00232493\r\n",
      "exploration/env_infos/final/reward_dist Min              1.65671e-113\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00308108\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0032242\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00762084\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.17281e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.00248013\r\n",
      "exploration/env_infos/reward_dist Std                    0.00827726\r\n",
      "exploration/env_infos/reward_dist Max                    0.0562742\r\n",
      "exploration/env_infos/reward_dist Min                    1.65671e-113\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.301587\r\n",
      "exploration/env_infos/final/reward_energy Std            0.394668\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.052845\r\n",
      "exploration/env_infos/final/reward_energy Min           -1.08618\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.556822\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.452989\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0409774\r\n",
      "exploration/env_infos/initial/reward_energy Min         -1.17281\r\n",
      "exploration/env_infos/reward_energy Mean                -0.264962\r\n",
      "exploration/env_infos/reward_energy Std                  0.261468\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0144653\r\n",
      "exploration/env_infos/reward_energy Min                 -1.1796\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0893208\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.580127\r\n",
      "exploration/env_infos/final/end_effector_loc Max         1\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00983063\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.023397\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0488282\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0215175\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.00672537\r\n",
      "exploration/env_infos/end_effector_loc Std               0.362464\r\n",
      "exploration/env_infos/end_effector_loc Max               1\r\n",
      "exploration/env_infos/end_effector_loc Min              -1\r\n",
      "evaluation/num steps total                           30000\r\n",
      "evaluation/num paths total                            1500\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.10604\r\n",
      "evaluation/Rewards Std                                   0.117725\r\n",
      "evaluation/Rewards Max                                   0.105469\r\n",
      "evaluation/Rewards Min                                  -0.830677\r\n",
      "evaluation/Returns Mean                                 -2.1208\r\n",
      "evaluation/Returns Std                                   1.67206\r\n",
      "evaluation/Returns Max                                   1.23452\r\n",
      "evaluation/Returns Min                                  -5.96855\r\n",
      "evaluation/Actions Mean                                  0.00448197\r\n",
      "evaluation/Actions Std                                   0.126624\r\n",
      "evaluation/Actions Max                                   0.992009\r\n",
      "evaluation/Actions Min                                  -0.813396\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.1208\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0713793\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.15923\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.606754\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.46183e-110\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00787185\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0191132\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.124202\r\n",
      "evaluation/env_infos/initial/reward_dist Min             6.01189e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0766745\r\n",
      "evaluation/env_infos/reward_dist Std                     0.175254\r\n",
      "evaluation/env_infos/reward_dist Max                     0.975767\r\n",
      "evaluation/env_infos/reward_dist Min                     2.46183e-110\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0781738\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.133758\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00299994\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.732299\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.361459\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.344752\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0125669\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.37745\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.100211\r\n",
      "evaluation/env_infos/reward_energy Std                   0.148543\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00158585\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.37745\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0745218\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.488648\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00393624\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0172159\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0496005\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0406698\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0579497\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.307813\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.173519\r\n",
      "time/evaluation sampling (s)                             0.866788\r\n",
      "time/exploration sampling (s)                            0.117964\r\n",
      "time/logging (s)                                         0.019337\r\n",
      "time/saving (s)                                          0.0280213\r\n",
      "time/training (s)                                       45.6972\r\n",
      "time/epoch (s)                                          46.9029\r\n",
      "time/total (s)                                        1358.16\r\n",
      "Epoch                                                   29\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:35:56.249985 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 30 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00314856\n",
      "trainer/QF2 Loss                                         0.00218968\n",
      "trainer/Policy Loss                                      2.89082\n",
      "trainer/Q1 Predictions Mean                             -1.01686\n",
      "trainer/Q1 Predictions Std                               0.889554\n",
      "trainer/Q1 Predictions Max                               1.11764\n",
      "trainer/Q1 Predictions Min                              -4.04743\n",
      "trainer/Q2 Predictions Mean                             -1.01821\n",
      "trainer/Q2 Predictions Std                               0.889662\n",
      "trainer/Q2 Predictions Max                               1.09532\n",
      "trainer/Q2 Predictions Min                              -4.13533\n",
      "trainer/Q Targets Mean                                  -1.02764\n",
      "trainer/Q Targets Std                                    0.891887\n",
      "trainer/Q Targets Max                                    1.08586\n",
      "trainer/Q Targets Min                                   -4.42039\n",
      "trainer/Log Pis Mean                                     1.89414\n",
      "trainer/Log Pis Std                                      1.74313\n",
      "trainer/Log Pis Max                                     14.9726\n",
      "trainer/Log Pis Min                                     -4.20587\n",
      "trainer/Policy mu Mean                                   0.00176186\n",
      "trainer/Policy mu Std                                    0.59399\n",
      "trainer/Policy mu Max                                    3.91048\n",
      "trainer/Policy mu Min                                   -3.56765\n",
      "trainer/Policy log std Mean                             -2.15446\n",
      "trainer/Policy log std Std                               0.719731\n",
      "trainer/Policy log std Max                               0.42539\n",
      "trainer/Policy log std Min                              -3.33449\n",
      "trainer/Alpha                                            0.0182084\n",
      "trainer/Alpha Loss                                      -0.424066\n",
      "exploration/num steps total                           4100\n",
      "exploration/num paths total                            205\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0779768\n",
      "exploration/Rewards Std                                  0.0846135\n",
      "exploration/Rewards Max                                  0.110444\n",
      "exploration/Rewards Min                                 -0.321509\n",
      "exploration/Returns Mean                                -1.55954\n",
      "exploration/Returns Std                                  1.36931\n",
      "exploration/Returns Max                                  0.59173\n",
      "exploration/Returns Min                                 -3.1103\n",
      "exploration/Actions Mean                                -0.00172171\n",
      "exploration/Actions Std                                  0.227904\n",
      "exploration/Actions Max                                  0.872084\n",
      "exploration/Actions Min                                 -0.932481\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.55954\n",
      "exploration/env_infos/final/reward_dist Mean             0.124614\n",
      "exploration/env_infos/final/reward_dist Std              0.248134\n",
      "exploration/env_infos/final/reward_dist Max              0.620879\n",
      "exploration/env_infos/final/reward_dist Min              1.42042e-20\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0132191\n",
      "exploration/env_infos/initial/reward_dist Std            0.0251206\n",
      "exploration/env_infos/initial/reward_dist Max            0.0634479\n",
      "exploration/env_infos/initial/reward_dist Min            5.40124e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.11252\n",
      "exploration/env_infos/reward_dist Std                    0.24553\n",
      "exploration/env_infos/reward_dist Max                    0.842144\n",
      "exploration/env_infos/reward_dist Min                    1.42042e-20\n",
      "exploration/env_infos/final/reward_energy Mean          -0.199359\n",
      "exploration/env_infos/final/reward_energy Std            0.144534\n",
      "exploration/env_infos/final/reward_energy Max           -0.0816369\n",
      "exploration/env_infos/final/reward_energy Min           -0.479582\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.469969\n",
      "exploration/env_infos/initial/reward_energy Std          0.434318\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0684267\n",
      "exploration/env_infos/initial/reward_energy Min         -1.22985\n",
      "exploration/env_infos/reward_energy Mean                -0.220436\n",
      "exploration/env_infos/reward_energy Std                  0.235147\n",
      "exploration/env_infos/reward_energy Max                 -0.0137787\n",
      "exploration/env_infos/reward_energy Min                 -1.22985\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00431046\n",
      "exploration/env_infos/final/end_effector_loc Std         0.202943\n",
      "exploration/env_infos/final/end_effector_loc Max         0.252245\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.366353\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00294726\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.022432\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0414718\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.045403\n",
      "exploration/env_infos/end_effector_loc Mean              0.0193043\n",
      "exploration/env_infos/end_effector_loc Std               0.182468\n",
      "exploration/env_infos/end_effector_loc Max               0.557622\n",
      "exploration/env_infos/end_effector_loc Min              -0.366353\n",
      "evaluation/num steps total                           31000\n",
      "evaluation/num paths total                            1550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0828047\n",
      "evaluation/Rewards Std                                   0.110159\n",
      "evaluation/Rewards Max                                   0.137853\n",
      "evaluation/Rewards Min                                  -0.701096\n",
      "evaluation/Returns Mean                                 -1.65609\n",
      "evaluation/Returns Std                                   1.79558\n",
      "evaluation/Returns Max                                   0.4914\n",
      "evaluation/Returns Min                                  -7.48319\n",
      "evaluation/Actions Mean                                  0.00253998\n",
      "evaluation/Actions Std                                   0.089598\n",
      "evaluation/Actions Max                                   0.689687\n",
      "evaluation/Actions Min                                  -0.803528\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.65609\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0721369\n",
      "evaluation/env_infos/final/reward_dist Std               0.146772\n",
      "evaluation/env_infos/final/reward_dist Max               0.694401\n",
      "evaluation/env_infos/final/reward_dist Min               1.11929e-115\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00557175\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0110464\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0486916\n",
      "evaluation/env_infos/initial/reward_dist Min             1.06557e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0883192\n",
      "evaluation/env_infos/reward_dist Std                     0.184564\n",
      "evaluation/env_infos/reward_dist Max                     0.995623\n",
      "evaluation/env_infos/reward_dist Min                     1.11929e-115\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0827256\n",
      "evaluation/env_infos/final/reward_energy Std             0.0728098\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00441708\n",
      "evaluation/env_infos/final/reward_energy Min            -0.347376\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.20732\n",
      "evaluation/env_infos/initial/reward_energy Std           0.191915\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0288333\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.814071\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0855881\n",
      "evaluation/env_infos/reward_energy Std                   0.093505\n",
      "evaluation/env_infos/reward_energy Max                  -0.00118791\n",
      "evaluation/env_infos/reward_energy Min                  -0.814071\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0190387\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.405034\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.896117\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000263618\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00998482\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0344843\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0401764\n",
      "evaluation/env_infos/end_effector_loc Mean               0.015835\n",
      "evaluation/env_infos/end_effector_loc Std                0.244442\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.896117\n",
      "time/data storing (s)                                    0.177508\n",
      "time/evaluation sampling (s)                             0.971025\n",
      "time/exploration sampling (s)                            0.127857\n",
      "time/logging (s)                                         0.0186397\n",
      "time/saving (s)                                          0.0309786\n",
      "time/training (s)                                       46.4412\n",
      "time/epoch (s)                                          47.7672\n",
      "time/total (s)                                        1406.34\n",
      "Epoch                                                   30\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:36:44.308693 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 31 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00172667\n",
      "trainer/QF2 Loss                                         0.00158133\n",
      "trainer/Policy Loss                                      2.91751\n",
      "trainer/Q1 Predictions Mean                             -0.949267\n",
      "trainer/Q1 Predictions Std                               0.805986\n",
      "trainer/Q1 Predictions Max                               1.0583\n",
      "trainer/Q1 Predictions Min                              -3.06498\n",
      "trainer/Q2 Predictions Mean                             -0.923018\n",
      "trainer/Q2 Predictions Std                               0.806126\n",
      "trainer/Q2 Predictions Max                               1.11676\n",
      "trainer/Q2 Predictions Min                              -3.02187\n",
      "trainer/Q Targets Mean                                  -0.933193\n",
      "trainer/Q Targets Std                                    0.809896\n",
      "trainer/Q Targets Max                                    1.09084\n",
      "trainer/Q Targets Min                                   -3.0758\n",
      "trainer/Log Pis Mean                                     2.00595\n",
      "trainer/Log Pis Std                                      1.4148\n",
      "trainer/Log Pis Max                                      6.34824\n",
      "trainer/Log Pis Min                                     -4.17389\n",
      "trainer/Policy mu Mean                                  -0.043612\n",
      "trainer/Policy mu Std                                    0.513986\n",
      "trainer/Policy mu Max                                    2.89016\n",
      "trainer/Policy mu Min                                   -2.96633\n",
      "trainer/Policy log std Mean                             -2.18346\n",
      "trainer/Policy log std Std                               0.708833\n",
      "trainer/Policy log std Max                               0.41294\n",
      "trainer/Policy log std Min                              -3.28155\n",
      "trainer/Alpha                                            0.0180003\n",
      "trainer/Alpha Loss                                       0.0238913\n",
      "exploration/num steps total                           4200\n",
      "exploration/num paths total                            210\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.113195\n",
      "exploration/Rewards Std                                  0.100594\n",
      "exploration/Rewards Max                                  0.00140983\n",
      "exploration/Rewards Min                                 -0.432015\n",
      "exploration/Returns Mean                                -2.26389\n",
      "exploration/Returns Std                                  1.62551\n",
      "exploration/Returns Max                                 -0.946749\n",
      "exploration/Returns Min                                 -5.00363\n",
      "exploration/Actions Mean                                -0.00627546\n",
      "exploration/Actions Std                                  0.131361\n",
      "exploration/Actions Max                                  0.625298\n",
      "exploration/Actions Min                                 -0.604377\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.26389\n",
      "exploration/env_infos/final/reward_dist Mean             0.00270717\n",
      "exploration/env_infos/final/reward_dist Std              0.00536986\n",
      "exploration/env_infos/final/reward_dist Max              0.0134467\n",
      "exploration/env_infos/final/reward_dist Min              2.87361e-09\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0104025\n",
      "exploration/env_infos/initial/reward_dist Std            0.00848612\n",
      "exploration/env_infos/initial/reward_dist Max            0.0193409\n",
      "exploration/env_infos/initial/reward_dist Min            0.000220008\n",
      "exploration/env_infos/reward_dist Mean                   0.0203045\n",
      "exploration/env_infos/reward_dist Std                    0.0366685\n",
      "exploration/env_infos/reward_dist Max                    0.145983\n",
      "exploration/env_infos/reward_dist Min                    2.98003e-20\n",
      "exploration/env_infos/final/reward_energy Mean          -0.167357\n",
      "exploration/env_infos/final/reward_energy Std            0.12074\n",
      "exploration/env_infos/final/reward_energy Max           -0.0720853\n",
      "exploration/env_infos/final/reward_energy Min           -0.405662\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.218724\n",
      "exploration/env_infos/initial/reward_energy Std          0.211335\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0657541\n",
      "exploration/env_infos/initial/reward_energy Min         -0.632753\n",
      "exploration/env_infos/reward_energy Mean                -0.129993\n",
      "exploration/env_infos/reward_energy Std                  0.133011\n",
      "exploration/env_infos/reward_energy Max                 -0.0104496\n",
      "exploration/env_infos/reward_energy Min                 -0.632753\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0902952\n",
      "exploration/env_infos/final/end_effector_loc Std         0.164883\n",
      "exploration/env_infos/final/end_effector_loc Max         0.314388\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.329369\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000787871\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0107241\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0312649\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00896834\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0330355\n",
      "exploration/env_infos/end_effector_loc Std               0.169327\n",
      "exploration/env_infos/end_effector_loc Max               0.600037\n",
      "exploration/env_infos/end_effector_loc Min              -0.329369\n",
      "evaluation/num steps total                           32000\n",
      "evaluation/num paths total                            1600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0643655\n",
      "evaluation/Rewards Std                                   0.0906276\n",
      "evaluation/Rewards Max                                   0.0998147\n",
      "evaluation/Rewards Min                                  -0.876912\n",
      "evaluation/Returns Mean                                 -1.28731\n",
      "evaluation/Returns Std                                   1.24393\n",
      "evaluation/Returns Max                                   0.897076\n",
      "evaluation/Returns Min                                  -5.00116\n",
      "evaluation/Actions Mean                                  0.00134581\n",
      "evaluation/Actions Std                                   0.11088\n",
      "evaluation/Actions Max                                   0.941584\n",
      "evaluation/Actions Min                                  -0.648562\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.28731\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0933417\n",
      "evaluation/env_infos/final/reward_dist Std               0.213681\n",
      "evaluation/env_infos/final/reward_dist Max               0.92667\n",
      "evaluation/env_infos/final/reward_dist Min               1.02363e-142\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.009574\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0188034\n",
      "evaluation/env_infos/initial/reward_dist Max             0.113589\n",
      "evaluation/env_infos/initial/reward_dist Min             1.01181e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.113312\n",
      "evaluation/env_infos/reward_dist Std                     0.212472\n",
      "evaluation/env_infos/reward_dist Max                     0.97379\n",
      "evaluation/env_infos/reward_dist Min                     1.02363e-142\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0794607\n",
      "evaluation/env_infos/final/reward_energy Std             0.119395\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00418249\n",
      "evaluation/env_infos/final/reward_energy Min            -0.581941\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.306777\n",
      "evaluation/env_infos/initial/reward_energy Std           0.303537\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0107812\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.28959\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0881696\n",
      "evaluation/env_infos/reward_energy Std                   0.129686\n",
      "evaluation/env_infos/reward_energy Max                  -0.00212604\n",
      "evaluation/env_infos/reward_energy Min                  -1.28959\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0184784\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.453763\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000867425\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0152334\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0459906\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0324281\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0138715\n",
      "evaluation/env_infos/end_effector_loc Std                0.288582\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.171646\n",
      "time/evaluation sampling (s)                             0.954842\n",
      "time/exploration sampling (s)                            0.117477\n",
      "time/logging (s)                                         0.0200203\n",
      "time/saving (s)                                          0.0260873\n",
      "time/training (s)                                       46.3629\n",
      "time/epoch (s)                                          47.6529\n",
      "time/total (s)                                        1454.39\n",
      "Epoch                                                   31\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:37:32.771727 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 32 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00306434\n",
      "trainer/QF2 Loss                                         0.00172705\n",
      "trainer/Policy Loss                                      3.04516\n",
      "trainer/Q1 Predictions Mean                             -1.103\n",
      "trainer/Q1 Predictions Std                               0.775839\n",
      "trainer/Q1 Predictions Max                               0.804353\n",
      "trainer/Q1 Predictions Min                              -3.77792\n",
      "trainer/Q2 Predictions Mean                             -1.09301\n",
      "trainer/Q2 Predictions Std                               0.773286\n",
      "trainer/Q2 Predictions Max                               0.835352\n",
      "trainer/Q2 Predictions Min                              -3.68291\n",
      "trainer/Q Targets Mean                                  -1.0943\n",
      "trainer/Q Targets Std                                    0.781981\n",
      "trainer/Q Targets Max                                    0.805278\n",
      "trainer/Q Targets Min                                   -3.62343\n",
      "trainer/Log Pis Mean                                     1.96577\n",
      "trainer/Log Pis Std                                      1.30321\n",
      "trainer/Log Pis Max                                      4.25257\n",
      "trainer/Log Pis Min                                     -2.54474\n",
      "trainer/Policy mu Mean                                  -0.0274985\n",
      "trainer/Policy mu Std                                    0.460923\n",
      "trainer/Policy mu Max                                    2.2592\n",
      "trainer/Policy mu Min                                   -2.37595\n",
      "trainer/Policy log std Mean                             -2.24173\n",
      "trainer/Policy log std Std                               0.651464\n",
      "trainer/Policy log std Max                              -0.0260184\n",
      "trainer/Policy log std Min                              -3.36914\n",
      "trainer/Alpha                                            0.0178245\n",
      "trainer/Alpha Loss                                      -0.137849\n",
      "exploration/num steps total                           4300\n",
      "exploration/num paths total                            215\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.190264\n",
      "exploration/Rewards Std                                  0.215962\n",
      "exploration/Rewards Max                                  0.0216831\n",
      "exploration/Rewards Min                                 -1.03859\n",
      "exploration/Returns Mean                                -3.80527\n",
      "exploration/Returns Std                                  3.29456\n",
      "exploration/Returns Max                                 -1.34936\n",
      "exploration/Returns Min                                -10.2539\n",
      "exploration/Actions Mean                                 0.015009\n",
      "exploration/Actions Std                                  0.162002\n",
      "exploration/Actions Max                                  0.7513\n",
      "exploration/Actions Min                                 -0.692279\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.80527\n",
      "exploration/env_infos/final/reward_dist Mean             0.296974\n",
      "exploration/env_infos/final/reward_dist Std              0.349693\n",
      "exploration/env_infos/final/reward_dist Max              0.795497\n",
      "exploration/env_infos/final/reward_dist Min              8.55325e-127\n",
      "exploration/env_infos/initial/reward_dist Mean           3.46794e-05\n",
      "exploration/env_infos/initial/reward_dist Std            2.84714e-05\n",
      "exploration/env_infos/initial/reward_dist Max            8.57286e-05\n",
      "exploration/env_infos/initial/reward_dist Min            9.03115e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.116667\n",
      "exploration/env_infos/reward_dist Std                    0.273158\n",
      "exploration/env_infos/reward_dist Max                    0.979284\n",
      "exploration/env_infos/reward_dist Min                    8.55325e-127\n",
      "exploration/env_infos/final/reward_energy Mean          -0.121867\n",
      "exploration/env_infos/final/reward_energy Std            0.130763\n",
      "exploration/env_infos/final/reward_energy Max           -0.00344862\n",
      "exploration/env_infos/final/reward_energy Min           -0.373325\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.432403\n",
      "exploration/env_infos/initial/reward_energy Std          0.327833\n",
      "exploration/env_infos/initial/reward_energy Max         -0.020694\n",
      "exploration/env_infos/initial/reward_energy Min         -0.863842\n",
      "exploration/env_infos/reward_energy Mean                -0.16377\n",
      "exploration/env_infos/reward_energy Std                  0.161615\n",
      "exploration/env_infos/reward_energy Max                 -0.00344862\n",
      "exploration/env_infos/reward_energy Min                 -0.863842\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.222749\n",
      "exploration/env_infos/final/end_effector_loc Std         0.373452\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.200083\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000414002\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0191804\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0258347\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0346139\n",
      "exploration/env_infos/end_effector_loc Mean              0.0885771\n",
      "exploration/env_infos/end_effector_loc Std               0.235933\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -0.20081\n",
      "evaluation/num steps total                           33000\n",
      "evaluation/num paths total                            1650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.104709\n",
      "evaluation/Rewards Std                                   0.1459\n",
      "evaluation/Rewards Max                                   0.149993\n",
      "evaluation/Rewards Min                                  -1.09343\n",
      "evaluation/Returns Mean                                 -2.09418\n",
      "evaluation/Returns Std                                   2.29415\n",
      "evaluation/Returns Max                                   1.4044\n",
      "evaluation/Returns Min                                 -10.1239\n",
      "evaluation/Actions Mean                                  0.0161876\n",
      "evaluation/Actions Std                                   0.140588\n",
      "evaluation/Actions Max                                   0.940415\n",
      "evaluation/Actions Min                                  -0.936428\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.09418\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0851644\n",
      "evaluation/env_infos/final/reward_dist Std               0.216204\n",
      "evaluation/env_infos/final/reward_dist Max               0.879395\n",
      "evaluation/env_infos/final/reward_dist Min               5.5802e-152\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00457387\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0102883\n",
      "evaluation/env_infos/initial/reward_dist Max             0.047694\n",
      "evaluation/env_infos/initial/reward_dist Min             2.477e-08\n",
      "evaluation/env_infos/reward_dist Mean                    0.0886745\n",
      "evaluation/env_infos/reward_dist Std                     0.192804\n",
      "evaluation/env_infos/reward_dist Max                     0.960147\n",
      "evaluation/env_infos/reward_dist Min                     5.5802e-152\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.111607\n",
      "evaluation/env_infos/final/reward_energy Std             0.180047\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0125948\n",
      "evaluation/env_infos/final/reward_energy Min            -1.07719\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.379667\n",
      "evaluation/env_infos/initial/reward_energy Std           0.385361\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.015327\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.233\n",
      "evaluation/env_infos/reward_energy Mean                 -0.115471\n",
      "evaluation/env_infos/reward_energy Std                   0.163464\n",
      "evaluation/env_infos/reward_energy Max                  -0.00214073\n",
      "evaluation/env_infos/reward_energy Min                  -1.31384\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0985927\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.440067\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00093304\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0191035\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0448286\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0468214\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0504734\n",
      "evaluation/env_infos/end_effector_loc Std                0.301426\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.175477\n",
      "time/evaluation sampling (s)                             0.869179\n",
      "time/exploration sampling (s)                            0.116319\n",
      "time/logging (s)                                         0.0187862\n",
      "time/saving (s)                                          0.0268721\n",
      "time/training (s)                                       46.8502\n",
      "time/epoch (s)                                          48.0569\n",
      "time/total (s)                                        1502.85\n",
      "Epoch                                                   32\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:38:21.573339 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 33 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00199013\r\n",
      "trainer/QF2 Loss                                         0.00234316\r\n",
      "trainer/Policy Loss                                      3.095\r\n",
      "trainer/Q1 Predictions Mean                             -1.11492\r\n",
      "trainer/Q1 Predictions Std                               0.759445\r\n",
      "trainer/Q1 Predictions Max                               0.902226\r\n",
      "trainer/Q1 Predictions Min                              -3.37342\r\n",
      "trainer/Q2 Predictions Mean                             -1.12034\r\n",
      "trainer/Q2 Predictions Std                               0.756146\r\n",
      "trainer/Q2 Predictions Max                               0.897006\r\n",
      "trainer/Q2 Predictions Min                              -3.37743\r\n",
      "trainer/Q Targets Mean                                  -1.11477\r\n",
      "trainer/Q Targets Std                                    0.754814\r\n",
      "trainer/Q Targets Max                                    0.847135\r\n",
      "trainer/Q Targets Min                                   -3.49546\r\n",
      "trainer/Log Pis Mean                                     1.99687\r\n",
      "trainer/Log Pis Std                                      1.33061\r\n",
      "trainer/Log Pis Max                                      6.37205\r\n",
      "trainer/Log Pis Min                                     -2.06286\r\n",
      "trainer/Policy mu Mean                                  -0.042236\r\n",
      "trainer/Policy mu Std                                    0.505375\r\n",
      "trainer/Policy mu Max                                    2.27829\r\n",
      "trainer/Policy mu Min                                   -2.86697\r\n",
      "trainer/Policy log std Mean                             -2.19374\r\n",
      "trainer/Policy log std Std                               0.713219\r\n",
      "trainer/Policy log std Max                              -0.147931\r\n",
      "trainer/Policy log std Min                              -3.3637\r\n",
      "trainer/Alpha                                            0.0188133\r\n",
      "trainer/Alpha Loss                                      -0.0124437\r\n",
      "exploration/num steps total                           4400\r\n",
      "exploration/num paths total                            220\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0643021\r\n",
      "exploration/Rewards Std                                  0.0821261\r\n",
      "exploration/Rewards Max                                  0.060561\r\n",
      "exploration/Rewards Min                                 -0.56598\r\n",
      "exploration/Returns Mean                                -1.28604\r\n",
      "exploration/Returns Std                                  0.944931\r\n",
      "exploration/Returns Max                                 -0.257234\r\n",
      "exploration/Returns Min                                 -2.95621\r\n",
      "exploration/Actions Mean                                -0.0145947\r\n",
      "exploration/Actions Std                                  0.147261\r\n",
      "exploration/Actions Max                                  0.531737\r\n",
      "exploration/Actions Min                                 -0.740858\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.28604\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0467705\r\n",
      "exploration/env_infos/final/reward_dist Std              0.068886\r\n",
      "exploration/env_infos/final/reward_dist Max              0.178781\r\n",
      "exploration/env_infos/final/reward_dist Min              3.97192e-08\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.011923\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.012616\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.030152\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.41351e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.110067\r\n",
      "exploration/env_infos/reward_dist Std                    0.192174\r\n",
      "exploration/env_infos/reward_dist Max                    0.961527\r\n",
      "exploration/env_infos/reward_dist Min                    3.97192e-08\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.266358\r\n",
      "exploration/env_infos/final/reward_energy Std            0.219221\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0721263\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.685613\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.158449\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.118621\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0544831\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.38366\r\n",
      "exploration/env_infos/reward_energy Mean                -0.135531\r\n",
      "exploration/env_infos/reward_energy Std                  0.159463\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0133204\r\n",
      "exploration/env_infos/reward_energy Min                 -0.799\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.109907\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.16212\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.140429\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.316753\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00118165\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00689747\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.018615\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00814532\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0251782\r\n",
      "exploration/env_infos/end_effector_loc Std               0.113458\r\n",
      "exploration/env_infos/end_effector_loc Max               0.307444\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.316753\r\n",
      "evaluation/num steps total                           34000\r\n",
      "evaluation/num paths total                            1700\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.103613\r\n",
      "evaluation/Rewards Std                                   0.113402\r\n",
      "evaluation/Rewards Max                                   0.0818552\r\n",
      "evaluation/Rewards Min                                  -1.28185\r\n",
      "evaluation/Returns Mean                                 -2.07227\r\n",
      "evaluation/Returns Std                                   1.60252\r\n",
      "evaluation/Returns Max                                   0.0579606\r\n",
      "evaluation/Returns Min                                  -7.3884\r\n",
      "evaluation/Actions Mean                                 -0.0138977\r\n",
      "evaluation/Actions Std                                   0.134813\r\n",
      "evaluation/Actions Max                                   0.930222\r\n",
      "evaluation/Actions Min                                  -0.999469\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.07227\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.047562\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.134121\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.799367\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.25212e-154\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00634811\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0128882\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0754551\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.01967e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0869932\r\n",
      "evaluation/env_infos/reward_dist Std                     0.181191\r\n",
      "evaluation/env_infos/reward_dist Max                     0.987173\r\n",
      "evaluation/env_infos/reward_dist Min                     6.25212e-154\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.196537\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.372962\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00309954\r\n",
      "evaluation/env_infos/final/reward_energy Min            -1.41314\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.222513\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.263065\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0125574\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.2243\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0937293\r\n",
      "evaluation/env_infos/reward_energy Std                   0.167182\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000852826\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.41314\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0499597\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.395073\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00073548\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121595\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0465111\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0397991\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00825486\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.229679\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.168317\r\n",
      "time/evaluation sampling (s)                             0.909871\r\n",
      "time/exploration sampling (s)                            0.116373\r\n",
      "time/logging (s)                                         0.0192055\r\n",
      "time/saving (s)                                          0.031408\r\n",
      "time/training (s)                                       47.1265\r\n",
      "time/epoch (s)                                          48.3716\r\n",
      "time/total (s)                                        1551.65\r\n",
      "Epoch                                                   33\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:39:09.369105 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 34 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00152526\r\n",
      "trainer/QF2 Loss                                         0.00130736\r\n",
      "trainer/Policy Loss                                      3.06238\r\n",
      "trainer/Q1 Predictions Mean                             -1.04831\r\n",
      "trainer/Q1 Predictions Std                               0.762005\r\n",
      "trainer/Q1 Predictions Max                               0.707285\r\n",
      "trainer/Q1 Predictions Min                              -3.76325\r\n",
      "trainer/Q2 Predictions Mean                             -1.05025\r\n",
      "trainer/Q2 Predictions Std                               0.765514\r\n",
      "trainer/Q2 Predictions Max                               0.762165\r\n",
      "trainer/Q2 Predictions Min                              -3.7141\r\n",
      "trainer/Q Targets Mean                                  -1.0448\r\n",
      "trainer/Q Targets Std                                    0.761276\r\n",
      "trainer/Q Targets Max                                    0.729385\r\n",
      "trainer/Q Targets Min                                   -3.77293\r\n",
      "trainer/Log Pis Mean                                     2.02973\r\n",
      "trainer/Log Pis Std                                      1.2757\r\n",
      "trainer/Log Pis Max                                      5.62321\r\n",
      "trainer/Log Pis Min                                     -2.33532\r\n",
      "trainer/Policy mu Mean                                  -0.00310915\r\n",
      "trainer/Policy mu Std                                    0.512606\r\n",
      "trainer/Policy mu Max                                    2.69342\r\n",
      "trainer/Policy mu Min                                   -2.44107\r\n",
      "trainer/Policy log std Mean                             -2.23537\r\n",
      "trainer/Policy log std Std                               0.688476\r\n",
      "trainer/Policy log std Max                               0.155466\r\n",
      "trainer/Policy log std Min                              -3.39869\r\n",
      "trainer/Alpha                                            0.0198127\r\n",
      "trainer/Alpha Loss                                       0.116574\r\n",
      "exploration/num steps total                           4500\r\n",
      "exploration/num paths total                            225\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.219093\r\n",
      "exploration/Rewards Std                                  0.233873\r\n",
      "exploration/Rewards Max                                  0.0370405\r\n",
      "exploration/Rewards Min                                 -0.851542\r\n",
      "exploration/Returns Mean                                -4.38185\r\n",
      "exploration/Returns Std                                  3.87678\r\n",
      "exploration/Returns Max                                 -0.318212\r\n",
      "exploration/Returns Min                                -11.6428\r\n",
      "exploration/Actions Mean                                -0.0873321\r\n",
      "exploration/Actions Std                                  0.303815\r\n",
      "exploration/Actions Max                                  0.655903\r\n",
      "exploration/Actions Min                                 -0.998157\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -4.38185\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0528131\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0656369\r\n",
      "exploration/env_infos/final/reward_dist Max              0.149667\r\n",
      "exploration/env_infos/final/reward_dist Min              6.76783e-171\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0104923\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0131131\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0348518\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000235916\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0299705\r\n",
      "exploration/env_infos/reward_dist Std                    0.0785744\r\n",
      "exploration/env_infos/reward_dist Max                    0.521967\r\n",
      "exploration/env_infos/reward_dist Min                    6.76783e-171\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.3079\r\n",
      "exploration/env_infos/final/reward_energy Std            0.347625\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0387809\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.954324\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.32131\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.290629\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.072613\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.704553\r\n",
      "exploration/env_infos/reward_energy Mean                -0.272772\r\n",
      "exploration/env_infos/reward_energy Std                  0.354198\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00418161\r\n",
      "exploration/env_infos/reward_energy Min                 -1.34456\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0938672\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.551105\r\n",
      "exploration/env_infos/final/end_effector_loc Max         1\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00578133\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0141848\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00823747\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.034251\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.106231\r\n",
      "exploration/env_infos/end_effector_loc Std               0.373786\r\n",
      "exploration/env_infos/end_effector_loc Max               1\r\n",
      "exploration/env_infos/end_effector_loc Min              -1\r\n",
      "evaluation/num steps total                           35000\r\n",
      "evaluation/num paths total                            1750\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0840263\r\n",
      "evaluation/Rewards Std                                   0.111381\r\n",
      "evaluation/Rewards Max                                   0.125286\r\n",
      "evaluation/Rewards Min                                  -0.970032\r\n",
      "evaluation/Returns Mean                                 -1.68053\r\n",
      "evaluation/Returns Std                                   1.67393\r\n",
      "evaluation/Returns Max                                   1.06071\r\n",
      "evaluation/Returns Min                                  -9.38583\r\n",
      "evaluation/Actions Mean                                  0.0174299\r\n",
      "evaluation/Actions Std                                   0.100462\r\n",
      "evaluation/Actions Max                                   0.977519\r\n",
      "evaluation/Actions Min                                  -0.888088\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.68053\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.144037\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.243482\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.825908\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.77706e-109\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00588087\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00913213\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0394028\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.44801e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.134437\r\n",
      "evaluation/env_infos/reward_dist Std                     0.229288\r\n",
      "evaluation/env_infos/reward_dist Max                     0.992063\r\n",
      "evaluation/env_infos/reward_dist Min                     1.77706e-109\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0654332\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.085875\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0114704\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.51948\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.248385\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.308368\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00478408\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.3207\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0859814\r\n",
      "evaluation/env_infos/reward_energy Std                   0.115758\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000639106\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.3207\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.136942\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.350426\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.493133\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00134937\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0139342\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.048876\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0444044\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0510558\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.218802\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.493133\r\n",
      "time/data storing (s)                                    0.176608\r\n",
      "time/evaluation sampling (s)                             0.915819\r\n",
      "time/exploration sampling (s)                            0.117939\r\n",
      "time/logging (s)                                         0.0197735\r\n",
      "time/saving (s)                                          0.0271186\r\n",
      "time/training (s)                                       46.1002\r\n",
      "time/epoch (s)                                          47.3575\r\n",
      "time/total (s)                                        1599.45\r\n",
      "Epoch                                                   34\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:39:59.187991 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 35 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.012224\n",
      "trainer/QF2 Loss                                         0.00526055\n",
      "trainer/Policy Loss                                      3.22145\n",
      "trainer/Q1 Predictions Mean                             -1.06491\n",
      "trainer/Q1 Predictions Std                               0.795847\n",
      "trainer/Q1 Predictions Max                               0.469484\n",
      "trainer/Q1 Predictions Min                              -3.75406\n",
      "trainer/Q2 Predictions Mean                             -1.07549\n",
      "trainer/Q2 Predictions Std                               0.795532\n",
      "trainer/Q2 Predictions Max                               0.471552\n",
      "trainer/Q2 Predictions Min                              -3.76693\n",
      "trainer/Q Targets Mean                                  -1.07471\n",
      "trainer/Q Targets Std                                    0.792739\n",
      "trainer/Q Targets Max                                    0.446503\n",
      "trainer/Q Targets Min                                   -3.81039\n",
      "trainer/Log Pis Mean                                     2.16276\n",
      "trainer/Log Pis Std                                      1.45645\n",
      "trainer/Log Pis Max                                      9.02371\n",
      "trainer/Log Pis Min                                     -3.51333\n",
      "trainer/Policy mu Mean                                  -0.0286964\n",
      "trainer/Policy mu Std                                    0.501541\n",
      "trainer/Policy mu Max                                    2.91453\n",
      "trainer/Policy mu Min                                   -3.75788\n",
      "trainer/Policy log std Mean                             -2.29174\n",
      "trainer/Policy log std Std                               0.655314\n",
      "trainer/Policy log std Max                              -0.245445\n",
      "trainer/Policy log std Min                              -3.31939\n",
      "trainer/Alpha                                            0.0194953\n",
      "trainer/Alpha Loss                                       0.641411\n",
      "exploration/num steps total                           4600\n",
      "exploration/num paths total                            230\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.172898\n",
      "exploration/Rewards Std                                  0.0834282\n",
      "exploration/Rewards Max                                  0.0251971\n",
      "exploration/Rewards Min                                 -0.510038\n",
      "exploration/Returns Mean                                -3.45796\n",
      "exploration/Returns Std                                  0.968939\n",
      "exploration/Returns Max                                 -1.81652\n",
      "exploration/Returns Min                                 -4.72602\n",
      "exploration/Actions Mean                                 0.0345604\n",
      "exploration/Actions Std                                  0.181824\n",
      "exploration/Actions Max                                  0.877413\n",
      "exploration/Actions Min                                 -0.538117\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.45796\n",
      "exploration/env_infos/final/reward_dist Mean             0.0239943\n",
      "exploration/env_infos/final/reward_dist Std              0.0479886\n",
      "exploration/env_infos/final/reward_dist Max              0.119971\n",
      "exploration/env_infos/final/reward_dist Min              7.69615e-90\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00168545\n",
      "exploration/env_infos/initial/reward_dist Std            0.00266655\n",
      "exploration/env_infos/initial/reward_dist Max            0.00695808\n",
      "exploration/env_infos/initial/reward_dist Min            6.2801e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.00479838\n",
      "exploration/env_infos/reward_dist Std                    0.0171776\n",
      "exploration/env_infos/reward_dist Max                    0.119971\n",
      "exploration/env_infos/reward_dist Min                    7.69615e-90\n",
      "exploration/env_infos/final/reward_energy Mean          -0.257793\n",
      "exploration/env_infos/final/reward_energy Std            0.134001\n",
      "exploration/env_infos/final/reward_energy Max           -0.0610992\n",
      "exploration/env_infos/final/reward_energy Min           -0.474908\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.461184\n",
      "exploration/env_infos/initial/reward_energy Std          0.287078\n",
      "exploration/env_infos/initial/reward_energy Max         -0.103067\n",
      "exploration/env_infos/initial/reward_energy Min         -0.966228\n",
      "exploration/env_infos/reward_energy Mean                -0.203827\n",
      "exploration/env_infos/reward_energy Std                  0.164206\n",
      "exploration/env_infos/reward_energy Max                 -0.0101139\n",
      "exploration/env_infos/reward_energy Min                 -0.966228\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.534999\n",
      "exploration/env_infos/final/end_effector_loc Std         0.45365\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.327393\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0128939\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0142348\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0438706\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00509833\n",
      "exploration/env_infos/end_effector_loc Mean              0.264999\n",
      "exploration/env_infos/end_effector_loc Std               0.302235\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -0.327393\n",
      "evaluation/num steps total                           36000\n",
      "evaluation/num paths total                            1800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.091617\n",
      "evaluation/Rewards Std                                   0.135768\n",
      "evaluation/Rewards Max                                   0.181408\n",
      "evaluation/Rewards Min                                  -0.937864\n",
      "evaluation/Returns Mean                                 -1.83234\n",
      "evaluation/Returns Std                                   2.13161\n",
      "evaluation/Returns Max                                   2.01941\n",
      "evaluation/Returns Min                                  -7.58895\n",
      "evaluation/Actions Mean                                  0.0236808\n",
      "evaluation/Actions Std                                   0.107789\n",
      "evaluation/Actions Max                                   0.998619\n",
      "evaluation/Actions Min                                  -0.728805\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.83234\n",
      "evaluation/env_infos/final/reward_dist Mean              0.123867\n",
      "evaluation/env_infos/final/reward_dist Std               0.236654\n",
      "evaluation/env_infos/final/reward_dist Max               0.895186\n",
      "evaluation/env_infos/final/reward_dist Min               6.23068e-152\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00703986\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0150698\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0761223\n",
      "evaluation/env_infos/initial/reward_dist Min             3.18016e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.110601\n",
      "evaluation/env_infos/reward_dist Std                     0.226911\n",
      "evaluation/env_infos/reward_dist Max                     0.997179\n",
      "evaluation/env_infos/reward_dist Min                     6.23068e-152\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0592565\n",
      "evaluation/env_infos/final/reward_energy Std             0.0607965\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00478021\n",
      "evaluation/env_infos/final/reward_energy Min            -0.271894\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.264624\n",
      "evaluation/env_infos/initial/reward_energy Std           0.317222\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.012482\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.27168\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0867471\n",
      "evaluation/env_infos/reward_energy Std                   0.129743\n",
      "evaluation/env_infos/reward_energy Max                  -0.00126486\n",
      "evaluation/env_infos/reward_energy Min                  -1.32533\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.154049\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.432233\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.985658\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00216841\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0144436\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0499309\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0364402\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0774036\n",
      "evaluation/env_infos/end_effector_loc Std                0.287565\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.985658\n",
      "time/data storing (s)                                    0.173114\n",
      "time/evaluation sampling (s)                             0.870508\n",
      "time/exploration sampling (s)                            0.120219\n",
      "time/logging (s)                                         0.0191981\n",
      "time/saving (s)                                          0.0273851\n",
      "time/training (s)                                       48.1931\n",
      "time/epoch (s)                                          49.4035\n",
      "time/total (s)                                        1649.27\n",
      "Epoch                                                   35\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:40:48.982763 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 36 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00185959\r\n",
      "trainer/QF2 Loss                                         0.00247242\r\n",
      "trainer/Policy Loss                                      3.03411\r\n",
      "trainer/Q1 Predictions Mean                             -1.13624\r\n",
      "trainer/Q1 Predictions Std                               0.880449\r\n",
      "trainer/Q1 Predictions Max                               0.337447\r\n",
      "trainer/Q1 Predictions Min                              -3.7065\r\n",
      "trainer/Q2 Predictions Mean                             -1.12174\r\n",
      "trainer/Q2 Predictions Std                               0.866504\r\n",
      "trainer/Q2 Predictions Max                               0.317543\r\n",
      "trainer/Q2 Predictions Min                              -3.61667\r\n",
      "trainer/Q Targets Mean                                  -1.13847\r\n",
      "trainer/Q Targets Std                                    0.876552\r\n",
      "trainer/Q Targets Max                                    0.304195\r\n",
      "trainer/Q Targets Min                                   -3.53429\r\n",
      "trainer/Log Pis Mean                                     1.92922\r\n",
      "trainer/Log Pis Std                                      1.45057\r\n",
      "trainer/Log Pis Max                                      8.03698\r\n",
      "trainer/Log Pis Min                                     -2.45319\r\n",
      "trainer/Policy mu Mean                                   0.00497629\r\n",
      "trainer/Policy mu Std                                    0.502788\r\n",
      "trainer/Policy mu Max                                    3.3722\r\n",
      "trainer/Policy mu Min                                   -2.41053\r\n",
      "trainer/Policy log std Mean                             -2.17288\r\n",
      "trainer/Policy log std Std                               0.659101\r\n",
      "trainer/Policy log std Max                               0.659706\r\n",
      "trainer/Policy log std Min                              -3.44175\r\n",
      "trainer/Alpha                                            0.0185273\r\n",
      "trainer/Alpha Loss                                      -0.282231\r\n",
      "exploration/num steps total                           4700\r\n",
      "exploration/num paths total                            235\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0791572\r\n",
      "exploration/Rewards Std                                  0.0606856\r\n",
      "exploration/Rewards Max                                  0.069263\r\n",
      "exploration/Rewards Min                                 -0.185009\r\n",
      "exploration/Returns Mean                                -1.58314\r\n",
      "exploration/Returns Std                                  0.804329\r\n",
      "exploration/Returns Max                                 -0.165547\r\n",
      "exploration/Returns Min                                 -2.56465\r\n",
      "exploration/Actions Mean                                -0.000206678\r\n",
      "exploration/Actions Std                                  0.146396\r\n",
      "exploration/Actions Max                                  0.894713\r\n",
      "exploration/Actions Min                                 -0.519448\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.58314\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.106861\r\n",
      "exploration/env_infos/final/reward_dist Std              0.162096\r\n",
      "exploration/env_infos/final/reward_dist Max              0.426828\r\n",
      "exploration/env_infos/final/reward_dist Min              0.000583348\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000531699\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000647717\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00144245\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.83211e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0934007\r\n",
      "exploration/env_infos/reward_dist Std                    0.171734\r\n",
      "exploration/env_infos/reward_dist Max                    0.796689\r\n",
      "exploration/env_infos/reward_dist Min                    1.83211e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.137793\r\n",
      "exploration/env_infos/final/reward_energy Std            0.123526\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.00829831\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.352321\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.274016\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.32767\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0296824\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.913421\r\n",
      "exploration/env_infos/reward_energy Mean                -0.149582\r\n",
      "exploration/env_infos/reward_energy Std                  0.143139\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00777387\r\n",
      "exploration/env_infos/reward_energy Min                 -0.913421\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0343472\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.175583\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.181131\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.425943\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00190286\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0149815\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0447356\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0120412\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0251889\r\n",
      "exploration/env_infos/end_effector_loc Std               0.128952\r\n",
      "exploration/env_infos/end_effector_loc Max               0.247872\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.425943\r\n",
      "evaluation/num steps total                           37000\r\n",
      "evaluation/num paths total                            1850\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0561454\r\n",
      "evaluation/Rewards Std                                   0.0874441\r\n",
      "evaluation/Rewards Max                                   0.130907\r\n",
      "evaluation/Rewards Min                                  -0.750996\r\n",
      "evaluation/Returns Mean                                 -1.12291\r\n",
      "evaluation/Returns Std                                   1.34521\r\n",
      "evaluation/Returns Max                                   0.860222\r\n",
      "evaluation/Returns Min                                  -5.45096\r\n",
      "evaluation/Actions Mean                                  0.00162309\r\n",
      "evaluation/Actions Std                                   0.0685995\r\n",
      "evaluation/Actions Max                                   0.554501\r\n",
      "evaluation/Actions Min                                  -0.88793\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.12291\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.135342\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.219752\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.78164\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.66059e-116\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00389183\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00648097\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0269832\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.52985e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.124267\r\n",
      "evaluation/env_infos/reward_dist Std                     0.2343\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993765\r\n",
      "evaluation/env_infos/reward_dist Min                     4.66059e-116\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0459242\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0245308\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0101394\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.139333\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.154395\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.19965\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00748422\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.04685\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0532311\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0811389\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000894056\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.04685\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0256764\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.288637\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.876399\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00143592\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00880685\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0277251\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0443965\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0190179\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.177315\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.876399\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.177817\r\n",
      "time/evaluation sampling (s)                             1.02778\r\n",
      "time/exploration sampling (s)                            0.122151\r\n",
      "time/logging (s)                                         0.0191235\r\n",
      "time/saving (s)                                          0.0279308\r\n",
      "time/training (s)                                       47.9353\r\n",
      "time/epoch (s)                                          49.3101\r\n",
      "time/total (s)                                        1699.06\r\n",
      "Epoch                                                   36\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:41:37.494813 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 37 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0027127\n",
      "trainer/QF2 Loss                                         0.00153926\n",
      "trainer/Policy Loss                                      2.9233\n",
      "trainer/Q1 Predictions Mean                             -1.06874\n",
      "trainer/Q1 Predictions Std                               0.769515\n",
      "trainer/Q1 Predictions Max                               0.795677\n",
      "trainer/Q1 Predictions Min                              -3.89704\n",
      "trainer/Q2 Predictions Mean                             -1.05885\n",
      "trainer/Q2 Predictions Std                               0.769347\n",
      "trainer/Q2 Predictions Max                               0.801649\n",
      "trainer/Q2 Predictions Min                              -3.75367\n",
      "trainer/Q Targets Mean                                  -1.05298\n",
      "trainer/Q Targets Std                                    0.762283\n",
      "trainer/Q Targets Max                                    0.717686\n",
      "trainer/Q Targets Min                                   -3.67442\n",
      "trainer/Log Pis Mean                                     1.87542\n",
      "trainer/Log Pis Std                                      1.52591\n",
      "trainer/Log Pis Max                                      4.74976\n",
      "trainer/Log Pis Min                                     -4.21599\n",
      "trainer/Policy mu Mean                                  -0.0499001\n",
      "trainer/Policy mu Std                                    0.437022\n",
      "trainer/Policy mu Max                                    2.86875\n",
      "trainer/Policy mu Min                                   -2.09805\n",
      "trainer/Policy log std Mean                             -2.24364\n",
      "trainer/Policy log std Std                               0.661928\n",
      "trainer/Policy log std Max                              -0.160516\n",
      "trainer/Policy log std Min                              -3.5005\n",
      "trainer/Alpha                                            0.0194775\n",
      "trainer/Alpha Loss                                      -0.490631\n",
      "exploration/num steps total                           4800\n",
      "exploration/num paths total                            240\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0871072\n",
      "exploration/Rewards Std                                  0.0424386\n",
      "exploration/Rewards Max                                  0.0279117\n",
      "exploration/Rewards Min                                 -0.18492\n",
      "exploration/Returns Mean                                -1.74214\n",
      "exploration/Returns Std                                  0.655634\n",
      "exploration/Returns Max                                 -0.518477\n",
      "exploration/Returns Min                                 -2.3701\n",
      "exploration/Actions Mean                                 0.00675603\n",
      "exploration/Actions Std                                  0.0859025\n",
      "exploration/Actions Max                                  0.281342\n",
      "exploration/Actions Min                                 -0.318489\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.74214\n",
      "exploration/env_infos/final/reward_dist Mean             0.032049\n",
      "exploration/env_infos/final/reward_dist Std              0.0635395\n",
      "exploration/env_infos/final/reward_dist Max              0.159125\n",
      "exploration/env_infos/final/reward_dist Min              2.40169e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00107734\n",
      "exploration/env_infos/initial/reward_dist Std            0.00118951\n",
      "exploration/env_infos/initial/reward_dist Max            0.00309503\n",
      "exploration/env_infos/initial/reward_dist Min            2.86512e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0569757\n",
      "exploration/env_infos/reward_dist Std                    0.125327\n",
      "exploration/env_infos/reward_dist Max                    0.727784\n",
      "exploration/env_infos/reward_dist Min                    2.40169e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.171706\n",
      "exploration/env_infos/final/reward_energy Std            0.0536343\n",
      "exploration/env_infos/final/reward_energy Max           -0.0883443\n",
      "exploration/env_infos/final/reward_energy Min           -0.235447\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0721349\n",
      "exploration/env_infos/initial/reward_energy Std          0.0260748\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0367764\n",
      "exploration/env_infos/initial/reward_energy Min         -0.117772\n",
      "exploration/env_infos/reward_energy Mean                -0.095383\n",
      "exploration/env_infos/reward_energy Std                  0.075841\n",
      "exploration/env_infos/reward_energy Max                 -0.00440971\n",
      "exploration/env_infos/reward_energy Min                 -0.370999\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0501511\n",
      "exploration/env_infos/final/end_effector_loc Std         0.205469\n",
      "exploration/env_infos/final/end_effector_loc Max         0.350773\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.446422\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00157104\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00221043\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00523739\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00221909\n",
      "exploration/env_infos/end_effector_loc Mean              0.0209245\n",
      "exploration/env_infos/end_effector_loc Std               0.106794\n",
      "exploration/env_infos/end_effector_loc Max               0.350773\n",
      "exploration/env_infos/end_effector_loc Min              -0.446422\n",
      "evaluation/num steps total                           38000\n",
      "evaluation/num paths total                            1900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0613571\n",
      "evaluation/Rewards Std                                   0.0827929\n",
      "evaluation/Rewards Max                                   0.124778\n",
      "evaluation/Rewards Min                                  -0.394263\n",
      "evaluation/Returns Mean                                 -1.22714\n",
      "evaluation/Returns Std                                   1.41904\n",
      "evaluation/Returns Max                                   1.23048\n",
      "evaluation/Returns Min                                  -5.07002\n",
      "evaluation/Actions Mean                                 -0.00204459\n",
      "evaluation/Actions Std                                   0.0759583\n",
      "evaluation/Actions Max                                   0.834058\n",
      "evaluation/Actions Min                                  -0.801408\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.22714\n",
      "evaluation/env_infos/final/reward_dist Mean              0.199201\n",
      "evaluation/env_infos/final/reward_dist Std               0.278512\n",
      "evaluation/env_infos/final/reward_dist Max               0.920496\n",
      "evaluation/env_infos/final/reward_dist Min               3.05485e-74\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00408123\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00590307\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0292355\n",
      "evaluation/env_infos/initial/reward_dist Min             1.23004e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.134177\n",
      "evaluation/env_infos/reward_dist Std                     0.225807\n",
      "evaluation/env_infos/reward_dist Max                     0.98778\n",
      "evaluation/env_infos/reward_dist Min                     3.05485e-74\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0363999\n",
      "evaluation/env_infos/final/reward_energy Std             0.0224503\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00200623\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0945021\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.199456\n",
      "evaluation/env_infos/initial/reward_energy Std           0.250946\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0170466\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.987884\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0605353\n",
      "evaluation/env_infos/reward_energy Std                   0.0887871\n",
      "evaluation/env_infos/reward_energy Max                  -0.000670827\n",
      "evaluation/env_infos/reward_energy Min                  -0.987884\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0465734\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.268552\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000922242\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0112958\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0417029\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0400704\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0255421\n",
      "evaluation/env_infos/end_effector_loc Std                0.174131\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.174124\n",
      "time/evaluation sampling (s)                             0.924334\n",
      "time/exploration sampling (s)                            0.117317\n",
      "time/logging (s)                                         0.0196194\n",
      "time/saving (s)                                          0.0266353\n",
      "time/training (s)                                       46.7751\n",
      "time/epoch (s)                                          48.0371\n",
      "time/total (s)                                        1747.57\n",
      "Epoch                                                   37\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:42:28.802454 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 38 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00186219\n",
      "trainer/QF2 Loss                                         0.00132239\n",
      "trainer/Policy Loss                                      2.91056\n",
      "trainer/Q1 Predictions Mean                             -1.02648\n",
      "trainer/Q1 Predictions Std                               0.800937\n",
      "trainer/Q1 Predictions Max                               1.03474\n",
      "trainer/Q1 Predictions Min                              -4.10094\n",
      "trainer/Q2 Predictions Mean                             -1.03402\n",
      "trainer/Q2 Predictions Std                               0.796724\n",
      "trainer/Q2 Predictions Max                               1.0198\n",
      "trainer/Q2 Predictions Min                              -4.0618\n",
      "trainer/Q Targets Mean                                  -1.03671\n",
      "trainer/Q Targets Std                                    0.799356\n",
      "trainer/Q Targets Max                                    1.00016\n",
      "trainer/Q Targets Min                                   -4.07438\n",
      "trainer/Log Pis Mean                                     1.89457\n",
      "trainer/Log Pis Std                                      1.52162\n",
      "trainer/Log Pis Max                                      6.23577\n",
      "trainer/Log Pis Min                                     -4.77576\n",
      "trainer/Policy mu Mean                                   0.00574977\n",
      "trainer/Policy mu Std                                    0.545947\n",
      "trainer/Policy mu Max                                    2.81369\n",
      "trainer/Policy mu Min                                   -2.39415\n",
      "trainer/Policy log std Mean                             -2.17464\n",
      "trainer/Policy log std Std                               0.69664\n",
      "trainer/Policy log std Max                              -0.0645326\n",
      "trainer/Policy log std Min                              -3.4351\n",
      "trainer/Alpha                                            0.0207587\n",
      "trainer/Alpha Loss                                      -0.408624\n",
      "exploration/num steps total                           4900\n",
      "exploration/num paths total                            245\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.150363\n",
      "exploration/Rewards Std                                  0.124585\n",
      "exploration/Rewards Max                                  0.0308238\n",
      "exploration/Rewards Min                                 -0.728388\n",
      "exploration/Returns Mean                                -3.00726\n",
      "exploration/Returns Std                                  1.96131\n",
      "exploration/Returns Max                                 -1.23012\n",
      "exploration/Returns Min                                 -6.72695\n",
      "exploration/Actions Mean                                 0.00846952\n",
      "exploration/Actions Std                                  0.116296\n",
      "exploration/Actions Max                                  0.443941\n",
      "exploration/Actions Min                                 -0.420674\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.00726\n",
      "exploration/env_infos/final/reward_dist Mean             0.0974959\n",
      "exploration/env_infos/final/reward_dist Std              0.156306\n",
      "exploration/env_infos/final/reward_dist Max              0.403273\n",
      "exploration/env_infos/final/reward_dist Min              9.65352e-100\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00180529\n",
      "exploration/env_infos/initial/reward_dist Std            0.00280603\n",
      "exploration/env_infos/initial/reward_dist Max            0.00724735\n",
      "exploration/env_infos/initial/reward_dist Min            7.73e-07\n",
      "exploration/env_infos/reward_dist Mean                   0.0955133\n",
      "exploration/env_infos/reward_dist Std                    0.23056\n",
      "exploration/env_infos/reward_dist Max                    0.950234\n",
      "exploration/env_infos/reward_dist Min                    9.65352e-100\n",
      "exploration/env_infos/final/reward_energy Mean          -0.117201\n",
      "exploration/env_infos/final/reward_energy Std            0.0530852\n",
      "exploration/env_infos/final/reward_energy Max           -0.0346926\n",
      "exploration/env_infos/final/reward_energy Min           -0.181737\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.169009\n",
      "exploration/env_infos/initial/reward_energy Std          0.103059\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0730378\n",
      "exploration/env_infos/initial/reward_energy Min         -0.344646\n",
      "exploration/env_infos/reward_energy Mean                -0.129244\n",
      "exploration/env_infos/reward_energy Std                  0.102415\n",
      "exploration/env_infos/reward_energy Max                 -0.0101327\n",
      "exploration/env_infos/reward_energy Min                 -0.56327\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0712302\n",
      "exploration/env_infos/final/end_effector_loc Std         0.343882\n",
      "exploration/env_infos/final/end_effector_loc Max         0.744485\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.578359\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00108774\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00691363\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0161617\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00646459\n",
      "exploration/env_infos/end_effector_loc Mean              0.025775\n",
      "exploration/env_infos/end_effector_loc Std               0.175304\n",
      "exploration/env_infos/end_effector_loc Max               0.744485\n",
      "exploration/env_infos/end_effector_loc Min              -0.578359\n",
      "evaluation/num steps total                           39000\n",
      "evaluation/num paths total                            1950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0891901\n",
      "evaluation/Rewards Std                                   0.129754\n",
      "evaluation/Rewards Max                                   0.117488\n",
      "evaluation/Rewards Min                                  -0.96316\n",
      "evaluation/Returns Mean                                 -1.7838\n",
      "evaluation/Returns Std                                   2.02462\n",
      "evaluation/Returns Max                                   1.02061\n",
      "evaluation/Returns Min                                  -9.27667\n",
      "evaluation/Actions Mean                                 -0.000406596\n",
      "evaluation/Actions Std                                   0.104483\n",
      "evaluation/Actions Max                                   0.977645\n",
      "evaluation/Actions Min                                  -0.739753\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.7838\n",
      "evaluation/env_infos/final/reward_dist Mean              0.12638\n",
      "evaluation/env_infos/final/reward_dist Std               0.223949\n",
      "evaluation/env_infos/final/reward_dist Max               0.986555\n",
      "evaluation/env_infos/final/reward_dist Min               3.06093e-110\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00505955\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0135402\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0903536\n",
      "evaluation/env_infos/initial/reward_dist Min             1.04488e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0911003\n",
      "evaluation/env_infos/reward_dist Std                     0.184318\n",
      "evaluation/env_infos/reward_dist Max                     0.986555\n",
      "evaluation/env_infos/reward_dist Min                     3.06093e-110\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0402927\n",
      "evaluation/env_infos/final/reward_energy Std             0.0331205\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00245031\n",
      "evaluation/env_infos/final/reward_energy Min            -0.198384\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.25942\n",
      "evaluation/env_infos/initial/reward_energy Std           0.310466\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0105205\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.18532\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0787423\n",
      "evaluation/env_infos/reward_energy Std                   0.125033\n",
      "evaluation/env_infos/reward_energy Max                  -0.00127523\n",
      "evaluation/env_infos/reward_energy Min                  -1.18532\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0110364\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.355414\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000252758\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0143019\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0488822\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0369877\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0087169\n",
      "evaluation/env_infos/end_effector_loc Std                0.231902\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.184312\n",
      "time/evaluation sampling (s)                             1.00731\n",
      "time/exploration sampling (s)                            0.129712\n",
      "time/logging (s)                                         0.0196193\n",
      "time/saving (s)                                          0.0278515\n",
      "time/training (s)                                       49.3916\n",
      "time/epoch (s)                                          50.7604\n",
      "time/total (s)                                        1798.88\n",
      "Epoch                                                   38\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:43:17.230751 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 39 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0013355\n",
      "trainer/QF2 Loss                                         0.00209619\n",
      "trainer/Policy Loss                                      3.01782\n",
      "trainer/Q1 Predictions Mean                             -1.02182\n",
      "trainer/Q1 Predictions Std                               0.837077\n",
      "trainer/Q1 Predictions Max                               1.00271\n",
      "trainer/Q1 Predictions Min                              -3.18766\n",
      "trainer/Q2 Predictions Mean                             -1.01729\n",
      "trainer/Q2 Predictions Std                               0.825764\n",
      "trainer/Q2 Predictions Max                               1.02243\n",
      "trainer/Q2 Predictions Min                              -3.13383\n",
      "trainer/Q Targets Mean                                  -1.01674\n",
      "trainer/Q Targets Std                                    0.839875\n",
      "trainer/Q Targets Max                                    1.06598\n",
      "trainer/Q Targets Min                                   -3.17693\n",
      "trainer/Log Pis Mean                                     2.00749\n",
      "trainer/Log Pis Std                                      1.38279\n",
      "trainer/Log Pis Max                                      7.04079\n",
      "trainer/Log Pis Min                                     -3.64737\n",
      "trainer/Policy mu Mean                                  -0.00879326\n",
      "trainer/Policy mu Std                                    0.467833\n",
      "trainer/Policy mu Max                                    2.70222\n",
      "trainer/Policy mu Min                                   -2.40951\n",
      "trainer/Policy log std Mean                             -2.25418\n",
      "trainer/Policy log std Std                               0.687529\n",
      "trainer/Policy log std Max                               0.0258477\n",
      "trainer/Policy log std Min                              -3.40955\n",
      "trainer/Alpha                                            0.0200895\n",
      "trainer/Alpha Loss                                       0.0292685\n",
      "exploration/num steps total                           5000\n",
      "exploration/num paths total                            250\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.106933\n",
      "exploration/Rewards Std                                  0.0957668\n",
      "exploration/Rewards Max                                  0.0610605\n",
      "exploration/Rewards Min                                 -0.372837\n",
      "exploration/Returns Mean                                -2.13866\n",
      "exploration/Returns Std                                  1.73039\n",
      "exploration/Returns Max                                 -0.191166\n",
      "exploration/Returns Min                                 -5.1474\n",
      "exploration/Actions Mean                                -0.00254701\n",
      "exploration/Actions Std                                  0.114746\n",
      "exploration/Actions Max                                  0.437014\n",
      "exploration/Actions Min                                 -0.39251\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.13866\n",
      "exploration/env_infos/final/reward_dist Mean             0.064585\n",
      "exploration/env_infos/final/reward_dist Std              0.0768964\n",
      "exploration/env_infos/final/reward_dist Max              0.166595\n",
      "exploration/env_infos/final/reward_dist Min              1.12358e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00741681\n",
      "exploration/env_infos/initial/reward_dist Std            0.00641368\n",
      "exploration/env_infos/initial/reward_dist Max            0.0169488\n",
      "exploration/env_infos/initial/reward_dist Min            2.29418e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.134892\n",
      "exploration/env_infos/reward_dist Std                    0.224326\n",
      "exploration/env_infos/reward_dist Max                    0.870758\n",
      "exploration/env_infos/reward_dist Min                    1.35818e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0829338\n",
      "exploration/env_infos/final/reward_energy Std            0.011694\n",
      "exploration/env_infos/final/reward_energy Max           -0.0617734\n",
      "exploration/env_infos/final/reward_energy Min           -0.0965132\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.140572\n",
      "exploration/env_infos/initial/reward_energy Std          0.101648\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0559852\n",
      "exploration/env_infos/initial/reward_energy Min         -0.324652\n",
      "exploration/env_infos/reward_energy Mean                -0.131351\n",
      "exploration/env_infos/reward_energy Std                  0.0953584\n",
      "exploration/env_infos/reward_energy Max                 -0.0112133\n",
      "exploration/env_infos/reward_energy Min                 -0.43776\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0281411\n",
      "exploration/env_infos/final/end_effector_loc Std         0.165358\n",
      "exploration/env_infos/final/end_effector_loc Max         0.436629\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.145818\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00285646\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0054274\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0160861\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00280213\n",
      "exploration/env_infos/end_effector_loc Mean              0.0330479\n",
      "exploration/env_infos/end_effector_loc Std               0.112433\n",
      "exploration/env_infos/end_effector_loc Max               0.436629\n",
      "exploration/env_infos/end_effector_loc Min              -0.145818\n",
      "evaluation/num steps total                           40000\n",
      "evaluation/num paths total                            2000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0998304\n",
      "evaluation/Rewards Std                                   0.129041\n",
      "evaluation/Rewards Max                                   0.169852\n",
      "evaluation/Rewards Min                                  -0.886774\n",
      "evaluation/Returns Mean                                 -1.99661\n",
      "evaluation/Returns Std                                   1.9724\n",
      "evaluation/Returns Max                                   0.0629126\n",
      "evaluation/Returns Min                                  -8.80794\n",
      "evaluation/Actions Mean                                  0.00193881\n",
      "evaluation/Actions Std                                   0.113305\n",
      "evaluation/Actions Max                                   0.980219\n",
      "evaluation/Actions Min                                  -0.933563\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.99661\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0606272\n",
      "evaluation/env_infos/final/reward_dist Std               0.122865\n",
      "evaluation/env_infos/final/reward_dist Max               0.547452\n",
      "evaluation/env_infos/final/reward_dist Min               4.41609e-143\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00558204\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0121391\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0472133\n",
      "evaluation/env_infos/initial/reward_dist Min             9.86547e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0831082\n",
      "evaluation/env_infos/reward_dist Std                     0.180924\n",
      "evaluation/env_infos/reward_dist Max                     0.989718\n",
      "evaluation/env_infos/reward_dist Min                     4.41609e-143\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0400232\n",
      "evaluation/env_infos/final/reward_energy Std             0.0303064\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00406745\n",
      "evaluation/env_infos/final/reward_energy Min            -0.182338\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.31604\n",
      "evaluation/env_infos/initial/reward_energy Std           0.355306\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0132438\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.30189\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0798146\n",
      "evaluation/env_infos/reward_energy Std                   0.138973\n",
      "evaluation/env_infos/reward_energy Max                  -0.00160123\n",
      "evaluation/env_infos/reward_energy Min                  -1.30189\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0252684\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.447817\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00159823\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0167362\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.049011\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0466781\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0144755\n",
      "evaluation/env_infos/end_effector_loc Std                0.305385\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.173768\n",
      "time/evaluation sampling (s)                             0.900013\n",
      "time/exploration sampling (s)                            0.113261\n",
      "time/logging (s)                                         0.0188849\n",
      "time/saving (s)                                          0.0252127\n",
      "time/training (s)                                       46.7398\n",
      "time/epoch (s)                                          47.971\n",
      "time/total (s)                                        1847.3\n",
      "Epoch                                                   39\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:44:04.567156 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 40 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00168446\r\n",
      "trainer/QF2 Loss                                         0.00197178\r\n",
      "trainer/Policy Loss                                      2.99465\r\n",
      "trainer/Q1 Predictions Mean                             -0.85916\r\n",
      "trainer/Q1 Predictions Std                               0.759054\r\n",
      "trainer/Q1 Predictions Max                               0.475073\r\n",
      "trainer/Q1 Predictions Min                              -3.66608\r\n",
      "trainer/Q2 Predictions Mean                             -0.856626\r\n",
      "trainer/Q2 Predictions Std                               0.762826\r\n",
      "trainer/Q2 Predictions Max                               0.509567\r\n",
      "trainer/Q2 Predictions Min                              -3.60682\r\n",
      "trainer/Q Targets Mean                                  -0.846352\r\n",
      "trainer/Q Targets Std                                    0.762752\r\n",
      "trainer/Q Targets Max                                    0.490628\r\n",
      "trainer/Q Targets Min                                   -3.88192\r\n",
      "trainer/Log Pis Mean                                     2.15443\r\n",
      "trainer/Log Pis Std                                      1.15734\r\n",
      "trainer/Log Pis Max                                      5.42381\r\n",
      "trainer/Log Pis Min                                     -1.73564\r\n",
      "trainer/Policy mu Mean                                  -0.0529567\r\n",
      "trainer/Policy mu Std                                    0.426269\r\n",
      "trainer/Policy mu Max                                    2.4717\r\n",
      "trainer/Policy mu Min                                   -2.61617\r\n",
      "trainer/Policy log std Mean                             -2.27472\r\n",
      "trainer/Policy log std Std                               0.619279\r\n",
      "trainer/Policy log std Max                              -0.157526\r\n",
      "trainer/Policy log std Min                              -3.40085\r\n",
      "trainer/Alpha                                            0.0207928\r\n",
      "trainer/Alpha Loss                                       0.598212\r\n",
      "exploration/num steps total                           5100\r\n",
      "exploration/num paths total                            255\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0933591\r\n",
      "exploration/Rewards Std                                  0.0903314\r\n",
      "exploration/Rewards Max                                  0.0617067\r\n",
      "exploration/Rewards Min                                 -0.373638\r\n",
      "exploration/Returns Mean                                -1.86718\r\n",
      "exploration/Returns Std                                  1.40122\r\n",
      "exploration/Returns Max                                 -0.437086\r\n",
      "exploration/Returns Min                                 -4.40842\r\n",
      "exploration/Actions Mean                                 0.00703565\r\n",
      "exploration/Actions Std                                  0.191245\r\n",
      "exploration/Actions Max                                  0.893821\r\n",
      "exploration/Actions Min                                 -0.860061\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.86718\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0176885\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0175333\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0479975\r\n",
      "exploration/env_infos/final/reward_dist Min              1.99837e-05\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00233495\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00365375\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00945692\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.59058e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0899538\r\n",
      "exploration/env_infos/reward_dist Std                    0.211213\r\n",
      "exploration/env_infos/reward_dist Max                    0.995324\r\n",
      "exploration/env_infos/reward_dist Min                    1.33508e-08\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0825095\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0900013\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0107586\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.260259\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.315744\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.412973\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0342061\r\n",
      "exploration/env_infos/initial/reward_energy Min         -1.13575\r\n",
      "exploration/env_infos/reward_energy Mean                -0.19271\r\n",
      "exploration/env_infos/reward_energy Std                  0.190029\r\n",
      "exploration/env_infos/reward_energy Max                 -0.003094\r\n",
      "exploration/env_infos/reward_energy Min                 -1.13575\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00922404\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.208747\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.384251\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.407719\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000368198\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0183757\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.044691\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0350359\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0172236\r\n",
      "exploration/env_infos/end_effector_loc Std               0.131696\r\n",
      "exploration/env_infos/end_effector_loc Max               0.384251\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.407719\r\n",
      "evaluation/num steps total                           41000\r\n",
      "evaluation/num paths total                            2050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.060998\r\n",
      "evaluation/Rewards Std                                   0.109092\r\n",
      "evaluation/Rewards Max                                   0.133754\r\n",
      "evaluation/Rewards Min                                  -0.729617\r\n",
      "evaluation/Returns Mean                                 -1.21996\r\n",
      "evaluation/Returns Std                                   1.6458\r\n",
      "evaluation/Returns Max                                   1.33648\r\n",
      "evaluation/Returns Min                                  -7.94585\r\n",
      "evaluation/Actions Mean                                  2.13123e-05\r\n",
      "evaluation/Actions Std                                   0.0833446\r\n",
      "evaluation/Actions Max                                   0.750403\r\n",
      "evaluation/Actions Min                                  -0.862313\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.21996\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.140968\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.242512\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.855074\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.20642e-114\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00710241\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0166998\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0889306\r\n",
      "evaluation/env_infos/initial/reward_dist Min             4.37321e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.153829\r\n",
      "evaluation/env_infos/reward_dist Std                     0.243537\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999607\r\n",
      "evaluation/env_infos/reward_dist Min                     9.20642e-114\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0365629\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0260935\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00539069\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.12143\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.254022\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.272263\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0158435\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.05643\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0670117\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0969643\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000413336\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.05643\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0267692\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.318869\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000586031\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.013152\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0375201\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0431156\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0159002\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.204981\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.162037\r\n",
      "time/evaluation sampling (s)                             0.782224\r\n",
      "time/exploration sampling (s)                            0.106306\r\n",
      "time/logging (s)                                         0.0190986\r\n",
      "time/saving (s)                                          0.0282234\r\n",
      "time/training (s)                                       45.8366\r\n",
      "time/epoch (s)                                          46.9344\r\n",
      "time/total (s)                                        1894.64\r\n",
      "Epoch                                                   40\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:44:51.838026 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 41 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00113873\r\n",
      "trainer/QF2 Loss                                         0.000961678\r\n",
      "trainer/Policy Loss                                      3.01136\r\n",
      "trainer/Q1 Predictions Mean                             -0.958535\r\n",
      "trainer/Q1 Predictions Std                               0.801065\r\n",
      "trainer/Q1 Predictions Max                               0.375554\r\n",
      "trainer/Q1 Predictions Min                              -3.40878\r\n",
      "trainer/Q2 Predictions Mean                             -0.939474\r\n",
      "trainer/Q2 Predictions Std                               0.793701\r\n",
      "trainer/Q2 Predictions Max                               0.370022\r\n",
      "trainer/Q2 Predictions Min                              -3.35145\r\n",
      "trainer/Q Targets Mean                                  -0.94602\r\n",
      "trainer/Q Targets Std                                    0.800272\r\n",
      "trainer/Q Targets Max                                    0.370427\r\n",
      "trainer/Q Targets Min                                   -3.29701\r\n",
      "trainer/Log Pis Mean                                     2.07576\r\n",
      "trainer/Log Pis Std                                      1.44671\r\n",
      "trainer/Log Pis Max                                      6.04167\r\n",
      "trainer/Log Pis Min                                     -3.80006\r\n",
      "trainer/Policy mu Mean                                   0.00467152\r\n",
      "trainer/Policy mu Std                                    0.456241\r\n",
      "trainer/Policy mu Max                                    3.07645\r\n",
      "trainer/Policy mu Min                                   -2.93107\r\n",
      "trainer/Policy log std Mean                             -2.31593\r\n",
      "trainer/Policy log std Std                               0.601918\r\n",
      "trainer/Policy log std Max                               0.0579549\r\n",
      "trainer/Policy log std Min                              -3.35166\r\n",
      "trainer/Alpha                                            0.020113\r\n",
      "trainer/Alpha Loss                                       0.295989\r\n",
      "exploration/num steps total                           5200\r\n",
      "exploration/num paths total                            260\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0740818\r\n",
      "exploration/Rewards Std                                  0.0672178\r\n",
      "exploration/Rewards Max                                  0.0339679\r\n",
      "exploration/Rewards Min                                 -0.246533\r\n",
      "exploration/Returns Mean                                -1.48164\r\n",
      "exploration/Returns Std                                  1.07974\r\n",
      "exploration/Returns Max                                 -0.143908\r\n",
      "exploration/Returns Min                                 -3.09674\r\n",
      "exploration/Actions Mean                                 0.00948904\r\n",
      "exploration/Actions Std                                  0.146609\r\n",
      "exploration/Actions Max                                  0.786438\r\n",
      "exploration/Actions Min                                 -0.818288\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.48164\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0732172\r\n",
      "exploration/env_infos/final/reward_dist Std              0.144851\r\n",
      "exploration/env_infos/final/reward_dist Max              0.362916\r\n",
      "exploration/env_infos/final/reward_dist Min              5.77505e-05\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000767994\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000782956\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00182703\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.4178e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.10086\r\n",
      "exploration/env_infos/reward_dist Std                    0.214203\r\n",
      "exploration/env_infos/reward_dist Max                    0.78433\r\n",
      "exploration/env_infos/reward_dist Min                    1.4178e-05\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.127875\r\n",
      "exploration/env_infos/final/reward_energy Std            0.080296\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0637133\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.282186\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.329193\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.41145\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0433383\r\n",
      "exploration/env_infos/initial/reward_energy Min         -1.13494\r\n",
      "exploration/env_infos/reward_energy Mean                -0.159996\r\n",
      "exploration/env_infos/reward_energy Std                  0.132551\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0063851\r\n",
      "exploration/env_infos/reward_energy Min                 -1.13494\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0194949\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.170288\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.395505\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.231476\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00142905\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.018575\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0393219\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0409144\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0207407\r\n",
      "exploration/env_infos/end_effector_loc Std               0.122789\r\n",
      "exploration/env_infos/end_effector_loc Max               0.395505\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.234053\r\n",
      "evaluation/num steps total                           42000\r\n",
      "evaluation/num paths total                            2100\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0761911\r\n",
      "evaluation/Rewards Std                                   0.115335\r\n",
      "evaluation/Rewards Max                                   0.152629\r\n",
      "evaluation/Rewards Min                                  -0.877186\r\n",
      "evaluation/Returns Mean                                 -1.52382\r\n",
      "evaluation/Returns Std                                   1.81235\r\n",
      "evaluation/Returns Max                                   1.54127\r\n",
      "evaluation/Returns Min                                  -8.75766\r\n",
      "evaluation/Actions Mean                                 -0.000159912\r\n",
      "evaluation/Actions Std                                   0.094505\r\n",
      "evaluation/Actions Max                                   0.946022\r\n",
      "evaluation/Actions Min                                  -0.94843\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.52382\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.130454\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.235289\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.975131\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.53433e-98\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00491932\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0086032\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0385642\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.09412e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.118253\r\n",
      "evaluation/env_infos/reward_dist Std                     0.226973\r\n",
      "evaluation/env_infos/reward_dist Max                     0.984066\r\n",
      "evaluation/env_infos/reward_dist Min                     1.53433e-98\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0319252\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0291301\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00273974\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.149664\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.24047\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.309676\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0102092\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.33958\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.067145\r\n",
      "evaluation/env_infos/reward_energy Std                   0.11556\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000479153\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.33958\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0163811\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.307077\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000205248\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0138605\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0473011\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0474215\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.010892\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.209044\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.161971\r\n",
      "time/evaluation sampling (s)                             1.00231\r\n",
      "time/exploration sampling (s)                            0.110838\r\n",
      "time/logging (s)                                         0.0188945\r\n",
      "time/saving (s)                                          0.0274166\r\n",
      "time/training (s)                                       45.4514\r\n",
      "time/epoch (s)                                          46.7728\r\n",
      "time/total (s)                                        1941.91\r\n",
      "Epoch                                                   41\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:45:38.653917 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 42 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00838836\n",
      "trainer/QF2 Loss                                         0.00244127\n",
      "trainer/Policy Loss                                      2.96642\n",
      "trainer/Q1 Predictions Mean                             -0.904311\n",
      "trainer/Q1 Predictions Std                               0.820328\n",
      "trainer/Q1 Predictions Max                               0.600087\n",
      "trainer/Q1 Predictions Min                              -3.00399\n",
      "trainer/Q2 Predictions Mean                             -0.898896\n",
      "trainer/Q2 Predictions Std                               0.807051\n",
      "trainer/Q2 Predictions Max                               0.573696\n",
      "trainer/Q2 Predictions Min                              -2.99825\n",
      "trainer/Q Targets Mean                                  -0.903038\n",
      "trainer/Q Targets Std                                    0.813116\n",
      "trainer/Q Targets Max                                    0.577511\n",
      "trainer/Q Targets Min                                   -2.98916\n",
      "trainer/Log Pis Mean                                     2.06501\n",
      "trainer/Log Pis Std                                      1.32013\n",
      "trainer/Log Pis Max                                      4.62658\n",
      "trainer/Log Pis Min                                     -5.4186\n",
      "trainer/Policy mu Mean                                  -0.0182379\n",
      "trainer/Policy mu Std                                    0.382613\n",
      "trainer/Policy mu Max                                    1.7465\n",
      "trainer/Policy mu Min                                   -2.25148\n",
      "trainer/Policy log std Mean                             -2.29761\n",
      "trainer/Policy log std Std                               0.600134\n",
      "trainer/Policy log std Max                              -0.306346\n",
      "trainer/Policy log std Min                              -3.40071\n",
      "trainer/Alpha                                            0.0196606\n",
      "trainer/Alpha Loss                                       0.255468\n",
      "exploration/num steps total                           5300\n",
      "exploration/num paths total                            265\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.102566\n",
      "exploration/Rewards Std                                  0.10323\n",
      "exploration/Rewards Max                                  0.0155382\n",
      "exploration/Rewards Min                                 -0.52862\n",
      "exploration/Returns Mean                                -2.05133\n",
      "exploration/Returns Std                                  1.08237\n",
      "exploration/Returns Max                                 -1.18685\n",
      "exploration/Returns Min                                 -4.0444\n",
      "exploration/Actions Mean                                 0.00892773\n",
      "exploration/Actions Std                                  0.188707\n",
      "exploration/Actions Max                                  0.671976\n",
      "exploration/Actions Min                                 -0.855002\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.05133\n",
      "exploration/env_infos/final/reward_dist Mean             0.00251785\n",
      "exploration/env_infos/final/reward_dist Std              0.00455671\n",
      "exploration/env_infos/final/reward_dist Max              0.0115992\n",
      "exploration/env_infos/final/reward_dist Min              1.16393e-36\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00171393\n",
      "exploration/env_infos/initial/reward_dist Std            0.0012388\n",
      "exploration/env_infos/initial/reward_dist Max            0.00344658\n",
      "exploration/env_infos/initial/reward_dist Min            1.10673e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.032248\n",
      "exploration/env_infos/reward_dist Std                    0.107347\n",
      "exploration/env_infos/reward_dist Max                    0.564015\n",
      "exploration/env_infos/reward_dist Min                    1.16393e-36\n",
      "exploration/env_infos/final/reward_energy Mean          -0.158529\n",
      "exploration/env_infos/final/reward_energy Std            0.128725\n",
      "exploration/env_infos/final/reward_energy Max           -0.04217\n",
      "exploration/env_infos/final/reward_energy Min           -0.385365\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.329278\n",
      "exploration/env_infos/initial/reward_energy Std          0.255287\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0366736\n",
      "exploration/env_infos/initial/reward_energy Min         -0.723129\n",
      "exploration/env_infos/reward_energy Mean                -0.185579\n",
      "exploration/env_infos/reward_energy Std                  0.192198\n",
      "exploration/env_infos/reward_energy Max                 -0.00419879\n",
      "exploration/env_infos/reward_energy Min                 -0.859151\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0949737\n",
      "exploration/env_infos/final/end_effector_loc Std         0.30026\n",
      "exploration/env_infos/final/end_effector_loc Max         0.4684\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.661818\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00739377\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0127407\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0335988\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0135275\n",
      "exploration/env_infos/end_effector_loc Mean              0.0505131\n",
      "exploration/env_infos/end_effector_loc Std               0.179212\n",
      "exploration/env_infos/end_effector_loc Max               0.4684\n",
      "exploration/env_infos/end_effector_loc Min              -0.661818\n",
      "evaluation/num steps total                           43000\n",
      "evaluation/num paths total                            2150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0782741\n",
      "evaluation/Rewards Std                                   0.110678\n",
      "evaluation/Rewards Max                                   0.161343\n",
      "evaluation/Rewards Min                                  -0.639314\n",
      "evaluation/Returns Mean                                 -1.56548\n",
      "evaluation/Returns Std                                   1.88229\n",
      "evaluation/Returns Max                                   2.06929\n",
      "evaluation/Returns Min                                  -8.99332\n",
      "evaluation/Actions Mean                                 -0.000419616\n",
      "evaluation/Actions Std                                   0.0932629\n",
      "evaluation/Actions Max                                   0.866074\n",
      "evaluation/Actions Min                                  -0.884815\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.56548\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0819025\n",
      "evaluation/env_infos/final/reward_dist Std               0.151766\n",
      "evaluation/env_infos/final/reward_dist Max               0.589183\n",
      "evaluation/env_infos/final/reward_dist Min               2.80732e-131\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00717383\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0110344\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0435647\n",
      "evaluation/env_infos/initial/reward_dist Min             1.02195e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.082134\n",
      "evaluation/env_infos/reward_dist Std                     0.178841\n",
      "evaluation/env_infos/reward_dist Max                     0.945939\n",
      "evaluation/env_infos/reward_dist Min                     2.80732e-131\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0320998\n",
      "evaluation/env_infos/final/reward_energy Std             0.01576\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00596147\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0892429\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.234633\n",
      "evaluation/env_infos/initial/reward_energy Std           0.294581\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0050597\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.07366\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0671591\n",
      "evaluation/env_infos/reward_energy Std                   0.113516\n",
      "evaluation/env_infos/reward_energy Max                  -0.00194487\n",
      "evaluation/env_infos/reward_energy Min                  -1.07366\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0310978\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.321793\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00173758\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0132011\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0433037\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0442408\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0256427\n",
      "evaluation/env_infos/end_effector_loc Std                0.221537\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.15864\n",
      "time/evaluation sampling (s)                             0.801632\n",
      "time/exploration sampling (s)                            0.104679\n",
      "time/logging (s)                                         0.0181944\n",
      "time/saving (s)                                          0.0256543\n",
      "time/training (s)                                       45.2317\n",
      "time/epoch (s)                                          46.3405\n",
      "time/total (s)                                        1988.72\n",
      "Epoch                                                   42\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:46:26.229318 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 43 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00183698\r\n",
      "trainer/QF2 Loss                                         0.00274457\r\n",
      "trainer/Policy Loss                                      2.84613\r\n",
      "trainer/Q1 Predictions Mean                             -0.860339\r\n",
      "trainer/Q1 Predictions Std                               0.806894\r\n",
      "trainer/Q1 Predictions Max                               0.714445\r\n",
      "trainer/Q1 Predictions Min                              -3.41381\r\n",
      "trainer/Q2 Predictions Mean                             -0.871563\r\n",
      "trainer/Q2 Predictions Std                               0.816067\r\n",
      "trainer/Q2 Predictions Max                               0.695972\r\n",
      "trainer/Q2 Predictions Min                              -3.42576\r\n",
      "trainer/Q Targets Mean                                  -0.880103\r\n",
      "trainer/Q Targets Std                                    0.80233\r\n",
      "trainer/Q Targets Max                                    0.707283\r\n",
      "trainer/Q Targets Min                                   -3.42852\r\n",
      "trainer/Log Pis Mean                                     1.98648\r\n",
      "trainer/Log Pis Std                                      1.38559\r\n",
      "trainer/Log Pis Max                                      5.03994\r\n",
      "trainer/Log Pis Min                                     -2.29572\r\n",
      "trainer/Policy mu Mean                                  -0.00698555\r\n",
      "trainer/Policy mu Std                                    0.422879\r\n",
      "trainer/Policy mu Max                                    2.36769\r\n",
      "trainer/Policy mu Min                                   -2.49804\r\n",
      "trainer/Policy log std Mean                             -2.25523\r\n",
      "trainer/Policy log std Std                               0.635566\r\n",
      "trainer/Policy log std Max                              -0.358377\r\n",
      "trainer/Policy log std Min                              -3.43534\r\n",
      "trainer/Alpha                                            0.0190155\r\n",
      "trainer/Alpha Loss                                      -0.0535647\r\n",
      "exploration/num steps total                           5400\r\n",
      "exploration/num paths total                            270\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.109164\r\n",
      "exploration/Rewards Std                                  0.0584921\r\n",
      "exploration/Rewards Max                                  0.0355834\r\n",
      "exploration/Rewards Min                                 -0.333156\r\n",
      "exploration/Returns Mean                                -2.18328\r\n",
      "exploration/Returns Std                                  0.620374\r\n",
      "exploration/Returns Max                                 -1.28853\r\n",
      "exploration/Returns Min                                 -3.10777\r\n",
      "exploration/Actions Mean                                -0.00419696\r\n",
      "exploration/Actions Std                                  0.135514\r\n",
      "exploration/Actions Max                                  0.858537\r\n",
      "exploration/Actions Min                                 -0.817966\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.18328\r\n",
      "exploration/env_infos/final/reward_dist Mean             7.61253e-05\r\n",
      "exploration/env_infos/final/reward_dist Std              0.000139348\r\n",
      "exploration/env_infos/final/reward_dist Max              0.000354117\r\n",
      "exploration/env_infos/final/reward_dist Min              5.36286e-13\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00412507\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00782758\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0197775\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.41297e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0467\r\n",
      "exploration/env_infos/reward_dist Std                    0.125964\r\n",
      "exploration/env_infos/reward_dist Max                    0.60438\r\n",
      "exploration/env_infos/reward_dist Min                    5.36286e-13\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0858314\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0473268\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0327675\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.173482\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.298394\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.443838\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0640619\r\n",
      "exploration/env_infos/initial/reward_energy Min         -1.18581\r\n",
      "exploration/env_infos/reward_energy Mean                -0.117918\r\n",
      "exploration/env_infos/reward_energy Std                  0.15119\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0146645\r\n",
      "exploration/env_infos/reward_energy Min                 -1.18581\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0356145\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.230827\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.327722\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.522768\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00023695\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0189072\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0429268\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0408983\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0133904\r\n",
      "exploration/env_infos/end_effector_loc Std               0.139417\r\n",
      "exploration/env_infos/end_effector_loc Max               0.327722\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.522768\r\n",
      "evaluation/num steps total                           44000\r\n",
      "evaluation/num paths total                            2200\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0611286\r\n",
      "evaluation/Rewards Std                                   0.0856291\r\n",
      "evaluation/Rewards Max                                   0.148414\r\n",
      "evaluation/Rewards Min                                  -0.561241\r\n",
      "evaluation/Returns Mean                                 -1.22257\r\n",
      "evaluation/Returns Std                                   1.32698\r\n",
      "evaluation/Returns Max                                   1.43225\r\n",
      "evaluation/Returns Min                                  -5.73907\r\n",
      "evaluation/Actions Mean                                 -0.00527995\r\n",
      "evaluation/Actions Std                                   0.080631\r\n",
      "evaluation/Actions Max                                   0.571657\r\n",
      "evaluation/Actions Min                                  -0.842311\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.22257\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.102416\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.227492\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.979377\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.21456e-76\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0118972\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0257437\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.169144\r\n",
      "evaluation/env_infos/initial/reward_dist Min             9.04555e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.115204\r\n",
      "evaluation/env_infos/reward_dist Std                     0.221472\r\n",
      "evaluation/env_infos/reward_dist Max                     0.981302\r\n",
      "evaluation/env_infos/reward_dist Min                     2.21456e-76\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0354558\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0444817\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000861835\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.276077\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.194968\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.211174\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0240166\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.01798\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.068158\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0917222\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000861835\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.01798\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0390222\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.31188\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.94493\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000573321\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0101454\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0285829\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0421155\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.016854\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.193028\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.94493\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.164618\r\n",
      "time/evaluation sampling (s)                             0.806962\r\n",
      "time/exploration sampling (s)                            0.103443\r\n",
      "time/logging (s)                                         0.0201997\r\n",
      "time/saving (s)                                          0.0274174\r\n",
      "time/training (s)                                       46.0569\r\n",
      "time/epoch (s)                                          47.1795\r\n",
      "time/total (s)                                        2036.3\r\n",
      "Epoch                                                   43\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:47:13.223756 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 44 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00260276\n",
      "trainer/QF2 Loss                                         0.00128228\n",
      "trainer/Policy Loss                                      2.96969\n",
      "trainer/Q1 Predictions Mean                             -0.837493\n",
      "trainer/Q1 Predictions Std                               0.871463\n",
      "trainer/Q1 Predictions Max                               0.685219\n",
      "trainer/Q1 Predictions Min                              -3.23852\n",
      "trainer/Q2 Predictions Mean                             -0.857114\n",
      "trainer/Q2 Predictions Std                               0.875722\n",
      "trainer/Q2 Predictions Max                               0.738429\n",
      "trainer/Q2 Predictions Min                              -3.26774\n",
      "trainer/Q Targets Mean                                  -0.855322\n",
      "trainer/Q Targets Std                                    0.873833\n",
      "trainer/Q Targets Max                                    0.697564\n",
      "trainer/Q Targets Min                                   -3.30559\n",
      "trainer/Log Pis Mean                                     2.12451\n",
      "trainer/Log Pis Std                                      1.27973\n",
      "trainer/Log Pis Max                                      4.55552\n",
      "trainer/Log Pis Min                                     -2.71452\n",
      "trainer/Policy mu Mean                                   0.0152376\n",
      "trainer/Policy mu Std                                    0.303412\n",
      "trainer/Policy mu Max                                    1.7336\n",
      "trainer/Policy mu Min                                   -2.09571\n",
      "trainer/Policy log std Mean                             -2.33331\n",
      "trainer/Policy log std Std                               0.568998\n",
      "trainer/Policy log std Max                              -0.0887153\n",
      "trainer/Policy log std Min                              -3.25948\n",
      "trainer/Alpha                                            0.0201043\n",
      "trainer/Alpha Loss                                       0.486402\n",
      "exploration/num steps total                           5500\n",
      "exploration/num paths total                            275\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.127213\n",
      "exploration/Rewards Std                                  0.140516\n",
      "exploration/Rewards Max                                  0.139789\n",
      "exploration/Rewards Min                                 -0.423265\n",
      "exploration/Returns Mean                                -2.54426\n",
      "exploration/Returns Std                                  2.31226\n",
      "exploration/Returns Max                                  0.823146\n",
      "exploration/Returns Min                                 -6.08916\n",
      "exploration/Actions Mean                                -0.00869804\n",
      "exploration/Actions Std                                  0.204108\n",
      "exploration/Actions Max                                  0.707036\n",
      "exploration/Actions Min                                 -0.949637\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.54426\n",
      "exploration/env_infos/final/reward_dist Mean             0.291553\n",
      "exploration/env_infos/final/reward_dist Std              0.357125\n",
      "exploration/env_infos/final/reward_dist Max              0.73806\n",
      "exploration/env_infos/final/reward_dist Min              5.42612e-22\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0092032\n",
      "exploration/env_infos/initial/reward_dist Std            0.0149551\n",
      "exploration/env_infos/initial/reward_dist Max            0.038645\n",
      "exploration/env_infos/initial/reward_dist Min            1.25519e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.167374\n",
      "exploration/env_infos/reward_dist Std                    0.283164\n",
      "exploration/env_infos/reward_dist Max                    0.919665\n",
      "exploration/env_infos/reward_dist Min                    5.42612e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.103746\n",
      "exploration/env_infos/final/reward_energy Std            0.0585456\n",
      "exploration/env_infos/final/reward_energy Max           -0.0255818\n",
      "exploration/env_infos/final/reward_energy Min           -0.183165\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.457143\n",
      "exploration/env_infos/initial/reward_energy Std          0.358171\n",
      "exploration/env_infos/initial/reward_energy Max         -0.04034\n",
      "exploration/env_infos/initial/reward_energy Min         -1.09606\n",
      "exploration/env_infos/reward_energy Mean                -0.216797\n",
      "exploration/env_infos/reward_energy Std                  0.190974\n",
      "exploration/env_infos/reward_energy Max                 -0.0226963\n",
      "exploration/env_infos/reward_energy Min                 -1.09606\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.060265\n",
      "exploration/env_infos/final/end_effector_loc Std         0.304515\n",
      "exploration/env_infos/final/end_effector_loc Max         0.29558\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.700854\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0020269\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0204322\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0273651\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0474818\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0156637\n",
      "exploration/env_infos/end_effector_loc Std               0.190535\n",
      "exploration/env_infos/end_effector_loc Max               0.334803\n",
      "exploration/env_infos/end_effector_loc Min              -0.700854\n",
      "evaluation/num steps total                           45000\n",
      "evaluation/num paths total                            2250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0650592\n",
      "evaluation/Rewards Std                                   0.0959971\n",
      "evaluation/Rewards Max                                   0.155602\n",
      "evaluation/Rewards Min                                  -0.65376\n",
      "evaluation/Returns Mean                                 -1.30118\n",
      "evaluation/Returns Std                                   1.54402\n",
      "evaluation/Returns Max                                   1.73431\n",
      "evaluation/Returns Min                                  -4.86493\n",
      "evaluation/Actions Mean                                  8.28685e-05\n",
      "evaluation/Actions Std                                   0.0974818\n",
      "evaluation/Actions Max                                   0.891839\n",
      "evaluation/Actions Min                                  -0.909934\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.30118\n",
      "evaluation/env_infos/final/reward_dist Mean              0.151041\n",
      "evaluation/env_infos/final/reward_dist Std               0.278783\n",
      "evaluation/env_infos/final/reward_dist Max               0.961019\n",
      "evaluation/env_infos/final/reward_dist Min               6.40105e-52\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0102474\n",
      "evaluation/env_infos/initial/reward_dist Std             0.026442\n",
      "evaluation/env_infos/initial/reward_dist Max             0.125387\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0283e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.103333\n",
      "evaluation/env_infos/reward_dist Std                     0.207864\n",
      "evaluation/env_infos/reward_dist Max                     0.999973\n",
      "evaluation/env_infos/reward_dist Min                     6.40105e-52\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0282657\n",
      "evaluation/env_infos/final/reward_energy Std             0.0266427\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00536494\n",
      "evaluation/env_infos/final/reward_energy Min            -0.17032\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.206802\n",
      "evaluation/env_infos/initial/reward_energy Std           0.28821\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00441519\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.22687\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0695048\n",
      "evaluation/env_infos/reward_energy Std                   0.119057\n",
      "evaluation/env_infos/reward_energy Max                  -0.00168058\n",
      "evaluation/env_infos/reward_energy Min                  -1.22687\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0437554\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.247246\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.696744\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.729742\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000997232\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0125018\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.044592\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0454967\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0280501\n",
      "evaluation/env_infos/end_effector_loc Std                0.167146\n",
      "evaluation/env_infos/end_effector_loc Max                0.696744\n",
      "evaluation/env_infos/end_effector_loc Min               -0.729742\n",
      "time/data storing (s)                                    0.154047\n",
      "time/evaluation sampling (s)                             0.835792\n",
      "time/exploration sampling (s)                            0.100301\n",
      "time/logging (s)                                         0.0197984\n",
      "time/saving (s)                                          0.0290297\n",
      "time/training (s)                                       45.2973\n",
      "time/epoch (s)                                          46.4362\n",
      "time/total (s)                                        2083.29\n",
      "Epoch                                                   44\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:48:01.086544 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 45 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00141114\n",
      "trainer/QF2 Loss                                         0.0011243\n",
      "trainer/Policy Loss                                      2.93276\n",
      "trainer/Q1 Predictions Mean                             -0.846669\n",
      "trainer/Q1 Predictions Std                               0.811161\n",
      "trainer/Q1 Predictions Max                               0.856512\n",
      "trainer/Q1 Predictions Min                              -3.09181\n",
      "trainer/Q2 Predictions Mean                             -0.840398\n",
      "trainer/Q2 Predictions Std                               0.819888\n",
      "trainer/Q2 Predictions Max                               0.877309\n",
      "trainer/Q2 Predictions Min                              -3.08327\n",
      "trainer/Q Targets Mean                                  -0.852914\n",
      "trainer/Q Targets Std                                    0.821866\n",
      "trainer/Q Targets Max                                    0.87204\n",
      "trainer/Q Targets Min                                   -3.08826\n",
      "trainer/Log Pis Mean                                     2.08297\n",
      "trainer/Log Pis Std                                      1.49704\n",
      "trainer/Log Pis Max                                      5.56987\n",
      "trainer/Log Pis Min                                     -3.45767\n",
      "trainer/Policy mu Mean                                  -0.00408983\n",
      "trainer/Policy mu Std                                    0.358987\n",
      "trainer/Policy mu Max                                    1.71517\n",
      "trainer/Policy mu Min                                   -2.66401\n",
      "trainer/Policy log std Mean                             -2.34028\n",
      "trainer/Policy log std Std                               0.628944\n",
      "trainer/Policy log std Max                              -0.289951\n",
      "trainer/Policy log std Min                              -3.45229\n",
      "trainer/Alpha                                            0.0189875\n",
      "trainer/Alpha Loss                                       0.329011\n",
      "exploration/num steps total                           5600\n",
      "exploration/num paths total                            280\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0733158\n",
      "exploration/Rewards Std                                  0.0853013\n",
      "exploration/Rewards Max                                  0.11776\n",
      "exploration/Rewards Min                                 -0.245959\n",
      "exploration/Returns Mean                                -1.46632\n",
      "exploration/Returns Std                                  1.40786\n",
      "exploration/Returns Max                                  0.866496\n",
      "exploration/Returns Min                                 -3.43626\n",
      "exploration/Actions Mean                                -0.00162284\n",
      "exploration/Actions Std                                  0.178053\n",
      "exploration/Actions Max                                  0.937972\n",
      "exploration/Actions Min                                 -0.972728\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.46632\n",
      "exploration/env_infos/final/reward_dist Mean             0.223005\n",
      "exploration/env_infos/final/reward_dist Std              0.296262\n",
      "exploration/env_infos/final/reward_dist Max              0.769608\n",
      "exploration/env_infos/final/reward_dist Min              1.10508e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00469151\n",
      "exploration/env_infos/initial/reward_dist Std            0.00375137\n",
      "exploration/env_infos/initial/reward_dist Max            0.00842541\n",
      "exploration/env_infos/initial/reward_dist Min            2.9342e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.139081\n",
      "exploration/env_infos/reward_dist Std                    0.242557\n",
      "exploration/env_infos/reward_dist Max                    0.927993\n",
      "exploration/env_infos/reward_dist Min                    8.43045e-16\n",
      "exploration/env_infos/final/reward_energy Mean          -0.112721\n",
      "exploration/env_infos/final/reward_energy Std            0.073996\n",
      "exploration/env_infos/final/reward_energy Max           -0.0202914\n",
      "exploration/env_infos/final/reward_energy Min           -0.2231\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.387751\n",
      "exploration/env_infos/initial/reward_energy Std          0.493876\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0274743\n",
      "exploration/env_infos/initial/reward_energy Min         -1.35129\n",
      "exploration/env_infos/reward_energy Mean                -0.158049\n",
      "exploration/env_infos/reward_energy Std                  0.19604\n",
      "exploration/env_infos/reward_energy Max                 -0.0054952\n",
      "exploration/env_infos/reward_energy Min                 -1.35129\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.018175\n",
      "exploration/env_infos/final/end_effector_loc Std         0.19698\n",
      "exploration/env_infos/final/end_effector_loc Max         0.274709\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.387198\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00217582\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0220929\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0468986\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0486364\n",
      "exploration/env_infos/end_effector_loc Mean              0.00536719\n",
      "exploration/env_infos/end_effector_loc Std               0.163545\n",
      "exploration/env_infos/end_effector_loc Max               0.274709\n",
      "exploration/env_infos/end_effector_loc Min              -0.491664\n",
      "evaluation/num steps total                           46000\n",
      "evaluation/num paths total                            2300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0762462\n",
      "evaluation/Rewards Std                                   0.0717825\n",
      "evaluation/Rewards Max                                   0.107832\n",
      "evaluation/Rewards Min                                  -0.395426\n",
      "evaluation/Returns Mean                                 -1.52492\n",
      "evaluation/Returns Std                                   1.18586\n",
      "evaluation/Returns Max                                   0.961265\n",
      "evaluation/Returns Min                                  -6.08597\n",
      "evaluation/Actions Mean                                 -0.00232076\n",
      "evaluation/Actions Std                                   0.0885794\n",
      "evaluation/Actions Max                                   0.905081\n",
      "evaluation/Actions Min                                  -0.939155\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.52492\n",
      "evaluation/env_infos/final/reward_dist Mean              0.13757\n",
      "evaluation/env_infos/final/reward_dist Std               0.260667\n",
      "evaluation/env_infos/final/reward_dist Max               0.982627\n",
      "evaluation/env_infos/final/reward_dist Min               3.19328e-62\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0037321\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0067653\n",
      "evaluation/env_infos/initial/reward_dist Max             0.029999\n",
      "evaluation/env_infos/initial/reward_dist Min             7.67978e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0961338\n",
      "evaluation/env_infos/reward_dist Std                     0.196601\n",
      "evaluation/env_infos/reward_dist Max                     0.982627\n",
      "evaluation/env_infos/reward_dist Min                     3.19328e-62\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0251293\n",
      "evaluation/env_infos/final/reward_energy Std             0.021225\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00246521\n",
      "evaluation/env_infos/final/reward_energy Min            -0.139247\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.216395\n",
      "evaluation/env_infos/initial/reward_energy Std           0.25043\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00361998\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.21339\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0612286\n",
      "evaluation/env_infos/reward_energy Std                   0.109336\n",
      "evaluation/env_infos/reward_energy Max                  -0.000259396\n",
      "evaluation/env_infos/reward_energy Min                  -1.21339\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0420155\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.29108\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.885239\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000277319\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116983\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0452541\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0469577\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0213183\n",
      "evaluation/env_infos/end_effector_loc Std                0.188249\n",
      "evaluation/env_infos/end_effector_loc Max                0.885239\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.16394\n",
      "time/evaluation sampling (s)                             0.935201\n",
      "time/exploration sampling (s)                            0.108703\n",
      "time/logging (s)                                         0.0190733\n",
      "time/saving (s)                                          0.0264709\n",
      "time/training (s)                                       46.0274\n",
      "time/epoch (s)                                          47.2808\n",
      "time/total (s)                                        2131.15\n",
      "Epoch                                                   45\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:48:48.675265 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 46 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00100854\n",
      "trainer/QF2 Loss                                         0.000913151\n",
      "trainer/Policy Loss                                      2.95128\n",
      "trainer/Q1 Predictions Mean                             -0.755639\n",
      "trainer/Q1 Predictions Std                               0.792832\n",
      "trainer/Q1 Predictions Max                               0.855622\n",
      "trainer/Q1 Predictions Min                              -3.21199\n",
      "trainer/Q2 Predictions Mean                             -0.749809\n",
      "trainer/Q2 Predictions Std                               0.789968\n",
      "trainer/Q2 Predictions Max                               0.878825\n",
      "trainer/Q2 Predictions Min                              -3.21113\n",
      "trainer/Q Targets Mean                                  -0.747532\n",
      "trainer/Q Targets Std                                    0.788267\n",
      "trainer/Q Targets Max                                    0.83648\n",
      "trainer/Q Targets Min                                   -3.18181\n",
      "trainer/Log Pis Mean                                     2.20336\n",
      "trainer/Log Pis Std                                      1.28913\n",
      "trainer/Log Pis Max                                      4.72806\n",
      "trainer/Log Pis Min                                     -2.50069\n",
      "trainer/Policy mu Mean                                   0.00798187\n",
      "trainer/Policy mu Std                                    0.278789\n",
      "trainer/Policy mu Max                                    1.81659\n",
      "trainer/Policy mu Min                                   -1.85265\n",
      "trainer/Policy log std Mean                             -2.37074\n",
      "trainer/Policy log std Std                               0.623386\n",
      "trainer/Policy log std Max                              -0.165018\n",
      "trainer/Policy log std Min                              -3.45211\n",
      "trainer/Alpha                                            0.0191716\n",
      "trainer/Alpha Loss                                       0.804558\n",
      "exploration/num steps total                           5700\n",
      "exploration/num paths total                            285\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.135258\n",
      "exploration/Rewards Std                                  0.0548242\n",
      "exploration/Rewards Max                                 -0.0592163\n",
      "exploration/Rewards Min                                 -0.414004\n",
      "exploration/Returns Mean                                -2.70516\n",
      "exploration/Returns Std                                  0.52875\n",
      "exploration/Returns Max                                 -2.14525\n",
      "exploration/Returns Min                                 -3.48185\n",
      "exploration/Actions Mean                                 0.00596459\n",
      "exploration/Actions Std                                  0.0763711\n",
      "exploration/Actions Max                                  0.367063\n",
      "exploration/Actions Min                                 -0.180667\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.70516\n",
      "exploration/env_infos/final/reward_dist Mean             0.0974604\n",
      "exploration/env_infos/final/reward_dist Std              0.132209\n",
      "exploration/env_infos/final/reward_dist Max              0.339616\n",
      "exploration/env_infos/final/reward_dist Min              1.92985e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000182311\n",
      "exploration/env_infos/initial/reward_dist Std            0.000330965\n",
      "exploration/env_infos/initial/reward_dist Max            0.000844025\n",
      "exploration/env_infos/initial/reward_dist Min            1.06117e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0933566\n",
      "exploration/env_infos/reward_dist Std                    0.215627\n",
      "exploration/env_infos/reward_dist Max                    0.881244\n",
      "exploration/env_infos/reward_dist Min                    1.92985e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0899139\n",
      "exploration/env_infos/final/reward_energy Std            0.0428177\n",
      "exploration/env_infos/final/reward_energy Max           -0.0341429\n",
      "exploration/env_infos/final/reward_energy Min           -0.146942\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0715108\n",
      "exploration/env_infos/initial/reward_energy Std          0.0426721\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0355888\n",
      "exploration/env_infos/initial/reward_energy Min         -0.149123\n",
      "exploration/env_infos/reward_energy Mean                -0.085869\n",
      "exploration/env_infos/reward_energy Std                  0.0660511\n",
      "exploration/env_infos/reward_energy Max                 -0.00837996\n",
      "exploration/env_infos/reward_energy Min                 -0.459693\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0194579\n",
      "exploration/env_infos/final/end_effector_loc Std         0.239152\n",
      "exploration/env_infos/final/end_effector_loc Max         0.399723\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.350041\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00017819\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00293881\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00425711\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00670489\n",
      "exploration/env_infos/end_effector_loc Mean              0.00202623\n",
      "exploration/env_infos/end_effector_loc Std               0.123223\n",
      "exploration/env_infos/end_effector_loc Max               0.399723\n",
      "exploration/env_infos/end_effector_loc Min              -0.350041\n",
      "evaluation/num steps total                           47000\n",
      "evaluation/num paths total                            2350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.068327\n",
      "evaluation/Rewards Std                                   0.0901229\n",
      "evaluation/Rewards Max                                   0.154241\n",
      "evaluation/Rewards Min                                  -0.769343\n",
      "evaluation/Returns Mean                                 -1.36654\n",
      "evaluation/Returns Std                                   1.42637\n",
      "evaluation/Returns Max                                   1.20014\n",
      "evaluation/Returns Min                                  -6.94603\n",
      "evaluation/Actions Mean                                  0.00588269\n",
      "evaluation/Actions Std                                   0.0870373\n",
      "evaluation/Actions Max                                   0.876888\n",
      "evaluation/Actions Min                                  -0.851204\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.36654\n",
      "evaluation/env_infos/final/reward_dist Mean              0.161559\n",
      "evaluation/env_infos/final/reward_dist Std               0.245097\n",
      "evaluation/env_infos/final/reward_dist Max               0.954269\n",
      "evaluation/env_infos/final/reward_dist Min               3.21795e-111\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00534642\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00893849\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0461161\n",
      "evaluation/env_infos/initial/reward_dist Min             9.97558e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0921629\n",
      "evaluation/env_infos/reward_dist Std                     0.178486\n",
      "evaluation/env_infos/reward_dist Max                     0.985641\n",
      "evaluation/env_infos/reward_dist Min                     3.21795e-111\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.036353\n",
      "evaluation/env_infos/final/reward_energy Std             0.0409696\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00130328\n",
      "evaluation/env_infos/final/reward_energy Min            -0.239088\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.248808\n",
      "evaluation/env_infos/initial/reward_energy Std           0.235151\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00520656\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.951746\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0718301\n",
      "evaluation/env_infos/reward_energy Std                   0.100303\n",
      "evaluation/env_infos/reward_energy Max                  -0.00125089\n",
      "evaluation/env_infos/reward_energy Min                  -0.951746\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0703536\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.287675\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.967471\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.909834\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000263809\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121009\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0438444\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0425602\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0274078\n",
      "evaluation/env_infos/end_effector_loc Std                0.187224\n",
      "evaluation/env_infos/end_effector_loc Max                0.967471\n",
      "evaluation/env_infos/end_effector_loc Min               -0.909834\n",
      "time/data storing (s)                                    0.164658\n",
      "time/evaluation sampling (s)                             0.898472\n",
      "time/exploration sampling (s)                            0.106126\n",
      "time/logging (s)                                         0.0186602\n",
      "time/saving (s)                                          0.0283804\n",
      "time/training (s)                                       45.8115\n",
      "time/epoch (s)                                          47.0278\n",
      "time/total (s)                                        2178.74\n",
      "Epoch                                                   46\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:49:35.966328 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 47 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00114962\n",
      "trainer/QF2 Loss                                         0.00103702\n",
      "trainer/Policy Loss                                      2.90354\n",
      "trainer/Q1 Predictions Mean                             -0.850391\n",
      "trainer/Q1 Predictions Std                               0.836558\n",
      "trainer/Q1 Predictions Max                               0.881701\n",
      "trainer/Q1 Predictions Min                              -3.24204\n",
      "trainer/Q2 Predictions Mean                             -0.851389\n",
      "trainer/Q2 Predictions Std                               0.839042\n",
      "trainer/Q2 Predictions Max                               0.8755\n",
      "trainer/Q2 Predictions Min                              -3.24771\n",
      "trainer/Q Targets Mean                                  -0.860456\n",
      "trainer/Q Targets Std                                    0.835236\n",
      "trainer/Q Targets Max                                    0.865412\n",
      "trainer/Q Targets Min                                   -3.27231\n",
      "trainer/Log Pis Mean                                     2.04984\n",
      "trainer/Log Pis Std                                      1.51116\n",
      "trainer/Log Pis Max                                      5.93638\n",
      "trainer/Log Pis Min                                     -4.25997\n",
      "trainer/Policy mu Mean                                  -0.00365057\n",
      "trainer/Policy mu Std                                    0.401883\n",
      "trainer/Policy mu Max                                    1.83697\n",
      "trainer/Policy mu Min                                   -2.50393\n",
      "trainer/Policy log std Mean                             -2.33614\n",
      "trainer/Policy log std Std                               0.679046\n",
      "trainer/Policy log std Max                              -0.174876\n",
      "trainer/Policy log std Min                              -3.40641\n",
      "trainer/Alpha                                            0.0205462\n",
      "trainer/Alpha Loss                                       0.193639\n",
      "exploration/num steps total                           5800\n",
      "exploration/num paths total                            290\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.133629\n",
      "exploration/Rewards Std                                  0.116023\n",
      "exploration/Rewards Max                                  0.0191634\n",
      "exploration/Rewards Min                                 -0.559278\n",
      "exploration/Returns Mean                                -2.67259\n",
      "exploration/Returns Std                                  1.6766\n",
      "exploration/Returns Max                                 -0.838098\n",
      "exploration/Returns Min                                 -5.6436\n",
      "exploration/Actions Mean                                -0.00375899\n",
      "exploration/Actions Std                                  0.202372\n",
      "exploration/Actions Max                                  0.789883\n",
      "exploration/Actions Min                                 -0.98766\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.67259\n",
      "exploration/env_infos/final/reward_dist Mean             0.183355\n",
      "exploration/env_infos/final/reward_dist Std              0.342617\n",
      "exploration/env_infos/final/reward_dist Max              0.868064\n",
      "exploration/env_infos/final/reward_dist Min              2.79074e-77\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00272778\n",
      "exploration/env_infos/initial/reward_dist Std            0.00352035\n",
      "exploration/env_infos/initial/reward_dist Max            0.00931559\n",
      "exploration/env_infos/initial/reward_dist Min            6.90148e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.110493\n",
      "exploration/env_infos/reward_dist Std                    0.197279\n",
      "exploration/env_infos/reward_dist Max                    0.868064\n",
      "exploration/env_infos/reward_dist Min                    2.79074e-77\n",
      "exploration/env_infos/final/reward_energy Mean          -0.167033\n",
      "exploration/env_infos/final/reward_energy Std            0.100563\n",
      "exploration/env_infos/final/reward_energy Max           -0.0484415\n",
      "exploration/env_infos/final/reward_energy Min           -0.319539\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.41016\n",
      "exploration/env_infos/initial/reward_energy Std          0.474297\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0253742\n",
      "exploration/env_infos/initial/reward_energy Min         -1.26467\n",
      "exploration/env_infos/reward_energy Mean                -0.198084\n",
      "exploration/env_infos/reward_energy Std                  0.206641\n",
      "exploration/env_infos/reward_energy Max                 -0.00727872\n",
      "exploration/env_infos/reward_energy Min                 -1.26467\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0521126\n",
      "exploration/env_infos/final/end_effector_loc Std         0.440087\n",
      "exploration/env_infos/final/end_effector_loc Max         0.860281\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00419073\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0217698\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0394942\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.049383\n",
      "exploration/env_infos/end_effector_loc Mean             -0.062118\n",
      "exploration/env_infos/end_effector_loc Std               0.233569\n",
      "exploration/env_infos/end_effector_loc Max               0.860281\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           48000\n",
      "evaluation/num paths total                            2400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0755083\n",
      "evaluation/Rewards Std                                   0.0884623\n",
      "evaluation/Rewards Max                                   0.145081\n",
      "evaluation/Rewards Min                                  -0.619082\n",
      "evaluation/Returns Mean                                 -1.51017\n",
      "evaluation/Returns Std                                   1.39092\n",
      "evaluation/Returns Max                                   0.798347\n",
      "evaluation/Returns Min                                  -6.40411\n",
      "evaluation/Actions Mean                                  0.000433121\n",
      "evaluation/Actions Std                                   0.0869338\n",
      "evaluation/Actions Max                                   0.700265\n",
      "evaluation/Actions Min                                  -0.896165\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.51017\n",
      "evaluation/env_infos/final/reward_dist Mean              0.128536\n",
      "evaluation/env_infos/final/reward_dist Std               0.234512\n",
      "evaluation/env_infos/final/reward_dist Max               0.968605\n",
      "evaluation/env_infos/final/reward_dist Min               3.66772e-60\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00629087\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0215955\n",
      "evaluation/env_infos/initial/reward_dist Max             0.153015\n",
      "evaluation/env_infos/initial/reward_dist Min             6.67973e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.119883\n",
      "evaluation/env_infos/reward_dist Std                     0.22984\n",
      "evaluation/env_infos/reward_dist Max                     0.996842\n",
      "evaluation/env_infos/reward_dist Min                     3.66772e-60\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.034104\n",
      "evaluation/env_infos/final/reward_energy Std             0.0342246\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00233388\n",
      "evaluation/env_infos/final/reward_energy Min            -0.139762\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.292451\n",
      "evaluation/env_infos/initial/reward_energy Std           0.2827\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00402587\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.999829\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0672401\n",
      "evaluation/env_infos/reward_energy Std                   0.102928\n",
      "evaluation/env_infos/reward_energy Max                  -0.00144061\n",
      "evaluation/env_infos/reward_energy Min                  -0.999829\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00572318\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.24958\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.543854\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00324867\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0140091\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0350133\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0448083\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0106613\n",
      "evaluation/env_infos/end_effector_loc Std                0.158957\n",
      "evaluation/env_infos/end_effector_loc Max                0.543854\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.173538\n",
      "time/evaluation sampling (s)                             0.911316\n",
      "time/exploration sampling (s)                            0.106653\n",
      "time/logging (s)                                         0.0185625\n",
      "time/saving (s)                                          0.0260402\n",
      "time/training (s)                                       45.487\n",
      "time/epoch (s)                                          46.7231\n",
      "time/total (s)                                        2226.03\n",
      "Epoch                                                   47\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:50:23.884069 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 48 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000824198\n",
      "trainer/QF2 Loss                                         0.00117691\n",
      "trainer/Policy Loss                                      2.96648\n",
      "trainer/Q1 Predictions Mean                             -0.863603\n",
      "trainer/Q1 Predictions Std                               0.888238\n",
      "trainer/Q1 Predictions Max                               1.32209\n",
      "trainer/Q1 Predictions Min                              -3.40794\n",
      "trainer/Q2 Predictions Mean                             -0.860327\n",
      "trainer/Q2 Predictions Std                               0.886532\n",
      "trainer/Q2 Predictions Max                               1.27887\n",
      "trainer/Q2 Predictions Min                              -3.38176\n",
      "trainer/Q Targets Mean                                  -0.855717\n",
      "trainer/Q Targets Std                                    0.892831\n",
      "trainer/Q Targets Max                                    1.3312\n",
      "trainer/Q Targets Min                                   -3.43089\n",
      "trainer/Log Pis Mean                                     2.1165\n",
      "trainer/Log Pis Std                                      1.42759\n",
      "trainer/Log Pis Max                                      6.90933\n",
      "trainer/Log Pis Min                                     -2.8281\n",
      "trainer/Policy mu Mean                                   0.0243263\n",
      "trainer/Policy mu Std                                    0.415865\n",
      "trainer/Policy mu Max                                    2.58294\n",
      "trainer/Policy mu Min                                   -2.30729\n",
      "trainer/Policy log std Mean                             -2.36229\n",
      "trainer/Policy log std Std                               0.649998\n",
      "trainer/Policy log std Max                              -0.357283\n",
      "trainer/Policy log std Min                              -3.27973\n",
      "trainer/Alpha                                            0.019515\n",
      "trainer/Alpha Loss                                       0.458658\n",
      "exploration/num steps total                           5900\n",
      "exploration/num paths total                            295\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.110163\n",
      "exploration/Rewards Std                                  0.0823076\n",
      "exploration/Rewards Max                                  0.14854\n",
      "exploration/Rewards Min                                 -0.288041\n",
      "exploration/Returns Mean                                -2.20325\n",
      "exploration/Returns Std                                  1.15942\n",
      "exploration/Returns Max                                 -0.1086\n",
      "exploration/Returns Min                                 -3.66441\n",
      "exploration/Actions Mean                                -0.00431139\n",
      "exploration/Actions Std                                  0.114349\n",
      "exploration/Actions Max                                  0.337347\n",
      "exploration/Actions Min                                 -0.441768\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.20325\n",
      "exploration/env_infos/final/reward_dist Mean             0.125342\n",
      "exploration/env_infos/final/reward_dist Std              0.250134\n",
      "exploration/env_infos/final/reward_dist Max              0.62561\n",
      "exploration/env_infos/final/reward_dist Min              3.71886e-16\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00199778\n",
      "exploration/env_infos/initial/reward_dist Std            0.00327068\n",
      "exploration/env_infos/initial/reward_dist Max            0.00850039\n",
      "exploration/env_infos/initial/reward_dist Min            5.02531e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0871127\n",
      "exploration/env_infos/reward_dist Std                    0.221229\n",
      "exploration/env_infos/reward_dist Max                    0.942298\n",
      "exploration/env_infos/reward_dist Min                    3.71886e-16\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0810617\n",
      "exploration/env_infos/final/reward_energy Std            0.0365815\n",
      "exploration/env_infos/final/reward_energy Max           -0.0364075\n",
      "exploration/env_infos/final/reward_energy Min           -0.138318\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.20267\n",
      "exploration/env_infos/initial/reward_energy Std          0.0799164\n",
      "exploration/env_infos/initial/reward_energy Max         -0.116124\n",
      "exploration/env_infos/initial/reward_energy Min         -0.340146\n",
      "exploration/env_infos/reward_energy Mean                -0.123237\n",
      "exploration/env_infos/reward_energy Std                  0.104886\n",
      "exploration/env_infos/reward_energy Max                 -0.00232496\n",
      "exploration/env_infos/reward_energy Min                 -0.488526\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0353136\n",
      "exploration/env_infos/final/end_effector_loc Std         0.236798\n",
      "exploration/env_infos/final/end_effector_loc Max         0.380737\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.269231\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000920917\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00764716\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.016367\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0119239\n",
      "exploration/env_infos/end_effector_loc Mean              0.0306029\n",
      "exploration/env_infos/end_effector_loc Std               0.1287\n",
      "exploration/env_infos/end_effector_loc Max               0.380737\n",
      "exploration/env_infos/end_effector_loc Min              -0.269231\n",
      "evaluation/num steps total                           49000\n",
      "evaluation/num paths total                            2450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0874034\n",
      "evaluation/Rewards Std                                   0.0844753\n",
      "evaluation/Rewards Max                                   0.113913\n",
      "evaluation/Rewards Min                                  -0.455779\n",
      "evaluation/Returns Mean                                 -1.74807\n",
      "evaluation/Returns Std                                   1.26772\n",
      "evaluation/Returns Max                                   0.952357\n",
      "evaluation/Returns Min                                  -4.50533\n",
      "evaluation/Actions Mean                                  0.00867572\n",
      "evaluation/Actions Std                                   0.103333\n",
      "evaluation/Actions Max                                   0.980725\n",
      "evaluation/Actions Min                                  -0.971685\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.74807\n",
      "evaluation/env_infos/final/reward_dist Mean              0.107562\n",
      "evaluation/env_infos/final/reward_dist Std               0.210425\n",
      "evaluation/env_infos/final/reward_dist Max               0.852168\n",
      "evaluation/env_infos/final/reward_dist Min               1.26917e-27\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00566271\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0118525\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0637168\n",
      "evaluation/env_infos/initial/reward_dist Min             9.56703e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.109699\n",
      "evaluation/env_infos/reward_dist Std                     0.203275\n",
      "evaluation/env_infos/reward_dist Max                     0.998268\n",
      "evaluation/env_infos/reward_dist Min                     1.26917e-27\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0332418\n",
      "evaluation/env_infos/final/reward_energy Std             0.0260493\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0039089\n",
      "evaluation/env_infos/final/reward_energy Min            -0.12384\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.322025\n",
      "evaluation/env_infos/initial/reward_energy Std           0.315868\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0167569\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.38058\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0789525\n",
      "evaluation/env_infos/reward_energy Std                   0.123582\n",
      "evaluation/env_infos/reward_energy Max                  -0.00217914\n",
      "evaluation/env_infos/reward_energy Min                  -1.38058\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.101092\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23865\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.767686\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.398968\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0018878\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0158359\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0490363\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0485843\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0433333\n",
      "evaluation/env_infos/end_effector_loc Std                0.15988\n",
      "evaluation/env_infos/end_effector_loc Max                0.767686\n",
      "evaluation/env_infos/end_effector_loc Min               -0.398968\n",
      "time/data storing (s)                                    0.169933\n",
      "time/evaluation sampling (s)                             0.796576\n",
      "time/exploration sampling (s)                            0.114668\n",
      "time/logging (s)                                         0.0183659\n",
      "time/saving (s)                                          0.025443\n",
      "time/training (s)                                       46.3184\n",
      "time/epoch (s)                                          47.4434\n",
      "time/total (s)                                        2273.94\n",
      "Epoch                                                   48\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:51:11.014330 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 49 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.001099\n",
      "trainer/QF2 Loss                                         0.000644223\n",
      "trainer/Policy Loss                                      2.87765\n",
      "trainer/Q1 Predictions Mean                             -0.911661\n",
      "trainer/Q1 Predictions Std                               0.863914\n",
      "trainer/Q1 Predictions Max                               1.29434\n",
      "trainer/Q1 Predictions Min                              -2.95303\n",
      "trainer/Q2 Predictions Mean                             -0.917034\n",
      "trainer/Q2 Predictions Std                               0.863768\n",
      "trainer/Q2 Predictions Max                               1.3102\n",
      "trainer/Q2 Predictions Min                              -2.98073\n",
      "trainer/Q Targets Mean                                  -0.913007\n",
      "trainer/Q Targets Std                                    0.863757\n",
      "trainer/Q Targets Max                                    1.32815\n",
      "trainer/Q Targets Min                                   -2.95932\n",
      "trainer/Log Pis Mean                                     1.96682\n",
      "trainer/Log Pis Std                                      1.56448\n",
      "trainer/Log Pis Max                                      7.70452\n",
      "trainer/Log Pis Min                                     -2.94958\n",
      "trainer/Policy mu Mean                                   0.0201565\n",
      "trainer/Policy mu Std                                    0.396012\n",
      "trainer/Policy mu Max                                    2.68598\n",
      "trainer/Policy mu Min                                   -2.15035\n",
      "trainer/Policy log std Mean                             -2.28537\n",
      "trainer/Policy log std Std                               0.668614\n",
      "trainer/Policy log std Max                              -0.146029\n",
      "trainer/Policy log std Min                              -3.48745\n",
      "trainer/Alpha                                            0.0199521\n",
      "trainer/Alpha Loss                                      -0.129906\n",
      "exploration/num steps total                           6000\n",
      "exploration/num paths total                            300\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0686598\n",
      "exploration/Rewards Std                                  0.0575334\n",
      "exploration/Rewards Max                                  0.0518973\n",
      "exploration/Rewards Min                                 -0.257096\n",
      "exploration/Returns Mean                                -1.3732\n",
      "exploration/Returns Std                                  0.514701\n",
      "exploration/Returns Max                                 -0.824315\n",
      "exploration/Returns Min                                 -2.15102\n",
      "exploration/Actions Mean                                 0.00749138\n",
      "exploration/Actions Std                                  0.161279\n",
      "exploration/Actions Max                                  0.917854\n",
      "exploration/Actions Min                                 -0.52072\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.3732\n",
      "exploration/env_infos/final/reward_dist Mean             0.164164\n",
      "exploration/env_infos/final/reward_dist Std              0.194773\n",
      "exploration/env_infos/final/reward_dist Max              0.516743\n",
      "exploration/env_infos/final/reward_dist Min              1.29262e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00369666\n",
      "exploration/env_infos/initial/reward_dist Std            0.00648979\n",
      "exploration/env_infos/initial/reward_dist Max            0.0166023\n",
      "exploration/env_infos/initial/reward_dist Min            1.2494e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.177183\n",
      "exploration/env_infos/reward_dist Std                    0.229347\n",
      "exploration/env_infos/reward_dist Max                    0.910497\n",
      "exploration/env_infos/reward_dist Min                    1.2494e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.173949\n",
      "exploration/env_infos/final/reward_energy Std            0.112393\n",
      "exploration/env_infos/final/reward_energy Max           -0.103286\n",
      "exploration/env_infos/final/reward_energy Min           -0.397651\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.237781\n",
      "exploration/env_infos/initial/reward_energy Std          0.128751\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0258321\n",
      "exploration/env_infos/initial/reward_energy Min         -0.416616\n",
      "exploration/env_infos/reward_energy Mean                -0.169188\n",
      "exploration/env_infos/reward_energy Std                  0.153329\n",
      "exploration/env_infos/reward_energy Max                 -0.00247661\n",
      "exploration/env_infos/reward_energy Min                 -0.936455\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0255667\n",
      "exploration/env_infos/final/end_effector_loc Std         0.236399\n",
      "exploration/env_infos/final/end_effector_loc Max         0.408734\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.32372\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00149297\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0094428\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0156372\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0137623\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0182186\n",
      "exploration/env_infos/end_effector_loc Std               0.159946\n",
      "exploration/env_infos/end_effector_loc Max               0.408734\n",
      "exploration/env_infos/end_effector_loc Min              -0.32372\n",
      "evaluation/num steps total                           50000\n",
      "evaluation/num paths total                            2500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0707572\n",
      "evaluation/Rewards Std                                   0.0880494\n",
      "evaluation/Rewards Max                                   0.145173\n",
      "evaluation/Rewards Min                                  -0.68941\n",
      "evaluation/Returns Mean                                 -1.41514\n",
      "evaluation/Returns Std                                   1.33033\n",
      "evaluation/Returns Max                                   1.64681\n",
      "evaluation/Returns Min                                  -4.15807\n",
      "evaluation/Actions Mean                                 -0.00574494\n",
      "evaluation/Actions Std                                   0.113071\n",
      "evaluation/Actions Max                                   0.992968\n",
      "evaluation/Actions Min                                  -0.992947\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.41514\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0371565\n",
      "evaluation/env_infos/final/reward_dist Std               0.0745203\n",
      "evaluation/env_infos/final/reward_dist Max               0.352549\n",
      "evaluation/env_infos/final/reward_dist Min               6.82805e-73\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0106422\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0178377\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0658665\n",
      "evaluation/env_infos/initial/reward_dist Min             1.39102e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0947284\n",
      "evaluation/env_infos/reward_dist Std                     0.20354\n",
      "evaluation/env_infos/reward_dist Max                     0.994038\n",
      "evaluation/env_infos/reward_dist Min                     6.82805e-73\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.041598\n",
      "evaluation/env_infos/final/reward_energy Std             0.0469681\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0101185\n",
      "evaluation/env_infos/final/reward_energy Min            -0.23304\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.366315\n",
      "evaluation/env_infos/initial/reward_energy Std           0.360325\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0182604\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.40425\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0929041\n",
      "evaluation/env_infos/reward_energy Std                   0.130404\n",
      "evaluation/env_infos/reward_energy Max                  -0.000929178\n",
      "evaluation/env_infos/reward_energy Min                  -1.40425\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0509857\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.253662\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.679058\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.770143\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000745709\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0181513\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0496484\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0496474\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0164536\n",
      "evaluation/env_infos/end_effector_loc Std                0.179687\n",
      "evaluation/env_infos/end_effector_loc Max                0.68583\n",
      "evaluation/env_infos/end_effector_loc Min               -0.770143\n",
      "time/data storing (s)                                    0.163205\n",
      "time/evaluation sampling (s)                             0.765304\n",
      "time/exploration sampling (s)                            0.103634\n",
      "time/logging (s)                                         0.0182447\n",
      "time/saving (s)                                          0.0251147\n",
      "time/training (s)                                       45.5932\n",
      "time/epoch (s)                                          46.6687\n",
      "time/total (s)                                        2321.07\n",
      "Epoch                                                   49\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:51:58.452323 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 50 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00120574\n",
      "trainer/QF2 Loss                                         0.00224723\n",
      "trainer/Policy Loss                                      2.83459\n",
      "trainer/Q1 Predictions Mean                             -0.928429\n",
      "trainer/Q1 Predictions Std                               0.814618\n",
      "trainer/Q1 Predictions Max                               1.27193\n",
      "trainer/Q1 Predictions Min                              -3.30898\n",
      "trainer/Q2 Predictions Mean                             -0.930223\n",
      "trainer/Q2 Predictions Std                               0.819177\n",
      "trainer/Q2 Predictions Max                               1.29588\n",
      "trainer/Q2 Predictions Min                              -3.34519\n",
      "trainer/Q Targets Mean                                  -0.92994\n",
      "trainer/Q Targets Std                                    0.824143\n",
      "trainer/Q Targets Max                                    1.28833\n",
      "trainer/Q Targets Min                                   -3.30442\n",
      "trainer/Log Pis Mean                                     1.90362\n",
      "trainer/Log Pis Std                                      1.43581\n",
      "trainer/Log Pis Max                                      4.40752\n",
      "trainer/Log Pis Min                                     -3.82936\n",
      "trainer/Policy mu Mean                                   0.015544\n",
      "trainer/Policy mu Std                                    0.392713\n",
      "trainer/Policy mu Max                                    2.21434\n",
      "trainer/Policy mu Min                                   -1.85948\n",
      "trainer/Policy log std Mean                             -2.24948\n",
      "trainer/Policy log std Std                               0.690878\n",
      "trainer/Policy log std Max                              -0.312013\n",
      "trainer/Policy log std Min                              -3.28281\n",
      "trainer/Alpha                                            0.0201022\n",
      "trainer/Alpha Loss                                      -0.376583\n",
      "exploration/num steps total                           6100\n",
      "exploration/num paths total                            305\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.119244\n",
      "exploration/Rewards Std                                  0.117396\n",
      "exploration/Rewards Max                                  0.0506276\n",
      "exploration/Rewards Min                                 -0.854374\n",
      "exploration/Returns Mean                                -2.38487\n",
      "exploration/Returns Std                                  1.60439\n",
      "exploration/Returns Max                                 -1.15416\n",
      "exploration/Returns Min                                 -5.36644\n",
      "exploration/Actions Mean                                 0.00643895\n",
      "exploration/Actions Std                                  0.15888\n",
      "exploration/Actions Max                                  0.979365\n",
      "exploration/Actions Min                                 -0.827498\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.38487\n",
      "exploration/env_infos/final/reward_dist Mean             0.113873\n",
      "exploration/env_infos/final/reward_dist Std              0.227742\n",
      "exploration/env_infos/final/reward_dist Max              0.569356\n",
      "exploration/env_infos/final/reward_dist Min              3.38054e-21\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00478621\n",
      "exploration/env_infos/initial/reward_dist Std            0.00558253\n",
      "exploration/env_infos/initial/reward_dist Max            0.0147658\n",
      "exploration/env_infos/initial/reward_dist Min            1.79558e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0691763\n",
      "exploration/env_infos/reward_dist Std                    0.122255\n",
      "exploration/env_infos/reward_dist Max                    0.569356\n",
      "exploration/env_infos/reward_dist Min                    1.00239e-21\n",
      "exploration/env_infos/final/reward_energy Mean          -0.185609\n",
      "exploration/env_infos/final/reward_energy Std            0.102239\n",
      "exploration/env_infos/final/reward_energy Max           -0.0839146\n",
      "exploration/env_infos/final/reward_energy Min           -0.361549\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.336609\n",
      "exploration/env_infos/initial/reward_energy Std          0.363058\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0629023\n",
      "exploration/env_infos/initial/reward_energy Min         -1.04931\n",
      "exploration/env_infos/reward_energy Mean                -0.170391\n",
      "exploration/env_infos/reward_energy Std                  0.14675\n",
      "exploration/env_infos/reward_energy Max                 -0.00613736\n",
      "exploration/env_infos/reward_energy Min                 -1.04931\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.170986\n",
      "exploration/env_infos/final/end_effector_loc Std         0.320877\n",
      "exploration/env_infos/final/end_effector_loc Max         0.656829\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.332744\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00527785\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0166895\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0489683\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0188347\n",
      "exploration/env_infos/end_effector_loc Mean              0.0906953\n",
      "exploration/env_infos/end_effector_loc Std               0.223881\n",
      "exploration/env_infos/end_effector_loc Max               0.666291\n",
      "exploration/env_infos/end_effector_loc Min              -0.332744\n",
      "evaluation/num steps total                           51000\n",
      "evaluation/num paths total                            2550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0689408\n",
      "evaluation/Rewards Std                                   0.0941067\n",
      "evaluation/Rewards Max                                   0.108372\n",
      "evaluation/Rewards Min                                  -0.606908\n",
      "evaluation/Returns Mean                                 -1.37882\n",
      "evaluation/Returns Std                                   1.48447\n",
      "evaluation/Returns Max                                   0.636857\n",
      "evaluation/Returns Min                                  -6.68624\n",
      "evaluation/Actions Mean                                 -0.00458198\n",
      "evaluation/Actions Std                                   0.0925393\n",
      "evaluation/Actions Max                                   0.940888\n",
      "evaluation/Actions Min                                  -0.898554\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.37882\n",
      "evaluation/env_infos/final/reward_dist Mean              0.123578\n",
      "evaluation/env_infos/final/reward_dist Std               0.201722\n",
      "evaluation/env_infos/final/reward_dist Max               0.817904\n",
      "evaluation/env_infos/final/reward_dist Min               3.46908e-150\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00790145\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0176744\n",
      "evaluation/env_infos/initial/reward_dist Max             0.109541\n",
      "evaluation/env_infos/initial/reward_dist Min             1.98422e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.124002\n",
      "evaluation/env_infos/reward_dist Std                     0.23516\n",
      "evaluation/env_infos/reward_dist Max                     0.99516\n",
      "evaluation/env_infos/reward_dist Min                     3.46908e-150\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0303417\n",
      "evaluation/env_infos/final/reward_energy Std             0.0317163\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00206576\n",
      "evaluation/env_infos/final/reward_energy Min            -0.136737\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.281531\n",
      "evaluation/env_infos/initial/reward_energy Std           0.324615\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00217745\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.30103\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0656832\n",
      "evaluation/env_infos/reward_energy Std                   0.113379\n",
      "evaluation/env_infos/reward_energy Max                  -0.00108986\n",
      "evaluation/env_infos/reward_energy Min                  -1.30103\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0501645\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.296233\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.845307\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000233839\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0151901\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0470444\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0449277\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0224367\n",
      "evaluation/env_infos/end_effector_loc Std                0.198681\n",
      "evaluation/env_infos/end_effector_loc Max                0.845307\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.161031\n",
      "time/evaluation sampling (s)                             0.777856\n",
      "time/exploration sampling (s)                            0.104858\n",
      "time/logging (s)                                         0.0191647\n",
      "time/saving (s)                                          0.0539612\n",
      "time/training (s)                                       45.8378\n",
      "time/epoch (s)                                          46.9547\n",
      "time/total (s)                                        2368.51\n",
      "Epoch                                                   50\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:52:46.877997 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 51 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00100505\r\n",
      "trainer/QF2 Loss                                         0.000831015\r\n",
      "trainer/Policy Loss                                      2.8168\r\n",
      "trainer/Q1 Predictions Mean                             -0.860016\r\n",
      "trainer/Q1 Predictions Std                               0.898607\r\n",
      "trainer/Q1 Predictions Max                               1.47338\r\n",
      "trainer/Q1 Predictions Min                              -3.13237\r\n",
      "trainer/Q2 Predictions Mean                             -0.865569\r\n",
      "trainer/Q2 Predictions Std                               0.902135\r\n",
      "trainer/Q2 Predictions Max                               1.50789\r\n",
      "trainer/Q2 Predictions Min                              -3.10075\r\n",
      "trainer/Q Targets Mean                                  -0.868215\r\n",
      "trainer/Q Targets Std                                    0.905875\r\n",
      "trainer/Q Targets Max                                    1.50087\r\n",
      "trainer/Q Targets Min                                   -3.10471\r\n",
      "trainer/Log Pis Mean                                     1.96386\r\n",
      "trainer/Log Pis Std                                      1.47018\r\n",
      "trainer/Log Pis Max                                      4.50943\r\n",
      "trainer/Log Pis Min                                     -3.85503\r\n",
      "trainer/Policy mu Mean                                  -0.0533686\r\n",
      "trainer/Policy mu Std                                    0.469833\r\n",
      "trainer/Policy mu Max                                    1.75416\r\n",
      "trainer/Policy mu Min                                   -2.58261\r\n",
      "trainer/Policy log std Mean                             -2.20056\r\n",
      "trainer/Policy log std Std                               0.727412\r\n",
      "trainer/Policy log std Max                              -0.0211093\r\n",
      "trainer/Policy log std Min                              -3.20408\r\n",
      "trainer/Alpha                                            0.0191176\r\n",
      "trainer/Alpha Loss                                      -0.142997\r\n",
      "exploration/num steps total                           6200\r\n",
      "exploration/num paths total                            310\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.125959\r\n",
      "exploration/Rewards Std                                  0.0498158\r\n",
      "exploration/Rewards Max                                 -0.0219472\r\n",
      "exploration/Rewards Min                                 -0.240025\r\n",
      "exploration/Returns Mean                                -2.51917\r\n",
      "exploration/Returns Std                                  0.729517\r\n",
      "exploration/Returns Max                                 -1.72369\r\n",
      "exploration/Returns Min                                 -3.64428\r\n",
      "exploration/Actions Mean                                -0.0050614\r\n",
      "exploration/Actions Std                                  0.108125\r\n",
      "exploration/Actions Max                                  0.387088\r\n",
      "exploration/Actions Min                                 -0.350303\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.51917\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00924435\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0174563\r\n",
      "exploration/env_infos/final/reward_dist Max              0.044119\r\n",
      "exploration/env_infos/final/reward_dist Min              3.31912e-15\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00277207\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00312294\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0086654\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.48701e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0188968\r\n",
      "exploration/env_infos/reward_dist Std                    0.0376261\r\n",
      "exploration/env_infos/reward_dist Max                    0.171754\r\n",
      "exploration/env_infos/reward_dist Min                    3.31912e-15\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.150803\r\n",
      "exploration/env_infos/final/reward_energy Std            0.107282\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0757701\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.361874\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.140236\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0885727\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0402346\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.302714\r\n",
      "exploration/env_infos/reward_energy Mean                -0.12505\r\n",
      "exploration/env_infos/reward_energy Std                  0.0882938\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0109596\r\n",
      "exploration/env_infos/reward_energy Min                 -0.39274\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0252975\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.230169\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.423777\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.308093\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000862828\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00580038\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0143936\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0066589\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00136837\r\n",
      "exploration/env_infos/end_effector_loc Std               0.110616\r\n",
      "exploration/env_infos/end_effector_loc Max               0.423777\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.308093\r\n",
      "evaluation/num steps total                           52000\r\n",
      "evaluation/num paths total                            2600\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0584558\r\n",
      "evaluation/Rewards Std                                   0.0842404\r\n",
      "evaluation/Rewards Max                                   0.148332\r\n",
      "evaluation/Rewards Min                                  -0.546475\r\n",
      "evaluation/Returns Mean                                 -1.16912\r\n",
      "evaluation/Returns Std                                   1.34288\r\n",
      "evaluation/Returns Max                                   1.2472\r\n",
      "evaluation/Returns Min                                  -5.56759\r\n",
      "evaluation/Actions Mean                                 -0.00587601\r\n",
      "evaluation/Actions Std                                   0.102822\r\n",
      "evaluation/Actions Max                                   0.944809\r\n",
      "evaluation/Actions Min                                  -0.904681\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.16912\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.140594\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.212504\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.719833\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.1488e-166\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0072556\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0283332\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.201174\r\n",
      "evaluation/env_infos/initial/reward_dist Min             4.82109e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.131234\r\n",
      "evaluation/env_infos/reward_dist Std                     0.236872\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999671\r\n",
      "evaluation/env_infos/reward_dist Min                     1.1488e-166\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0409959\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0556639\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00546837\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.347324\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.289045\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.338004\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00274558\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.24316\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0747798\r\n",
      "evaluation/env_infos/reward_energy Std                   0.124987\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000558885\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.24316\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0824878\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.299946\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.547152\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00199378\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.015597\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0472405\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0452341\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.040838\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.191247\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.547152\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.162168\r\n",
      "time/evaluation sampling (s)                             0.774905\r\n",
      "time/exploration sampling (s)                            0.103414\r\n",
      "time/logging (s)                                         0.0186096\r\n",
      "time/saving (s)                                          0.030008\r\n",
      "time/training (s)                                       46.8341\r\n",
      "time/epoch (s)                                          47.9232\r\n",
      "time/total (s)                                        2416.93\r\n",
      "Epoch                                                   51\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:53:34.207738 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 52 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000826323\r\n",
      "trainer/QF2 Loss                                         0.00105103\r\n",
      "trainer/Policy Loss                                      3.08252\r\n",
      "trainer/Q1 Predictions Mean                             -1.04664\r\n",
      "trainer/Q1 Predictions Std                               0.858842\r\n",
      "trainer/Q1 Predictions Max                               1.44924\r\n",
      "trainer/Q1 Predictions Min                              -3.18474\r\n",
      "trainer/Q2 Predictions Mean                             -1.04396\r\n",
      "trainer/Q2 Predictions Std                               0.852254\r\n",
      "trainer/Q2 Predictions Max                               1.47039\r\n",
      "trainer/Q2 Predictions Min                              -3.18701\r\n",
      "trainer/Q Targets Mean                                  -1.04725\r\n",
      "trainer/Q Targets Std                                    0.856108\r\n",
      "trainer/Q Targets Max                                    1.46087\r\n",
      "trainer/Q Targets Min                                   -3.19841\r\n",
      "trainer/Log Pis Mean                                     2.04111\r\n",
      "trainer/Log Pis Std                                      1.4019\r\n",
      "trainer/Log Pis Max                                      5.33976\r\n",
      "trainer/Log Pis Min                                     -6.01033\r\n",
      "trainer/Policy mu Mean                                  -0.0424137\r\n",
      "trainer/Policy mu Std                                    0.380718\r\n",
      "trainer/Policy mu Max                                    2.04448\r\n",
      "trainer/Policy mu Min                                   -2.45501\r\n",
      "trainer/Policy log std Mean                             -2.25386\r\n",
      "trainer/Policy log std Std                               0.673482\r\n",
      "trainer/Policy log std Max                               0.187673\r\n",
      "trainer/Policy log std Min                              -3.16248\r\n",
      "trainer/Alpha                                            0.0200796\r\n",
      "trainer/Alpha Loss                                       0.1607\r\n",
      "exploration/num steps total                           6300\r\n",
      "exploration/num paths total                            315\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.133715\r\n",
      "exploration/Rewards Std                                  0.0823235\r\n",
      "exploration/Rewards Max                                 -0.00714368\r\n",
      "exploration/Rewards Min                                 -0.389335\r\n",
      "exploration/Returns Mean                                -2.6743\r\n",
      "exploration/Returns Std                                  1.21256\r\n",
      "exploration/Returns Max                                 -1.37664\r\n",
      "exploration/Returns Min                                 -4.86899\r\n",
      "exploration/Actions Mean                                -0.00877302\r\n",
      "exploration/Actions Std                                  0.0964092\r\n",
      "exploration/Actions Max                                  0.420887\r\n",
      "exploration/Actions Min                                 -0.358731\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.6743\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0450278\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0891146\r\n",
      "exploration/env_infos/final/reward_dist Max              0.223251\r\n",
      "exploration/env_infos/final/reward_dist Min              1.63653e-13\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000266863\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000516914\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00130056\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.55323e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0181127\r\n",
      "exploration/env_infos/reward_dist Std                    0.0469828\r\n",
      "exploration/env_infos/reward_dist Max                    0.223911\r\n",
      "exploration/env_infos/reward_dist Min                    1.63653e-13\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.138831\r\n",
      "exploration/env_infos/final/reward_energy Std            0.018992\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.10565\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.161447\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.116784\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0470176\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0839745\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.210107\r\n",
      "exploration/env_infos/reward_energy Mean                -0.109175\r\n",
      "exploration/env_infos/reward_energy Std                  0.0826089\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0169988\r\n",
      "exploration/env_infos/reward_energy Min                 -0.443562\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0774069\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.235775\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.203696\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.469976\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000664477\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00440112\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0101578\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00479676\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0240862\r\n",
      "exploration/env_infos/end_effector_loc Std               0.134678\r\n",
      "exploration/env_infos/end_effector_loc Max               0.203696\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.469976\r\n",
      "evaluation/num steps total                           53000\r\n",
      "evaluation/num paths total                            2650\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0502773\r\n",
      "evaluation/Rewards Std                                   0.111881\r\n",
      "evaluation/Rewards Max                                   0.157176\r\n",
      "evaluation/Rewards Min                                  -0.751158\r\n",
      "evaluation/Returns Mean                                 -1.00555\r\n",
      "evaluation/Returns Std                                   1.7862\r\n",
      "evaluation/Returns Max                                   1.44352\r\n",
      "evaluation/Returns Min                                  -7.19084\r\n",
      "evaluation/Actions Mean                                 -0.011909\r\n",
      "evaluation/Actions Std                                   0.10115\r\n",
      "evaluation/Actions Max                                   0.876922\r\n",
      "evaluation/Actions Min                                  -0.951704\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.00555\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.161871\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.257421\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.963205\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.40518e-148\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00403642\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00768614\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0380562\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.47751e-08\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.169752\r\n",
      "evaluation/env_infos/reward_dist Std                     0.257529\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997283\r\n",
      "evaluation/env_infos/reward_dist Min                     4.40518e-148\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0599315\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.140174\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00254525\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.779868\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.27073\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.278975\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00709215\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.953232\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.076557\r\n",
      "evaluation/env_infos/reward_energy Std                   0.122005\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000716495\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.953232\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0923213\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.286492\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.417624\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000401377\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0137383\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0438461\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0475852\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0334152\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.181329\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.417624\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.164423\r\n",
      "time/evaluation sampling (s)                             0.793773\r\n",
      "time/exploration sampling (s)                            0.103101\r\n",
      "time/logging (s)                                         0.0197434\r\n",
      "time/saving (s)                                          0.0391422\r\n",
      "time/training (s)                                       45.7273\r\n",
      "time/epoch (s)                                          46.8475\r\n",
      "time/total (s)                                        2464.26\r\n",
      "Epoch                                                   52\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:54:22.275804 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 53 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000862682\n",
      "trainer/QF2 Loss                                         0.000881523\n",
      "trainer/Policy Loss                                      2.98683\n",
      "trainer/Q1 Predictions Mean                             -0.979933\n",
      "trainer/Q1 Predictions Std                               0.745569\n",
      "trainer/Q1 Predictions Max                               1.25896\n",
      "trainer/Q1 Predictions Min                              -3.0024\n",
      "trainer/Q2 Predictions Mean                             -0.988381\n",
      "trainer/Q2 Predictions Std                               0.749079\n",
      "trainer/Q2 Predictions Max                               1.27325\n",
      "trainer/Q2 Predictions Min                              -2.98284\n",
      "trainer/Q Targets Mean                                  -0.987839\n",
      "trainer/Q Targets Std                                    0.750714\n",
      "trainer/Q Targets Max                                    1.31476\n",
      "trainer/Q Targets Min                                   -3.00107\n",
      "trainer/Log Pis Mean                                     2.00675\n",
      "trainer/Log Pis Std                                      1.39234\n",
      "trainer/Log Pis Max                                      4.48473\n",
      "trainer/Log Pis Min                                     -2.76146\n",
      "trainer/Policy mu Mean                                  -0.0162937\n",
      "trainer/Policy mu Std                                    0.351974\n",
      "trainer/Policy mu Max                                    2.31148\n",
      "trainer/Policy mu Min                                   -1.9495\n",
      "trainer/Policy log std Mean                             -2.33749\n",
      "trainer/Policy log std Std                               0.711543\n",
      "trainer/Policy log std Max                              -0.162722\n",
      "trainer/Policy log std Min                              -3.23434\n",
      "trainer/Alpha                                            0.0204263\n",
      "trainer/Alpha Loss                                       0.0262419\n",
      "exploration/num steps total                           6400\n",
      "exploration/num paths total                            320\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.174665\n",
      "exploration/Rewards Std                                  0.154998\n",
      "exploration/Rewards Max                                 -0.00192998\n",
      "exploration/Rewards Min                                 -0.733112\n",
      "exploration/Returns Mean                                -3.49331\n",
      "exploration/Returns Std                                  1.87008\n",
      "exploration/Returns Max                                 -1.54777\n",
      "exploration/Returns Min                                 -7.00692\n",
      "exploration/Actions Mean                                -0.0258201\n",
      "exploration/Actions Std                                  0.279718\n",
      "exploration/Actions Max                                  0.908073\n",
      "exploration/Actions Min                                 -0.994761\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.49331\n",
      "exploration/env_infos/final/reward_dist Mean             0.011569\n",
      "exploration/env_infos/final/reward_dist Std              0.023138\n",
      "exploration/env_infos/final/reward_dist Max              0.0578449\n",
      "exploration/env_infos/final/reward_dist Min              9.28849e-51\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0239844\n",
      "exploration/env_infos/initial/reward_dist Std            0.0234136\n",
      "exploration/env_infos/initial/reward_dist Max            0.0659915\n",
      "exploration/env_infos/initial/reward_dist Min            2.57154e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0150021\n",
      "exploration/env_infos/reward_dist Std                    0.0464958\n",
      "exploration/env_infos/reward_dist Max                    0.335502\n",
      "exploration/env_infos/reward_dist Min                    3.5328e-58\n",
      "exploration/env_infos/final/reward_energy Mean          -0.2458\n",
      "exploration/env_infos/final/reward_energy Std            0.109916\n",
      "exploration/env_infos/final/reward_energy Max           -0.105581\n",
      "exploration/env_infos/final/reward_energy Min           -0.409427\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.447922\n",
      "exploration/env_infos/initial/reward_energy Std          0.294762\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0994847\n",
      "exploration/env_infos/initial/reward_energy Min         -0.933134\n",
      "exploration/env_infos/reward_energy Mean                -0.304721\n",
      "exploration/env_infos/reward_energy Std                  0.254877\n",
      "exploration/env_infos/reward_energy Max                 -0.0078456\n",
      "exploration/env_infos/reward_energy Min                 -1.3469\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.183538\n",
      "exploration/env_infos/final/end_effector_loc Std         0.366146\n",
      "exploration/env_infos/final/end_effector_loc Max         0.445641\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00597679\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.017991\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0369647\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0284686\n",
      "exploration/env_infos/end_effector_loc Mean             -0.146948\n",
      "exploration/env_infos/end_effector_loc Std               0.269244\n",
      "exploration/env_infos/end_effector_loc Max               0.445641\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           54000\n",
      "evaluation/num paths total                            2700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0729484\n",
      "evaluation/Rewards Std                                   0.0925574\n",
      "evaluation/Rewards Max                                   0.132652\n",
      "evaluation/Rewards Min                                  -0.613649\n",
      "evaluation/Returns Mean                                 -1.45897\n",
      "evaluation/Returns Std                                   1.54682\n",
      "evaluation/Returns Max                                   1.55544\n",
      "evaluation/Returns Min                                  -5.4994\n",
      "evaluation/Actions Mean                                 -0.00746\n",
      "evaluation/Actions Std                                   0.123452\n",
      "evaluation/Actions Max                                   0.920547\n",
      "evaluation/Actions Min                                  -0.995006\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.45897\n",
      "evaluation/env_infos/final/reward_dist Mean              0.134192\n",
      "evaluation/env_infos/final/reward_dist Std               0.231604\n",
      "evaluation/env_infos/final/reward_dist Max               0.954529\n",
      "evaluation/env_infos/final/reward_dist Min               4.33648e-134\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00616447\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00975333\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0372705\n",
      "evaluation/env_infos/initial/reward_dist Min             3.55606e-08\n",
      "evaluation/env_infos/reward_dist Mean                    0.125127\n",
      "evaluation/env_infos/reward_dist Std                     0.226859\n",
      "evaluation/env_infos/reward_dist Max                     0.986929\n",
      "evaluation/env_infos/reward_dist Min                     4.33648e-134\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0306949\n",
      "evaluation/env_infos/final/reward_energy Std             0.0303721\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00405901\n",
      "evaluation/env_infos/final/reward_energy Min            -0.126624\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.271268\n",
      "evaluation/env_infos/initial/reward_energy Std           0.355735\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00908327\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.35552\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0812931\n",
      "evaluation/env_infos/reward_energy Std                   0.154867\n",
      "evaluation/env_infos/reward_energy Max                  -0.000827027\n",
      "evaluation/env_infos/reward_energy Min                  -1.35552\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0811139\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.289812\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.40646\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00274194\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0155772\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0460274\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0497503\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0514447\n",
      "evaluation/env_infos/end_effector_loc Std                0.204243\n",
      "evaluation/env_infos/end_effector_loc Max                0.40646\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.160906\n",
      "time/evaluation sampling (s)                             0.798352\n",
      "time/exploration sampling (s)                            0.103807\n",
      "time/logging (s)                                         0.0184647\n",
      "time/saving (s)                                          0.0279825\n",
      "time/training (s)                                       46.4762\n",
      "time/epoch (s)                                          47.5857\n",
      "time/total (s)                                        2512.33\n",
      "Epoch                                                   53\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:55:09.843324 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 54 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00600845\n",
      "trainer/QF2 Loss                                         0.00291153\n",
      "trainer/Policy Loss                                      2.90767\n",
      "trainer/Q1 Predictions Mean                             -0.990961\n",
      "trainer/Q1 Predictions Std                               0.77595\n",
      "trainer/Q1 Predictions Max                               1.01872\n",
      "trainer/Q1 Predictions Min                              -3.20121\n",
      "trainer/Q2 Predictions Mean                             -1.00012\n",
      "trainer/Q2 Predictions Std                               0.778719\n",
      "trainer/Q2 Predictions Max                               0.979442\n",
      "trainer/Q2 Predictions Min                              -3.20288\n",
      "trainer/Q Targets Mean                                  -1.00213\n",
      "trainer/Q Targets Std                                    0.785533\n",
      "trainer/Q Targets Max                                    1.00992\n",
      "trainer/Q Targets Min                                   -3.2034\n",
      "trainer/Log Pis Mean                                     1.90675\n",
      "trainer/Log Pis Std                                      1.52323\n",
      "trainer/Log Pis Max                                      4.48783\n",
      "trainer/Log Pis Min                                     -4.40909\n",
      "trainer/Policy mu Mean                                  -0.0129997\n",
      "trainer/Policy mu Std                                    0.278664\n",
      "trainer/Policy mu Max                                    2.30978\n",
      "trainer/Policy mu Min                                   -1.51421\n",
      "trainer/Policy log std Mean                             -2.28241\n",
      "trainer/Policy log std Std                               0.644985\n",
      "trainer/Policy log std Max                              -0.386973\n",
      "trainer/Policy log std Min                              -3.33593\n",
      "trainer/Alpha                                            0.0195695\n",
      "trainer/Alpha Loss                                      -0.366833\n",
      "exploration/num steps total                           6500\n",
      "exploration/num paths total                            325\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.158021\n",
      "exploration/Rewards Std                                  0.167045\n",
      "exploration/Rewards Max                                  0.0452106\n",
      "exploration/Rewards Min                                 -0.573563\n",
      "exploration/Returns Mean                                -3.16042\n",
      "exploration/Returns Std                                  3.15758\n",
      "exploration/Returns Max                                 -0.578852\n",
      "exploration/Returns Min                                 -9.37981\n",
      "exploration/Actions Mean                                -0.012392\n",
      "exploration/Actions Std                                  0.177341\n",
      "exploration/Actions Max                                  0.605224\n",
      "exploration/Actions Min                                 -0.843212\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.16042\n",
      "exploration/env_infos/final/reward_dist Mean             0.107966\n",
      "exploration/env_infos/final/reward_dist Std              0.215932\n",
      "exploration/env_infos/final/reward_dist Max              0.539829\n",
      "exploration/env_infos/final/reward_dist Min              5.32258e-86\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000775664\n",
      "exploration/env_infos/initial/reward_dist Std            0.00149789\n",
      "exploration/env_infos/initial/reward_dist Max            0.00377103\n",
      "exploration/env_infos/initial/reward_dist Min            3.5637e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0566332\n",
      "exploration/env_infos/reward_dist Std                    0.133139\n",
      "exploration/env_infos/reward_dist Max                    0.678642\n",
      "exploration/env_infos/reward_dist Min                    5.32258e-86\n",
      "exploration/env_infos/final/reward_energy Mean          -0.120583\n",
      "exploration/env_infos/final/reward_energy Std            0.0631558\n",
      "exploration/env_infos/final/reward_energy Max           -0.0268106\n",
      "exploration/env_infos/final/reward_energy Min           -0.208412\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.338215\n",
      "exploration/env_infos/initial/reward_energy Std          0.305111\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0745011\n",
      "exploration/env_infos/initial/reward_energy Min         -0.908155\n",
      "exploration/env_infos/reward_energy Mean                -0.179388\n",
      "exploration/env_infos/reward_energy Std                  0.176144\n",
      "exploration/env_infos/reward_energy Max                 -0.0116299\n",
      "exploration/env_infos/reward_energy Min                 -0.908155\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.23174\n",
      "exploration/env_infos/final/end_effector_loc Std         0.397633\n",
      "exploration/env_infos/final/end_effector_loc Max         0.576206\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.853528\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00484734\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0153576\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0161847\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0421606\n",
      "exploration/env_infos/end_effector_loc Mean             -0.115019\n",
      "exploration/env_infos/end_effector_loc Std               0.263809\n",
      "exploration/env_infos/end_effector_loc Max               0.576206\n",
      "exploration/env_infos/end_effector_loc Min              -0.853528\n",
      "evaluation/num steps total                           55000\n",
      "evaluation/num paths total                            2750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0737428\n",
      "evaluation/Rewards Std                                   0.0834161\n",
      "evaluation/Rewards Max                                   0.146926\n",
      "evaluation/Rewards Min                                  -0.994646\n",
      "evaluation/Returns Mean                                 -1.47486\n",
      "evaluation/Returns Std                                   1.09098\n",
      "evaluation/Returns Max                                   0.791234\n",
      "evaluation/Returns Min                                  -5.48211\n",
      "evaluation/Actions Mean                                 -0.00484916\n",
      "evaluation/Actions Std                                   0.0747494\n",
      "evaluation/Actions Max                                   0.782432\n",
      "evaluation/Actions Min                                  -0.794962\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.47486\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0582011\n",
      "evaluation/env_infos/final/reward_dist Std               0.178535\n",
      "evaluation/env_infos/final/reward_dist Max               0.872506\n",
      "evaluation/env_infos/final/reward_dist Min               4.21005e-153\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00665661\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0128097\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0761865\n",
      "evaluation/env_infos/initial/reward_dist Min             1.33914e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0675897\n",
      "evaluation/env_infos/reward_dist Std                     0.166997\n",
      "evaluation/env_infos/reward_dist Max                     0.956089\n",
      "evaluation/env_infos/reward_dist Min                     4.21005e-153\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0335962\n",
      "evaluation/env_infos/final/reward_energy Std             0.0288958\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00140162\n",
      "evaluation/env_infos/final/reward_energy Min            -0.172686\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.200382\n",
      "evaluation/env_infos/initial/reward_energy Std           0.279238\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00514273\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.11542\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0547732\n",
      "evaluation/env_infos/reward_energy Std                   0.0906745\n",
      "evaluation/env_infos/reward_energy Max                  -0.00112546\n",
      "evaluation/env_infos/reward_energy Min                  -1.11542\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0407728\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.330148\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.823758\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000531428\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121398\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0391216\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0397481\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0136842\n",
      "evaluation/env_infos/end_effector_loc Std                0.198432\n",
      "evaluation/env_infos/end_effector_loc Max                0.823758\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.161087\n",
      "time/evaluation sampling (s)                             0.764204\n",
      "time/exploration sampling (s)                            0.104537\n",
      "time/logging (s)                                         0.0189958\n",
      "time/saving (s)                                          0.0257229\n",
      "time/training (s)                                       45.9522\n",
      "time/epoch (s)                                          47.0268\n",
      "time/total (s)                                        2559.89\n",
      "Epoch                                                   54\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:55:57.879163 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 55 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00103867\n",
      "trainer/QF2 Loss                                         0.00229839\n",
      "trainer/Policy Loss                                      2.99426\n",
      "trainer/Q1 Predictions Mean                             -0.96197\n",
      "trainer/Q1 Predictions Std                               0.822099\n",
      "trainer/Q1 Predictions Max                               1.34288\n",
      "trainer/Q1 Predictions Min                              -3.01481\n",
      "trainer/Q2 Predictions Mean                             -0.947068\n",
      "trainer/Q2 Predictions Std                               0.819129\n",
      "trainer/Q2 Predictions Max                               1.33228\n",
      "trainer/Q2 Predictions Min                              -2.95872\n",
      "trainer/Q Targets Mean                                  -0.962166\n",
      "trainer/Q Targets Std                                    0.819814\n",
      "trainer/Q Targets Max                                    1.35498\n",
      "trainer/Q Targets Min                                   -2.95045\n",
      "trainer/Log Pis Mean                                     2.04382\n",
      "trainer/Log Pis Std                                      1.42978\n",
      "trainer/Log Pis Max                                      4.44806\n",
      "trainer/Log Pis Min                                     -2.39223\n",
      "trainer/Policy mu Mean                                  -0.000802543\n",
      "trainer/Policy mu Std                                    0.307457\n",
      "trainer/Policy mu Max                                    1.90979\n",
      "trainer/Policy mu Min                                   -2.02329\n",
      "trainer/Policy log std Mean                             -2.34458\n",
      "trainer/Policy log std Std                               0.662366\n",
      "trainer/Policy log std Max                               0.0481424\n",
      "trainer/Policy log std Min                              -3.31804\n",
      "trainer/Alpha                                            0.0192732\n",
      "trainer/Alpha Loss                                       0.173013\n",
      "exploration/num steps total                           6600\n",
      "exploration/num paths total                            330\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117496\n",
      "exploration/Rewards Std                                  0.063569\n",
      "exploration/Rewards Max                                  0.0451169\n",
      "exploration/Rewards Min                                 -0.247655\n",
      "exploration/Returns Mean                                -2.34993\n",
      "exploration/Returns Std                                  0.624831\n",
      "exploration/Returns Max                                 -1.34727\n",
      "exploration/Returns Min                                 -2.96896\n",
      "exploration/Actions Mean                                -0.000525223\n",
      "exploration/Actions Std                                  0.119181\n",
      "exploration/Actions Max                                  0.423058\n",
      "exploration/Actions Min                                 -0.325603\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.34993\n",
      "exploration/env_infos/final/reward_dist Mean             1.00712e-06\n",
      "exploration/env_infos/final/reward_dist Std              1.99168e-06\n",
      "exploration/env_infos/final/reward_dist Max              4.99033e-06\n",
      "exploration/env_infos/final/reward_dist Min              2.603e-36\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000441546\n",
      "exploration/env_infos/initial/reward_dist Std            0.000449566\n",
      "exploration/env_infos/initial/reward_dist Max            0.00105514\n",
      "exploration/env_infos/initial/reward_dist Min            2.0424e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.103542\n",
      "exploration/env_infos/reward_dist Std                    0.214152\n",
      "exploration/env_infos/reward_dist Max                    0.839416\n",
      "exploration/env_infos/reward_dist Min                    2.603e-36\n",
      "exploration/env_infos/final/reward_energy Mean          -0.2155\n",
      "exploration/env_infos/final/reward_energy Std            0.114602\n",
      "exploration/env_infos/final/reward_energy Max           -0.0991813\n",
      "exploration/env_infos/final/reward_energy Min           -0.427818\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.135887\n",
      "exploration/env_infos/initial/reward_energy Std          0.143495\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0368113\n",
      "exploration/env_infos/initial/reward_energy Min         -0.420412\n",
      "exploration/env_infos/reward_energy Mean                -0.140978\n",
      "exploration/env_infos/reward_energy Std                  0.0923781\n",
      "exploration/env_infos/reward_energy Max                 -0.00730938\n",
      "exploration/env_infos/reward_energy Min                 -0.427818\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0445495\n",
      "exploration/env_infos/final/end_effector_loc Std         0.42262\n",
      "exploration/env_infos/final/end_effector_loc Max         0.549696\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.599863\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00143037\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00683917\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0180835\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.010717\n",
      "exploration/env_infos/end_effector_loc Mean              0.032159\n",
      "exploration/env_infos/end_effector_loc Std               0.224695\n",
      "exploration/env_infos/end_effector_loc Max               0.549696\n",
      "exploration/env_infos/end_effector_loc Min              -0.599863\n",
      "evaluation/num steps total                           56000\n",
      "evaluation/num paths total                            2800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0470331\n",
      "evaluation/Rewards Std                                   0.0671366\n",
      "evaluation/Rewards Max                                   0.132711\n",
      "evaluation/Rewards Min                                  -0.426529\n",
      "evaluation/Returns Mean                                 -0.940662\n",
      "evaluation/Returns Std                                   0.997079\n",
      "evaluation/Returns Max                                   1.4879\n",
      "evaluation/Returns Min                                  -3.32997\n",
      "evaluation/Actions Mean                                  0.000361326\n",
      "evaluation/Actions Std                                   0.0801792\n",
      "evaluation/Actions Max                                   0.866095\n",
      "evaluation/Actions Min                                  -0.772119\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.940662\n",
      "evaluation/env_infos/final/reward_dist Mean              0.11839\n",
      "evaluation/env_infos/final/reward_dist Std               0.251019\n",
      "evaluation/env_infos/final/reward_dist Max               0.953429\n",
      "evaluation/env_infos/final/reward_dist Min               9.1337e-174\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0112365\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0233353\n",
      "evaluation/env_infos/initial/reward_dist Max             0.124698\n",
      "evaluation/env_infos/initial/reward_dist Min             1.08684e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.123442\n",
      "evaluation/env_infos/reward_dist Std                     0.222107\n",
      "evaluation/env_infos/reward_dist Max                     0.980162\n",
      "evaluation/env_infos/reward_dist Min                     9.1337e-174\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0289896\n",
      "evaluation/env_infos/final/reward_energy Std             0.0319836\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00149514\n",
      "evaluation/env_infos/final/reward_energy Min            -0.164562\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.219918\n",
      "evaluation/env_infos/initial/reward_energy Std           0.275482\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00870761\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.992634\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0584122\n",
      "evaluation/env_infos/reward_energy Std                   0.0971889\n",
      "evaluation/env_infos/reward_energy Max                  -0.000842331\n",
      "evaluation/env_infos/reward_energy Min                  -0.992634\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.012842\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.311202\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.771357\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0002745\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0124596\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0433048\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0386059\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00214151\n",
      "evaluation/env_infos/end_effector_loc Std                0.193528\n",
      "evaluation/env_infos/end_effector_loc Max                0.771357\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.16759\n",
      "time/evaluation sampling (s)                             0.771306\n",
      "time/exploration sampling (s)                            0.108854\n",
      "time/logging (s)                                         0.0194391\n",
      "time/saving (s)                                          0.0271076\n",
      "time/training (s)                                       46.3967\n",
      "time/epoch (s)                                          47.491\n",
      "time/total (s)                                        2607.93\n",
      "Epoch                                                   55\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:56:46.137537 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 56 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00115764\n",
      "trainer/QF2 Loss                                         0.00130119\n",
      "trainer/Policy Loss                                      3.17569\n",
      "trainer/Q1 Predictions Mean                             -1.02359\n",
      "trainer/Q1 Predictions Std                               0.814576\n",
      "trainer/Q1 Predictions Max                               0.999042\n",
      "trainer/Q1 Predictions Min                              -3.35257\n",
      "trainer/Q2 Predictions Mean                             -1.00126\n",
      "trainer/Q2 Predictions Std                               0.80227\n",
      "trainer/Q2 Predictions Max                               1.0005\n",
      "trainer/Q2 Predictions Min                              -3.24757\n",
      "trainer/Q Targets Mean                                  -1.01681\n",
      "trainer/Q Targets Std                                    0.80808\n",
      "trainer/Q Targets Max                                    1.0504\n",
      "trainer/Q Targets Min                                   -3.30964\n",
      "trainer/Log Pis Mean                                     2.16493\n",
      "trainer/Log Pis Std                                      1.43123\n",
      "trainer/Log Pis Max                                      6.43724\n",
      "trainer/Log Pis Min                                     -2.78793\n",
      "trainer/Policy mu Mean                                   0.00852657\n",
      "trainer/Policy mu Std                                    0.390113\n",
      "trainer/Policy mu Max                                    2.6295\n",
      "trainer/Policy mu Min                                   -2.00187\n",
      "trainer/Policy log std Mean                             -2.33071\n",
      "trainer/Policy log std Std                               0.683611\n",
      "trainer/Policy log std Max                              -0.122156\n",
      "trainer/Policy log std Min                              -3.35047\n",
      "trainer/Alpha                                            0.0198784\n",
      "trainer/Alpha Loss                                       0.646526\n",
      "exploration/num steps total                           6700\n",
      "exploration/num paths total                            335\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0990339\n",
      "exploration/Rewards Std                                  0.0918832\n",
      "exploration/Rewards Max                                  0.103073\n",
      "exploration/Rewards Min                                 -0.402273\n",
      "exploration/Returns Mean                                -1.98068\n",
      "exploration/Returns Std                                  0.829886\n",
      "exploration/Returns Max                                 -0.959943\n",
      "exploration/Returns Min                                 -3.18177\n",
      "exploration/Actions Mean                                -0.00678772\n",
      "exploration/Actions Std                                  0.144394\n",
      "exploration/Actions Max                                  0.88034\n",
      "exploration/Actions Min                                 -0.682768\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.98068\n",
      "exploration/env_infos/final/reward_dist Mean             0.0229347\n",
      "exploration/env_infos/final/reward_dist Std              0.0307673\n",
      "exploration/env_infos/final/reward_dist Max              0.077188\n",
      "exploration/env_infos/final/reward_dist Min              1.00257e-41\n",
      "exploration/env_infos/initial/reward_dist Mean           0.012941\n",
      "exploration/env_infos/initial/reward_dist Std            0.0132707\n",
      "exploration/env_infos/initial/reward_dist Max            0.0333263\n",
      "exploration/env_infos/initial/reward_dist Min            8.63862e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0743002\n",
      "exploration/env_infos/reward_dist Std                    0.166218\n",
      "exploration/env_infos/reward_dist Max                    0.857987\n",
      "exploration/env_infos/reward_dist Min                    1.00257e-41\n",
      "exploration/env_infos/final/reward_energy Mean          -0.125847\n",
      "exploration/env_infos/final/reward_energy Std            0.0622849\n",
      "exploration/env_infos/final/reward_energy Max           -0.0593507\n",
      "exploration/env_infos/final/reward_energy Min           -0.231884\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.281775\n",
      "exploration/env_infos/initial/reward_energy Std          0.388279\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0199545\n",
      "exploration/env_infos/initial/reward_energy Min         -1.05042\n",
      "exploration/env_infos/reward_energy Mean                -0.138897\n",
      "exploration/env_infos/reward_energy Std                  0.149998\n",
      "exploration/env_infos/reward_energy Max                 -0.00669839\n",
      "exploration/env_infos/reward_energy Min                 -1.05042\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0928689\n",
      "exploration/env_infos/final/end_effector_loc Std         0.363501\n",
      "exploration/env_infos/final/end_effector_loc Max         0.631847\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.697836\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00251763\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0167737\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.044017\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0286525\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0383815\n",
      "exploration/env_infos/end_effector_loc Std               0.207639\n",
      "exploration/env_infos/end_effector_loc Max               0.631847\n",
      "exploration/env_infos/end_effector_loc Min              -0.697836\n",
      "evaluation/num steps total                           57000\n",
      "evaluation/num paths total                            2850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0632565\n",
      "evaluation/Rewards Std                                   0.0720024\n",
      "evaluation/Rewards Max                                   0.142553\n",
      "evaluation/Rewards Min                                  -0.436734\n",
      "evaluation/Returns Mean                                 -1.26513\n",
      "evaluation/Returns Std                                   1.00539\n",
      "evaluation/Returns Max                                   1.17742\n",
      "evaluation/Returns Min                                  -3.48869\n",
      "evaluation/Actions Mean                                 -0.00205844\n",
      "evaluation/Actions Std                                   0.0728399\n",
      "evaluation/Actions Max                                   0.915305\n",
      "evaluation/Actions Min                                  -0.8487\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.26513\n",
      "evaluation/env_infos/final/reward_dist Mean              0.128638\n",
      "evaluation/env_infos/final/reward_dist Std               0.232797\n",
      "evaluation/env_infos/final/reward_dist Max               0.959402\n",
      "evaluation/env_infos/final/reward_dist Min               4.46352e-146\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00726824\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0198929\n",
      "evaluation/env_infos/initial/reward_dist Max             0.116863\n",
      "evaluation/env_infos/initial/reward_dist Min             8.74277e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.116962\n",
      "evaluation/env_infos/reward_dist Std                     0.204023\n",
      "evaluation/env_infos/reward_dist Max                     0.999133\n",
      "evaluation/env_infos/reward_dist Min                     4.46352e-146\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0369407\n",
      "evaluation/env_infos/final/reward_energy Std             0.037739\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00880826\n",
      "evaluation/env_infos/final/reward_energy Min            -0.261202\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.240755\n",
      "evaluation/env_infos/initial/reward_energy Std           0.248817\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00506723\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.24823\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0557838\n",
      "evaluation/env_infos/reward_energy Std                   0.0866484\n",
      "evaluation/env_infos/reward_energy Max                  -0.00127297\n",
      "evaluation/env_infos/reward_energy Min                  -1.24823\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00770115\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.307783\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.676979\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.975478\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000633307\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122246\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0457652\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.042435\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00238574\n",
      "evaluation/env_infos/end_effector_loc Std                0.183747\n",
      "evaluation/env_infos/end_effector_loc Max                0.676979\n",
      "evaluation/env_infos/end_effector_loc Min               -0.975478\n",
      "time/data storing (s)                                    0.163561\n",
      "time/evaluation sampling (s)                             0.803658\n",
      "time/exploration sampling (s)                            0.110535\n",
      "time/logging (s)                                         0.0187572\n",
      "time/saving (s)                                          0.0266015\n",
      "time/training (s)                                       46.5903\n",
      "time/epoch (s)                                          47.7134\n",
      "time/total (s)                                        2656.18\n",
      "Epoch                                                   56\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:57:34.210457 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 57 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00210147\r\n",
      "trainer/QF2 Loss                                         0.00192353\r\n",
      "trainer/Policy Loss                                      2.90284\r\n",
      "trainer/Q1 Predictions Mean                             -0.969891\r\n",
      "trainer/Q1 Predictions Std                               0.833097\r\n",
      "trainer/Q1 Predictions Max                               1.33374\r\n",
      "trainer/Q1 Predictions Min                              -3.33951\r\n",
      "trainer/Q2 Predictions Mean                             -0.968514\r\n",
      "trainer/Q2 Predictions Std                               0.822646\r\n",
      "trainer/Q2 Predictions Max                               1.32447\r\n",
      "trainer/Q2 Predictions Min                              -3.31945\r\n",
      "trainer/Q Targets Mean                                  -0.980794\r\n",
      "trainer/Q Targets Std                                    0.821473\r\n",
      "trainer/Q Targets Max                                    1.29904\r\n",
      "trainer/Q Targets Min                                   -3.28\r\n",
      "trainer/Log Pis Mean                                     1.95188\r\n",
      "trainer/Log Pis Std                                      1.50544\r\n",
      "trainer/Log Pis Max                                      7.06976\r\n",
      "trainer/Log Pis Min                                     -2.25483\r\n",
      "trainer/Policy mu Mean                                   0.00309846\r\n",
      "trainer/Policy mu Std                                    0.437501\r\n",
      "trainer/Policy mu Max                                    2.695\r\n",
      "trainer/Policy mu Min                                   -2.44038\r\n",
      "trainer/Policy log std Mean                             -2.24274\r\n",
      "trainer/Policy log std Std                               0.68792\r\n",
      "trainer/Policy log std Max                              -0.0621926\r\n",
      "trainer/Policy log std Min                              -3.89496\r\n",
      "trainer/Alpha                                            0.0197668\r\n",
      "trainer/Alpha Loss                                      -0.188845\r\n",
      "exploration/num steps total                           6800\r\n",
      "exploration/num paths total                            340\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.127738\r\n",
      "exploration/Rewards Std                                  0.094683\r\n",
      "exploration/Rewards Max                                  0.0699141\r\n",
      "exploration/Rewards Min                                 -0.657042\r\n",
      "exploration/Returns Mean                                -2.55475\r\n",
      "exploration/Returns Std                                  1.17311\r\n",
      "exploration/Returns Max                                 -0.864777\r\n",
      "exploration/Returns Min                                 -4.3992\r\n",
      "exploration/Actions Mean                                 0.00989452\r\n",
      "exploration/Actions Std                                  0.137981\r\n",
      "exploration/Actions Max                                  0.730225\r\n",
      "exploration/Actions Min                                 -0.367749\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.55475\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0458628\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0912417\r\n",
      "exploration/env_infos/final/reward_dist Max              0.228345\r\n",
      "exploration/env_infos/final/reward_dist Min              6.45932e-49\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00229583\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0024627\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00709093\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.00028444\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0582086\r\n",
      "exploration/env_infos/reward_dist Std                    0.126176\r\n",
      "exploration/env_infos/reward_dist Max                    0.763748\r\n",
      "exploration/env_infos/reward_dist Min                    5.21777e-50\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0634269\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0286979\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0280777\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.110462\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.377166\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.272322\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0718745\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.730799\r\n",
      "exploration/env_infos/reward_energy Mean                -0.145828\r\n",
      "exploration/env_infos/reward_energy Std                  0.130412\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0132702\r\n",
      "exploration/env_infos/reward_energy Min                 -0.730799\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.149233\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.348734\r\n",
      "exploration/env_infos/final/end_effector_loc Max         1\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.31523\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00856168\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0140433\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0365112\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0122962\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.098815\r\n",
      "exploration/env_infos/end_effector_loc Std               0.233221\r\n",
      "exploration/env_infos/end_effector_loc Max               1\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.31523\r\n",
      "evaluation/num steps total                           58000\r\n",
      "evaluation/num paths total                            2900\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0787508\r\n",
      "evaluation/Rewards Std                                   0.0766965\r\n",
      "evaluation/Rewards Max                                   0.115043\r\n",
      "evaluation/Rewards Min                                  -0.474671\r\n",
      "evaluation/Returns Mean                                 -1.57502\r\n",
      "evaluation/Returns Std                                   1.03647\r\n",
      "evaluation/Returns Max                                   1.29276\r\n",
      "evaluation/Returns Min                                  -3.74101\r\n",
      "evaluation/Actions Mean                                  0.00958409\r\n",
      "evaluation/Actions Std                                   0.0757043\r\n",
      "evaluation/Actions Max                                   0.779124\r\n",
      "evaluation/Actions Min                                  -0.721916\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.57502\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0397911\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.136598\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.814124\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.43651e-65\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00669417\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0180346\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.120333\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.39764e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0815068\r\n",
      "evaluation/env_infos/reward_dist Std                     0.175625\r\n",
      "evaluation/env_infos/reward_dist Max                     0.927402\r\n",
      "evaluation/env_infos/reward_dist Min                     5.43651e-65\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0292181\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0261619\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0118058\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.126578\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.233872\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.258702\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.014564\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.974522\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0625704\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0879257\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00356214\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.974522\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.154857\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.314642\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.987252\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.648174\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00331852\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.011875\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0389562\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0360958\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.074532\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.194927\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.987252\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.648174\r\n",
      "time/data storing (s)                                    0.165228\r\n",
      "time/evaluation sampling (s)                             0.776962\r\n",
      "time/exploration sampling (s)                            0.106685\r\n",
      "time/logging (s)                                         0.0192185\r\n",
      "time/saving (s)                                          0.0270718\r\n",
      "time/training (s)                                       46.4207\r\n",
      "time/epoch (s)                                          47.5159\r\n",
      "time/total (s)                                        2704.25\r\n",
      "Epoch                                                   57\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:58:23.278569 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 58 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00184059\n",
      "trainer/QF2 Loss                                         0.00575\n",
      "trainer/Policy Loss                                      2.88952\n",
      "trainer/Q1 Predictions Mean                             -0.964069\n",
      "trainer/Q1 Predictions Std                               0.840193\n",
      "trainer/Q1 Predictions Max                               1.45593\n",
      "trainer/Q1 Predictions Min                              -3.27062\n",
      "trainer/Q2 Predictions Mean                             -0.953143\n",
      "trainer/Q2 Predictions Std                               0.832525\n",
      "trainer/Q2 Predictions Max                               1.47658\n",
      "trainer/Q2 Predictions Min                              -3.30099\n",
      "trainer/Q Targets Mean                                  -0.964894\n",
      "trainer/Q Targets Std                                    0.839064\n",
      "trainer/Q Targets Max                                    1.48591\n",
      "trainer/Q Targets Min                                   -3.25885\n",
      "trainer/Log Pis Mean                                     1.93306\n",
      "trainer/Log Pis Std                                      1.5376\n",
      "trainer/Log Pis Max                                      4.50253\n",
      "trainer/Log Pis Min                                     -3.13127\n",
      "trainer/Policy mu Mean                                  -0.0379953\n",
      "trainer/Policy mu Std                                    0.362721\n",
      "trainer/Policy mu Max                                    2.34611\n",
      "trainer/Policy mu Min                                   -2.20577\n",
      "trainer/Policy log std Mean                             -2.28857\n",
      "trainer/Policy log std Std                               0.655905\n",
      "trainer/Policy log std Max                              -0.283302\n",
      "trainer/Policy log std Min                              -3.42083\n",
      "trainer/Alpha                                            0.0198451\n",
      "trainer/Alpha Loss                                      -0.262393\n",
      "exploration/num steps total                           6900\n",
      "exploration/num paths total                            345\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0680982\n",
      "exploration/Rewards Std                                  0.0919352\n",
      "exploration/Rewards Max                                  0.143712\n",
      "exploration/Rewards Min                                 -0.259572\n",
      "exploration/Returns Mean                                -1.36196\n",
      "exploration/Returns Std                                  1.49229\n",
      "exploration/Returns Max                                  1.16161\n",
      "exploration/Returns Min                                 -3.38137\n",
      "exploration/Actions Mean                                -0.0105333\n",
      "exploration/Actions Std                                  0.114926\n",
      "exploration/Actions Max                                  0.435921\n",
      "exploration/Actions Min                                 -0.407137\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.36196\n",
      "exploration/env_infos/final/reward_dist Mean             0.093408\n",
      "exploration/env_infos/final/reward_dist Std              0.186812\n",
      "exploration/env_infos/final/reward_dist Max              0.467032\n",
      "exploration/env_infos/final/reward_dist Min              1.92505e-41\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00755368\n",
      "exploration/env_infos/initial/reward_dist Std            0.00972587\n",
      "exploration/env_infos/initial/reward_dist Max            0.0242203\n",
      "exploration/env_infos/initial/reward_dist Min            4.18302e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.14393\n",
      "exploration/env_infos/reward_dist Std                    0.26699\n",
      "exploration/env_infos/reward_dist Max                    0.969938\n",
      "exploration/env_infos/reward_dist Min                    1.92505e-41\n",
      "exploration/env_infos/final/reward_energy Mean          -0.125666\n",
      "exploration/env_infos/final/reward_energy Std            0.0722963\n",
      "exploration/env_infos/final/reward_energy Max           -0.0441598\n",
      "exploration/env_infos/final/reward_energy Min           -0.258861\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.178743\n",
      "exploration/env_infos/initial/reward_energy Std          0.102985\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0395888\n",
      "exploration/env_infos/initial/reward_energy Min         -0.342426\n",
      "exploration/env_infos/reward_energy Mean                -0.131902\n",
      "exploration/env_infos/reward_energy Std                  0.0961228\n",
      "exploration/env_infos/reward_energy Max                 -0.00248172\n",
      "exploration/env_infos/reward_energy Min                 -0.45194\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0445898\n",
      "exploration/env_infos/final/end_effector_loc Std         0.310985\n",
      "exploration/env_infos/final/end_effector_loc Max         0.49015\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.585432\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00197876\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00701985\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0155568\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00720617\n",
      "exploration/env_infos/end_effector_loc Mean              0.00590448\n",
      "exploration/env_infos/end_effector_loc Std               0.175297\n",
      "exploration/env_infos/end_effector_loc Max               0.49015\n",
      "exploration/env_infos/end_effector_loc Min              -0.585432\n",
      "evaluation/num steps total                           59000\n",
      "evaluation/num paths total                            2950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0780378\n",
      "evaluation/Rewards Std                                   0.118007\n",
      "evaluation/Rewards Max                                   0.168642\n",
      "evaluation/Rewards Min                                  -0.877773\n",
      "evaluation/Returns Mean                                 -1.56076\n",
      "evaluation/Returns Std                                   1.82717\n",
      "evaluation/Returns Max                                   2.00054\n",
      "evaluation/Returns Min                                  -8.05811\n",
      "evaluation/Actions Mean                                 -0.00240063\n",
      "evaluation/Actions Std                                   0.101403\n",
      "evaluation/Actions Max                                   0.764494\n",
      "evaluation/Actions Min                                  -0.995683\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.56076\n",
      "evaluation/env_infos/final/reward_dist Mean              0.057832\n",
      "evaluation/env_infos/final/reward_dist Std               0.141494\n",
      "evaluation/env_infos/final/reward_dist Max               0.553733\n",
      "evaluation/env_infos/final/reward_dist Min               1.64586e-116\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00569697\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0119924\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0579123\n",
      "evaluation/env_infos/initial/reward_dist Min             1.17782e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0978411\n",
      "evaluation/env_infos/reward_dist Std                     0.200859\n",
      "evaluation/env_infos/reward_dist Max                     0.997089\n",
      "evaluation/env_infos/reward_dist Min                     1.64586e-116\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0284898\n",
      "evaluation/env_infos/final/reward_energy Std             0.0334018\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00243916\n",
      "evaluation/env_infos/final/reward_energy Min            -0.171169\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.301808\n",
      "evaluation/env_infos/initial/reward_energy Std           0.30654\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00316237\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.25532\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0776081\n",
      "evaluation/env_infos/reward_energy Std                   0.120639\n",
      "evaluation/env_infos/reward_energy Max                  -0.000828617\n",
      "evaluation/env_infos/reward_energy Min                  -1.25532\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0321718\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.360399\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.996174\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.99605\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000977928\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0151777\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0382247\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0497842\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0153198\n",
      "evaluation/env_infos/end_effector_loc Std                0.222003\n",
      "evaluation/env_infos/end_effector_loc Max                0.996174\n",
      "evaluation/env_infos/end_effector_loc Min               -0.99605\n",
      "time/data storing (s)                                    0.176213\n",
      "time/evaluation sampling (s)                             0.836938\n",
      "time/exploration sampling (s)                            0.120723\n",
      "time/logging (s)                                         0.0191511\n",
      "time/saving (s)                                          0.0258531\n",
      "time/training (s)                                       47.29\n",
      "time/epoch (s)                                          48.4689\n",
      "time/total (s)                                        2753.32\n",
      "Epoch                                                   58\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:59:11.130850 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 59 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00304762\r\n",
      "trainer/QF2 Loss                                         0.00174929\r\n",
      "trainer/Policy Loss                                      3.04516\r\n",
      "trainer/Q1 Predictions Mean                             -1.03742\r\n",
      "trainer/Q1 Predictions Std                               0.822831\r\n",
      "trainer/Q1 Predictions Max                               1.26625\r\n",
      "trainer/Q1 Predictions Min                              -3.37864\r\n",
      "trainer/Q2 Predictions Mean                             -1.03409\r\n",
      "trainer/Q2 Predictions Std                               0.83172\r\n",
      "trainer/Q2 Predictions Max                               1.31259\r\n",
      "trainer/Q2 Predictions Min                              -3.36687\r\n",
      "trainer/Q Targets Mean                                  -1.02905\r\n",
      "trainer/Q Targets Std                                    0.828039\r\n",
      "trainer/Q Targets Max                                    1.35627\r\n",
      "trainer/Q Targets Min                                   -3.39472\r\n",
      "trainer/Log Pis Mean                                     2.02218\r\n",
      "trainer/Log Pis Std                                      1.5757\r\n",
      "trainer/Log Pis Max                                      5.55332\r\n",
      "trainer/Log Pis Min                                     -5.29699\r\n",
      "trainer/Policy mu Mean                                   0.0296851\r\n",
      "trainer/Policy mu Std                                    0.358014\r\n",
      "trainer/Policy mu Max                                    2.59077\r\n",
      "trainer/Policy mu Min                                   -2.43296\r\n",
      "trainer/Policy log std Mean                             -2.30121\r\n",
      "trainer/Policy log std Std                               0.694884\r\n",
      "trainer/Policy log std Max                              -0.122941\r\n",
      "trainer/Policy log std Min                              -3.42854\r\n",
      "trainer/Alpha                                            0.0191801\r\n",
      "trainer/Alpha Loss                                       0.0876612\r\n",
      "exploration/num steps total                           7000\r\n",
      "exploration/num paths total                            350\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.102625\r\n",
      "exploration/Rewards Std                                  0.0712676\r\n",
      "exploration/Rewards Max                                  0.0256944\r\n",
      "exploration/Rewards Min                                 -0.34325\r\n",
      "exploration/Returns Mean                                -2.0525\r\n",
      "exploration/Returns Std                                  0.899044\r\n",
      "exploration/Returns Max                                 -1.07913\r\n",
      "exploration/Returns Min                                 -3.39558\r\n",
      "exploration/Actions Mean                                 0.00527815\r\n",
      "exploration/Actions Std                                  0.128094\r\n",
      "exploration/Actions Max                                  0.545069\r\n",
      "exploration/Actions Min                                 -0.653521\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.0525\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.131462\r\n",
      "exploration/env_infos/final/reward_dist Std              0.121693\r\n",
      "exploration/env_infos/final/reward_dist Max              0.323298\r\n",
      "exploration/env_infos/final/reward_dist Min              7.52741e-26\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0015701\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00179796\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00477858\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.88437e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.121802\r\n",
      "exploration/env_infos/reward_dist Std                    0.226043\r\n",
      "exploration/env_infos/reward_dist Max                    0.983669\r\n",
      "exploration/env_infos/reward_dist Min                    7.52741e-26\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.140913\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0927449\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0536636\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.30841\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.271157\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.257936\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0558589\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.714277\r\n",
      "exploration/env_infos/reward_energy Mean                -0.123697\r\n",
      "exploration/env_infos/reward_energy Std                  0.132554\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00639585\r\n",
      "exploration/env_infos/reward_energy Min                 -0.714277\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0981311\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.309869\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.826556\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.362558\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00210654\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0130627\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0197461\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.032676\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0423947\r\n",
      "exploration/env_infos/end_effector_loc Std               0.191507\r\n",
      "exploration/env_infos/end_effector_loc Max               0.826556\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.36339\r\n",
      "evaluation/num steps total                           60000\r\n",
      "evaluation/num paths total                            3000\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0711769\r\n",
      "evaluation/Rewards Std                                   0.0929224\r\n",
      "evaluation/Rewards Max                                   0.148752\r\n",
      "evaluation/Rewards Min                                  -0.599864\r\n",
      "evaluation/Returns Mean                                 -1.42354\r\n",
      "evaluation/Returns Std                                   1.44706\r\n",
      "evaluation/Returns Max                                   1.74887\r\n",
      "evaluation/Returns Min                                  -5.24303\r\n",
      "evaluation/Actions Mean                                  0.00152763\r\n",
      "evaluation/Actions Std                                   0.0797688\r\n",
      "evaluation/Actions Max                                   0.878409\r\n",
      "evaluation/Actions Min                                  -0.968627\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.42354\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0880773\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.195748\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.982644\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.22868e-106\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/reward_dist Mean            0.00533359\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00862165\n",
      "evaluation/env_infos/initial/reward_dist Max             0.041111\n",
      "evaluation/env_infos/initial/reward_dist Min             8.86369e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0995578\n",
      "evaluation/env_infos/reward_dist Std                     0.21203\n",
      "evaluation/env_infos/reward_dist Max                     0.989345\n",
      "evaluation/env_infos/reward_dist Min                     2.22868e-106\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0283557\n",
      "evaluation/env_infos/final/reward_energy Std             0.0381386\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00332063\n",
      "evaluation/env_infos/final/reward_energy Min            -0.218991\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.250676\n",
      "evaluation/env_infos/initial/reward_energy Std           0.299394\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00250722\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.12394\n",
      "evaluation/env_infos/reward_energy Mean                 -0.054651\n",
      "evaluation/env_infos/reward_energy Std                   0.098712\n",
      "evaluation/env_infos/reward_energy Max                  -0.00173461\n",
      "evaluation/env_infos/reward_energy Min                  -1.12394\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0265969\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.363501\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00121412\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0137521\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0439204\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0484313\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0166347\n",
      "evaluation/env_infos/end_effector_loc Std                0.227428\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.161849\n",
      "time/evaluation sampling (s)                             0.801942\n",
      "time/exploration sampling (s)                            0.105792\n",
      "time/logging (s)                                         0.0187589\n",
      "time/saving (s)                                          0.0261085\n",
      "time/training (s)                                       46.1894\n",
      "time/epoch (s)                                          47.3039\n",
      "time/total (s)                                        2801.17\n",
      "Epoch                                                   59\n",
      "---------------------------------------------------  ----------------\n",
      "2021-05-25 13:59:59.217769 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 60 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000851304\n",
      "trainer/QF2 Loss                                         0.000915504\n",
      "trainer/Policy Loss                                      3.1057\n",
      "trainer/Q1 Predictions Mean                             -1.05222\n",
      "trainer/Q1 Predictions Std                               0.806036\n",
      "trainer/Q1 Predictions Max                               0.758872\n",
      "trainer/Q1 Predictions Min                              -3.40804\n",
      "trainer/Q2 Predictions Mean                             -1.0565\n",
      "trainer/Q2 Predictions Std                               0.807972\n",
      "trainer/Q2 Predictions Max                               0.733944\n",
      "trainer/Q2 Predictions Min                              -3.45939\n",
      "trainer/Q Targets Mean                                  -1.05655\n",
      "trainer/Q Targets Std                                    0.803195\n",
      "trainer/Q Targets Max                                    0.755261\n",
      "trainer/Q Targets Min                                   -3.4265\n",
      "trainer/Log Pis Mean                                     2.06038\n",
      "trainer/Log Pis Std                                      1.4088\n",
      "trainer/Log Pis Max                                      4.72192\n",
      "trainer/Log Pis Min                                     -3.21287\n",
      "trainer/Policy mu Mean                                   0.0102308\n",
      "trainer/Policy mu Std                                    0.410497\n",
      "trainer/Policy mu Max                                    2.363\n",
      "trainer/Policy mu Min                                   -2.92846\n",
      "trainer/Policy log std Mean                             -2.27742\n",
      "trainer/Policy log std Std                               0.731744\n",
      "trainer/Policy log std Max                               0.131539\n",
      "trainer/Policy log std Min                              -3.48691\n",
      "trainer/Alpha                                            0.0203702\n",
      "trainer/Alpha Loss                                       0.235055\n",
      "exploration/num steps total                           7100\n",
      "exploration/num paths total                            355\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0967365\n",
      "exploration/Rewards Std                                  0.0591718\n",
      "exploration/Rewards Max                                  0.00680483\n",
      "exploration/Rewards Min                                 -0.32118\n",
      "exploration/Returns Mean                                -1.93473\n",
      "exploration/Returns Std                                  0.842618\n",
      "exploration/Returns Max                                 -0.942286\n",
      "exploration/Returns Min                                 -3.37079\n",
      "exploration/Actions Mean                                -0.00542649\n",
      "exploration/Actions Std                                  0.133675\n",
      "exploration/Actions Max                                  0.918312\n",
      "exploration/Actions Min                                 -0.471796\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.93473\n",
      "exploration/env_infos/final/reward_dist Mean             0.122255\n",
      "exploration/env_infos/final/reward_dist Std              0.186363\n",
      "exploration/env_infos/final/reward_dist Max              0.482462\n",
      "exploration/env_infos/final/reward_dist Min              2.24653e-09\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00287525\n",
      "exploration/env_infos/initial/reward_dist Std            0.00317838\n",
      "exploration/env_infos/initial/reward_dist Max            0.0079267\n",
      "exploration/env_infos/initial/reward_dist Min            4.58413e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0323658\n",
      "exploration/env_infos/reward_dist Std                    0.0865021\n",
      "exploration/env_infos/reward_dist Max                    0.482462\n",
      "exploration/env_infos/reward_dist Min                    2.24653e-09\n",
      "exploration/env_infos/final/reward_energy Mean          -0.167496\n",
      "exploration/env_infos/final/reward_energy Std            0.144005\n",
      "exploration/env_infos/final/reward_energy Max           -0.0118317\n",
      "exploration/env_infos/final/reward_energy Min           -0.436874\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.295133\n",
      "exploration/env_infos/initial/reward_energy Std          0.3972\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0636978\n",
      "exploration/env_infos/initial/reward_energy Min         -1.08815\n",
      "exploration/env_infos/reward_energy Mean                -0.120902\n",
      "exploration/env_infos/reward_energy Std                  0.145532\n",
      "exploration/env_infos/reward_energy Max                 -0.0103725\n",
      "exploration/env_infos/reward_energy Min                 -1.08815\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0248172\n",
      "exploration/env_infos/final/end_effector_loc Std         0.20717\n",
      "exploration/env_infos/final/end_effector_loc Max         0.322666\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.395039\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00754908\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0157829\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0459156\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00344262\n",
      "exploration/env_infos/end_effector_loc Mean              0.0146562\n",
      "exploration/env_infos/end_effector_loc Std               0.128865\n",
      "exploration/env_infos/end_effector_loc Max               0.322666\n",
      "exploration/env_infos/end_effector_loc Min              -0.395039\n",
      "evaluation/num steps total                           61000\n",
      "evaluation/num paths total                            3050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0721956\n",
      "evaluation/Rewards Std                                   0.10475\n",
      "evaluation/Rewards Max                                   0.138475\n",
      "evaluation/Rewards Min                                  -0.698077\n",
      "evaluation/Returns Mean                                 -1.44391\n",
      "evaluation/Returns Std                                   1.51794\n",
      "evaluation/Returns Max                                   1.1916\n",
      "evaluation/Returns Min                                  -5.71758\n",
      "evaluation/Actions Mean                                  0.00119286\n",
      "evaluation/Actions Std                                   0.10485\n",
      "evaluation/Actions Max                                   0.836456\n",
      "evaluation/Actions Min                                  -0.941803\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.44391\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0832631\n",
      "evaluation/env_infos/final/reward_dist Std               0.167839\n",
      "evaluation/env_infos/final/reward_dist Max               0.677691\n",
      "evaluation/env_infos/final/reward_dist Min               2.46583e-98\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00481965\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00991573\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0488609\n",
      "evaluation/env_infos/initial/reward_dist Min             8.4307e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.102648\n",
      "evaluation/env_infos/reward_dist Std                     0.208195\n",
      "evaluation/env_infos/reward_dist Max                     0.987581\n",
      "evaluation/env_infos/reward_dist Min                     2.46583e-98\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0291433\n",
      "evaluation/env_infos/final/reward_energy Std             0.0515636\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000799669\n",
      "evaluation/env_infos/final/reward_energy Min            -0.253153\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.287964\n",
      "evaluation/env_infos/initial/reward_energy Std           0.313976\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00782049\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.25962\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0777292\n",
      "evaluation/env_infos/reward_energy Std                   0.126286\n",
      "evaluation/env_infos/reward_energy Max                  -0.000799669\n",
      "evaluation/env_infos/reward_energy Min                  -1.25962\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0292824\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.354953\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00174479\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0149611\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0418228\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0470902\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00107656\n",
      "evaluation/env_infos/end_effector_loc Std                0.2289\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.159159\n",
      "time/evaluation sampling (s)                             0.786698\n",
      "time/exploration sampling (s)                            0.10324\n",
      "time/logging (s)                                         0.0187614\n",
      "time/saving (s)                                          0.0287139\n",
      "time/training (s)                                       46.4137\n",
      "time/epoch (s)                                          47.5103\n",
      "time/total (s)                                        2849.25\n",
      "Epoch                                                   60\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:00:47.622846 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 61 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00101856\r\n",
      "trainer/QF2 Loss                                         0.00104101\r\n",
      "trainer/Policy Loss                                      2.97594\r\n",
      "trainer/Q1 Predictions Mean                             -0.999702\r\n",
      "trainer/Q1 Predictions Std                               0.776755\r\n",
      "trainer/Q1 Predictions Max                               1.51826\r\n",
      "trainer/Q1 Predictions Min                              -3.5695\r\n",
      "trainer/Q2 Predictions Mean                             -0.988367\r\n",
      "trainer/Q2 Predictions Std                               0.774571\r\n",
      "trainer/Q2 Predictions Max                               1.52726\r\n",
      "trainer/Q2 Predictions Min                              -3.53124\r\n",
      "trainer/Q Targets Mean                                  -0.992413\r\n",
      "trainer/Q Targets Std                                    0.783093\r\n",
      "trainer/Q Targets Max                                    1.55715\r\n",
      "trainer/Q Targets Min                                   -3.5477\r\n",
      "trainer/Log Pis Mean                                     1.98543\r\n",
      "trainer/Log Pis Std                                      1.39947\r\n",
      "trainer/Log Pis Max                                      4.57553\r\n",
      "trainer/Log Pis Min                                     -2.93086\r\n",
      "trainer/Policy mu Mean                                   0.00447644\r\n",
      "trainer/Policy mu Std                                    0.339056\r\n",
      "trainer/Policy mu Max                                    2.29427\r\n",
      "trainer/Policy mu Min                                   -2.40257\r\n",
      "trainer/Policy log std Mean                             -2.29723\r\n",
      "trainer/Policy log std Std                               0.667168\r\n",
      "trainer/Policy log std Max                              -0.154444\r\n",
      "trainer/Policy log std Min                              -3.50252\r\n",
      "trainer/Alpha                                            0.0210364\r\n",
      "trainer/Alpha Loss                                      -0.0562688\r\n",
      "exploration/num steps total                           7200\r\n",
      "exploration/num paths total                            360\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.117969\r\n",
      "exploration/Rewards Std                                  0.0505931\r\n",
      "exploration/Rewards Max                                 -0.0390378\r\n",
      "exploration/Rewards Min                                 -0.29627\r\n",
      "exploration/Returns Mean                                -2.35937\r\n",
      "exploration/Returns Std                                  0.705068\r\n",
      "exploration/Returns Max                                 -1.51256\r\n",
      "exploration/Returns Min                                 -3.65813\r\n",
      "exploration/Actions Mean                                 0.00507132\r\n",
      "exploration/Actions Std                                  0.0959602\r\n",
      "exploration/Actions Max                                  0.391032\r\n",
      "exploration/Actions Min                                 -0.341823\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.35937\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0489278\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0858955\r\n",
      "exploration/env_infos/final/reward_dist Max              0.21978\r\n",
      "exploration/env_infos/final/reward_dist Min              1.43197e-19\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000137468\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000224078\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.000584122\r\n",
      "exploration/env_infos/initial/reward_dist Min            9.72861e-07\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0345611\r\n",
      "exploration/env_infos/reward_dist Std                    0.0954866\r\n",
      "exploration/env_infos/reward_dist Max                    0.506975\r\n",
      "exploration/env_infos/reward_dist Min                    1.43197e-19\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.139276\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0540153\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0678513\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.20854\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.145612\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.044816\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0800402\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.209734\r\n",
      "exploration/env_infos/reward_energy Mean                -0.109966\r\n",
      "exploration/env_infos/reward_energy Std                  0.0798473\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00794503\r\n",
      "exploration/env_infos/reward_energy Min                 -0.396308\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00994262\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.149696\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.214453\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.278561\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000459638\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00536683\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0101502\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0069803\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00939958\r\n",
      "exploration/env_infos/end_effector_loc Std               0.0997002\r\n",
      "exploration/env_infos/end_effector_loc Max               0.214453\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.278561\r\n",
      "evaluation/num steps total                           62000\r\n",
      "evaluation/num paths total                            3100\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0660838\r\n",
      "evaluation/Rewards Std                                   0.091061\r\n",
      "evaluation/Rewards Max                                   0.124457\r\n",
      "evaluation/Rewards Min                                  -0.757337\r\n",
      "evaluation/Returns Mean                                 -1.32168\r\n",
      "evaluation/Returns Std                                   1.30002\r\n",
      "evaluation/Returns Max                                   0.959017\r\n",
      "evaluation/Returns Min                                  -5.30221\r\n",
      "evaluation/Actions Mean                                  0.00297643\r\n",
      "evaluation/Actions Std                                   0.110076\r\n",
      "evaluation/Actions Max                                   0.878762\r\n",
      "evaluation/Actions Min                                  -0.974813\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.32168\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.115164\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.234723\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.857532\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.36434e-84\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00952383\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0214759\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.13629\r\n",
      "evaluation/env_infos/initial/reward_dist Min             8.74643e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0881996\r\n",
      "evaluation/env_infos/reward_dist Std                     0.18493\r\n",
      "evaluation/env_infos/reward_dist Max                     0.988646\r\n",
      "evaluation/env_infos/reward_dist Min                     9.36434e-84\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0429579\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0674603\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00261031\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.411876\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.310747\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.366741\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00946628\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.29566\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0741611\r\n",
      "evaluation/env_infos/reward_energy Std                   0.136936\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000730733\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.29566\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0247067\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.356052\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       9.12642e-05\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0169947\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0439381\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0487407\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0131649\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.226668\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.164842\r\n",
      "time/evaluation sampling (s)                             0.797299\r\n",
      "time/exploration sampling (s)                            0.106145\r\n",
      "time/logging (s)                                         0.0184885\r\n",
      "time/saving (s)                                          0.0265323\r\n",
      "time/training (s)                                       46.6765\r\n",
      "time/epoch (s)                                          47.7899\r\n",
      "time/total (s)                                        2897.66\r\n",
      "Epoch                                                   61\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:01:35.680587 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 62 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00114571\r\n",
      "trainer/QF2 Loss                                         0.000975294\r\n",
      "trainer/Policy Loss                                      2.79352\r\n",
      "trainer/Q1 Predictions Mean                             -0.926199\r\n",
      "trainer/Q1 Predictions Std                               0.810606\r\n",
      "trainer/Q1 Predictions Max                               0.831909\r\n",
      "trainer/Q1 Predictions Min                              -3.69216\r\n",
      "trainer/Q2 Predictions Mean                             -0.908882\r\n",
      "trainer/Q2 Predictions Std                               0.810841\r\n",
      "trainer/Q2 Predictions Max                               0.870796\r\n",
      "trainer/Q2 Predictions Min                              -3.66753\r\n",
      "trainer/Q Targets Mean                                  -0.915005\r\n",
      "trainer/Q Targets Std                                    0.812402\r\n",
      "trainer/Q Targets Max                                    0.844203\r\n",
      "trainer/Q Targets Min                                   -3.69374\r\n",
      "trainer/Log Pis Mean                                     1.87194\r\n",
      "trainer/Log Pis Std                                      1.34286\r\n",
      "trainer/Log Pis Max                                      5.8427\r\n",
      "trainer/Log Pis Min                                     -2.9138\r\n",
      "trainer/Policy mu Mean                                   0.0236174\r\n",
      "trainer/Policy mu Std                                    0.322533\r\n",
      "trainer/Policy mu Max                                    2.64231\r\n",
      "trainer/Policy mu Min                                   -1.97651\r\n",
      "trainer/Policy log std Mean                             -2.31129\r\n",
      "trainer/Policy log std Std                               0.603706\r\n",
      "trainer/Policy log std Max                              -0.182327\r\n",
      "trainer/Policy log std Min                              -3.41518\r\n",
      "trainer/Alpha                                            0.0221938\r\n",
      "trainer/Alpha Loss                                      -0.487668\r\n",
      "exploration/num steps total                           7300\r\n",
      "exploration/num paths total                            365\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.146665\r\n",
      "exploration/Rewards Std                                  0.113516\r\n",
      "exploration/Rewards Max                                  0.108014\r\n",
      "exploration/Rewards Min                                 -0.40141\r\n",
      "exploration/Returns Mean                                -2.93329\r\n",
      "exploration/Returns Std                                  1.63569\r\n",
      "exploration/Returns Max                                 -0.979668\r\n",
      "exploration/Returns Min                                 -4.9816\r\n",
      "exploration/Actions Mean                                 0.00562543\r\n",
      "exploration/Actions Std                                  0.214103\r\n",
      "exploration/Actions Max                                  0.776363\r\n",
      "exploration/Actions Min                                 -0.59019\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.93329\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0718258\r\n",
      "exploration/env_infos/final/reward_dist Std              0.14365\r\n",
      "exploration/env_infos/final/reward_dist Max              0.359126\r\n",
      "exploration/env_infos/final/reward_dist Min              1.20864e-43\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00183802\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00301357\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00783906\r\n",
      "exploration/env_infos/initial/reward_dist Min            7.17028e-07\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0264237\r\n",
      "exploration/env_infos/reward_dist Std                    0.0722433\r\n",
      "exploration/env_infos/reward_dist Max                    0.359126\r\n",
      "exploration/env_infos/reward_dist Min                    1.20864e-43\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.17007\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0720289\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.119917\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.308179\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.53751\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.240847\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.235159\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.910513\r\n",
      "exploration/env_infos/reward_energy Mean                -0.247327\r\n",
      "exploration/env_infos/reward_energy Std                  0.17485\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0265707\r\n",
      "exploration/env_infos/reward_energy Min                 -0.910513\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.123533\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.414513\r\n",
      "exploration/env_infos/final/end_effector_loc Max         1\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.376817\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00221851\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0207059\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0388181\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0295095\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0814308\r\n",
      "exploration/env_infos/end_effector_loc Std               0.302785\r\n",
      "exploration/env_infos/end_effector_loc Max               1\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.376817\r\n",
      "evaluation/num steps total                           63000\r\n",
      "evaluation/num paths total                            3150\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0879986\r\n",
      "evaluation/Rewards Std                                   0.0992036\r\n",
      "evaluation/Rewards Max                                   0.140388\r\n",
      "evaluation/Rewards Min                                  -0.492079\r\n",
      "evaluation/Returns Mean                                 -1.75997\r\n",
      "evaluation/Returns Std                                   1.65186\r\n",
      "evaluation/Returns Max                                   1.47415\r\n",
      "evaluation/Returns Min                                  -5.84075\r\n",
      "evaluation/Actions Mean                                  0.00810564\r\n",
      "evaluation/Actions Std                                   0.129831\r\n",
      "evaluation/Actions Max                                   0.991942\r\n",
      "evaluation/Actions Min                                  -0.991554\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.75997\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0682706\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.184698\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.839247\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.82691e-121\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0109166\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0305064\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.179678\r\n",
      "evaluation/env_infos/initial/reward_dist Min             5.88217e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.057776\r\n",
      "evaluation/env_infos/reward_dist Std                     0.151219\r\n",
      "evaluation/env_infos/reward_dist Max                     0.973313\r\n",
      "evaluation/env_infos/reward_dist Min                     1.82691e-121\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.034875\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0336444\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00346251\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.162141\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.371128\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.382729\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00315124\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.40254\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0962527\r\n",
      "evaluation/env_infos/reward_energy Std                   0.156777\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00172852\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.40254\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0324985\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.379696\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00160529\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0187802\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0495971\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0495777\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0223301\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.255152\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.163026\r\n",
      "time/evaluation sampling (s)                             0.800554\r\n",
      "time/exploration sampling (s)                            0.103487\r\n",
      "time/logging (s)                                         0.0185928\r\n",
      "time/saving (s)                                          0.0285215\r\n",
      "time/training (s)                                       46.324\r\n",
      "time/epoch (s)                                          47.4382\r\n",
      "time/total (s)                                        2945.71\r\n",
      "Epoch                                                   62\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:02:24.334166 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 63 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00192869\n",
      "trainer/QF2 Loss                                         0.00106215\n",
      "trainer/Policy Loss                                      2.88253\n",
      "trainer/Q1 Predictions Mean                             -0.933449\n",
      "trainer/Q1 Predictions Std                               0.820789\n",
      "trainer/Q1 Predictions Max                               0.866776\n",
      "trainer/Q1 Predictions Min                              -3.27318\n",
      "trainer/Q2 Predictions Mean                             -0.934555\n",
      "trainer/Q2 Predictions Std                               0.820158\n",
      "trainer/Q2 Predictions Max                               0.872211\n",
      "trainer/Q2 Predictions Min                              -3.34149\n",
      "trainer/Q Targets Mean                                  -0.940719\n",
      "trainer/Q Targets Std                                    0.832033\n",
      "trainer/Q Targets Max                                    0.885778\n",
      "trainer/Q Targets Min                                   -3.46213\n",
      "trainer/Log Pis Mean                                     1.95631\n",
      "trainer/Log Pis Std                                      1.36938\n",
      "trainer/Log Pis Max                                      5.32116\n",
      "trainer/Log Pis Min                                     -3.85963\n",
      "trainer/Policy mu Mean                                   0.0233985\n",
      "trainer/Policy mu Std                                    0.306618\n",
      "trainer/Policy mu Max                                    2.15676\n",
      "trainer/Policy mu Min                                   -1.92904\n",
      "trainer/Policy log std Mean                             -2.32511\n",
      "trainer/Policy log std Std                               0.59358\n",
      "trainer/Policy log std Max                               0.0357393\n",
      "trainer/Policy log std Min                              -3.22471\n",
      "trainer/Alpha                                            0.0199703\n",
      "trainer/Alpha Loss                                      -0.170879\n",
      "exploration/num steps total                           7400\n",
      "exploration/num paths total                            370\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.097732\n",
      "exploration/Rewards Std                                  0.0603468\n",
      "exploration/Rewards Max                                  0.025961\n",
      "exploration/Rewards Min                                 -0.347245\n",
      "exploration/Returns Mean                                -1.95464\n",
      "exploration/Returns Std                                  0.636217\n",
      "exploration/Returns Max                                 -1.13552\n",
      "exploration/Returns Min                                 -3.04227\n",
      "exploration/Actions Mean                                -0.00117781\n",
      "exploration/Actions Std                                  0.0764473\n",
      "exploration/Actions Max                                  0.24807\n",
      "exploration/Actions Min                                 -0.373948\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.95464\n",
      "exploration/env_infos/final/reward_dist Mean             0.00294964\n",
      "exploration/env_infos/final/reward_dist Std              0.00404484\n",
      "exploration/env_infos/final/reward_dist Max              0.0105873\n",
      "exploration/env_infos/final/reward_dist Min              1.3406e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00350628\n",
      "exploration/env_infos/initial/reward_dist Std            0.00651982\n",
      "exploration/env_infos/initial/reward_dist Max            0.0165399\n",
      "exploration/env_infos/initial/reward_dist Min            1.90843e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0569729\n",
      "exploration/env_infos/reward_dist Std                    0.156533\n",
      "exploration/env_infos/reward_dist Max                    0.81047\n",
      "exploration/env_infos/reward_dist Min                    1.3406e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.12597\n",
      "exploration/env_infos/final/reward_energy Std            0.133696\n",
      "exploration/env_infos/final/reward_energy Max           -0.0328689\n",
      "exploration/env_infos/final/reward_energy Min           -0.385384\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.104751\n",
      "exploration/env_infos/initial/reward_energy Std          0.0526546\n",
      "exploration/env_infos/initial/reward_energy Max         -0.042376\n",
      "exploration/env_infos/initial/reward_energy Min         -0.164225\n",
      "exploration/env_infos/reward_energy Mean                -0.0890242\n",
      "exploration/env_infos/reward_energy Std                  0.0613665\n",
      "exploration/env_infos/reward_energy Max                 -0.00624627\n",
      "exploration/env_infos/reward_energy Min                 -0.385384\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0889242\n",
      "exploration/env_infos/final/end_effector_loc Std         0.183901\n",
      "exploration/env_infos/final/end_effector_loc Max         0.420078\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.154209\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000829628\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0040612\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00810845\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00802428\n",
      "exploration/env_infos/end_effector_loc Mean              0.0534479\n",
      "exploration/env_infos/end_effector_loc Std               0.10141\n",
      "exploration/env_infos/end_effector_loc Max               0.420078\n",
      "exploration/env_infos/end_effector_loc Min              -0.154209\n",
      "evaluation/num steps total                           64000\n",
      "evaluation/num paths total                            3200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0736179\n",
      "evaluation/Rewards Std                                   0.0863151\n",
      "evaluation/Rewards Max                                   0.13998\n",
      "evaluation/Rewards Min                                  -0.610988\n",
      "evaluation/Returns Mean                                 -1.47236\n",
      "evaluation/Returns Std                                   1.40292\n",
      "evaluation/Returns Max                                   0.697066\n",
      "evaluation/Returns Min                                  -5.53411\n",
      "evaluation/Actions Mean                                 -0.00609971\n",
      "evaluation/Actions Std                                   0.0890572\n",
      "evaluation/Actions Max                                   0.79074\n",
      "evaluation/Actions Min                                  -0.931942\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.47236\n",
      "evaluation/env_infos/final/reward_dist Mean              0.114089\n",
      "evaluation/env_infos/final/reward_dist Std               0.217827\n",
      "evaluation/env_infos/final/reward_dist Max               0.99101\n",
      "evaluation/env_infos/final/reward_dist Min               2.643e-90\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00436528\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00877456\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0436736\n",
      "evaluation/env_infos/initial/reward_dist Min             2.0048e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0858933\n",
      "evaluation/env_infos/reward_dist Std                     0.183076\n",
      "evaluation/env_infos/reward_dist Max                     0.999911\n",
      "evaluation/env_infos/reward_dist Min                     2.643e-90\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0240682\n",
      "evaluation/env_infos/final/reward_energy Std             0.0329746\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00457556\n",
      "evaluation/env_infos/final/reward_energy Min            -0.166007\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.276636\n",
      "evaluation/env_infos/initial/reward_energy Std           0.279409\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.010665\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.948545\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0673424\n",
      "evaluation/env_infos/reward_energy Std                   0.106779\n",
      "evaluation/env_infos/reward_energy Max                  -0.00173148\n",
      "evaluation/env_infos/reward_energy Min                  -0.948545\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0613271\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.268003\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.678722\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.831036\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000188727\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0139\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.039537\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0465971\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0205714\n",
      "evaluation/env_infos/end_effector_loc Std                0.175948\n",
      "evaluation/env_infos/end_effector_loc Max                0.678722\n",
      "evaluation/env_infos/end_effector_loc Min               -0.831036\n",
      "time/data storing (s)                                    0.168996\n",
      "time/evaluation sampling (s)                             0.786116\n",
      "time/exploration sampling (s)                            0.116215\n",
      "time/logging (s)                                         0.01874\n",
      "time/saving (s)                                          0.0271061\n",
      "time/training (s)                                       46.9223\n",
      "time/epoch (s)                                          48.0394\n",
      "time/total (s)                                        2994.37\n",
      "Epoch                                                   63\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:03:12.942688 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 64 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000734641\n",
      "trainer/QF2 Loss                                         0.00161139\n",
      "trainer/Policy Loss                                      3.02806\n",
      "trainer/Q1 Predictions Mean                             -1.02526\n",
      "trainer/Q1 Predictions Std                               0.824852\n",
      "trainer/Q1 Predictions Max                               0.639767\n",
      "trainer/Q1 Predictions Min                              -3.6956\n",
      "trainer/Q2 Predictions Mean                             -1.01829\n",
      "trainer/Q2 Predictions Std                               0.818542\n",
      "trainer/Q2 Predictions Max                               0.670397\n",
      "trainer/Q2 Predictions Min                              -3.69638\n",
      "trainer/Q Targets Mean                                  -1.02456\n",
      "trainer/Q Targets Std                                    0.828275\n",
      "trainer/Q Targets Max                                    0.672857\n",
      "trainer/Q Targets Min                                   -3.70572\n",
      "trainer/Log Pis Mean                                     2.00821\n",
      "trainer/Log Pis Std                                      1.42187\n",
      "trainer/Log Pis Max                                      5.91679\n",
      "trainer/Log Pis Min                                     -3.13739\n",
      "trainer/Policy mu Mean                                  -0.00778767\n",
      "trainer/Policy mu Std                                    0.346324\n",
      "trainer/Policy mu Max                                    2.24278\n",
      "trainer/Policy mu Min                                   -2.17853\n",
      "trainer/Policy log std Mean                             -2.31481\n",
      "trainer/Policy log std Std                               0.640398\n",
      "trainer/Policy log std Max                              -0.166301\n",
      "trainer/Policy log std Min                              -3.34063\n",
      "trainer/Alpha                                            0.0234681\n",
      "trainer/Alpha Loss                                       0.0307769\n",
      "exploration/num steps total                           7500\n",
      "exploration/num paths total                            375\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.112996\n",
      "exploration/Rewards Std                                  0.0663292\n",
      "exploration/Rewards Max                                  0.0344551\n",
      "exploration/Rewards Min                                 -0.282947\n",
      "exploration/Returns Mean                                -2.25993\n",
      "exploration/Returns Std                                  0.779694\n",
      "exploration/Returns Max                                 -1.24811\n",
      "exploration/Returns Min                                 -3.2146\n",
      "exploration/Actions Mean                                -0.000853169\n",
      "exploration/Actions Std                                  0.102153\n",
      "exploration/Actions Max                                  0.310989\n",
      "exploration/Actions Min                                 -0.34111\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.25993\n",
      "exploration/env_infos/final/reward_dist Mean             0.255436\n",
      "exploration/env_infos/final/reward_dist Std              0.253192\n",
      "exploration/env_infos/final/reward_dist Max              0.668569\n",
      "exploration/env_infos/final/reward_dist Min              8.22131e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0109706\n",
      "exploration/env_infos/initial/reward_dist Std            0.0165306\n",
      "exploration/env_infos/initial/reward_dist Max            0.0426739\n",
      "exploration/env_infos/initial/reward_dist Min            1.60837e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.217152\n",
      "exploration/env_infos/reward_dist Std                    0.292072\n",
      "exploration/env_infos/reward_dist Max                    0.98301\n",
      "exploration/env_infos/reward_dist Min                    8.22131e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0828243\n",
      "exploration/env_infos/final/reward_energy Std            0.0859404\n",
      "exploration/env_infos/final/reward_energy Max           -0.0174562\n",
      "exploration/env_infos/final/reward_energy Min           -0.252454\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.184108\n",
      "exploration/env_infos/initial/reward_energy Std          0.129666\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0486003\n",
      "exploration/env_infos/initial/reward_energy Min         -0.389657\n",
      "exploration/env_infos/reward_energy Mean                -0.116811\n",
      "exploration/env_infos/reward_energy Std                  0.0850122\n",
      "exploration/env_infos/reward_energy Max                 -0.0109356\n",
      "exploration/env_infos/reward_energy Min                 -0.389657\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0296627\n",
      "exploration/env_infos/final/end_effector_loc Std         0.199101\n",
      "exploration/env_infos/final/end_effector_loc Max         0.242606\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.381139\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00220224\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0076509\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00841995\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0140748\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0228906\n",
      "exploration/env_infos/end_effector_loc Std               0.133182\n",
      "exploration/env_infos/end_effector_loc Max               0.242606\n",
      "exploration/env_infos/end_effector_loc Min              -0.381139\n",
      "evaluation/num steps total                           65000\n",
      "evaluation/num paths total                            3250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0653641\n",
      "evaluation/Rewards Std                                   0.0814982\n",
      "evaluation/Rewards Max                                   0.124336\n",
      "evaluation/Rewards Min                                  -0.609426\n",
      "evaluation/Returns Mean                                 -1.30728\n",
      "evaluation/Returns Std                                   1.26638\n",
      "evaluation/Returns Max                                   1.02188\n",
      "evaluation/Returns Min                                  -5.26931\n",
      "evaluation/Actions Mean                                 -0.00388076\n",
      "evaluation/Actions Std                                   0.0890994\n",
      "evaluation/Actions Max                                   0.893674\n",
      "evaluation/Actions Min                                  -0.770719\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.30728\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0883835\n",
      "evaluation/env_infos/final/reward_dist Std               0.218116\n",
      "evaluation/env_infos/final/reward_dist Max               0.983767\n",
      "evaluation/env_infos/final/reward_dist Min               1.07857e-95\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00856786\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0226966\n",
      "evaluation/env_infos/initial/reward_dist Max             0.15231\n",
      "evaluation/env_infos/initial/reward_dist Min             1.15474e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.104943\n",
      "evaluation/env_infos/reward_dist Std                     0.208364\n",
      "evaluation/env_infos/reward_dist Max                     0.991524\n",
      "evaluation/env_infos/reward_dist Min                     1.07857e-95\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0250551\n",
      "evaluation/env_infos/final/reward_energy Std             0.0270114\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00258743\n",
      "evaluation/env_infos/final/reward_energy Min            -0.122893\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.262523\n",
      "evaluation/env_infos/initial/reward_energy Std           0.283434\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0127271\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.14232\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0681943\n",
      "evaluation/env_infos/reward_energy Std                   0.106099\n",
      "evaluation/env_infos/reward_energy Max                  -0.000366728\n",
      "evaluation/env_infos/reward_energy Min                  -1.14232\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0510055\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.321997\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.873898\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00115834\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0136097\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0446837\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.038536\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0202603\n",
      "evaluation/env_infos/end_effector_loc Std                0.202487\n",
      "evaluation/env_infos/end_effector_loc Max                0.873898\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.162646\n",
      "time/evaluation sampling (s)                             0.804318\n",
      "time/exploration sampling (s)                            0.106322\n",
      "time/logging (s)                                         0.0196007\n",
      "time/saving (s)                                          0.0276541\n",
      "time/training (s)                                       46.9012\n",
      "time/epoch (s)                                          48.0217\n",
      "time/total (s)                                        3042.97\n",
      "Epoch                                                   64\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:04:01.316437 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 65 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00289595\n",
      "trainer/QF2 Loss                                         0.00214712\n",
      "trainer/Policy Loss                                      2.86773\n",
      "trainer/Q1 Predictions Mean                             -0.964478\n",
      "trainer/Q1 Predictions Std                               0.849913\n",
      "trainer/Q1 Predictions Max                               0.894145\n",
      "trainer/Q1 Predictions Min                              -3.51038\n",
      "trainer/Q2 Predictions Mean                             -0.96142\n",
      "trainer/Q2 Predictions Std                               0.852986\n",
      "trainer/Q2 Predictions Max                               0.877846\n",
      "trainer/Q2 Predictions Min                              -3.61646\n",
      "trainer/Q Targets Mean                                  -0.954327\n",
      "trainer/Q Targets Std                                    0.844287\n",
      "trainer/Q Targets Max                                    0.89733\n",
      "trainer/Q Targets Min                                   -3.50179\n",
      "trainer/Log Pis Mean                                     1.90098\n",
      "trainer/Log Pis Std                                      1.45281\n",
      "trainer/Log Pis Max                                      5.23816\n",
      "trainer/Log Pis Min                                     -5.23276\n",
      "trainer/Policy mu Mean                                  -0.0148471\n",
      "trainer/Policy mu Std                                    0.275617\n",
      "trainer/Policy mu Max                                    1.14457\n",
      "trainer/Policy mu Min                                   -2.39992\n",
      "trainer/Policy log std Mean                             -2.31505\n",
      "trainer/Policy log std Std                               0.613355\n",
      "trainer/Policy log std Max                               1.57423\n",
      "trainer/Policy log std Min                              -3.12588\n",
      "trainer/Alpha                                            0.0239029\n",
      "trainer/Alpha Loss                                      -0.369627\n",
      "exploration/num steps total                           7600\n",
      "exploration/num paths total                            380\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117331\n",
      "exploration/Rewards Std                                  0.0749383\n",
      "exploration/Rewards Max                                 -0.00184194\n",
      "exploration/Rewards Min                                 -0.480061\n",
      "exploration/Returns Mean                                -2.34662\n",
      "exploration/Returns Std                                  0.698538\n",
      "exploration/Returns Max                                 -1.45994\n",
      "exploration/Returns Min                                 -3.34653\n",
      "exploration/Actions Mean                                 0.0058909\n",
      "exploration/Actions Std                                  0.19642\n",
      "exploration/Actions Max                                  0.808145\n",
      "exploration/Actions Min                                 -0.620522\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.34662\n",
      "exploration/env_infos/final/reward_dist Mean             0.0499181\n",
      "exploration/env_infos/final/reward_dist Std              0.0995622\n",
      "exploration/env_infos/final/reward_dist Max              0.249042\n",
      "exploration/env_infos/final/reward_dist Min              7.126e-35\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00629786\n",
      "exploration/env_infos/initial/reward_dist Std            0.00599739\n",
      "exploration/env_infos/initial/reward_dist Max            0.0177169\n",
      "exploration/env_infos/initial/reward_dist Min            0.00041514\n",
      "exploration/env_infos/reward_dist Mean                   0.0571059\n",
      "exploration/env_infos/reward_dist Std                    0.147447\n",
      "exploration/env_infos/reward_dist Max                    0.937389\n",
      "exploration/env_infos/reward_dist Min                    7.126e-35\n",
      "exploration/env_infos/final/reward_energy Mean          -0.171119\n",
      "exploration/env_infos/final/reward_energy Std            0.0666446\n",
      "exploration/env_infos/final/reward_energy Max           -0.0655858\n",
      "exploration/env_infos/final/reward_energy Min           -0.25926\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.394573\n",
      "exploration/env_infos/initial/reward_energy Std          0.318706\n",
      "exploration/env_infos/initial/reward_energy Max         -0.106813\n",
      "exploration/env_infos/initial/reward_energy Min         -0.939502\n",
      "exploration/env_infos/reward_energy Mean                -0.195826\n",
      "exploration/env_infos/reward_energy Std                  0.197189\n",
      "exploration/env_infos/reward_energy Max                 -0.0169084\n",
      "exploration/env_infos/reward_energy Min                 -0.962074\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.134825\n",
      "exploration/env_infos/final/end_effector_loc Std         0.328908\n",
      "exploration/env_infos/final/end_effector_loc Max         0.896209\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.252104\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0104247\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0145912\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0337866\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0056576\n",
      "exploration/env_infos/end_effector_loc Mean              0.0879316\n",
      "exploration/env_infos/end_effector_loc Std               0.217027\n",
      "exploration/env_infos/end_effector_loc Max               0.896209\n",
      "exploration/env_infos/end_effector_loc Min              -0.271109\n",
      "evaluation/num steps total                           66000\n",
      "evaluation/num paths total                            3300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.089387\n",
      "evaluation/Rewards Std                                   0.0914965\n",
      "evaluation/Rewards Max                                   0.143605\n",
      "evaluation/Rewards Min                                  -0.751899\n",
      "evaluation/Returns Mean                                 -1.78774\n",
      "evaluation/Returns Std                                   1.25152\n",
      "evaluation/Returns Max                                   0.405426\n",
      "evaluation/Returns Min                                  -4.47929\n",
      "evaluation/Actions Mean                                 -0.00127964\n",
      "evaluation/Actions Std                                   0.0972726\n",
      "evaluation/Actions Max                                   0.878659\n",
      "evaluation/Actions Min                                  -0.94013\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.78774\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0496465\n",
      "evaluation/env_infos/final/reward_dist Std               0.150708\n",
      "evaluation/env_infos/final/reward_dist Max               0.937509\n",
      "evaluation/env_infos/final/reward_dist Min               5.00737e-98\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00547156\n",
      "evaluation/env_infos/initial/reward_dist Std             0.018707\n",
      "evaluation/env_infos/initial/reward_dist Max             0.132283\n",
      "evaluation/env_infos/initial/reward_dist Min             2.05265e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0694009\n",
      "evaluation/env_infos/reward_dist Std                     0.163966\n",
      "evaluation/env_infos/reward_dist Max                     0.995708\n",
      "evaluation/env_infos/reward_dist Min                     5.00737e-98\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0318927\n",
      "evaluation/env_infos/final/reward_energy Std             0.0228\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00372186\n",
      "evaluation/env_infos/final/reward_energy Min            -0.0976739\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.302805\n",
      "evaluation/env_infos/initial/reward_energy Std           0.332363\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0148869\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.17531\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0702689\n",
      "evaluation/env_infos/reward_energy Std                   0.118277\n",
      "evaluation/env_infos/reward_energy Max                  -0.0016437\n",
      "evaluation/env_infos/reward_energy Min                  -1.17531\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0336393\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.394773\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000833705\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0158745\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0439329\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0470065\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0123055\n",
      "evaluation/env_infos/end_effector_loc Std                0.253017\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.162874\n",
      "time/evaluation sampling (s)                             0.781577\n",
      "time/exploration sampling (s)                            0.106755\n",
      "time/logging (s)                                         0.0191794\n",
      "time/saving (s)                                          0.02607\n",
      "time/training (s)                                       46.6851\n",
      "time/epoch (s)                                          47.7816\n",
      "time/total (s)                                        3091.34\n",
      "Epoch                                                   65\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:04:49.810356 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 66 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00104579\r\n",
      "trainer/QF2 Loss                                         0.000909716\r\n",
      "trainer/Policy Loss                                      3.03782\r\n",
      "trainer/Q1 Predictions Mean                             -1.00146\r\n",
      "trainer/Q1 Predictions Std                               0.794636\r\n",
      "trainer/Q1 Predictions Max                               0.283873\r\n",
      "trainer/Q1 Predictions Min                              -3.317\r\n",
      "trainer/Q2 Predictions Mean                             -0.995505\r\n",
      "trainer/Q2 Predictions Std                               0.797175\r\n",
      "trainer/Q2 Predictions Max                               0.295767\r\n",
      "trainer/Q2 Predictions Min                              -3.35438\r\n",
      "trainer/Q Targets Mean                                  -1.00181\r\n",
      "trainer/Q Targets Std                                    0.798748\r\n",
      "trainer/Q Targets Max                                    0.293755\r\n",
      "trainer/Q Targets Min                                   -3.39875\r\n",
      "trainer/Log Pis Mean                                     2.04066\r\n",
      "trainer/Log Pis Std                                      1.39451\r\n",
      "trainer/Log Pis Max                                      4.32175\r\n",
      "trainer/Log Pis Min                                     -5.21273\r\n",
      "trainer/Policy mu Mean                                  -0.0304294\r\n",
      "trainer/Policy mu Std                                    0.277693\r\n",
      "trainer/Policy mu Max                                    1.268\r\n",
      "trainer/Policy mu Min                                   -2.09378\r\n",
      "trainer/Policy log std Mean                             -2.34906\r\n",
      "trainer/Policy log std Std                               0.586061\r\n",
      "trainer/Policy log std Max                               0.066098\r\n",
      "trainer/Policy log std Min                              -3.16853\r\n",
      "trainer/Alpha                                            0.0236193\r\n",
      "trainer/Alpha Loss                                       0.15237\r\n",
      "exploration/num steps total                           7700\r\n",
      "exploration/num paths total                            385\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0389713\r\n",
      "exploration/Rewards Std                                  0.0761514\r\n",
      "exploration/Rewards Max                                  0.133528\r\n",
      "exploration/Rewards Min                                 -0.212345\r\n",
      "exploration/Returns Mean                                -0.779426\r\n",
      "exploration/Returns Std                                  1.26534\r\n",
      "exploration/Returns Max                                  1.47286\r\n",
      "exploration/Returns Min                                 -2.10939\r\n",
      "exploration/Actions Mean                                -0.00318674\r\n",
      "exploration/Actions Std                                  0.136825\r\n",
      "exploration/Actions Max                                  0.562753\r\n",
      "exploration/Actions Min                                 -0.38024\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.779426\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0381941\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0536352\r\n",
      "exploration/env_infos/final/reward_dist Max              0.143205\r\n",
      "exploration/env_infos/final/reward_dist Min              1.47678e-05\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00846112\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0111558\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0302515\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.56002e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.139052\r\n",
      "exploration/env_infos/reward_dist Std                    0.236402\r\n",
      "exploration/env_infos/reward_dist Max                    0.968319\r\n",
      "exploration/env_infos/reward_dist Min                    3.56002e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.139106\r\n",
      "exploration/env_infos/final/reward_energy Std            0.114898\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0496047\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.353539\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.31521\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.218247\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0631805\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.701368\r\n",
      "exploration/env_infos/reward_energy Mean                -0.158914\r\n",
      "exploration/env_infos/reward_energy Std                  0.110493\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0234137\r\n",
      "exploration/env_infos/reward_energy Min                 -0.701368\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0386982\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.241081\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.312902\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.318853\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00575177\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0122741\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0281377\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00946868\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0419936\r\n",
      "exploration/env_infos/end_effector_loc Std               0.166016\r\n",
      "exploration/env_infos/end_effector_loc Max               0.416837\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.318853\r\n",
      "evaluation/num steps total                           67000\r\n",
      "evaluation/num paths total                            3350\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0567739\r\n",
      "evaluation/Rewards Std                                   0.0703835\r\n",
      "evaluation/Rewards Max                                   0.167379\r\n",
      "evaluation/Rewards Min                                  -0.295523\r\n",
      "evaluation/Returns Mean                                 -1.13548\r\n",
      "evaluation/Returns Std                                   1.08452\r\n",
      "evaluation/Returns Max                                   1.6178\r\n",
      "evaluation/Returns Min                                  -3.61326\r\n",
      "evaluation/Actions Mean                                  0.000252514\r\n",
      "evaluation/Actions Std                                   0.0694137\r\n",
      "evaluation/Actions Max                                   0.678338\r\n",
      "evaluation/Actions Min                                  -0.63179\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.13548\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.128182\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.239178\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.854779\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.37546e-103\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00533345\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0143777\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0920444\r\n",
      "evaluation/env_infos/initial/reward_dist Min             4.75061e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.110991\r\n",
      "evaluation/env_infos/reward_dist Std                     0.213637\r\n",
      "evaluation/env_infos/reward_dist Max                     0.99805\r\n",
      "evaluation/env_infos/reward_dist Min                     3.37546e-103\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0407305\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0521811\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00652997\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.3059\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.180436\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.18076\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00336714\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.783977\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0575136\r\n",
      "evaluation/env_infos/reward_energy Std                   0.079554\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00113014\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.791777\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00837885\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.280696\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.8318\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000982594\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00897628\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0339169\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0315895\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0002452\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.171544\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.8318\r\n",
      "time/data storing (s)                                    0.15956\r\n",
      "time/evaluation sampling (s)                             0.803607\r\n",
      "time/exploration sampling (s)                            0.104343\r\n",
      "time/logging (s)                                         0.0185763\r\n",
      "time/saving (s)                                          0.0373553\r\n",
      "time/training (s)                                       46.7548\r\n",
      "time/epoch (s)                                          47.8783\r\n",
      "time/total (s)                                        3139.84\r\n",
      "Epoch                                                   66\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:05:37.842303 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 67 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.0013588\r\n",
      "trainer/QF2 Loss                                         0.00105898\r\n",
      "trainer/Policy Loss                                      3.06028\r\n",
      "trainer/Q1 Predictions Mean                             -0.982781\r\n",
      "trainer/Q1 Predictions Std                               0.812794\r\n",
      "trainer/Q1 Predictions Max                               0.527124\r\n",
      "trainer/Q1 Predictions Min                              -3.29649\r\n",
      "trainer/Q2 Predictions Mean                             -0.979742\r\n",
      "trainer/Q2 Predictions Std                               0.806895\r\n",
      "trainer/Q2 Predictions Max                               0.527894\r\n",
      "trainer/Q2 Predictions Min                              -3.26228\r\n",
      "trainer/Q Targets Mean                                  -0.972892\r\n",
      "trainer/Q Targets Std                                    0.810882\r\n",
      "trainer/Q Targets Max                                    0.544006\r\n",
      "trainer/Q Targets Min                                   -3.24785\r\n",
      "trainer/Log Pis Mean                                     2.0711\r\n",
      "trainer/Log Pis Std                                      1.23041\r\n",
      "trainer/Log Pis Max                                      4.25163\r\n",
      "trainer/Log Pis Min                                     -1.61311\r\n",
      "trainer/Policy mu Mean                                  -0.0122333\r\n",
      "trainer/Policy mu Std                                    0.259655\r\n",
      "trainer/Policy mu Max                                    1.11858\r\n",
      "trainer/Policy mu Min                                   -2.35215\r\n",
      "trainer/Policy log std Mean                             -2.308\r\n",
      "trainer/Policy log std Std                               0.557224\r\n",
      "trainer/Policy log std Max                              -0.48241\r\n",
      "trainer/Policy log std Min                              -3.13039\r\n",
      "trainer/Alpha                                            0.0220742\r\n",
      "trainer/Alpha Loss                                       0.271168\r\n",
      "exploration/num steps total                           7800\r\n",
      "exploration/num paths total                            390\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.109767\r\n",
      "exploration/Rewards Std                                  0.0998436\r\n",
      "exploration/Rewards Max                                  0.127492\r\n",
      "exploration/Rewards Min                                 -0.384705\r\n",
      "exploration/Returns Mean                                -2.19535\r\n",
      "exploration/Returns Std                                  1.68351\r\n",
      "exploration/Returns Max                                  1.1095\r\n",
      "exploration/Returns Min                                 -3.49718\r\n",
      "exploration/Actions Mean                                 0.00196862\r\n",
      "exploration/Actions Std                                  0.156584\r\n",
      "exploration/Actions Max                                  0.454698\r\n",
      "exploration/Actions Min                                 -0.625338\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.19535\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0305465\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0573062\r\n",
      "exploration/env_infos/final/reward_dist Max              0.145002\r\n",
      "exploration/env_infos/final/reward_dist Min              3.61627e-09\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00542338\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.010658\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0267386\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.02057e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.124502\r\n",
      "exploration/env_infos/reward_dist Std                    0.262123\r\n",
      "exploration/env_infos/reward_dist Max                    0.906065\r\n",
      "exploration/env_infos/reward_dist Min                    3.61627e-09\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.146241\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0394136\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0774261\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.189405\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.159306\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.107623\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0553117\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.344603\r\n",
      "exploration/env_infos/reward_energy Mean                -0.177112\r\n",
      "exploration/env_infos/reward_energy Std                  0.132952\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0092786\r\n",
      "exploration/env_infos/reward_energy Min                 -0.654467\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0672672\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.222819\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.382072\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.38946\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00254921\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00630105\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0128943\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00553601\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0465311\r\n",
      "exploration/env_infos/end_effector_loc Std               0.134665\r\n",
      "exploration/env_infos/end_effector_loc Max               0.402562\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.38946\r\n",
      "evaluation/num steps total                           68000\r\n",
      "evaluation/num paths total                            3400\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0667682\r\n",
      "evaluation/Rewards Std                                   0.0811289\r\n",
      "evaluation/Rewards Max                                   0.179216\r\n",
      "evaluation/Rewards Min                                  -0.402158\r\n",
      "evaluation/Returns Mean                                 -1.33536\r\n",
      "evaluation/Returns Std                                   1.34229\r\n",
      "evaluation/Returns Max                                   1.25274\r\n",
      "evaluation/Returns Min                                  -4.70204\r\n",
      "evaluation/Actions Mean                                  0.00707928\r\n",
      "evaluation/Actions Std                                   0.0641219\r\n",
      "evaluation/Actions Max                                   0.561421\r\n",
      "evaluation/Actions Min                                  -0.531887\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.33536\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0687029\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.161862\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.76222\r\n",
      "evaluation/env_infos/final/reward_dist Min               7.26988e-88\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00629762\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111706\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0534999\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.13576e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.100576\r\n",
      "evaluation/env_infos/reward_dist Std                     0.199461\r\n",
      "evaluation/env_infos/reward_dist Max                     0.976798\r\n",
      "evaluation/env_infos/reward_dist Min                     7.26988e-88\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0497271\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0410643\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0132891\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.174649\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.150333\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.14879\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00746647\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.593708\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.060323\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0684442\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00289992\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.593708\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0633601\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.270318\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.984823\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.554145\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000882785\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00742589\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0280711\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0265944\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0249392\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.168471\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.984823\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.554145\r\n",
      "time/data storing (s)                                    0.162971\r\n",
      "time/evaluation sampling (s)                             0.795679\r\n",
      "time/exploration sampling (s)                            0.104273\r\n",
      "time/logging (s)                                         0.0196542\r\n",
      "time/saving (s)                                          0.0271793\r\n",
      "time/training (s)                                       46.3141\r\n",
      "time/epoch (s)                                          47.4239\r\n",
      "time/total (s)                                        3187.87\r\n",
      "Epoch                                                   67\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:06:26.752890 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 68 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00109642\n",
      "trainer/QF2 Loss                                         0.00144616\n",
      "trainer/Policy Loss                                      2.87152\n",
      "trainer/Q1 Predictions Mean                             -0.832745\n",
      "trainer/Q1 Predictions Std                               0.7964\n",
      "trainer/Q1 Predictions Max                               0.746812\n",
      "trainer/Q1 Predictions Min                              -2.9226\n",
      "trainer/Q2 Predictions Mean                             -0.821789\n",
      "trainer/Q2 Predictions Std                               0.793342\n",
      "trainer/Q2 Predictions Max                               0.769967\n",
      "trainer/Q2 Predictions Min                              -2.89995\n",
      "trainer/Q Targets Mean                                  -0.832164\n",
      "trainer/Q Targets Std                                    0.797042\n",
      "trainer/Q Targets Max                                    0.725536\n",
      "trainer/Q Targets Min                                   -2.96379\n",
      "trainer/Log Pis Mean                                     2.04069\n",
      "trainer/Log Pis Std                                      1.33296\n",
      "trainer/Log Pis Max                                      4.58964\n",
      "trainer/Log Pis Min                                     -2.76711\n",
      "trainer/Policy mu Mean                                  -0.0412591\n",
      "trainer/Policy mu Std                                    0.309899\n",
      "trainer/Policy mu Max                                    1.72031\n",
      "trainer/Policy mu Min                                   -2.29963\n",
      "trainer/Policy log std Mean                             -2.32338\n",
      "trainer/Policy log std Std                               0.588056\n",
      "trainer/Policy log std Max                              -0.303581\n",
      "trainer/Policy log std Min                              -3.34138\n",
      "trainer/Alpha                                            0.0209724\n",
      "trainer/Alpha Loss                                       0.157294\n",
      "exploration/num steps total                           7900\n",
      "exploration/num paths total                            395\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0658147\n",
      "exploration/Rewards Std                                  0.0752064\n",
      "exploration/Rewards Max                                  0.123962\n",
      "exploration/Rewards Min                                 -0.293828\n",
      "exploration/Returns Mean                                -1.31629\n",
      "exploration/Returns Std                                  1.07292\n",
      "exploration/Returns Max                                  0.461763\n",
      "exploration/Returns Min                                 -2.83961\n",
      "exploration/Actions Mean                                 0.00168038\n",
      "exploration/Actions Std                                  0.142367\n",
      "exploration/Actions Max                                  0.710704\n",
      "exploration/Actions Min                                 -0.561618\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.31629\n",
      "exploration/env_infos/final/reward_dist Mean             0.00395059\n",
      "exploration/env_infos/final/reward_dist Std              0.00691852\n",
      "exploration/env_infos/final/reward_dist Max              0.0177153\n",
      "exploration/env_infos/final/reward_dist Min              1.23907e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00134901\n",
      "exploration/env_infos/initial/reward_dist Std            0.0015455\n",
      "exploration/env_infos/initial/reward_dist Max            0.00381436\n",
      "exploration/env_infos/initial/reward_dist Min            2.73742e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.108129\n",
      "exploration/env_infos/reward_dist Std                    0.199981\n",
      "exploration/env_infos/reward_dist Max                    0.942253\n",
      "exploration/env_infos/reward_dist Min                    1.23907e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.120419\n",
      "exploration/env_infos/final/reward_energy Std            0.0393218\n",
      "exploration/env_infos/final/reward_energy Max           -0.0602164\n",
      "exploration/env_infos/final/reward_energy Min           -0.158066\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.293734\n",
      "exploration/env_infos/initial/reward_energy Std          0.251011\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0780824\n",
      "exploration/env_infos/initial/reward_energy Min         -0.721624\n",
      "exploration/env_infos/reward_energy Mean                -0.157022\n",
      "exploration/env_infos/reward_energy Std                  0.126042\n",
      "exploration/env_infos/reward_energy Max                 -0.0125485\n",
      "exploration/env_infos/reward_energy Min                 -0.722475\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0198189\n",
      "exploration/env_infos/final/end_effector_loc Std         0.190618\n",
      "exploration/env_infos/final/end_effector_loc Max         0.301718\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.369006\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00254176\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0134219\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0226565\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0280809\n",
      "exploration/env_infos/end_effector_loc Mean             -0.023422\n",
      "exploration/env_infos/end_effector_loc Std               0.124981\n",
      "exploration/env_infos/end_effector_loc Max               0.301718\n",
      "exploration/env_infos/end_effector_loc Min              -0.369006\n",
      "evaluation/num steps total                           69000\n",
      "evaluation/num paths total                            3450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0568168\n",
      "evaluation/Rewards Std                                   0.0740869\n",
      "evaluation/Rewards Max                                   0.157247\n",
      "evaluation/Rewards Min                                  -0.403481\n",
      "evaluation/Returns Mean                                 -1.13634\n",
      "evaluation/Returns Std                                   1.08179\n",
      "evaluation/Returns Max                                   1.02809\n",
      "evaluation/Returns Min                                  -5.01564\n",
      "evaluation/Actions Mean                                 -0.000556198\n",
      "evaluation/Actions Std                                   0.0675125\n",
      "evaluation/Actions Max                                   0.770149\n",
      "evaluation/Actions Min                                  -0.557961\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.13634\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0747436\n",
      "evaluation/env_infos/final/reward_dist Std               0.164486\n",
      "evaluation/env_infos/final/reward_dist Max               0.87797\n",
      "evaluation/env_infos/final/reward_dist Min               4.84856e-40\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00455474\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00912716\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0453449\n",
      "evaluation/env_infos/initial/reward_dist Min             1.10426e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.122999\n",
      "evaluation/env_infos/reward_dist Std                     0.214297\n",
      "evaluation/env_infos/reward_dist Max                     0.993251\n",
      "evaluation/env_infos/reward_dist Min                     4.84856e-40\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0400656\n",
      "evaluation/env_infos/final/reward_energy Std             0.0218183\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00750569\n",
      "evaluation/env_infos/final/reward_energy Min            -0.107516\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.182406\n",
      "evaluation/env_infos/initial/reward_energy Std           0.174422\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00295487\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.797171\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0610369\n",
      "evaluation/env_infos/reward_energy Std                   0.0734233\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178794\n",
      "evaluation/env_infos/reward_energy Min                  -0.797171\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.047895\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.268959\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.566693\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000746895\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00889162\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0385074\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0265127\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0266665\n",
      "evaluation/env_infos/end_effector_loc Std                0.167676\n",
      "evaluation/env_infos/end_effector_loc Max                0.566693\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.163533\n",
      "time/evaluation sampling (s)                             0.774224\n",
      "time/exploration sampling (s)                            0.110235\n",
      "time/logging (s)                                         0.0195039\n",
      "time/saving (s)                                          0.0276299\n",
      "time/training (s)                                       47.1922\n",
      "time/epoch (s)                                          48.2873\n",
      "time/total (s)                                        3236.77\n",
      "Epoch                                                   68\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:07:15.125999 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 69 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00110645\r\n",
      "trainer/QF2 Loss                                         0.00105111\r\n",
      "trainer/Policy Loss                                      2.87902\r\n",
      "trainer/Q1 Predictions Mean                             -0.851405\r\n",
      "trainer/Q1 Predictions Std                               0.812987\r\n",
      "trainer/Q1 Predictions Max                               0.840193\r\n",
      "trainer/Q1 Predictions Min                              -2.86218\r\n",
      "trainer/Q2 Predictions Mean                             -0.851208\r\n",
      "trainer/Q2 Predictions Std                               0.813134\r\n",
      "trainer/Q2 Predictions Max                               0.788473\r\n",
      "trainer/Q2 Predictions Min                              -2.87872\r\n",
      "trainer/Q Targets Mean                                  -0.855005\r\n",
      "trainer/Q Targets Std                                    0.809554\r\n",
      "trainer/Q Targets Max                                    0.86943\r\n",
      "trainer/Q Targets Min                                   -2.85722\r\n",
      "trainer/Log Pis Mean                                     2.02899\r\n",
      "trainer/Log Pis Std                                      1.28053\r\n",
      "trainer/Log Pis Max                                      4.42482\r\n",
      "trainer/Log Pis Min                                     -6.5029\r\n",
      "trainer/Policy mu Mean                                  -0.0385676\r\n",
      "trainer/Policy mu Std                                    0.283863\r\n",
      "trainer/Policy mu Max                                    1.77244\r\n",
      "trainer/Policy mu Min                                   -1.94663\r\n",
      "trainer/Policy log std Mean                             -2.36932\r\n",
      "trainer/Policy log std Std                               0.514883\r\n",
      "trainer/Policy log std Max                              -0.521891\r\n",
      "trainer/Policy log std Min                              -3.267\r\n",
      "trainer/Alpha                                            0.0215015\r\n",
      "trainer/Alpha Loss                                       0.111309\r\n",
      "exploration/num steps total                           8000\r\n",
      "exploration/num paths total                            400\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.1152\r\n",
      "exploration/Rewards Std                                  0.0482758\r\n",
      "exploration/Rewards Max                                  0.0131315\r\n",
      "exploration/Rewards Min                                 -0.245138\r\n",
      "exploration/Returns Mean                                -2.304\r\n",
      "exploration/Returns Std                                  0.326007\r\n",
      "exploration/Returns Max                                 -1.85332\r\n",
      "exploration/Returns Min                                 -2.69099\r\n",
      "exploration/Actions Mean                                 0.0047692\r\n",
      "exploration/Actions Std                                  0.148479\r\n",
      "exploration/Actions Max                                  0.842216\r\n",
      "exploration/Actions Min                                 -0.425666\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.304\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.127102\r\n",
      "exploration/env_infos/final/reward_dist Std              0.250894\r\n",
      "exploration/env_infos/final/reward_dist Max              0.628864\r\n",
      "exploration/env_infos/final/reward_dist Min              7.17526e-17\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00651269\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00627473\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0141007\r\n",
      "exploration/env_infos/initial/reward_dist Min            6.57948e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.130881\r\n",
      "exploration/env_infos/reward_dist Std                    0.239563\r\n",
      "exploration/env_infos/reward_dist Max                    0.939862\r\n",
      "exploration/env_infos/reward_dist Min                    7.17526e-17\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.117196\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0844691\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.027139\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.267452\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.35448\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.411546\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0327703\r\n",
      "exploration/env_infos/initial/reward_energy Min         -1.16614\r\n",
      "exploration/env_infos/reward_energy Mean                -0.153202\r\n",
      "exploration/env_infos/reward_energy Std                  0.143759\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0101022\r\n",
      "exploration/env_infos/reward_energy Min                 -1.16614\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.01261\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.333877\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.619765\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.431039\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00855895\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0171909\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0421108\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0107483\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0194311\r\n",
      "exploration/env_infos/end_effector_loc Std               0.203891\r\n",
      "exploration/env_infos/end_effector_loc Max               0.619765\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.431039\r\n",
      "evaluation/num steps total                           70000\r\n",
      "evaluation/num paths total                            3500\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.063741\r\n",
      "evaluation/Rewards Std                                   0.0867763\r\n",
      "evaluation/Rewards Max                                   0.166372\r\n",
      "evaluation/Rewards Min                                  -0.497699\r\n",
      "evaluation/Returns Mean                                 -1.27482\r\n",
      "evaluation/Returns Std                                   1.2937\r\n",
      "evaluation/Returns Max                                   1.35185\r\n",
      "evaluation/Returns Min                                  -4.3331\r\n",
      "evaluation/Actions Mean                                 -0.00793113\r\n",
      "evaluation/Actions Std                                   0.0906309\r\n",
      "evaluation/Actions Max                                   0.943342\r\n",
      "evaluation/Actions Min                                  -0.776306\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.27482\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0569979\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.14243\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.758467\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.43704e-49\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.011545\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0369236\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.246037\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.43542e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.141423\r\n",
      "evaluation/env_infos/reward_dist Std                     0.236375\r\n",
      "evaluation/env_infos/reward_dist Max                     0.985802\r\n",
      "evaluation/env_infos/reward_dist Min                     2.43704e-49\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0533638\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0496903\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00496731\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.332304\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.31514\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.270452\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0110533\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.1836\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0788789\r\n",
      "evaluation/env_infos/reward_energy Std                   0.101646\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00224808\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.1836\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.109829\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.340651\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.693079\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00195962\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.014551\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0471671\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0388153\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0395456\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.219889\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.693079\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.16508\r\n",
      "time/evaluation sampling (s)                             0.794298\r\n",
      "time/exploration sampling (s)                            0.108806\r\n",
      "time/logging (s)                                         0.0189725\r\n",
      "time/saving (s)                                          0.0266064\r\n",
      "time/training (s)                                       46.6064\r\n",
      "time/epoch (s)                                          47.7201\r\n",
      "time/total (s)                                        3285.15\r\n",
      "Epoch                                                   69\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:08:04.255677 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 70 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000891427\r\n",
      "trainer/QF2 Loss                                         0.00108219\r\n",
      "trainer/Policy Loss                                      2.78052\r\n",
      "trainer/Q1 Predictions Mean                             -0.857611\r\n",
      "trainer/Q1 Predictions Std                               0.785509\r\n",
      "trainer/Q1 Predictions Max                               0.978426\r\n",
      "trainer/Q1 Predictions Min                              -3.00915\r\n",
      "trainer/Q2 Predictions Mean                             -0.85297\r\n",
      "trainer/Q2 Predictions Std                               0.788019\r\n",
      "trainer/Q2 Predictions Max                               0.960983\r\n",
      "trainer/Q2 Predictions Min                              -3.08578\r\n",
      "trainer/Q Targets Mean                                  -0.856035\r\n",
      "trainer/Q Targets Std                                    0.787585\r\n",
      "trainer/Q Targets Max                                    0.977752\r\n",
      "trainer/Q Targets Min                                   -3.03151\r\n",
      "trainer/Log Pis Mean                                     1.92868\r\n",
      "trainer/Log Pis Std                                      1.4227\r\n",
      "trainer/Log Pis Max                                      4.80617\r\n",
      "trainer/Log Pis Min                                     -3.37327\r\n",
      "trainer/Policy mu Mean                                  -0.0636968\r\n",
      "trainer/Policy mu Std                                    0.326367\r\n",
      "trainer/Policy mu Max                                    1.66129\r\n",
      "trainer/Policy mu Min                                   -2.28552\r\n",
      "trainer/Policy log std Mean                             -2.294\r\n",
      "trainer/Policy log std Std                               0.588425\r\n",
      "trainer/Policy log std Max                              -0.0444373\r\n",
      "trainer/Policy log std Min                              -3.23474\r\n",
      "trainer/Alpha                                            0.020591\r\n",
      "trainer/Alpha Loss                                      -0.276956\r\n",
      "exploration/num steps total                           8100\r\n",
      "exploration/num paths total                            405\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0894478\r\n",
      "exploration/Rewards Std                                  0.0991256\r\n",
      "exploration/Rewards Max                                  0.0672844\r\n",
      "exploration/Rewards Min                                 -0.345014\r\n",
      "exploration/Returns Mean                                -1.78896\r\n",
      "exploration/Returns Std                                  1.46855\r\n",
      "exploration/Returns Max                                 -0.47198\r\n",
      "exploration/Returns Min                                 -4.49735\r\n",
      "exploration/Actions Mean                                 0.00472755\r\n",
      "exploration/Actions Std                                  0.135268\r\n",
      "exploration/Actions Max                                  0.420403\r\n",
      "exploration/Actions Min                                 -0.42299\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.78896\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0574926\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0866709\r\n",
      "exploration/env_infos/final/reward_dist Max              0.228289\r\n",
      "exploration/env_infos/final/reward_dist Min              1.35027e-54\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0127114\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0169431\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0457463\r\n",
      "exploration/env_infos/initial/reward_dist Min            6.02606e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.148406\r\n",
      "exploration/env_infos/reward_dist Std                    0.24306\r\n",
      "exploration/env_infos/reward_dist Max                    0.968861\r\n",
      "exploration/env_infos/reward_dist Min                    1.35027e-54\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.185163\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0777247\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.061393\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.296576\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.31789\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.166139\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0224057\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.524475\r\n",
      "exploration/env_infos/reward_energy Mean                -0.159618\r\n",
      "exploration/env_infos/reward_energy Std                  0.105649\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0170994\r\n",
      "exploration/env_infos/reward_energy Min                 -0.565461\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00662064\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.384357\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.908248\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.435905\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00215898\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0124964\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0155318\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0211294\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0156972\r\n",
      "exploration/env_infos/end_effector_loc Std               0.234745\r\n",
      "exploration/env_infos/end_effector_loc Max               0.908248\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.435905\r\n",
      "evaluation/num steps total                           71000\r\n",
      "evaluation/num paths total                            3550\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0571053\r\n",
      "evaluation/Rewards Std                                   0.078134\r\n",
      "evaluation/Rewards Max                                   0.155648\r\n",
      "evaluation/Rewards Min                                  -0.503066\r\n",
      "evaluation/Returns Mean                                 -1.14211\r\n",
      "evaluation/Returns Std                                   1.19568\r\n",
      "evaluation/Returns Max                                   1.66067\r\n",
      "evaluation/Returns Min                                  -3.64556\r\n",
      "evaluation/Actions Mean                                 -0.00038318\r\n",
      "evaluation/Actions Std                                   0.0725679\r\n",
      "evaluation/Actions Max                                   0.773754\r\n",
      "evaluation/Actions Min                                  -0.744275\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.14211\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.13188\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.25883\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.936755\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.28659e-47\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00597383\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00979334\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.046344\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.57014e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.118419\r\n",
      "evaluation/env_infos/reward_dist Std                     0.207549\r\n",
      "evaluation/env_infos/reward_dist Max                     0.991633\r\n",
      "evaluation/env_infos/reward_dist Min                     1.28659e-47\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.045944\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0289111\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00190743\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.11985\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.233085\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.241685\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00403823\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.949152\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0614237\r\n",
      "evaluation/env_infos/reward_energy Std                   0.082217\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000198978\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.949152\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00448723\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.304372\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.925147\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.941529\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00124825\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0118054\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0386877\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0372138\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00328131\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.191784\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.925147\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.941529\r\n",
      "time/data storing (s)                                    0.166646\r\n",
      "time/evaluation sampling (s)                             0.853354\r\n",
      "time/exploration sampling (s)                            0.111598\r\n",
      "time/logging (s)                                         0.0194131\r\n",
      "time/saving (s)                                          0.0276339\r\n",
      "time/training (s)                                       47.328\r\n",
      "time/epoch (s)                                          48.5066\r\n",
      "time/total (s)                                        3334.27\r\n",
      "Epoch                                                   70\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:08:53.601251 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 71 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00145531\n",
      "trainer/QF2 Loss                                         0.00152263\n",
      "trainer/Policy Loss                                      2.8959\n",
      "trainer/Q1 Predictions Mean                             -0.803196\n",
      "trainer/Q1 Predictions Std                               0.834007\n",
      "trainer/Q1 Predictions Max                               1.01602\n",
      "trainer/Q1 Predictions Min                              -3.15786\n",
      "trainer/Q2 Predictions Mean                             -0.804168\n",
      "trainer/Q2 Predictions Std                               0.836166\n",
      "trainer/Q2 Predictions Max                               1.10673\n",
      "trainer/Q2 Predictions Min                              -3.1121\n",
      "trainer/Q Targets Mean                                  -0.800322\n",
      "trainer/Q Targets Std                                    0.832772\n",
      "trainer/Q Targets Max                                    1.09313\n",
      "trainer/Q Targets Min                                   -3.0882\n",
      "trainer/Log Pis Mean                                     2.08874\n",
      "trainer/Log Pis Std                                      1.24646\n",
      "trainer/Log Pis Max                                      4.49069\n",
      "trainer/Log Pis Min                                     -3.06999\n",
      "trainer/Policy mu Mean                                  -0.0490769\n",
      "trainer/Policy mu Std                                    0.271692\n",
      "trainer/Policy mu Max                                    1.31239\n",
      "trainer/Policy mu Min                                   -2.22112\n",
      "trainer/Policy log std Mean                             -2.36542\n",
      "trainer/Policy log std Std                               0.558091\n",
      "trainer/Policy log std Max                              -0.149153\n",
      "trainer/Policy log std Min                              -3.33301\n",
      "trainer/Alpha                                            0.0200165\n",
      "trainer/Alpha Loss                                       0.347158\n",
      "exploration/num steps total                           8200\n",
      "exploration/num paths total                            410\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0975803\n",
      "exploration/Rewards Std                                  0.0936812\n",
      "exploration/Rewards Max                                  0.0872194\n",
      "exploration/Rewards Min                                 -0.337786\n",
      "exploration/Returns Mean                                -1.95161\n",
      "exploration/Returns Std                                  1.34586\n",
      "exploration/Returns Max                                  0.383835\n",
      "exploration/Returns Min                                 -3.81123\n",
      "exploration/Actions Mean                                -0.0107855\n",
      "exploration/Actions Std                                  0.133429\n",
      "exploration/Actions Max                                  0.630453\n",
      "exploration/Actions Min                                 -0.424007\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.95161\n",
      "exploration/env_infos/final/reward_dist Mean             0.0836337\n",
      "exploration/env_infos/final/reward_dist Std              0.167267\n",
      "exploration/env_infos/final/reward_dist Max              0.418169\n",
      "exploration/env_infos/final/reward_dist Min              2.00136e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0248328\n",
      "exploration/env_infos/initial/reward_dist Std            0.0151923\n",
      "exploration/env_infos/initial/reward_dist Max            0.039845\n",
      "exploration/env_infos/initial/reward_dist Min            0.00100139\n",
      "exploration/env_infos/reward_dist Mean                   0.15289\n",
      "exploration/env_infos/reward_dist Std                    0.216954\n",
      "exploration/env_infos/reward_dist Max                    0.82062\n",
      "exploration/env_infos/reward_dist Min                    2.00136e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.179852\n",
      "exploration/env_infos/final/reward_energy Std            0.0759648\n",
      "exploration/env_infos/final/reward_energy Max           -0.0723736\n",
      "exploration/env_infos/final/reward_energy Min           -0.298487\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.347818\n",
      "exploration/env_infos/initial/reward_energy Std          0.182724\n",
      "exploration/env_infos/initial/reward_energy Max         -0.160703\n",
      "exploration/env_infos/initial/reward_energy Min         -0.686753\n",
      "exploration/env_infos/reward_energy Mean                -0.155217\n",
      "exploration/env_infos/reward_energy Std                  0.108384\n",
      "exploration/env_infos/reward_energy Max                 -0.0263635\n",
      "exploration/env_infos/reward_energy Min                 -0.686753\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0692866\n",
      "exploration/env_infos/final/end_effector_loc Std         0.314408\n",
      "exploration/env_infos/final/end_effector_loc Max         0.527189\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.484318\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00571463\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.012661\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0315227\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0148209\n",
      "exploration/env_infos/end_effector_loc Mean              0.010301\n",
      "exploration/env_infos/end_effector_loc Std               0.214777\n",
      "exploration/env_infos/end_effector_loc Max               0.527189\n",
      "exploration/env_infos/end_effector_loc Min              -0.484318\n",
      "evaluation/num steps total                           72000\n",
      "evaluation/num paths total                            3600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0643154\n",
      "evaluation/Rewards Std                                   0.0916732\n",
      "evaluation/Rewards Max                                   0.173232\n",
      "evaluation/Rewards Min                                  -0.631538\n",
      "evaluation/Returns Mean                                 -1.28631\n",
      "evaluation/Returns Std                                   1.50642\n",
      "evaluation/Returns Max                                   2.2777\n",
      "evaluation/Returns Min                                  -7.36786\n",
      "evaluation/Actions Mean                                 -0.00074231\n",
      "evaluation/Actions Std                                   0.0850121\n",
      "evaluation/Actions Max                                   0.814558\n",
      "evaluation/Actions Min                                  -0.543052\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.28631\n",
      "evaluation/env_infos/final/reward_dist Mean              0.102756\n",
      "evaluation/env_infos/final/reward_dist Std               0.160524\n",
      "evaluation/env_infos/final/reward_dist Max               0.703931\n",
      "evaluation/env_infos/final/reward_dist Min               2.06458e-59\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00560481\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0101943\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0430241\n",
      "evaluation/env_infos/initial/reward_dist Min             1.98267e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.126247\n",
      "evaluation/env_infos/reward_dist Std                     0.217245\n",
      "evaluation/env_infos/reward_dist Max                     0.999633\n",
      "evaluation/env_infos/reward_dist Min                     2.06458e-59\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0418167\n",
      "evaluation/env_infos/final/reward_energy Std             0.0314945\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00405343\n",
      "evaluation/env_infos/final/reward_energy Min            -0.134584\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.268534\n",
      "evaluation/env_infos/initial/reward_energy Std           0.254125\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00270151\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.0313\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0710271\n",
      "evaluation/env_infos/reward_energy Std                   0.0970071\n",
      "evaluation/env_infos/reward_energy Max                  -0.00201832\n",
      "evaluation/env_infos/reward_energy Min                  -1.0313\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0188167\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.321792\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.78099\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00292404\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127402\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0407279\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0271526\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00247016\n",
      "evaluation/env_infos/end_effector_loc Std                0.209625\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.78099\n",
      "time/data storing (s)                                    0.167158\n",
      "time/evaluation sampling (s)                             1.12425\n",
      "time/exploration sampling (s)                            0.111454\n",
      "time/logging (s)                                         0.0189503\n",
      "time/saving (s)                                          0.0266545\n",
      "time/training (s)                                       46.9329\n",
      "time/epoch (s)                                          48.3814\n",
      "time/total (s)                                        3383.62\n",
      "Epoch                                                   71\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:09:42.054177 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 72 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00142788\n",
      "trainer/QF2 Loss                                         0.00135513\n",
      "trainer/Policy Loss                                      2.88382\n",
      "trainer/Q1 Predictions Mean                             -0.798937\n",
      "trainer/Q1 Predictions Std                               0.81679\n",
      "trainer/Q1 Predictions Max                               1.05354\n",
      "trainer/Q1 Predictions Min                              -2.90704\n",
      "trainer/Q2 Predictions Mean                             -0.791587\n",
      "trainer/Q2 Predictions Std                               0.817213\n",
      "trainer/Q2 Predictions Max                               1.04933\n",
      "trainer/Q2 Predictions Min                              -2.89636\n",
      "trainer/Q Targets Mean                                  -0.782208\n",
      "trainer/Q Targets Std                                    0.819943\n",
      "trainer/Q Targets Max                                    1.07419\n",
      "trainer/Q Targets Min                                   -2.89446\n",
      "trainer/Log Pis Mean                                     2.09614\n",
      "trainer/Log Pis Std                                      1.28834\n",
      "trainer/Log Pis Max                                      4.83306\n",
      "trainer/Log Pis Min                                     -2.07575\n",
      "trainer/Policy mu Mean                                  -0.0310331\n",
      "trainer/Policy mu Std                                    0.32318\n",
      "trainer/Policy mu Max                                    1.91801\n",
      "trainer/Policy mu Min                                   -2.03005\n",
      "trainer/Policy log std Mean                             -2.31796\n",
      "trainer/Policy log std Std                               0.556814\n",
      "trainer/Policy log std Max                              -0.529967\n",
      "trainer/Policy log std Min                              -3.41486\n",
      "trainer/Alpha                                            0.0201437\n",
      "trainer/Alpha Loss                                       0.375476\n",
      "exploration/num steps total                           8300\n",
      "exploration/num paths total                            415\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.130061\n",
      "exploration/Rewards Std                                  0.089653\n",
      "exploration/Rewards Max                                  0.0480793\n",
      "exploration/Rewards Min                                 -0.630063\n",
      "exploration/Returns Mean                                -2.60121\n",
      "exploration/Returns Std                                  1.21619\n",
      "exploration/Returns Max                                 -0.884479\n",
      "exploration/Returns Min                                 -4.34758\n",
      "exploration/Actions Mean                                -0.0118182\n",
      "exploration/Actions Std                                  0.0984458\n",
      "exploration/Actions Max                                  0.259133\n",
      "exploration/Actions Min                                 -0.878233\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.60121\n",
      "exploration/env_infos/final/reward_dist Mean             0.00660127\n",
      "exploration/env_infos/final/reward_dist Std              0.00859268\n",
      "exploration/env_infos/final/reward_dist Max              0.0211165\n",
      "exploration/env_infos/final/reward_dist Min              1.28602e-65\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00334106\n",
      "exploration/env_infos/initial/reward_dist Std            0.00431267\n",
      "exploration/env_infos/initial/reward_dist Max            0.0107332\n",
      "exploration/env_infos/initial/reward_dist Min            1.6406e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0222145\n",
      "exploration/env_infos/reward_dist Std                    0.0608461\n",
      "exploration/env_infos/reward_dist Max                    0.327273\n",
      "exploration/env_infos/reward_dist Min                    1.28602e-65\n",
      "exploration/env_infos/final/reward_energy Mean          -0.115969\n",
      "exploration/env_infos/final/reward_energy Std            0.0444587\n",
      "exploration/env_infos/final/reward_energy Max           -0.0609994\n",
      "exploration/env_infos/final/reward_energy Min           -0.177214\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.258935\n",
      "exploration/env_infos/initial/reward_energy Std          0.330519\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0505179\n",
      "exploration/env_infos/initial/reward_energy Min         -0.915665\n",
      "exploration/env_infos/reward_energy Mean                -0.1005\n",
      "exploration/env_infos/reward_energy Std                  0.097787\n",
      "exploration/env_infos/reward_energy Max                 -0.00891952\n",
      "exploration/env_infos/reward_energy Min                 -0.915665\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.217376\n",
      "exploration/env_infos/final/end_effector_loc Std         0.32727\n",
      "exploration/env_infos/final/end_effector_loc Max         0.261947\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00430195\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0142076\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0129567\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0439116\n",
      "exploration/env_infos/end_effector_loc Mean             -0.101746\n",
      "exploration/env_infos/end_effector_loc Std               0.209687\n",
      "exploration/env_infos/end_effector_loc Max               0.261947\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           73000\n",
      "evaluation/num paths total                            3650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0598433\n",
      "evaluation/Rewards Std                                   0.0811109\n",
      "evaluation/Rewards Max                                   0.152147\n",
      "evaluation/Rewards Min                                  -0.563123\n",
      "evaluation/Returns Mean                                 -1.19687\n",
      "evaluation/Returns Std                                   1.18617\n",
      "evaluation/Returns Max                                   1.5365\n",
      "evaluation/Returns Min                                  -4.72268\n",
      "evaluation/Actions Mean                                  0.00460092\n",
      "evaluation/Actions Std                                   0.0812788\n",
      "evaluation/Actions Max                                   0.843915\n",
      "evaluation/Actions Min                                  -0.868523\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.19687\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0866092\n",
      "evaluation/env_infos/final/reward_dist Std               0.18942\n",
      "evaluation/env_infos/final/reward_dist Max               0.924576\n",
      "evaluation/env_infos/final/reward_dist Min               2.68261e-96\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.008785\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0140511\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0554042\n",
      "evaluation/env_infos/initial/reward_dist Min             1.56761e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.142005\n",
      "evaluation/env_infos/reward_dist Std                     0.245847\n",
      "evaluation/env_infos/reward_dist Max                     0.994933\n",
      "evaluation/env_infos/reward_dist Min                     2.68261e-96\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0305857\n",
      "evaluation/env_infos/final/reward_energy Std             0.0308116\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0025645\n",
      "evaluation/env_infos/final/reward_energy Min            -0.155842\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.267064\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255155\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00443102\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.00033\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0665778\n",
      "evaluation/env_infos/reward_energy Std                   0.0939266\n",
      "evaluation/env_infos/reward_energy Max                  -0.000647161\n",
      "evaluation/env_infos/reward_energy Min                  -1.00033\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0200636\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.362969\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.793887\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000344109\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0130543\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0421958\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0434262\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00400434\n",
      "evaluation/env_infos/end_effector_loc Std                0.229749\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.793887\n",
      "time/data storing (s)                                    0.164325\n",
      "time/evaluation sampling (s)                             0.768367\n",
      "time/exploration sampling (s)                            0.107178\n",
      "time/logging (s)                                         0.0190129\n",
      "time/saving (s)                                          0.0273273\n",
      "time/training (s)                                       46.6394\n",
      "time/epoch (s)                                          47.7256\n",
      "time/total (s)                                        3432.07\n",
      "Epoch                                                   72\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:10:32.127857 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 73 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00126714\r\n",
      "trainer/QF2 Loss                                         0.00225946\r\n",
      "trainer/Policy Loss                                      2.88533\r\n",
      "trainer/Q1 Predictions Mean                             -0.974914\r\n",
      "trainer/Q1 Predictions Std                               0.758248\r\n",
      "trainer/Q1 Predictions Max                               0.635518\r\n",
      "trainer/Q1 Predictions Min                              -2.74514\r\n",
      "trainer/Q2 Predictions Mean                             -0.972667\r\n",
      "trainer/Q2 Predictions Std                               0.76481\r\n",
      "trainer/Q2 Predictions Max                               0.64446\r\n",
      "trainer/Q2 Predictions Min                              -2.78013\r\n",
      "trainer/Q Targets Mean                                  -0.971817\r\n",
      "trainer/Q Targets Std                                    0.763859\r\n",
      "trainer/Q Targets Max                                    0.640672\r\n",
      "trainer/Q Targets Min                                   -2.79463\r\n",
      "trainer/Log Pis Mean                                     1.90058\r\n",
      "trainer/Log Pis Std                                      1.27135\r\n",
      "trainer/Log Pis Max                                      4.25212\r\n",
      "trainer/Log Pis Min                                     -4.5051\r\n",
      "trainer/Policy mu Mean                                  -0.0173874\r\n",
      "trainer/Policy mu Std                                    0.19682\r\n",
      "trainer/Policy mu Max                                    1.09775\r\n",
      "trainer/Policy mu Min                                   -1.44059\r\n",
      "trainer/Policy log std Mean                             -2.33868\r\n",
      "trainer/Policy log std Std                               0.529098\r\n",
      "trainer/Policy log std Max                              -0.631195\r\n",
      "trainer/Policy log std Min                              -3.31972\r\n",
      "trainer/Alpha                                            0.0204847\r\n",
      "trainer/Alpha Loss                                      -0.386569\r\n",
      "exploration/num steps total                           8400\r\n",
      "exploration/num paths total                            420\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.103644\r\n",
      "exploration/Rewards Std                                  0.0453537\r\n",
      "exploration/Rewards Max                                 -0.028761\r\n",
      "exploration/Rewards Min                                 -0.246965\r\n",
      "exploration/Returns Mean                                -2.07288\r\n",
      "exploration/Returns Std                                  0.41811\r\n",
      "exploration/Returns Max                                 -1.37992\r\n",
      "exploration/Returns Min                                 -2.6616\r\n",
      "exploration/Actions Mean                                 0.00285621\r\n",
      "exploration/Actions Std                                  0.123034\r\n",
      "exploration/Actions Max                                  0.494762\r\n",
      "exploration/Actions Min                                 -0.456777\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.07288\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.000329585\r\n",
      "exploration/env_infos/final/reward_dist Std              0.000390509\r\n",
      "exploration/env_infos/final/reward_dist Max              0.00101656\r\n",
      "exploration/env_infos/final/reward_dist Min              2.16237e-46\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00422906\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00461178\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0126591\r\n",
      "exploration/env_infos/initial/reward_dist Min            9.02808e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.117479\r\n",
      "exploration/env_infos/reward_dist Std                    0.220434\r\n",
      "exploration/env_infos/reward_dist Max                    0.948325\r\n",
      "exploration/env_infos/reward_dist Min                    2.16237e-46\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.178816\r\n",
      "exploration/env_infos/final/reward_energy Std            0.133788\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0389955\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.407716\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.280625\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.185593\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0759274\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.53068\r\n",
      "exploration/env_infos/reward_energy Mean                -0.138332\r\n",
      "exploration/env_infos/reward_energy Std                  0.105618\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0123783\r\n",
      "exploration/env_infos/reward_energy Min                 -0.53068\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0701589\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.430905\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.850975\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.680466\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00116872\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0118376\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0213097\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0228389\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0313116\r\n",
      "exploration/env_infos/end_effector_loc Std               0.258984\r\n",
      "exploration/env_infos/end_effector_loc Max               0.850975\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.680466\r\n",
      "evaluation/num steps total                           74000\r\n",
      "evaluation/num paths total                            3700\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.064552\r\n",
      "evaluation/Rewards Std                                   0.0747919\r\n",
      "evaluation/Rewards Max                                   0.0991203\r\n",
      "evaluation/Rewards Min                                  -0.496348\r\n",
      "evaluation/Returns Mean                                 -1.29104\r\n",
      "evaluation/Returns Std                                   1.01437\r\n",
      "evaluation/Returns Max                                   0.921625\r\n",
      "evaluation/Returns Min                                  -3.01171\r\n",
      "evaluation/Actions Mean                                 -0.0025643\r\n",
      "evaluation/Actions Std                                   0.075654\r\n",
      "evaluation/Actions Max                                   0.55161\r\n",
      "evaluation/Actions Min                                  -0.752748\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.29104\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.100738\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.191017\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.760824\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.96009e-77\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00776799\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0128593\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0660154\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.1181e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.157409\r\n",
      "evaluation/env_infos/reward_dist Std                     0.247105\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998565\r\n",
      "evaluation/env_infos/reward_dist Min                     4.96009e-77\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.038154\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0238547\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00547405\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.127318\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.247162\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.184202\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00902379\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.80111\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0686351\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0821549\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.001017\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.80111\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0832379\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.317161\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/end_effector_loc Max          0.840122\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.889725\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00195833\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010721\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0275805\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0376374\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0444986\n",
      "evaluation/env_infos/end_effector_loc Std                0.205307\n",
      "evaluation/env_infos/end_effector_loc Max                0.840122\n",
      "evaluation/env_infos/end_effector_loc Min               -0.889725\n",
      "time/data storing (s)                                    0.167164\n",
      "time/evaluation sampling (s)                             0.776407\n",
      "time/exploration sampling (s)                            0.111704\n",
      "time/logging (s)                                         0.0186286\n",
      "time/saving (s)                                          0.0264941\n",
      "time/training (s)                                       48.3137\n",
      "time/epoch (s)                                          49.4141\n",
      "time/total (s)                                        3482.14\n",
      "Epoch                                                   73\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 14:11:20.749009 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 74 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00137203\n",
      "trainer/QF2 Loss                                         0.00134952\n",
      "trainer/Policy Loss                                      2.92009\n",
      "trainer/Q1 Predictions Mean                             -0.965522\n",
      "trainer/Q1 Predictions Std                               0.81573\n",
      "trainer/Q1 Predictions Max                               1.1655\n",
      "trainer/Q1 Predictions Min                              -2.85185\n",
      "trainer/Q2 Predictions Mean                             -0.960854\n",
      "trainer/Q2 Predictions Std                               0.814889\n",
      "trainer/Q2 Predictions Max                               1.15552\n",
      "trainer/Q2 Predictions Min                              -2.81634\n",
      "trainer/Q Targets Mean                                  -0.95975\n",
      "trainer/Q Targets Std                                    0.820011\n",
      "trainer/Q Targets Max                                    1.13265\n",
      "trainer/Q Targets Min                                   -2.83139\n",
      "trainer/Log Pis Mean                                     1.95563\n",
      "trainer/Log Pis Std                                      1.3276\n",
      "trainer/Log Pis Max                                      4.63315\n",
      "trainer/Log Pis Min                                     -2.12516\n",
      "trainer/Policy mu Mean                                  -0.0240595\n",
      "trainer/Policy mu Std                                    0.228693\n",
      "trainer/Policy mu Max                                    1.21991\n",
      "trainer/Policy mu Min                                   -1.87915\n",
      "trainer/Policy log std Mean                             -2.33637\n",
      "trainer/Policy log std Std                               0.551882\n",
      "trainer/Policy log std Max                               0.160458\n",
      "trainer/Policy log std Min                              -3.26375\n",
      "trainer/Alpha                                            0.0191825\n",
      "trainer/Alpha Loss                                      -0.175405\n",
      "exploration/num steps total                           8500\n",
      "exploration/num paths total                            425\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.05609\n",
      "exploration/Rewards Std                                  0.0684455\n",
      "exploration/Rewards Max                                  0.152427\n",
      "exploration/Rewards Min                                 -0.271727\n",
      "exploration/Returns Mean                                -1.1218\n",
      "exploration/Returns Std                                  0.585192\n",
      "exploration/Returns Max                                 -0.484939\n",
      "exploration/Returns Min                                 -2.09027\n",
      "exploration/Actions Mean                                 0.0188677\n",
      "exploration/Actions Std                                  0.143239\n",
      "exploration/Actions Max                                  0.756489\n",
      "exploration/Actions Min                                 -0.420707\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.1218\n",
      "exploration/env_infos/final/reward_dist Mean             0.0349039\n",
      "exploration/env_infos/final/reward_dist Std              0.0465911\n",
      "exploration/env_infos/final/reward_dist Max              0.1178\n",
      "exploration/env_infos/final/reward_dist Min              4.14649e-45\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00362786\n",
      "exploration/env_infos/initial/reward_dist Std            0.0037037\n",
      "exploration/env_infos/initial/reward_dist Max            0.0101225\n",
      "exploration/env_infos/initial/reward_dist Min            3.94907e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0887152\n",
      "exploration/env_infos/reward_dist Std                    0.155858\n",
      "exploration/env_infos/reward_dist Max                    0.85719\n",
      "exploration/env_infos/reward_dist Min                    4.14649e-45\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0910274\n",
      "exploration/env_infos/final/reward_energy Std            0.0606946\n",
      "exploration/env_infos/final/reward_energy Max           -0.0138986\n",
      "exploration/env_infos/final/reward_energy Min           -0.180116\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.325468\n",
      "exploration/env_infos/initial/reward_energy Std          0.273386\n",
      "exploration/env_infos/initial/reward_energy Max         -0.106946\n",
      "exploration/env_infos/initial/reward_energy Min         -0.859052\n",
      "exploration/env_infos/reward_energy Mean                -0.152422\n",
      "exploration/env_infos/reward_energy Std                  0.136068\n",
      "exploration/env_infos/reward_energy Max                 -0.00335587\n",
      "exploration/env_infos/reward_energy Min                 -0.859052\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.223539\n",
      "exploration/env_infos/final/end_effector_loc Std         0.320003\n",
      "exploration/env_infos/final/end_effector_loc Max         0.768592\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.280755\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00698075\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0133081\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0378245\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00959225\n",
      "exploration/env_infos/end_effector_loc Mean              0.0957837\n",
      "exploration/env_infos/end_effector_loc Std               0.203856\n",
      "exploration/env_infos/end_effector_loc Max               0.768592\n",
      "exploration/env_infos/end_effector_loc Min              -0.314827\n",
      "evaluation/num steps total                           75000\n",
      "evaluation/num paths total                            3750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0587412\n",
      "evaluation/Rewards Std                                   0.0933635\n",
      "evaluation/Rewards Max                                   0.151236\n",
      "evaluation/Rewards Min                                  -0.947632\n",
      "evaluation/Returns Mean                                 -1.17482\n",
      "evaluation/Returns Std                                   1.34243\n",
      "evaluation/Returns Max                                   1.00765\n",
      "evaluation/Returns Min                                  -4.23699\n",
      "evaluation/Actions Mean                                  0.0039149\n",
      "evaluation/Actions Std                                   0.0867878\n",
      "evaluation/Actions Max                                   0.780367\n",
      "evaluation/Actions Min                                  -0.933564\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.17482\n",
      "evaluation/env_infos/final/reward_dist Mean              0.118439\n",
      "evaluation/env_infos/final/reward_dist Std               0.231896\n",
      "evaluation/env_infos/final/reward_dist Max               0.97853\n",
      "evaluation/env_infos/final/reward_dist Min               2.67237e-58\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0134199\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0254206\n",
      "evaluation/env_infos/initial/reward_dist Max             0.13215\n",
      "evaluation/env_infos/initial/reward_dist Min             3.38288e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.187059\n",
      "evaluation/env_infos/reward_dist Std                     0.267738\n",
      "evaluation/env_infos/reward_dist Max                     0.996472\n",
      "evaluation/env_infos/reward_dist Min                     2.67237e-58\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0428787\n",
      "evaluation/env_infos/final/reward_energy Std             0.0300595\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00929539\n",
      "evaluation/env_infos/final/reward_energy Min            -0.186069\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.306453\n",
      "evaluation/env_infos/initial/reward_energy Std           0.276229\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0288803\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.20162\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0708659\n",
      "evaluation/env_infos/reward_energy Std                   0.100364\n",
      "evaluation/env_infos/reward_energy Max                  -0.00239083\n",
      "evaluation/env_infos/reward_energy Min                  -1.20162\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0257889\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.301485\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.794313\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0012399\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0145338\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0390183\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0466782\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0101003\n",
      "evaluation/env_infos/end_effector_loc Std                0.195949\n",
      "evaluation/env_infos/end_effector_loc Max                0.794313\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.166777\n",
      "time/evaluation sampling (s)                             0.812633\n",
      "time/exploration sampling (s)                            0.107538\n",
      "time/logging (s)                                         0.018777\n",
      "time/saving (s)                                          0.0280876\n",
      "time/training (s)                                       46.8089\n",
      "time/epoch (s)                                          47.9427\n",
      "time/total (s)                                        3530.76\n",
      "Epoch                                                   74\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:12:10.591738 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 75 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00206685\r\n",
      "trainer/QF2 Loss                                         0.00113072\r\n",
      "trainer/Policy Loss                                      2.82597\r\n",
      "trainer/Q1 Predictions Mean                             -0.869192\r\n",
      "trainer/Q1 Predictions Std                               0.713722\r\n",
      "trainer/Q1 Predictions Max                               1.13503\r\n",
      "trainer/Q1 Predictions Min                              -2.69807\r\n",
      "trainer/Q2 Predictions Mean                             -0.867744\r\n",
      "trainer/Q2 Predictions Std                               0.715093\r\n",
      "trainer/Q2 Predictions Max                               1.16099\r\n",
      "trainer/Q2 Predictions Min                              -2.7211\r\n",
      "trainer/Q Targets Mean                                  -0.861789\r\n",
      "trainer/Q Targets Std                                    0.71455\r\n",
      "trainer/Q Targets Max                                    1.14898\r\n",
      "trainer/Q Targets Min                                   -2.66803\r\n",
      "trainer/Log Pis Mean                                     1.9531\r\n",
      "trainer/Log Pis Std                                      1.35311\r\n",
      "trainer/Log Pis Max                                      4.62962\r\n",
      "trainer/Log Pis Min                                     -1.86519\r\n",
      "trainer/Policy mu Mean                                  -0.0286248\r\n",
      "trainer/Policy mu Std                                    0.207842\r\n",
      "trainer/Policy mu Max                                    0.645282\r\n",
      "trainer/Policy mu Min                                   -1.24068\r\n",
      "trainer/Policy log std Mean                             -2.36538\r\n",
      "trainer/Policy log std Std                               0.540142\r\n",
      "trainer/Policy log std Max                              -0.603033\r\n",
      "trainer/Policy log std Min                              -3.32955\r\n",
      "trainer/Alpha                                            0.0186099\r\n",
      "trainer/Alpha Loss                                      -0.186828\r\n",
      "exploration/num steps total                           8600\r\n",
      "exploration/num paths total                            430\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0933084\r\n",
      "exploration/Rewards Std                                  0.0637044\r\n",
      "exploration/Rewards Max                                  0.000955281\r\n",
      "exploration/Rewards Min                                 -0.30883\r\n",
      "exploration/Returns Mean                                -1.86617\r\n",
      "exploration/Returns Std                                  1.07698\r\n",
      "exploration/Returns Max                                 -0.845961\r\n",
      "exploration/Returns Min                                 -3.90998\r\n",
      "exploration/Actions Mean                                 0.00233031\r\n",
      "exploration/Actions Std                                  0.128346\r\n",
      "exploration/Actions Max                                  0.42197\r\n",
      "exploration/Actions Min                                 -0.717944\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.86617\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.173371\r\n",
      "exploration/env_infos/final/reward_dist Std              0.325754\r\n",
      "exploration/env_infos/final/reward_dist Max              0.824472\r\n",
      "exploration/env_infos/final/reward_dist Min              3.63333e-44\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00595519\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00623853\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0165123\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000182635\r\n",
      "exploration/env_infos/reward_dist Mean                   0.124883\r\n",
      "exploration/env_infos/reward_dist Std                    0.220588\r\n",
      "exploration/env_infos/reward_dist Max                    0.970077\r\n",
      "exploration/env_infos/reward_dist Min                    3.63333e-44\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.123343\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0574819\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0469887\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.215728\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.305349\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.275377\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.049373\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.832768\r\n",
      "exploration/env_infos/reward_energy Mean                -0.142219\r\n",
      "exploration/env_infos/reward_energy Std                  0.112828\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00901246\r\n",
      "exploration/env_infos/reward_energy Min                 -0.832768\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0411491\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.281424\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.49878\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.399694\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00477541\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0137308\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0210985\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0358972\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0353092\r\n",
      "exploration/env_infos/end_effector_loc Std               0.174848\r\n",
      "exploration/env_infos/end_effector_loc Max               0.49878\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.404497\r\n",
      "evaluation/num steps total                           76000\r\n",
      "evaluation/num paths total                            3800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0737405\r\n",
      "evaluation/Rewards Std                                   0.102614\r\n",
      "evaluation/Rewards Max                                   0.157104\r\n",
      "evaluation/Rewards Min                                  -0.796734\r\n",
      "evaluation/Returns Mean                                 -1.47481\r\n",
      "evaluation/Returns Std                                   1.61133\r\n",
      "evaluation/Returns Max                                   1.63606\r\n",
      "evaluation/Returns Min                                  -8.57255\r\n",
      "evaluation/Actions Mean                                 -0.00264512\r\n",
      "evaluation/Actions Std                                   0.108019\r\n",
      "evaluation/Actions Max                                   0.867847\r\n",
      "evaluation/Actions Min                                  -0.878797\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.47481\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.106346\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.216313\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.92052\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.30267e-128\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00493286\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0118293\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0632614\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.64718e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0948826\r\n",
      "evaluation/env_infos/reward_dist Std                     0.196705\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996801\r\n",
      "evaluation/env_infos/reward_dist Min                     2.30267e-128\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0546446\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.139209\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000731382\r\n",
      "evaluation/env_infos/final/reward_energy Min            -1.0091\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.233487\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.244959\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00363257\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.13407\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.07956\r\n",
      "evaluation/env_infos/reward_energy Std                   0.130462\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000326616\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.13407\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0577612\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.326394\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00129923\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0118938\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0398734\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0439398\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0308238\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.208009\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.164981\r\n",
      "time/evaluation sampling (s)                             0.799401\r\n",
      "time/exploration sampling (s)                            0.108087\r\n",
      "time/logging (s)                                         0.0203394\r\n",
      "time/saving (s)                                          0.0261301\r\n",
      "time/training (s)                                       47.9878\r\n",
      "time/epoch (s)                                          49.1067\r\n",
      "time/total (s)                                        3580.6\r\n",
      "Epoch                                                   75\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:12:59.446125 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 76 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00171979\n",
      "trainer/QF2 Loss                                         0.000888384\n",
      "trainer/Policy Loss                                      3.17894\n",
      "trainer/Q1 Predictions Mean                             -0.998217\n",
      "trainer/Q1 Predictions Std                               0.765731\n",
      "trainer/Q1 Predictions Max                               1.13298\n",
      "trainer/Q1 Predictions Min                              -2.66103\n",
      "trainer/Q2 Predictions Mean                             -1.00156\n",
      "trainer/Q2 Predictions Std                               0.770912\n",
      "trainer/Q2 Predictions Max                               1.16661\n",
      "trainer/Q2 Predictions Min                              -2.65416\n",
      "trainer/Q Targets Mean                                  -1.00116\n",
      "trainer/Q Targets Std                                    0.773023\n",
      "trainer/Q Targets Max                                    1.13715\n",
      "trainer/Q Targets Min                                   -2.66048\n",
      "trainer/Log Pis Mean                                     2.17848\n",
      "trainer/Log Pis Std                                      1.48785\n",
      "trainer/Log Pis Max                                      4.83691\n",
      "trainer/Log Pis Min                                     -2.96215\n",
      "trainer/Policy mu Mean                                  -0.0238717\n",
      "trainer/Policy mu Std                                    0.188655\n",
      "trainer/Policy mu Max                                    0.751805\n",
      "trainer/Policy mu Min                                   -1.3829\n",
      "trainer/Policy log std Mean                             -2.43608\n",
      "trainer/Policy log std Std                               0.588696\n",
      "trainer/Policy log std Max                              -0.432903\n",
      "trainer/Policy log std Min                              -3.45681\n",
      "trainer/Alpha                                            0.0196169\n",
      "trainer/Alpha Loss                                       0.701842\n",
      "exploration/num steps total                           8700\n",
      "exploration/num paths total                            435\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0673062\n",
      "exploration/Rewards Std                                  0.0972882\n",
      "exploration/Rewards Max                                  0.154643\n",
      "exploration/Rewards Min                                 -0.425778\n",
      "exploration/Returns Mean                                -1.34612\n",
      "exploration/Returns Std                                  1.2302\n",
      "exploration/Returns Max                                  0.661957\n",
      "exploration/Returns Min                                 -3.20447\n",
      "exploration/Actions Mean                                -0.00235587\n",
      "exploration/Actions Std                                  0.161586\n",
      "exploration/Actions Max                                  0.659475\n",
      "exploration/Actions Min                                 -0.469457\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.34612\n",
      "exploration/env_infos/final/reward_dist Mean             0.00183706\n",
      "exploration/env_infos/final/reward_dist Std              0.00241863\n",
      "exploration/env_infos/final/reward_dist Max              0.0059997\n",
      "exploration/env_infos/final/reward_dist Min              3.11264e-29\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0094094\n",
      "exploration/env_infos/initial/reward_dist Std            0.0135771\n",
      "exploration/env_infos/initial/reward_dist Max            0.036172\n",
      "exploration/env_infos/initial/reward_dist Min            9.40095e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0933105\n",
      "exploration/env_infos/reward_dist Std                    0.179927\n",
      "exploration/env_infos/reward_dist Max                    0.927369\n",
      "exploration/env_infos/reward_dist Min                    3.11264e-29\n",
      "exploration/env_infos/final/reward_energy Mean          -0.177175\n",
      "exploration/env_infos/final/reward_energy Std            0.0399084\n",
      "exploration/env_infos/final/reward_energy Max           -0.106465\n",
      "exploration/env_infos/final/reward_energy Min           -0.214091\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.261542\n",
      "exploration/env_infos/initial/reward_energy Std          0.115779\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0780291\n",
      "exploration/env_infos/initial/reward_energy Min         -0.427863\n",
      "exploration/env_infos/reward_energy Mean                -0.178115\n",
      "exploration/env_infos/reward_energy Std                  0.1432\n",
      "exploration/env_infos/reward_energy Max                 -0.00371719\n",
      "exploration/env_infos/reward_energy Min                 -0.663686\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00662582\n",
      "exploration/env_infos/final/end_effector_loc Std         0.245185\n",
      "exploration/env_infos/final/end_effector_loc Max         0.421243\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.34402\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00172427\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00996434\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0104682\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0190869\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00422964\n",
      "exploration/env_infos/end_effector_loc Std               0.158882\n",
      "exploration/env_infos/end_effector_loc Max               0.421243\n",
      "exploration/env_infos/end_effector_loc Min              -0.34402\n",
      "evaluation/num steps total                           77000\n",
      "evaluation/num paths total                            3850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0613266\n",
      "evaluation/Rewards Std                                   0.0959769\n",
      "evaluation/Rewards Max                                   0.168125\n",
      "evaluation/Rewards Min                                  -0.649148\n",
      "evaluation/Returns Mean                                 -1.22653\n",
      "evaluation/Returns Std                                   1.5654\n",
      "evaluation/Returns Max                                   1.62949\n",
      "evaluation/Returns Min                                  -7.81073\n",
      "evaluation/Actions Mean                                  0.000997759\n",
      "evaluation/Actions Std                                   0.0931671\n",
      "evaluation/Actions Max                                   0.94607\n",
      "evaluation/Actions Min                                  -0.646744\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.22653\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0813231\n",
      "evaluation/env_infos/final/reward_dist Std               0.175217\n",
      "evaluation/env_infos/final/reward_dist Max               0.950606\n",
      "evaluation/env_infos/final/reward_dist Min               9.57957e-162\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00878383\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0162957\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0745796\n",
      "evaluation/env_infos/initial/reward_dist Min             3.8741e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.115485\n",
      "evaluation/env_infos/reward_dist Std                     0.208985\n",
      "evaluation/env_infos/reward_dist Max                     0.989402\n",
      "evaluation/env_infos/reward_dist Min                     9.57957e-162\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0444896\n",
      "evaluation/env_infos/final/reward_energy Std             0.0358497\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00221954\n",
      "evaluation/env_infos/final/reward_energy Min            -0.177137\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.248885\n",
      "evaluation/env_infos/initial/reward_energy Std           0.207136\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00295722\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.95471\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0764049\n",
      "evaluation/env_infos/reward_energy Std                   0.107352\n",
      "evaluation/env_infos/reward_energy Max                  -0.000976439\n",
      "evaluation/env_infos/reward_energy Min                  -0.95471\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0160253\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.376884\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -3.39248e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0114481\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0473035\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0323372\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00776573\n",
      "evaluation/env_infos/end_effector_loc Std                0.226699\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.185116\n",
      "time/evaluation sampling (s)                             0.801253\n",
      "time/exploration sampling (s)                            0.10603\n",
      "time/logging (s)                                         0.0195311\n",
      "time/saving (s)                                          0.0292808\n",
      "time/training (s)                                       47.0011\n",
      "time/epoch (s)                                          48.1423\n",
      "time/total (s)                                        3629.45\n",
      "Epoch                                                   76\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:13:49.200629 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 77 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00110795\n",
      "trainer/QF2 Loss                                         0.000827104\n",
      "trainer/Policy Loss                                      3.12704\n",
      "trainer/Q1 Predictions Mean                             -1.00945\n",
      "trainer/Q1 Predictions Std                               0.775589\n",
      "trainer/Q1 Predictions Max                               0.714811\n",
      "trainer/Q1 Predictions Min                              -2.79336\n",
      "trainer/Q2 Predictions Mean                             -1.02044\n",
      "trainer/Q2 Predictions Std                               0.770218\n",
      "trainer/Q2 Predictions Max                               0.668219\n",
      "trainer/Q2 Predictions Min                              -2.7995\n",
      "trainer/Q Targets Mean                                  -1.01822\n",
      "trainer/Q Targets Std                                    0.772421\n",
      "trainer/Q Targets Max                                    0.739905\n",
      "trainer/Q Targets Min                                   -2.79491\n",
      "trainer/Log Pis Mean                                     2.11587\n",
      "trainer/Log Pis Std                                      1.4248\n",
      "trainer/Log Pis Max                                      4.97303\n",
      "trainer/Log Pis Min                                     -2.67502\n",
      "trainer/Policy mu Mean                                  -0.00238687\n",
      "trainer/Policy mu Std                                    0.207801\n",
      "trainer/Policy mu Max                                    1.41259\n",
      "trainer/Policy mu Min                                   -1.94105\n",
      "trainer/Policy log std Mean                             -2.42977\n",
      "trainer/Policy log std Std                               0.55707\n",
      "trainer/Policy log std Max                               0.188823\n",
      "trainer/Policy log std Min                              -3.51696\n",
      "trainer/Alpha                                            0.019817\n",
      "trainer/Alpha Loss                                       0.45454\n",
      "exploration/num steps total                           8800\n",
      "exploration/num paths total                            440\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0675318\n",
      "exploration/Rewards Std                                  0.0885791\n",
      "exploration/Rewards Max                                  0.100044\n",
      "exploration/Rewards Min                                 -0.268861\n",
      "exploration/Returns Mean                                -1.35064\n",
      "exploration/Returns Std                                  1.55394\n",
      "exploration/Returns Max                                  0.867229\n",
      "exploration/Returns Min                                 -3.33703\n",
      "exploration/Actions Mean                                -0.00301566\n",
      "exploration/Actions Std                                  0.119214\n",
      "exploration/Actions Max                                  0.504202\n",
      "exploration/Actions Min                                 -0.37133\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.35064\n",
      "exploration/env_infos/final/reward_dist Mean             0.151851\n",
      "exploration/env_infos/final/reward_dist Std              0.1896\n",
      "exploration/env_infos/final/reward_dist Max              0.438025\n",
      "exploration/env_infos/final/reward_dist Min              3.05527e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0122169\n",
      "exploration/env_infos/initial/reward_dist Std            0.0173703\n",
      "exploration/env_infos/initial/reward_dist Max            0.0467163\n",
      "exploration/env_infos/initial/reward_dist Min            0.0016311\n",
      "exploration/env_infos/reward_dist Mean                   0.208338\n",
      "exploration/env_infos/reward_dist Std                    0.305415\n",
      "exploration/env_infos/reward_dist Max                    0.98363\n",
      "exploration/env_infos/reward_dist Min                    3.05527e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.109247\n",
      "exploration/env_infos/final/reward_energy Std            0.0277887\n",
      "exploration/env_infos/final/reward_energy Max           -0.0780446\n",
      "exploration/env_infos/final/reward_energy Min           -0.142522\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.18097\n",
      "exploration/env_infos/initial/reward_energy Std          0.148055\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0356375\n",
      "exploration/env_infos/initial/reward_energy Min         -0.428801\n",
      "exploration/env_infos/reward_energy Mean                -0.130654\n",
      "exploration/env_infos/reward_energy Std                  0.106639\n",
      "exploration/env_infos/reward_energy Max                 -0.00564222\n",
      "exploration/env_infos/reward_energy Min                 -0.507058\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0492708\n",
      "exploration/env_infos/final/end_effector_loc Std         0.167962\n",
      "exploration/env_infos/final/end_effector_loc Max         0.257791\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.391853\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000810462\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00822687\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.01995\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0127993\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0113719\n",
      "exploration/env_infos/end_effector_loc Std               0.135702\n",
      "exploration/env_infos/end_effector_loc Max               0.295062\n",
      "exploration/env_infos/end_effector_loc Min              -0.425263\n",
      "evaluation/num steps total                           78000\n",
      "evaluation/num paths total                            3900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0756477\n",
      "evaluation/Rewards Std                                   0.0917813\n",
      "evaluation/Rewards Max                                   0.129945\n",
      "evaluation/Rewards Min                                  -0.519991\n",
      "evaluation/Returns Mean                                 -1.51295\n",
      "evaluation/Returns Std                                   1.55168\n",
      "evaluation/Returns Max                                   1.30793\n",
      "evaluation/Returns Min                                  -5.98747\n",
      "evaluation/Actions Mean                                 -0.000865104\n",
      "evaluation/Actions Std                                   0.0968799\n",
      "evaluation/Actions Max                                   0.911359\n",
      "evaluation/Actions Min                                  -0.701245\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.51295\n",
      "evaluation/env_infos/final/reward_dist Mean              0.134075\n",
      "evaluation/env_infos/final/reward_dist Std               0.252561\n",
      "evaluation/env_infos/final/reward_dist Max               0.869987\n",
      "evaluation/env_infos/final/reward_dist Min               1.36171e-86\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00827718\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0139591\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0530587\n",
      "evaluation/env_infos/initial/reward_dist Min             2.45254e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.132334\n",
      "evaluation/env_infos/reward_dist Std                     0.237835\n",
      "evaluation/env_infos/reward_dist Max                     0.983904\n",
      "evaluation/env_infos/reward_dist Min                     1.36171e-86\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0811466\n",
      "evaluation/env_infos/final/reward_energy Std             0.153219\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00448588\n",
      "evaluation/env_infos/final/reward_energy Min            -1.05404\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.230407\n",
      "evaluation/env_infos/initial/reward_energy Std           0.190201\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0109234\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.819407\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0851554\n",
      "evaluation/env_infos/reward_energy Std                   0.107338\n",
      "evaluation/env_infos/reward_energy Max                  -0.00138689\n",
      "evaluation/env_infos/reward_energy Min                  -1.05404\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0347153\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.363541\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00058999\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0105466\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0399486\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0350623\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.015253\n",
      "evaluation/env_infos/end_effector_loc Std                0.220956\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.168766\n",
      "time/evaluation sampling (s)                             0.878351\n",
      "time/exploration sampling (s)                            0.109968\n",
      "time/logging (s)                                         0.019267\n",
      "time/saving (s)                                          0.0279251\n",
      "time/training (s)                                       47.8118\n",
      "time/epoch (s)                                          49.0161\n",
      "time/total (s)                                        3679.2\n",
      "Epoch                                                   77\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:14:38.363215 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 78 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000995741\r\n",
      "trainer/QF2 Loss                                         0.00123484\r\n",
      "trainer/Policy Loss                                      2.91196\r\n",
      "trainer/Q1 Predictions Mean                             -0.890503\r\n",
      "trainer/Q1 Predictions Std                               0.715548\r\n",
      "trainer/Q1 Predictions Max                               0.534796\r\n",
      "trainer/Q1 Predictions Min                              -2.91955\r\n",
      "trainer/Q2 Predictions Mean                             -0.89482\r\n",
      "trainer/Q2 Predictions Std                               0.711678\r\n",
      "trainer/Q2 Predictions Max                               0.498249\r\n",
      "trainer/Q2 Predictions Min                              -2.90916\r\n",
      "trainer/Q Targets Mean                                  -0.894999\r\n",
      "trainer/Q Targets Std                                    0.715693\r\n",
      "trainer/Q Targets Max                                    0.503565\r\n",
      "trainer/Q Targets Min                                   -2.93461\r\n",
      "trainer/Log Pis Mean                                     2.01961\r\n",
      "trainer/Log Pis Std                                      1.3547\r\n",
      "trainer/Log Pis Max                                      4.53787\r\n",
      "trainer/Log Pis Min                                     -3.21065\r\n",
      "trainer/Policy mu Mean                                  -0.0164609\r\n",
      "trainer/Policy mu Std                                    0.236441\r\n",
      "trainer/Policy mu Max                                    1.25163\r\n",
      "trainer/Policy mu Min                                   -2.03738\r\n",
      "trainer/Policy log std Mean                             -2.35259\r\n",
      "trainer/Policy log std Std                               0.56543\r\n",
      "trainer/Policy log std Max                              -0.00691622\r\n",
      "trainer/Policy log std Min                              -3.39692\r\n",
      "trainer/Alpha                                            0.021927\r\n",
      "trainer/Alpha Loss                                       0.074883\r\n",
      "exploration/num steps total                           8900\r\n",
      "exploration/num paths total                            445\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0803004\r\n",
      "exploration/Rewards Std                                  0.0565282\r\n",
      "exploration/Rewards Max                                  0.0393279\r\n",
      "exploration/Rewards Min                                 -0.274374\r\n",
      "exploration/Returns Mean                                -1.60601\r\n",
      "exploration/Returns Std                                  0.765521\r\n",
      "exploration/Returns Max                                 -0.453335\r\n",
      "exploration/Returns Min                                 -2.81123\r\n",
      "exploration/Actions Mean                                -0.0119663\r\n",
      "exploration/Actions Std                                  0.131311\r\n",
      "exploration/Actions Max                                  0.428774\r\n",
      "exploration/Actions Min                                 -0.659495\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.60601\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.000794481\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0013369\r\n",
      "exploration/env_infos/final/reward_dist Max              0.00343593\r\n",
      "exploration/env_infos/final/reward_dist Min              8.56463e-09\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00742389\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00900178\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0195636\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.26427e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0409134\r\n",
      "exploration/env_infos/reward_dist Std                    0.0800854\r\n",
      "exploration/env_infos/reward_dist Max                    0.403858\r\n",
      "exploration/env_infos/reward_dist Min                    8.56463e-09\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0974704\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0635631\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0460469\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.222667\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.174283\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.138588\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0275565\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.406558\r\n",
      "exploration/env_infos/reward_energy Mean                -0.137271\r\n",
      "exploration/env_infos/reward_energy Std                  0.126208\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00634318\r\n",
      "exploration/env_infos/reward_energy Min                 -0.73008\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.11788\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.198571\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.21726\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.552844\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00435986\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00655501\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00187682\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0200295\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0539136\r\n",
      "exploration/env_infos/end_effector_loc Std               0.137921\r\n",
      "exploration/env_infos/end_effector_loc Max               0.21726\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.552844\r\n",
      "evaluation/num steps total                           79000\r\n",
      "evaluation/num paths total                            3950\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0719975\r\n",
      "evaluation/Rewards Std                                   0.0796377\r\n",
      "evaluation/Rewards Max                                   0.113853\r\n",
      "evaluation/Rewards Min                                  -0.485531\r\n",
      "evaluation/Returns Mean                                 -1.43995\r\n",
      "evaluation/Returns Std                                   1.30247\r\n",
      "evaluation/Returns Max                                   1.06626\r\n",
      "evaluation/Returns Min                                  -5.70039\r\n",
      "evaluation/Actions Mean                                  0.00352856\r\n",
      "evaluation/Actions Std                                   0.0985486\r\n",
      "evaluation/Actions Max                                   0.897455\r\n",
      "evaluation/Actions Min                                  -0.610177\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.43995\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0583093\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.149926\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.787966\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.76113e-134\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00966053\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0144328\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0552133\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.04476e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0852351\r\n",
      "evaluation/env_infos/reward_dist Std                     0.185913\r\n",
      "evaluation/env_infos/reward_dist Max                     0.99625\r\n",
      "evaluation/env_infos/reward_dist Min                     3.76113e-134\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0700889\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.127982\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00458358\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.907187\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.253214\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.275231\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00423212\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.08142\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0744658\r\n",
      "evaluation/env_infos/reward_energy Std                   0.117913\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00123174\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.08142\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0255336\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.386307\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00218238\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0130413\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0448727\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0305088\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0143242\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.245925\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.161299\r\n",
      "time/evaluation sampling (s)                             0.78933\r\n",
      "time/exploration sampling (s)                            0.102705\r\n",
      "time/logging (s)                                         0.0186474\r\n",
      "time/saving (s)                                          0.0260822\r\n",
      "time/training (s)                                       47.3125\r\n",
      "time/epoch (s)                                          48.4106\r\n",
      "time/total (s)                                        3728.36\r\n",
      "Epoch                                                   78\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:15:28.118089 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 79 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00179798\r\n",
      "trainer/QF2 Loss                                         0.00150672\r\n",
      "trainer/Policy Loss                                      2.95829\r\n",
      "trainer/Q1 Predictions Mean                             -0.912899\r\n",
      "trainer/Q1 Predictions Std                               0.715824\r\n",
      "trainer/Q1 Predictions Max                               0.491061\r\n",
      "trainer/Q1 Predictions Min                              -2.99608\r\n",
      "trainer/Q2 Predictions Mean                             -0.911117\r\n",
      "trainer/Q2 Predictions Std                               0.716958\r\n",
      "trainer/Q2 Predictions Max                               0.503467\r\n",
      "trainer/Q2 Predictions Min                              -3.02471\r\n",
      "trainer/Q Targets Mean                                  -0.921767\r\n",
      "trainer/Q Targets Std                                    0.715505\r\n",
      "trainer/Q Targets Max                                    0.451362\r\n",
      "trainer/Q Targets Min                                   -3.00876\r\n",
      "trainer/Log Pis Mean                                     2.04773\r\n",
      "trainer/Log Pis Std                                      1.49071\r\n",
      "trainer/Log Pis Max                                      4.70996\r\n",
      "trainer/Log Pis Min                                     -3.51775\r\n",
      "trainer/Policy mu Mean                                  -0.00740262\r\n",
      "trainer/Policy mu Std                                    0.275604\r\n",
      "trainer/Policy mu Max                                    1.57785\r\n",
      "trainer/Policy mu Min                                   -1.57467\r\n",
      "trainer/Policy log std Mean                             -2.36767\r\n",
      "trainer/Policy log std Std                               0.651942\r\n",
      "trainer/Policy log std Max                              -0.186648\r\n",
      "trainer/Policy log std Min                              -3.47776\r\n",
      "trainer/Alpha                                            0.0226104\r\n",
      "trainer/Alpha Loss                                       0.180925\r\n",
      "exploration/num steps total                           9000\r\n",
      "exploration/num paths total                            450\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.133598\r\n",
      "exploration/Rewards Std                                  0.113006\r\n",
      "exploration/Rewards Max                                  0.0696401\r\n",
      "exploration/Rewards Min                                 -0.422433\r\n",
      "exploration/Returns Mean                                -2.67196\r\n",
      "exploration/Returns Std                                  1.99707\r\n",
      "exploration/Returns Max                                 -0.379139\r\n",
      "exploration/Returns Min                                 -5.98709\r\n",
      "exploration/Actions Mean                                 0.01517\r\n",
      "exploration/Actions Std                                  0.159679\r\n",
      "exploration/Actions Max                                  0.693813\r\n",
      "exploration/Actions Min                                 -0.877837\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.67196\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0374122\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0687581\r\n",
      "exploration/env_infos/final/reward_dist Max              0.174597\r\n",
      "exploration/env_infos/final/reward_dist Min              4.18702e-96\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0024981\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00497625\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0124506\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.2496e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0834753\r\n",
      "exploration/env_infos/reward_dist Std                    0.16813\r\n",
      "exploration/env_infos/reward_dist Max                    0.748613\r\n",
      "exploration/env_infos/reward_dist Min                    4.18702e-96\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0947521\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0468338\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0302742\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.152016\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.247849\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.259926\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0211661\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.755095\r\n",
      "exploration/env_infos/reward_energy Mean                -0.146555\r\n",
      "exploration/env_infos/reward_energy Std                  0.173138\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00807547\r\n",
      "exploration/env_infos/reward_energy Min                 -0.949666\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.166816\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.352206\r\n",
      "exploration/env_infos/final/end_effector_loc Max         1\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.346298\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00594194\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0112219\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0294662\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00563068\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.094999\r\n",
      "exploration/env_infos/end_effector_loc Std               0.226723\r\n",
      "exploration/env_infos/end_effector_loc Max               1\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.346298\r\n",
      "evaluation/num steps total                           80000\r\n",
      "evaluation/num paths total                            4000\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0583988\r\n",
      "evaluation/Rewards Std                                   0.0754713\r\n",
      "evaluation/Rewards Max                                   0.148776\r\n",
      "evaluation/Rewards Min                                  -0.634219\r\n",
      "evaluation/Returns Mean                                 -1.16798\r\n",
      "evaluation/Returns Std                                   1.07186\r\n",
      "evaluation/Returns Max                                   1.219\r\n",
      "evaluation/Returns Min                                  -3.98006\r\n",
      "evaluation/Actions Mean                                  0.000848266\r\n",
      "evaluation/Actions Std                                   0.0803707\r\n",
      "evaluation/Actions Max                                   0.952646\r\n",
      "evaluation/Actions Min                                  -0.623839\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.16798\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.195105\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.329622\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.978292\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.59739e-108\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00581132\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0124246\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0561774\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.72835e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.12915\r\n",
      "evaluation/env_infos/reward_dist Std                     0.240947\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999739\r\n",
      "evaluation/env_infos/reward_dist Min                     8.59739e-108\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0327427\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.02885\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00157239\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.146928\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.230387\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.259493\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00486015\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.30708\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0604534\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0962587\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000230055\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.30708\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.000788496\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.313918\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0010382\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122246\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0476323\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.031192\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00158877\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.21865\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.172919\r\n",
      "time/evaluation sampling (s)                             0.90461\r\n",
      "time/exploration sampling (s)                            0.111688\r\n",
      "time/logging (s)                                         0.0193987\r\n",
      "time/saving (s)                                          0.0261054\r\n",
      "time/training (s)                                       47.783\r\n",
      "time/epoch (s)                                          49.0177\r\n",
      "time/total (s)                                        3778.11\r\n",
      "Epoch                                                   79\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:16:17.871010 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 80 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00112689\n",
      "trainer/QF2 Loss                                         0.000869385\n",
      "trainer/Policy Loss                                      2.9129\n",
      "trainer/Q1 Predictions Mean                             -0.945534\n",
      "trainer/Q1 Predictions Std                               0.729045\n",
      "trainer/Q1 Predictions Max                               0.613055\n",
      "trainer/Q1 Predictions Min                              -2.72666\n",
      "trainer/Q2 Predictions Mean                             -0.943259\n",
      "trainer/Q2 Predictions Std                               0.724768\n",
      "trainer/Q2 Predictions Max                               0.626861\n",
      "trainer/Q2 Predictions Min                              -2.70086\n",
      "trainer/Q Targets Mean                                  -0.942454\n",
      "trainer/Q Targets Std                                    0.727479\n",
      "trainer/Q Targets Max                                    0.629731\n",
      "trainer/Q Targets Min                                   -2.75956\n",
      "trainer/Log Pis Mean                                     1.96076\n",
      "trainer/Log Pis Std                                      1.48105\n",
      "trainer/Log Pis Max                                      4.698\n",
      "trainer/Log Pis Min                                     -2.94558\n",
      "trainer/Policy mu Mean                                  -0.0128259\n",
      "trainer/Policy mu Std                                    0.226949\n",
      "trainer/Policy mu Max                                    1.31298\n",
      "trainer/Policy mu Min                                   -1.68454\n",
      "trainer/Policy log std Mean                             -2.34819\n",
      "trainer/Policy log std Std                               0.608243\n",
      "trainer/Policy log std Max                              -0.445355\n",
      "trainer/Policy log std Min                              -3.50351\n",
      "trainer/Alpha                                            0.0235597\n",
      "trainer/Alpha Loss                                      -0.147091\n",
      "exploration/num steps total                           9100\n",
      "exploration/num paths total                            455\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.164369\n",
      "exploration/Rewards Std                                  0.0660545\n",
      "exploration/Rewards Max                                 -0.0608998\n",
      "exploration/Rewards Min                                 -0.42163\n",
      "exploration/Returns Mean                                -3.28738\n",
      "exploration/Returns Std                                  0.770451\n",
      "exploration/Returns Max                                 -2.34847\n",
      "exploration/Returns Min                                 -4.13567\n",
      "exploration/Actions Mean                                -0.00626952\n",
      "exploration/Actions Std                                  0.148581\n",
      "exploration/Actions Max                                  0.554801\n",
      "exploration/Actions Min                                 -0.563589\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.28738\n",
      "exploration/env_infos/final/reward_dist Mean             0.0352879\n",
      "exploration/env_infos/final/reward_dist Std              0.070201\n",
      "exploration/env_infos/final/reward_dist Max              0.175689\n",
      "exploration/env_infos/final/reward_dist Min              1.17461e-58\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000369947\n",
      "exploration/env_infos/initial/reward_dist Std            0.000710521\n",
      "exploration/env_infos/initial/reward_dist Max            0.00179074\n",
      "exploration/env_infos/initial/reward_dist Min            1.55419e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.016752\n",
      "exploration/env_infos/reward_dist Std                    0.0608224\n",
      "exploration/env_infos/reward_dist Max                    0.420016\n",
      "exploration/env_infos/reward_dist Min                    1.17461e-58\n",
      "exploration/env_infos/final/reward_energy Mean          -0.124814\n",
      "exploration/env_infos/final/reward_energy Std            0.086819\n",
      "exploration/env_infos/final/reward_energy Max           -0.0382156\n",
      "exploration/env_infos/final/reward_energy Min           -0.289357\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.340418\n",
      "exploration/env_infos/initial/reward_energy Std          0.18934\n",
      "exploration/env_infos/initial/reward_energy Max         -0.115091\n",
      "exploration/env_infos/initial/reward_energy Min         -0.563663\n",
      "exploration/env_infos/reward_energy Mean                -0.174676\n",
      "exploration/env_infos/reward_energy Std                  0.11713\n",
      "exploration/env_infos/reward_energy Max                 -0.0112104\n",
      "exploration/env_infos/reward_energy Min                 -0.563663\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0985198\n",
      "exploration/env_infos/final/end_effector_loc Std         0.426086\n",
      "exploration/env_infos/final/end_effector_loc Max         0.623377\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000970668\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0137377\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0277401\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0281794\n",
      "exploration/env_infos/end_effector_loc Mean             -0.038885\n",
      "exploration/env_infos/end_effector_loc Std               0.252912\n",
      "exploration/env_infos/end_effector_loc Max               0.630513\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           81000\n",
      "evaluation/num paths total                            4050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0539246\n",
      "evaluation/Rewards Std                                   0.080175\n",
      "evaluation/Rewards Max                                   0.114677\n",
      "evaluation/Rewards Min                                  -0.512612\n",
      "evaluation/Returns Mean                                 -1.07849\n",
      "evaluation/Returns Std                                   1.26932\n",
      "evaluation/Returns Max                                   1.1888\n",
      "evaluation/Returns Min                                  -5.87191\n",
      "evaluation/Actions Mean                                  0.00279918\n",
      "evaluation/Actions Std                                   0.0773453\n",
      "evaluation/Actions Max                                   0.769774\n",
      "evaluation/Actions Min                                  -0.881074\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.07849\n",
      "evaluation/env_infos/final/reward_dist Mean              0.12453\n",
      "evaluation/env_infos/final/reward_dist Std               0.23501\n",
      "evaluation/env_infos/final/reward_dist Max               0.93934\n",
      "evaluation/env_infos/final/reward_dist Min               3.79258e-112\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00692002\n",
      "evaluation/env_infos/initial/reward_dist Std             0.013523\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0709879\n",
      "evaluation/env_infos/initial/reward_dist Min             1.11733e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.136579\n",
      "evaluation/env_infos/reward_dist Std                     0.239557\n",
      "evaluation/env_infos/reward_dist Max                     0.996533\n",
      "evaluation/env_infos/reward_dist Min                     3.79258e-112\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0521617\n",
      "evaluation/env_infos/final/reward_energy Std             0.0547431\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000660291\n",
      "evaluation/env_infos/final/reward_energy Min            -0.283412\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.239775\n",
      "evaluation/env_infos/initial/reward_energy Std           0.224726\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00765694\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.970821\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0655145\n",
      "evaluation/env_infos/reward_energy Std                   0.0876819\n",
      "evaluation/env_infos/reward_energy Max                  -0.000181001\n",
      "evaluation/env_infos/reward_energy Min                  -0.970821\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0150107\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.321463\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.960296\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.985261\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00020516\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116168\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0384887\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0440537\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0129728\n",
      "evaluation/env_infos/end_effector_loc Std                0.201332\n",
      "evaluation/env_infos/end_effector_loc Max                0.960296\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.166855\n",
      "time/evaluation sampling (s)                             0.803698\n",
      "time/exploration sampling (s)                            0.106366\n",
      "time/logging (s)                                         0.018979\n",
      "time/saving (s)                                          0.0276835\n",
      "time/training (s)                                       47.8713\n",
      "time/epoch (s)                                          48.9949\n",
      "time/total (s)                                        3827.86\n",
      "Epoch                                                   80\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:17:06.629630 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 81 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00207317\n",
      "trainer/QF2 Loss                                         0.00531105\n",
      "trainer/Policy Loss                                      3.03213\n",
      "trainer/Q1 Predictions Mean                             -0.959116\n",
      "trainer/Q1 Predictions Std                               0.764484\n",
      "trainer/Q1 Predictions Max                               0.869727\n",
      "trainer/Q1 Predictions Min                              -2.90116\n",
      "trainer/Q2 Predictions Mean                             -0.949219\n",
      "trainer/Q2 Predictions Std                               0.755985\n",
      "trainer/Q2 Predictions Max                               0.834615\n",
      "trainer/Q2 Predictions Min                              -2.89419\n",
      "trainer/Q Targets Mean                                  -0.960831\n",
      "trainer/Q Targets Std                                    0.760967\n",
      "trainer/Q Targets Max                                    0.875593\n",
      "trainer/Q Targets Min                                   -2.94722\n",
      "trainer/Log Pis Mean                                     2.0716\n",
      "trainer/Log Pis Std                                      1.48123\n",
      "trainer/Log Pis Max                                      5.02742\n",
      "trainer/Log Pis Min                                     -4.4611\n",
      "trainer/Policy mu Mean                                   0.0201521\n",
      "trainer/Policy mu Std                                    0.306831\n",
      "trainer/Policy mu Max                                    2.07574\n",
      "trainer/Policy mu Min                                   -1.45683\n",
      "trainer/Policy log std Mean                             -2.36676\n",
      "trainer/Policy log std Std                               0.623969\n",
      "trainer/Policy log std Max                              -0.146844\n",
      "trainer/Policy log std Min                              -3.33244\n",
      "trainer/Alpha                                            0.0227796\n",
      "trainer/Alpha Loss                                       0.270784\n",
      "exploration/num steps total                           9200\n",
      "exploration/num paths total                            460\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.108013\n",
      "exploration/Rewards Std                                  0.0607457\n",
      "exploration/Rewards Max                                  0.0680973\n",
      "exploration/Rewards Min                                 -0.230769\n",
      "exploration/Returns Mean                                -2.16026\n",
      "exploration/Returns Std                                  0.774547\n",
      "exploration/Returns Max                                 -1.09082\n",
      "exploration/Returns Min                                 -3.34287\n",
      "exploration/Actions Mean                                 0.00442101\n",
      "exploration/Actions Std                                  0.182877\n",
      "exploration/Actions Max                                  0.659668\n",
      "exploration/Actions Min                                 -0.77718\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.16026\n",
      "exploration/env_infos/final/reward_dist Mean             0.004216\n",
      "exploration/env_infos/final/reward_dist Std              0.00792711\n",
      "exploration/env_infos/final/reward_dist Max              0.0200648\n",
      "exploration/env_infos/final/reward_dist Min              3.30012e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000324746\n",
      "exploration/env_infos/initial/reward_dist Std            0.000546739\n",
      "exploration/env_infos/initial/reward_dist Max            0.00141274\n",
      "exploration/env_infos/initial/reward_dist Min            4.34249e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.143278\n",
      "exploration/env_infos/reward_dist Std                    0.25408\n",
      "exploration/env_infos/reward_dist Max                    0.917953\n",
      "exploration/env_infos/reward_dist Min                    1.0057e-16\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135772\n",
      "exploration/env_infos/final/reward_energy Std            0.100971\n",
      "exploration/env_infos/final/reward_energy Max           -0.0105622\n",
      "exploration/env_infos/final/reward_energy Min           -0.302166\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.280127\n",
      "exploration/env_infos/initial/reward_energy Std          0.239069\n",
      "exploration/env_infos/initial/reward_energy Max         -0.00908834\n",
      "exploration/env_infos/initial/reward_energy Min         -0.689416\n",
      "exploration/env_infos/reward_energy Mean                -0.201883\n",
      "exploration/env_infos/reward_energy Std                  0.161773\n",
      "exploration/env_infos/reward_energy Max                 -0.00463268\n",
      "exploration/env_infos/reward_energy Min                 -0.894733\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.033232\n",
      "exploration/env_infos/final/end_effector_loc Std         0.309167\n",
      "exploration/env_infos/final/end_effector_loc Max         0.67317\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.45499\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00187618\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0128845\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0196837\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0322379\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0245359\n",
      "exploration/env_infos/end_effector_loc Std               0.229273\n",
      "exploration/env_infos/end_effector_loc Max               0.67317\n",
      "exploration/env_infos/end_effector_loc Min              -0.664838\n",
      "evaluation/num steps total                           82000\n",
      "evaluation/num paths total                            4100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0721898\n",
      "evaluation/Rewards Std                                   0.0842394\n",
      "evaluation/Rewards Max                                   0.151446\n",
      "evaluation/Rewards Min                                  -0.450421\n",
      "evaluation/Returns Mean                                 -1.4438\n",
      "evaluation/Returns Std                                   1.36365\n",
      "evaluation/Returns Max                                   1.90249\n",
      "evaluation/Returns Min                                  -4.88939\n",
      "evaluation/Actions Mean                                 -0.0043155\n",
      "evaluation/Actions Std                                   0.0858879\n",
      "evaluation/Actions Max                                   0.846899\n",
      "evaluation/Actions Min                                  -0.679298\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.4438\n",
      "evaluation/env_infos/final/reward_dist Mean              0.103203\n",
      "evaluation/env_infos/final/reward_dist Std               0.207827\n",
      "evaluation/env_infos/final/reward_dist Max               0.979209\n",
      "evaluation/env_infos/final/reward_dist Min               7.33457e-117\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00666041\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0171658\n",
      "evaluation/env_infos/initial/reward_dist Max             0.111521\n",
      "evaluation/env_infos/initial/reward_dist Min             1.49084e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0901818\n",
      "evaluation/env_infos/reward_dist Std                     0.189526\n",
      "evaluation/env_infos/reward_dist Max                     0.979209\n",
      "evaluation/env_infos/reward_dist Min                     7.33457e-117\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0527342\n",
      "evaluation/env_infos/final/reward_energy Std             0.043099\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0040979\n",
      "evaluation/env_infos/final/reward_energy Min            -0.22812\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.26176\n",
      "evaluation/env_infos/initial/reward_energy Std           0.276735\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0113733\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.12055\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0702656\n",
      "evaluation/env_infos/reward_energy Std                   0.0992645\n",
      "evaluation/env_infos/reward_energy Max                  -0.000315301\n",
      "evaluation/env_infos/reward_energy Min                  -1.12055\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0658142\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.395719\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00127926\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134067\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.042345\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0339649\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0231355\n",
      "evaluation/env_infos/end_effector_loc Std                0.256487\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.162782\n",
      "time/evaluation sampling (s)                             0.806544\n",
      "time/exploration sampling (s)                            0.104653\n",
      "time/logging (s)                                         0.0191086\n",
      "time/saving (s)                                          0.0268054\n",
      "time/training (s)                                       46.9101\n",
      "time/epoch (s)                                          48.03\n",
      "time/total (s)                                        3876.62\n",
      "Epoch                                                   81\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:17:55.792935 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 82 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000865562\n",
      "trainer/QF2 Loss                                         0.00259693\n",
      "trainer/Policy Loss                                      2.82694\n",
      "trainer/Q1 Predictions Mean                             -0.993209\n",
      "trainer/Q1 Predictions Std                               0.828042\n",
      "trainer/Q1 Predictions Max                               0.866579\n",
      "trainer/Q1 Predictions Min                              -3.18482\n",
      "trainer/Q2 Predictions Mean                             -0.987918\n",
      "trainer/Q2 Predictions Std                               0.828752\n",
      "trainer/Q2 Predictions Max                               0.883907\n",
      "trainer/Q2 Predictions Min                              -3.25956\n",
      "trainer/Q Targets Mean                                  -0.992097\n",
      "trainer/Q Targets Std                                    0.831098\n",
      "trainer/Q Targets Max                                    0.884598\n",
      "trainer/Q Targets Min                                   -3.17753\n",
      "trainer/Log Pis Mean                                     1.82847\n",
      "trainer/Log Pis Std                                      1.5171\n",
      "trainer/Log Pis Max                                      4.59808\n",
      "trainer/Log Pis Min                                     -5.69802\n",
      "trainer/Policy mu Mean                                   0.0130114\n",
      "trainer/Policy mu Std                                    0.232791\n",
      "trainer/Policy mu Max                                    1.41999\n",
      "trainer/Policy mu Min                                   -1.70298\n",
      "trainer/Policy log std Mean                             -2.28462\n",
      "trainer/Policy log std Std                               0.611787\n",
      "trainer/Policy log std Max                              -0.292772\n",
      "trainer/Policy log std Min                              -3.28018\n",
      "trainer/Alpha                                            0.0219047\n",
      "trainer/Alpha Loss                                      -0.655295\n",
      "exploration/num steps total                           9300\n",
      "exploration/num paths total                            465\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0714679\n",
      "exploration/Rewards Std                                  0.0637818\n",
      "exploration/Rewards Max                                  0.0766386\n",
      "exploration/Rewards Min                                 -0.262365\n",
      "exploration/Returns Mean                                -1.42936\n",
      "exploration/Returns Std                                  0.813011\n",
      "exploration/Returns Max                                 -0.463607\n",
      "exploration/Returns Min                                 -2.41601\n",
      "exploration/Actions Mean                                 0.0179162\n",
      "exploration/Actions Std                                  0.140372\n",
      "exploration/Actions Max                                  0.699607\n",
      "exploration/Actions Min                                 -0.425291\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.42936\n",
      "exploration/env_infos/final/reward_dist Mean             0.00116172\n",
      "exploration/env_infos/final/reward_dist Std              0.00209297\n",
      "exploration/env_infos/final/reward_dist Max              0.00533227\n",
      "exploration/env_infos/final/reward_dist Min              1.43857e-96\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0018635\n",
      "exploration/env_infos/initial/reward_dist Std            0.00316551\n",
      "exploration/env_infos/initial/reward_dist Max            0.00818293\n",
      "exploration/env_infos/initial/reward_dist Min            5.55453e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0536988\n",
      "exploration/env_infos/reward_dist Std                    0.106115\n",
      "exploration/env_infos/reward_dist Max                    0.497061\n",
      "exploration/env_infos/reward_dist Min                    1.43857e-96\n",
      "exploration/env_infos/final/reward_energy Mean          -0.104279\n",
      "exploration/env_infos/final/reward_energy Std            0.0349865\n",
      "exploration/env_infos/final/reward_energy Max           -0.0599302\n",
      "exploration/env_infos/final/reward_energy Min           -0.159715\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.305234\n",
      "exploration/env_infos/initial/reward_energy Std          0.280043\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0134768\n",
      "exploration/env_infos/initial/reward_energy Min         -0.835737\n",
      "exploration/env_infos/reward_energy Mean                -0.139098\n",
      "exploration/env_infos/reward_energy Std                  0.143884\n",
      "exploration/env_infos/reward_energy Max                 -0.0134768\n",
      "exploration/env_infos/reward_energy Min                 -0.835737\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.238274\n",
      "exploration/env_infos/final/end_effector_loc Std         0.442064\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.31144\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00683705\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0129516\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0321548\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00797823\n",
      "exploration/env_infos/end_effector_loc Mean              0.130132\n",
      "exploration/env_infos/end_effector_loc Std               0.291163\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -0.31144\n",
      "evaluation/num steps total                           83000\n",
      "evaluation/num paths total                            4150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0542183\n",
      "evaluation/Rewards Std                                   0.0895698\n",
      "evaluation/Rewards Max                                   0.17938\n",
      "evaluation/Rewards Min                                  -0.697338\n",
      "evaluation/Returns Mean                                 -1.08437\n",
      "evaluation/Returns Std                                   1.38512\n",
      "evaluation/Returns Max                                   2.40489\n",
      "evaluation/Returns Min                                  -3.73243\n",
      "evaluation/Actions Mean                                  0.00859152\n",
      "evaluation/Actions Std                                   0.0812369\n",
      "evaluation/Actions Max                                   0.945477\n",
      "evaluation/Actions Min                                  -0.708505\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.08437\n",
      "evaluation/env_infos/final/reward_dist Mean              0.133424\n",
      "evaluation/env_infos/final/reward_dist Std               0.244139\n",
      "evaluation/env_infos/final/reward_dist Max               0.898588\n",
      "evaluation/env_infos/final/reward_dist Min               1.02738e-56\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0056226\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00915809\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0298817\n",
      "evaluation/env_infos/initial/reward_dist Min             9.05607e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.156544\n",
      "evaluation/env_infos/reward_dist Std                     0.260923\n",
      "evaluation/env_infos/reward_dist Max                     0.999032\n",
      "evaluation/env_infos/reward_dist Min                     1.02738e-56\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0466104\n",
      "evaluation/env_infos/final/reward_energy Std             0.0341931\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00971171\n",
      "evaluation/env_infos/final/reward_energy Min            -0.158645\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.252063\n",
      "evaluation/env_infos/initial/reward_energy Std           0.27898\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0058088\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.06043\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0633236\n",
      "evaluation/env_infos/reward_energy Std                   0.0966262\n",
      "evaluation/env_infos/reward_energy Max                  -0.00171425\n",
      "evaluation/env_infos/reward_energy Min                  -1.06043\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0861342\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.323635\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.667516\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00255077\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0130461\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0472738\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0354252\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0414428\n",
      "evaluation/env_infos/end_effector_loc Std                0.210644\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.667516\n",
      "time/data storing (s)                                    0.170137\n",
      "time/evaluation sampling (s)                             0.803029\n",
      "time/exploration sampling (s)                            0.109728\n",
      "time/logging (s)                                         0.0186955\n",
      "time/saving (s)                                          0.027751\n",
      "time/training (s)                                       47.3054\n",
      "time/epoch (s)                                          48.4348\n",
      "time/total (s)                                        3925.78\n",
      "Epoch                                                   82\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:18:46.086945 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 83 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0010359\n",
      "trainer/QF2 Loss                                         0.00129027\n",
      "trainer/Policy Loss                                      2.88458\n",
      "trainer/Q1 Predictions Mean                             -0.969311\n",
      "trainer/Q1 Predictions Std                               0.817368\n",
      "trainer/Q1 Predictions Max                               0.768433\n",
      "trainer/Q1 Predictions Min                              -3.05786\n",
      "trainer/Q2 Predictions Mean                             -0.967957\n",
      "trainer/Q2 Predictions Std                               0.811616\n",
      "trainer/Q2 Predictions Max                               0.754023\n",
      "trainer/Q2 Predictions Min                              -3.06294\n",
      "trainer/Q Targets Mean                                  -0.97213\n",
      "trainer/Q Targets Std                                    0.815041\n",
      "trainer/Q Targets Max                                    0.729699\n",
      "trainer/Q Targets Min                                   -3.07332\n",
      "trainer/Log Pis Mean                                     1.91952\n",
      "trainer/Log Pis Std                                      1.51839\n",
      "trainer/Log Pis Max                                      4.49197\n",
      "trainer/Log Pis Min                                     -3.66877\n",
      "trainer/Policy mu Mean                                   0.033372\n",
      "trainer/Policy mu Std                                    0.298963\n",
      "trainer/Policy mu Max                                    1.89527\n",
      "trainer/Policy mu Min                                   -1.33897\n",
      "trainer/Policy log std Mean                             -2.28452\n",
      "trainer/Policy log std Std                               0.660483\n",
      "trainer/Policy log std Max                              -0.196243\n",
      "trainer/Policy log std Min                              -3.42905\n",
      "trainer/Alpha                                            0.0209953\n",
      "trainer/Alpha Loss                                      -0.31098\n",
      "exploration/num steps total                           9400\n",
      "exploration/num paths total                            470\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0196373\n",
      "exploration/Rewards Std                                  0.065216\n",
      "exploration/Rewards Max                                  0.0822276\n",
      "exploration/Rewards Min                                 -0.202109\n",
      "exploration/Returns Mean                                -0.392747\n",
      "exploration/Returns Std                                  0.799318\n",
      "exploration/Returns Max                                  0.171301\n",
      "exploration/Returns Min                                 -1.95688\n",
      "exploration/Actions Mean                                 0.00261313\n",
      "exploration/Actions Std                                  0.110026\n",
      "exploration/Actions Max                                  0.706663\n",
      "exploration/Actions Min                                 -0.555571\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.392747\n",
      "exploration/env_infos/final/reward_dist Mean             0.157114\n",
      "exploration/env_infos/final/reward_dist Std              0.172526\n",
      "exploration/env_infos/final/reward_dist Max              0.4898\n",
      "exploration/env_infos/final/reward_dist Min              6.25798e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00321929\n",
      "exploration/env_infos/initial/reward_dist Std            0.00557753\n",
      "exploration/env_infos/initial/reward_dist Max            0.0143368\n",
      "exploration/env_infos/initial/reward_dist Min            3.11566e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.212734\n",
      "exploration/env_infos/reward_dist Std                    0.253095\n",
      "exploration/env_infos/reward_dist Max                    0.949601\n",
      "exploration/env_infos/reward_dist Min                    6.25798e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0616936\n",
      "exploration/env_infos/final/reward_energy Std            0.0269541\n",
      "exploration/env_infos/final/reward_energy Max           -0.0208003\n",
      "exploration/env_infos/final/reward_energy Min           -0.100662\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.122191\n",
      "exploration/env_infos/initial/reward_energy Std          0.0725199\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0562804\n",
      "exploration/env_infos/initial/reward_energy Min         -0.240464\n",
      "exploration/env_infos/reward_energy Mean                -0.108194\n",
      "exploration/env_infos/reward_energy Std                  0.111888\n",
      "exploration/env_infos/reward_energy Max                 -0.0113792\n",
      "exploration/env_infos/reward_energy Min                 -0.719133\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0740544\n",
      "exploration/env_infos/final/end_effector_loc Std         0.164648\n",
      "exploration/env_infos/final/end_effector_loc Max         0.353358\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.224613\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00285874\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00413098\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0113542\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00309659\n",
      "exploration/env_infos/end_effector_loc Mean              0.0501779\n",
      "exploration/env_infos/end_effector_loc Std               0.105957\n",
      "exploration/env_infos/end_effector_loc Max               0.353358\n",
      "exploration/env_infos/end_effector_loc Min              -0.224613\n",
      "evaluation/num steps total                           84000\n",
      "evaluation/num paths total                            4200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0454632\n",
      "evaluation/Rewards Std                                   0.0903997\n",
      "evaluation/Rewards Max                                   0.163158\n",
      "evaluation/Rewards Min                                  -0.503951\n",
      "evaluation/Returns Mean                                 -0.909265\n",
      "evaluation/Returns Std                                   1.45851\n",
      "evaluation/Returns Max                                   2.22621\n",
      "evaluation/Returns Min                                  -4.57313\n",
      "evaluation/Actions Mean                                  0.00588081\n",
      "evaluation/Actions Std                                   0.0887554\n",
      "evaluation/Actions Max                                   0.87231\n",
      "evaluation/Actions Min                                  -0.596252\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.909265\n",
      "evaluation/env_infos/final/reward_dist Mean              0.124554\n",
      "evaluation/env_infos/final/reward_dist Std               0.229626\n",
      "evaluation/env_infos/final/reward_dist Max               0.94187\n",
      "evaluation/env_infos/final/reward_dist Min               1.26244e-99\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0142387\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0257552\n",
      "evaluation/env_infos/initial/reward_dist Max             0.148711\n",
      "evaluation/env_infos/initial/reward_dist Min             7.41257e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.184514\n",
      "evaluation/env_infos/reward_dist Std                     0.279952\n",
      "evaluation/env_infos/reward_dist Max                     0.996955\n",
      "evaluation/env_infos/reward_dist Min                     1.26244e-99\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0526305\n",
      "evaluation/env_infos/final/reward_energy Std             0.0381119\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00439221\n",
      "evaluation/env_infos/final/reward_energy Min            -0.200675\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.296199\n",
      "evaluation/env_infos/initial/reward_energy Std           0.269284\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00251856\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.04338\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0762031\n",
      "evaluation/env_infos/reward_energy Std                   0.100086\n",
      "evaluation/env_infos/reward_energy Max                  -0.000554598\n",
      "evaluation/env_infos/reward_energy Min                  -1.04338\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0573659\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.340208\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.965719\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00233864\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0139585\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0436155\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0298126\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0330629\n",
      "evaluation/env_infos/end_effector_loc Std                0.227406\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.965719\n",
      "time/data storing (s)                                    0.16562\n",
      "time/evaluation sampling (s)                             0.798143\n",
      "time/exploration sampling (s)                            0.106572\n",
      "time/logging (s)                                         0.0202217\n",
      "time/saving (s)                                          0.0267891\n",
      "time/training (s)                                       48.4044\n",
      "time/epoch (s)                                          49.5217\n",
      "time/total (s)                                        3976.08\n",
      "Epoch                                                   83\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:19:35.119965 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 84 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000864172\n",
      "trainer/QF2 Loss                                         0.000860045\n",
      "trainer/Policy Loss                                      2.99468\n",
      "trainer/Q1 Predictions Mean                             -0.921766\n",
      "trainer/Q1 Predictions Std                               0.742588\n",
      "trainer/Q1 Predictions Max                               0.701585\n",
      "trainer/Q1 Predictions Min                              -2.99837\n",
      "trainer/Q2 Predictions Mean                             -0.92486\n",
      "trainer/Q2 Predictions Std                               0.739596\n",
      "trainer/Q2 Predictions Max                               0.702439\n",
      "trainer/Q2 Predictions Min                              -2.98723\n",
      "trainer/Q Targets Mean                                  -0.921954\n",
      "trainer/Q Targets Std                                    0.735025\n",
      "trainer/Q Targets Max                                    0.704487\n",
      "trainer/Q Targets Min                                   -2.97312\n",
      "trainer/Log Pis Mean                                     2.06698\n",
      "trainer/Log Pis Std                                      1.46645\n",
      "trainer/Log Pis Max                                      4.98747\n",
      "trainer/Log Pis Min                                     -2.94125\n",
      "trainer/Policy mu Mean                                   0.0287122\n",
      "trainer/Policy mu Std                                    0.248679\n",
      "trainer/Policy mu Max                                    1.64843\n",
      "trainer/Policy mu Min                                   -1.47855\n",
      "trainer/Policy log std Mean                             -2.3774\n",
      "trainer/Policy log std Std                               0.641524\n",
      "trainer/Policy log std Max                              -0.524609\n",
      "trainer/Policy log std Min                              -3.48706\n",
      "trainer/Alpha                                            0.0220143\n",
      "trainer/Alpha Loss                                       0.255546\n",
      "exploration/num steps total                           9500\n",
      "exploration/num paths total                            475\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.122666\n",
      "exploration/Rewards Std                                  0.145846\n",
      "exploration/Rewards Max                                  0.149597\n",
      "exploration/Rewards Min                                 -0.665653\n",
      "exploration/Returns Mean                                -2.45333\n",
      "exploration/Returns Std                                  1.99709\n",
      "exploration/Returns Max                                  0.257124\n",
      "exploration/Returns Min                                 -5.42481\n",
      "exploration/Actions Mean                                 0.0226363\n",
      "exploration/Actions Std                                  0.158012\n",
      "exploration/Actions Max                                  0.953102\n",
      "exploration/Actions Min                                 -0.926985\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.45333\n",
      "exploration/env_infos/final/reward_dist Mean             0.000360282\n",
      "exploration/env_infos/final/reward_dist Std              0.000705547\n",
      "exploration/env_infos/final/reward_dist Max              0.00177119\n",
      "exploration/env_infos/final/reward_dist Min              4.34326e-86\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0126398\n",
      "exploration/env_infos/initial/reward_dist Std            0.0122343\n",
      "exploration/env_infos/initial/reward_dist Max            0.0303175\n",
      "exploration/env_infos/initial/reward_dist Min            3.27899e-07\n",
      "exploration/env_infos/reward_dist Mean                   0.0663994\n",
      "exploration/env_infos/reward_dist Std                    0.184735\n",
      "exploration/env_infos/reward_dist Max                    0.942419\n",
      "exploration/env_infos/reward_dist Min                    1.26815e-103\n",
      "exploration/env_infos/final/reward_energy Mean          -0.133789\n",
      "exploration/env_infos/final/reward_energy Std            0.0538852\n",
      "exploration/env_infos/final/reward_energy Max           -0.0457382\n",
      "exploration/env_infos/final/reward_energy Min           -0.20879\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.427983\n",
      "exploration/env_infos/initial/reward_energy Std          0.454716\n",
      "exploration/env_infos/initial/reward_energy Max         -0.134588\n",
      "exploration/env_infos/initial/reward_energy Min         -1.32955\n",
      "exploration/env_infos/reward_energy Mean                -0.161628\n",
      "exploration/env_infos/reward_energy Std                  0.157597\n",
      "exploration/env_infos/reward_energy Max                 -0.0137613\n",
      "exploration/env_infos/reward_energy Min                 -1.32955\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.18151\n",
      "exploration/env_infos/final/end_effector_loc Std         0.46971\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.400594\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000990812\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0220554\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0476551\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0463493\n",
      "exploration/env_infos/end_effector_loc Mean              0.0871652\n",
      "exploration/env_infos/end_effector_loc Std               0.332745\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -0.407408\n",
      "evaluation/num steps total                           85000\n",
      "evaluation/num paths total                            4250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0607895\n",
      "evaluation/Rewards Std                                   0.0885767\n",
      "evaluation/Rewards Max                                   0.138403\n",
      "evaluation/Rewards Min                                  -0.503359\n",
      "evaluation/Returns Mean                                 -1.21579\n",
      "evaluation/Returns Std                                   1.38962\n",
      "evaluation/Returns Max                                   1.36268\n",
      "evaluation/Returns Min                                  -5.10886\n",
      "evaluation/Actions Mean                                  0.00582676\n",
      "evaluation/Actions Std                                   0.109267\n",
      "evaluation/Actions Max                                   0.91298\n",
      "evaluation/Actions Min                                  -0.879982\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.21579\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0619647\n",
      "evaluation/env_infos/final/reward_dist Std               0.131486\n",
      "evaluation/env_infos/final/reward_dist Max               0.533405\n",
      "evaluation/env_infos/final/reward_dist Min               1.32275e-122\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00642908\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0110328\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0475726\n",
      "evaluation/env_infos/initial/reward_dist Min             1.88931e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.138052\n",
      "evaluation/env_infos/reward_dist Std                     0.239135\n",
      "evaluation/env_infos/reward_dist Max                     0.995706\n",
      "evaluation/env_infos/reward_dist Min                     1.32275e-122\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0588202\n",
      "evaluation/env_infos/final/reward_energy Std             0.0452146\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00558981\n",
      "evaluation/env_infos/final/reward_energy Min            -0.182592\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.366789\n",
      "evaluation/env_infos/initial/reward_energy Std           0.302205\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00548325\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.18969\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0913659\n",
      "evaluation/env_infos/reward_energy Std                   0.124894\n",
      "evaluation/env_infos/reward_energy Max                  -0.00189773\n",
      "evaluation/env_infos/reward_energy Min                  -1.18969\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0494958\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.38394\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00157659\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0167285\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.045649\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0439991\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0254106\n",
      "evaluation/env_infos/end_effector_loc Std                0.264626\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.163513\n",
      "time/evaluation sampling (s)                             0.805628\n",
      "time/exploration sampling (s)                            0.105809\n",
      "time/logging (s)                                         0.0191573\n",
      "time/saving (s)                                          0.0264649\n",
      "time/training (s)                                       47.1598\n",
      "time/epoch (s)                                          48.2804\n",
      "time/total (s)                                        4025.1\n",
      "Epoch                                                   84\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:20:25.396929 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 85 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00146155\n",
      "trainer/QF2 Loss                                         0.00157782\n",
      "trainer/Policy Loss                                      3.1244\n",
      "trainer/Q1 Predictions Mean                             -0.940803\n",
      "trainer/Q1 Predictions Std                               0.767218\n",
      "trainer/Q1 Predictions Max                               0.609191\n",
      "trainer/Q1 Predictions Min                              -2.99745\n",
      "trainer/Q2 Predictions Mean                             -0.942725\n",
      "trainer/Q2 Predictions Std                               0.760672\n",
      "trainer/Q2 Predictions Max                               0.566353\n",
      "trainer/Q2 Predictions Min                              -2.94265\n",
      "trainer/Q Targets Mean                                  -0.951818\n",
      "trainer/Q Targets Std                                    0.764175\n",
      "trainer/Q Targets Max                                    0.601887\n",
      "trainer/Q Targets Min                                   -2.94729\n",
      "trainer/Log Pis Mean                                     2.19061\n",
      "trainer/Log Pis Std                                      1.36569\n",
      "trainer/Log Pis Max                                      5.03969\n",
      "trainer/Log Pis Min                                     -2.49474\n",
      "trainer/Policy mu Mean                                   0.074805\n",
      "trainer/Policy mu Std                                    0.343754\n",
      "trainer/Policy mu Max                                    2.23396\n",
      "trainer/Policy mu Min                                   -1.30349\n",
      "trainer/Policy log std Mean                             -2.33623\n",
      "trainer/Policy log std Std                               0.709394\n",
      "trainer/Policy log std Max                              -0.177061\n",
      "trainer/Policy log std Min                              -3.51242\n",
      "trainer/Alpha                                            0.0220799\n",
      "trainer/Alpha Loss                                       0.727035\n",
      "exploration/num steps total                           9600\n",
      "exploration/num paths total                            480\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0607917\n",
      "exploration/Rewards Std                                  0.0613478\n",
      "exploration/Rewards Max                                  0.0697872\n",
      "exploration/Rewards Min                                 -0.207007\n",
      "exploration/Returns Mean                                -1.21583\n",
      "exploration/Returns Std                                  0.862845\n",
      "exploration/Returns Max                                 -0.232148\n",
      "exploration/Returns Min                                 -2.41302\n",
      "exploration/Actions Mean                                 0.000211793\n",
      "exploration/Actions Std                                  0.094173\n",
      "exploration/Actions Max                                  0.308453\n",
      "exploration/Actions Min                                 -0.291095\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.21583\n",
      "exploration/env_infos/final/reward_dist Mean             0.154528\n",
      "exploration/env_infos/final/reward_dist Std              0.303958\n",
      "exploration/env_infos/final/reward_dist Max              0.762407\n",
      "exploration/env_infos/final/reward_dist Min              3.89342e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103281\n",
      "exploration/env_infos/initial/reward_dist Std            0.00702775\n",
      "exploration/env_infos/initial/reward_dist Max            0.0189206\n",
      "exploration/env_infos/initial/reward_dist Min            5.25062e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.134066\n",
      "exploration/env_infos/reward_dist Std                    0.279299\n",
      "exploration/env_infos/reward_dist Max                    0.978121\n",
      "exploration/env_infos/reward_dist Min                    3.89342e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.173691\n",
      "exploration/env_infos/final/reward_energy Std            0.0862323\n",
      "exploration/env_infos/final/reward_energy Max           -0.0753345\n",
      "exploration/env_infos/final/reward_energy Min           -0.315663\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.224171\n",
      "exploration/env_infos/initial/reward_energy Std          0.0708301\n",
      "exploration/env_infos/initial/reward_energy Max         -0.15605\n",
      "exploration/env_infos/initial/reward_energy Min         -0.31884\n",
      "exploration/env_infos/reward_energy Mean                -0.108929\n",
      "exploration/env_infos/reward_energy Std                  0.0766264\n",
      "exploration/env_infos/reward_energy Max                 -0.0124473\n",
      "exploration/env_infos/reward_energy Min                 -0.368554\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0268689\n",
      "exploration/env_infos/final/end_effector_loc Std         0.190151\n",
      "exploration/env_infos/final/end_effector_loc Max         0.29061\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.34802\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00157428\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00816141\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0114083\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0145412\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00450414\n",
      "exploration/env_infos/end_effector_loc Std               0.11792\n",
      "exploration/env_infos/end_effector_loc Max               0.29061\n",
      "exploration/env_infos/end_effector_loc Min              -0.34802\n",
      "evaluation/num steps total                           86000\n",
      "evaluation/num paths total                            4300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0806339\n",
      "evaluation/Rewards Std                                   0.11976\n",
      "evaluation/Rewards Max                                   0.155701\n",
      "evaluation/Rewards Min                                  -0.815316\n",
      "evaluation/Returns Mean                                 -1.61268\n",
      "evaluation/Returns Std                                   1.92055\n",
      "evaluation/Returns Max                                   2.08365\n",
      "evaluation/Returns Min                                  -7.28728\n",
      "evaluation/Actions Mean                                  0.0205416\n",
      "evaluation/Actions Std                                   0.102917\n",
      "evaluation/Actions Max                                   0.971614\n",
      "evaluation/Actions Min                                  -0.612559\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.61268\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0810242\n",
      "evaluation/env_infos/final/reward_dist Std               0.192026\n",
      "evaluation/env_infos/final/reward_dist Max               0.942102\n",
      "evaluation/env_infos/final/reward_dist Min               1.52612e-148\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0100436\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0190141\n",
      "evaluation/env_infos/initial/reward_dist Max             0.107787\n",
      "evaluation/env_infos/initial/reward_dist Min             9.975e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.112129\n",
      "evaluation/env_infos/reward_dist Std                     0.221749\n",
      "evaluation/env_infos/reward_dist Max                     0.982678\n",
      "evaluation/env_infos/reward_dist Min                     1.52612e-148\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0605238\n",
      "evaluation/env_infos/final/reward_energy Std             0.0401128\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00129525\n",
      "evaluation/env_infos/final/reward_energy Min            -0.168712\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.359522\n",
      "evaluation/env_infos/initial/reward_energy Std           0.353627\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00563142\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.13136\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0777533\n",
      "evaluation/env_infos/reward_energy Std                   0.12642\n",
      "evaluation/env_infos/reward_energy Max                  -0.00129525\n",
      "evaluation/env_infos/reward_energy Min                  -1.13136\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.180686\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.39373\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.667718\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00636017\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0166563\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0485807\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0306279\n",
      "evaluation/env_infos/end_effector_loc Mean               0.097299\n",
      "evaluation/env_infos/end_effector_loc Std                0.266532\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.667718\n",
      "time/data storing (s)                                    0.172637\n",
      "time/evaluation sampling (s)                             0.789501\n",
      "time/exploration sampling (s)                            0.114955\n",
      "time/logging (s)                                         0.0190642\n",
      "time/saving (s)                                          0.0264536\n",
      "time/training (s)                                       48.3588\n",
      "time/epoch (s)                                          49.4814\n",
      "time/total (s)                                        4075.38\n",
      "Epoch                                                   85\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:21:14.638255 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 86 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00139276\r\n",
      "trainer/QF2 Loss                                         0.00222677\r\n",
      "trainer/Policy Loss                                      2.86652\r\n",
      "trainer/Q1 Predictions Mean                             -0.889548\r\n",
      "trainer/Q1 Predictions Std                               0.788552\r\n",
      "trainer/Q1 Predictions Max                               1.17049\r\n",
      "trainer/Q1 Predictions Min                              -3.1979\r\n",
      "trainer/Q2 Predictions Mean                             -0.897323\r\n",
      "trainer/Q2 Predictions Std                               0.793294\r\n",
      "trainer/Q2 Predictions Max                               1.15185\r\n",
      "trainer/Q2 Predictions Min                              -3.25711\r\n",
      "trainer/Q Targets Mean                                  -0.887976\r\n",
      "trainer/Q Targets Std                                    0.78946\r\n",
      "trainer/Q Targets Max                                    1.20983\r\n",
      "trainer/Q Targets Min                                   -3.23056\r\n",
      "trainer/Log Pis Mean                                     1.97203\r\n",
      "trainer/Log Pis Std                                      1.47176\r\n",
      "trainer/Log Pis Max                                      4.96178\r\n",
      "trainer/Log Pis Min                                     -2.78142\r\n",
      "trainer/Policy mu Mean                                   0.0504947\r\n",
      "trainer/Policy mu Std                                    0.355354\r\n",
      "trainer/Policy mu Max                                    2.23448\r\n",
      "trainer/Policy mu Min                                   -1.66797\r\n",
      "trainer/Policy log std Mean                             -2.28661\r\n",
      "trainer/Policy log std Std                               0.683784\r\n",
      "trainer/Policy log std Max                               0.00592566\r\n",
      "trainer/Policy log std Min                              -3.46912\r\n",
      "trainer/Alpha                                            0.022547\r\n",
      "trainer/Alpha Loss                                      -0.106032\r\n",
      "exploration/num steps total                           9700\r\n",
      "exploration/num paths total                            485\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0899662\r\n",
      "exploration/Rewards Std                                  0.101396\r\n",
      "exploration/Rewards Max                                  0.0992793\r\n",
      "exploration/Rewards Min                                 -0.460669\r\n",
      "exploration/Returns Mean                                -1.79932\r\n",
      "exploration/Returns Std                                  1.28709\r\n",
      "exploration/Returns Max                                 -0.391238\r\n",
      "exploration/Returns Min                                 -3.31722\r\n",
      "exploration/Actions Mean                                 0.0326654\r\n",
      "exploration/Actions Std                                  0.169769\r\n",
      "exploration/Actions Max                                  0.640827\r\n",
      "exploration/Actions Min                                 -0.485229\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.79932\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0579677\r\n",
      "exploration/env_infos/final/reward_dist Std              0.109026\r\n",
      "exploration/env_infos/final/reward_dist Max              0.275747\r\n",
      "exploration/env_infos/final/reward_dist Min              6.90286e-141\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0020475\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00252047\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00639035\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.04252e-07\r\n",
      "exploration/env_infos/reward_dist Mean                   0.138707\r\n",
      "exploration/env_infos/reward_dist Std                    0.225113\r\n",
      "exploration/env_infos/reward_dist Max                    0.829954\r\n",
      "exploration/env_infos/reward_dist Min                    6.90286e-141\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.270445\r\n",
      "exploration/env_infos/final/reward_energy Std            0.170695\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.112671\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.594273\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.260489\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.157469\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0503483\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.480332\r\n",
      "exploration/env_infos/reward_energy Mean                -0.194445\r\n",
      "exploration/env_infos/reward_energy Std                  0.148216\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0171644\r\n",
      "exploration/env_infos/reward_energy Min                 -0.733324\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.313005\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.586211\r\n",
      "exploration/env_infos/final/end_effector_loc Max         1\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00415892\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00992559\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0237284\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0101116\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.181057\r\n",
      "exploration/env_infos/end_effector_loc Std               0.39574\r\n",
      "exploration/env_infos/end_effector_loc Max               1\r\n",
      "exploration/env_infos/end_effector_loc Min              -1\r\n",
      "evaluation/num steps total                           87000\r\n",
      "evaluation/num paths total                            4350\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0892788\r\n",
      "evaluation/Rewards Std                                   0.141005\r\n",
      "evaluation/Rewards Max                                   0.155905\r\n",
      "evaluation/Rewards Min                                  -0.831587\r\n",
      "evaluation/Returns Mean                                 -1.78558\r\n",
      "evaluation/Returns Std                                   2.3401\r\n",
      "evaluation/Returns Max                                   1.00185\r\n",
      "evaluation/Returns Min                                  -8.7892\r\n",
      "evaluation/Actions Mean                                  0.0319743\r\n",
      "evaluation/Actions Std                                   0.145968\r\n",
      "evaluation/Actions Max                                   0.986567\r\n",
      "evaluation/Actions Min                                  -0.783166\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.78558\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.102989\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.195547\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.882095\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.84486e-114\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0107043\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0212805\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.104136\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.46563e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.113157\r\n",
      "evaluation/env_infos/reward_dist Std                     0.215037\r\n",
      "evaluation/env_infos/reward_dist Max                     0.994096\r\n",
      "evaluation/env_infos/reward_dist Min                     8.84486e-114\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0539599\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0397783\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00595369\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.125695\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.351889\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.360852\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00709693\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.12849\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.110759\r\n",
      "evaluation/env_infos/reward_energy Std                   0.179974\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00298353\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.2339\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.182787\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.448716\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00704413\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0163686\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0479878\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0391583\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.123436\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.322814\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.166801\r\n",
      "time/evaluation sampling (s)                             0.854743\r\n",
      "time/exploration sampling (s)                            0.106736\r\n",
      "time/logging (s)                                         0.018717\r\n",
      "time/saving (s)                                          0.0263557\r\n",
      "time/training (s)                                       47.3044\r\n",
      "time/epoch (s)                                          48.4777\r\n",
      "time/total (s)                                        4124.62\r\n",
      "Epoch                                                   86\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:22:04.208950 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 87 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00143119\n",
      "trainer/QF2 Loss                                         0.0018854\n",
      "trainer/Policy Loss                                      3.00775\n",
      "trainer/Q1 Predictions Mean                             -0.944837\n",
      "trainer/Q1 Predictions Std                               0.783792\n",
      "trainer/Q1 Predictions Max                               1.2512\n",
      "trainer/Q1 Predictions Min                              -3.16169\n",
      "trainer/Q2 Predictions Mean                             -0.931347\n",
      "trainer/Q2 Predictions Std                               0.78726\n",
      "trainer/Q2 Predictions Max                               1.28355\n",
      "trainer/Q2 Predictions Min                              -3.15165\n",
      "trainer/Q Targets Mean                                  -0.928359\n",
      "trainer/Q Targets Std                                    0.788026\n",
      "trainer/Q Targets Max                                    1.27601\n",
      "trainer/Q Targets Min                                   -3.16495\n",
      "trainer/Log Pis Mean                                     2.07631\n",
      "trainer/Log Pis Std                                      1.2665\n",
      "trainer/Log Pis Max                                      4.3818\n",
      "trainer/Log Pis Min                                     -1.8126\n",
      "trainer/Policy mu Mean                                   0.096781\n",
      "trainer/Policy mu Std                                    0.407491\n",
      "trainer/Policy mu Max                                    2.18003\n",
      "trainer/Policy mu Min                                   -1.22911\n",
      "trainer/Policy log std Mean                             -2.24918\n",
      "trainer/Policy log std Std                               0.702923\n",
      "trainer/Policy log std Max                              -0.335737\n",
      "trainer/Policy log std Min                              -3.308\n",
      "trainer/Alpha                                            0.023686\n",
      "trainer/Alpha Loss                                       0.285667\n",
      "exploration/num steps total                           9800\n",
      "exploration/num paths total                            490\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.181526\n",
      "exploration/Rewards Std                                  0.144532\n",
      "exploration/Rewards Max                                 -0.0241603\n",
      "exploration/Rewards Min                                 -0.602573\n",
      "exploration/Returns Mean                                -3.63052\n",
      "exploration/Returns Std                                  2.60096\n",
      "exploration/Returns Max                                 -1.49103\n",
      "exploration/Returns Min                                 -8.51446\n",
      "exploration/Actions Mean                                 0.123892\n",
      "exploration/Actions Std                                  0.272951\n",
      "exploration/Actions Max                                  0.999214\n",
      "exploration/Actions Min                                 -0.317538\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.63052\n",
      "exploration/env_infos/final/reward_dist Mean             0.0409292\n",
      "exploration/env_infos/final/reward_dist Std              0.0773989\n",
      "exploration/env_infos/final/reward_dist Max              0.195567\n",
      "exploration/env_infos/final/reward_dist Min              4.90512e-101\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00118732\n",
      "exploration/env_infos/initial/reward_dist Std            0.00134562\n",
      "exploration/env_infos/initial/reward_dist Max            0.00370532\n",
      "exploration/env_infos/initial/reward_dist Min            9.54666e-07\n",
      "exploration/env_infos/reward_dist Mean                   0.0453387\n",
      "exploration/env_infos/reward_dist Std                    0.125577\n",
      "exploration/env_infos/reward_dist Max                    0.61584\n",
      "exploration/env_infos/reward_dist Min                    4.90512e-101\n",
      "exploration/env_infos/final/reward_energy Mean          -0.150025\n",
      "exploration/env_infos/final/reward_energy Std            0.105849\n",
      "exploration/env_infos/final/reward_energy Max           -0.0467614\n",
      "exploration/env_infos/final/reward_energy Min           -0.347915\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.436071\n",
      "exploration/env_infos/initial/reward_energy Std          0.468201\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0410231\n",
      "exploration/env_infos/initial/reward_energy Min         -1.32999\n",
      "exploration/env_infos/reward_energy Mean                -0.264994\n",
      "exploration/env_infos/reward_energy Std                  0.33088\n",
      "exploration/env_infos/reward_energy Max                 -0.00987299\n",
      "exploration/env_infos/reward_energy Min                 -1.32999\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.376505\n",
      "exploration/env_infos/final/end_effector_loc Std         0.576866\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.510906\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0125752\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0188037\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0494913\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00379438\n",
      "exploration/env_infos/end_effector_loc Mean              0.252165\n",
      "exploration/env_infos/end_effector_loc Std               0.431428\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -0.510906\n",
      "evaluation/num steps total                           88000\n",
      "evaluation/num paths total                            4400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0743607\n",
      "evaluation/Rewards Std                                   0.108636\n",
      "evaluation/Rewards Max                                   0.184917\n",
      "evaluation/Rewards Min                                  -0.883706\n",
      "evaluation/Returns Mean                                 -1.48721\n",
      "evaluation/Returns Std                                   1.82235\n",
      "evaluation/Returns Max                                   2.27303\n",
      "evaluation/Returns Min                                  -9.19738\n",
      "evaluation/Actions Mean                                  0.0179586\n",
      "evaluation/Actions Std                                   0.123696\n",
      "evaluation/Actions Max                                   0.968244\n",
      "evaluation/Actions Min                                  -0.489704\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.48721\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0798651\n",
      "evaluation/env_infos/final/reward_dist Std               0.199491\n",
      "evaluation/env_infos/final/reward_dist Max               0.970463\n",
      "evaluation/env_infos/final/reward_dist Min               2.08158e-119\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00931202\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0200603\n",
      "evaluation/env_infos/initial/reward_dist Max             0.119247\n",
      "evaluation/env_infos/initial/reward_dist Min             5.18135e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.139221\n",
      "evaluation/env_infos/reward_dist Std                     0.238939\n",
      "evaluation/env_infos/reward_dist Max                     0.995413\n",
      "evaluation/env_infos/reward_dist Min                     2.08158e-119\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0640965\n",
      "evaluation/env_infos/final/reward_energy Std             0.0735436\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00637286\n",
      "evaluation/env_infos/final/reward_energy Min            -0.417731\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.303713\n",
      "evaluation/env_infos/initial/reward_energy Std           0.316398\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0048015\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.35121\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0984673\n",
      "evaluation/env_infos/reward_energy Std                   0.146801\n",
      "evaluation/env_infos/reward_energy Max                  -0.00214145\n",
      "evaluation/env_infos/reward_energy Min                  -1.35121\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.114805\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.367767\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.598788\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0052521\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0145894\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0484122\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0244852\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0764762\n",
      "evaluation/env_infos/end_effector_loc Std                0.251018\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.598788\n",
      "time/data storing (s)                                    0.163604\n",
      "time/evaluation sampling (s)                             0.809132\n",
      "time/exploration sampling (s)                            0.107326\n",
      "time/logging (s)                                         0.0197163\n",
      "time/saving (s)                                          0.0259937\n",
      "time/training (s)                                       47.671\n",
      "time/epoch (s)                                          48.7968\n",
      "time/total (s)                                        4174.19\n",
      "Epoch                                                   87\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:22:54.056728 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 88 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00123431\n",
      "trainer/QF2 Loss                                         0.00108164\n",
      "trainer/Policy Loss                                      2.84208\n",
      "trainer/Q1 Predictions Mean                             -1.03705\n",
      "trainer/Q1 Predictions Std                               0.722135\n",
      "trainer/Q1 Predictions Max                               0.426493\n",
      "trainer/Q1 Predictions Min                              -3.19499\n",
      "trainer/Q2 Predictions Mean                             -1.03796\n",
      "trainer/Q2 Predictions Std                               0.715781\n",
      "trainer/Q2 Predictions Max                               0.400247\n",
      "trainer/Q2 Predictions Min                              -3.19511\n",
      "trainer/Q Targets Mean                                  -1.04011\n",
      "trainer/Q Targets Std                                    0.719013\n",
      "trainer/Q Targets Max                                    0.392147\n",
      "trainer/Q Targets Min                                   -3.19577\n",
      "trainer/Log Pis Mean                                     1.80159\n",
      "trainer/Log Pis Std                                      1.41382\n",
      "trainer/Log Pis Max                                      4.45182\n",
      "trainer/Log Pis Min                                     -3.19177\n",
      "trainer/Policy mu Mean                                   0.0519693\n",
      "trainer/Policy mu Std                                    0.264729\n",
      "trainer/Policy mu Max                                    1.63253\n",
      "trainer/Policy mu Min                                   -1.09025\n",
      "trainer/Policy log std Mean                             -2.25548\n",
      "trainer/Policy log std Std                               0.599475\n",
      "trainer/Policy log std Max                              -0.343079\n",
      "trainer/Policy log std Min                              -3.22694\n",
      "trainer/Alpha                                            0.024104\n",
      "trainer/Alpha Loss                                      -0.739216\n",
      "exploration/num steps total                           9900\n",
      "exploration/num paths total                            495\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.126753\n",
      "exploration/Rewards Std                                  0.101903\n",
      "exploration/Rewards Max                                  0.0409857\n",
      "exploration/Rewards Min                                 -0.387946\n",
      "exploration/Returns Mean                                -2.53505\n",
      "exploration/Returns Std                                  1.47397\n",
      "exploration/Returns Max                                 -0.840934\n",
      "exploration/Returns Min                                 -4.79155\n",
      "exploration/Actions Mean                                 0.0050803\n",
      "exploration/Actions Std                                  0.177229\n",
      "exploration/Actions Max                                  0.864776\n",
      "exploration/Actions Min                                 -0.611492\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.53505\n",
      "exploration/env_infos/final/reward_dist Mean             0.12731\n",
      "exploration/env_infos/final/reward_dist Std              0.213472\n",
      "exploration/env_infos/final/reward_dist Max              0.551471\n",
      "exploration/env_infos/final/reward_dist Min              3.22034e-56\n",
      "exploration/env_infos/initial/reward_dist Mean           0.013844\n",
      "exploration/env_infos/initial/reward_dist Std            0.0158605\n",
      "exploration/env_infos/initial/reward_dist Max            0.0407365\n",
      "exploration/env_infos/initial/reward_dist Min            5.59774e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.155936\n",
      "exploration/env_infos/reward_dist Std                    0.259623\n",
      "exploration/env_infos/reward_dist Max                    0.935508\n",
      "exploration/env_infos/reward_dist Min                    3.22034e-56\n",
      "exploration/env_infos/final/reward_energy Mean          -0.143218\n",
      "exploration/env_infos/final/reward_energy Std            0.0540683\n",
      "exploration/env_infos/final/reward_energy Max           -0.0535961\n",
      "exploration/env_infos/final/reward_energy Min           -0.195594\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.31171\n",
      "exploration/env_infos/initial/reward_energy Std          0.342881\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0403505\n",
      "exploration/env_infos/initial/reward_energy Min         -0.927864\n",
      "exploration/env_infos/reward_energy Mean                -0.190026\n",
      "exploration/env_infos/reward_energy Std                  0.163592\n",
      "exploration/env_infos/reward_energy Max                 -0.0191199\n",
      "exploration/env_infos/reward_energy Min                 -0.927864\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.115506\n",
      "exploration/env_infos/final/end_effector_loc Std         0.459902\n",
      "exploration/env_infos/final/end_effector_loc Max         0.740644\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.923626\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00633737\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.015108\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0432388\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0149878\n",
      "exploration/env_infos/end_effector_loc Mean              0.0750756\n",
      "exploration/env_infos/end_effector_loc Std               0.263141\n",
      "exploration/env_infos/end_effector_loc Max               0.740644\n",
      "exploration/env_infos/end_effector_loc Min              -0.923626\n",
      "evaluation/num steps total                           89000\n",
      "evaluation/num paths total                            4450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0903817\n",
      "evaluation/Rewards Std                                   0.126637\n",
      "evaluation/Rewards Max                                   0.155493\n",
      "evaluation/Rewards Min                                  -0.643867\n",
      "evaluation/Returns Mean                                 -1.80763\n",
      "evaluation/Returns Std                                   2.0603\n",
      "evaluation/Returns Max                                   2.07506\n",
      "evaluation/Returns Min                                  -7.98779\n",
      "evaluation/Actions Mean                                  0.0217718\n",
      "evaluation/Actions Std                                   0.131223\n",
      "evaluation/Actions Max                                   0.935577\n",
      "evaluation/Actions Min                                  -0.630426\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.80763\n",
      "evaluation/env_infos/final/reward_dist Mean              0.07464\n",
      "evaluation/env_infos/final/reward_dist Std               0.19845\n",
      "evaluation/env_infos/final/reward_dist Max               0.96979\n",
      "evaluation/env_infos/final/reward_dist Min               6.74357e-165\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00817146\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0200275\n",
      "evaluation/env_infos/initial/reward_dist Max             0.109881\n",
      "evaluation/env_infos/initial/reward_dist Min             6.69432e-09\n",
      "evaluation/env_infos/reward_dist Mean                    0.0846608\n",
      "evaluation/env_infos/reward_dist Std                     0.189766\n",
      "evaluation/env_infos/reward_dist Max                     0.992097\n",
      "evaluation/env_infos/reward_dist Min                     6.74357e-165\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0460979\n",
      "evaluation/env_infos/final/reward_energy Std             0.0372139\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00570571\n",
      "evaluation/env_infos/final/reward_energy Min            -0.118504\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.336853\n",
      "evaluation/env_infos/initial/reward_energy Std           0.338446\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0109134\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.23509\n",
      "evaluation/env_infos/reward_energy Mean                 -0.100438\n",
      "evaluation/env_infos/reward_energy Std                   0.159057\n",
      "evaluation/env_infos/reward_energy Max                  -0.000708192\n",
      "evaluation/env_infos/reward_energy Min                  -1.23509\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.171056\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.442834\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00574209\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.015876\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0467788\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0296109\n",
      "evaluation/env_infos/end_effector_loc Mean               0.1023\n",
      "evaluation/env_infos/end_effector_loc Std                0.311497\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.170823\n",
      "time/evaluation sampling (s)                             1.16891\n",
      "time/exploration sampling (s)                            0.111309\n",
      "time/logging (s)                                         0.0189073\n",
      "time/saving (s)                                          0.0268508\n",
      "time/training (s)                                       47.2843\n",
      "time/epoch (s)                                          48.7811\n",
      "time/total (s)                                        4224.03\n",
      "Epoch                                                   88\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:23:43.886261 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 89 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00350714\n",
      "trainer/QF2 Loss                                         0.00178048\n",
      "trainer/Policy Loss                                      2.86015\n",
      "trainer/Q1 Predictions Mean                             -0.948761\n",
      "trainer/Q1 Predictions Std                               0.738708\n",
      "trainer/Q1 Predictions Max                               0.295797\n",
      "trainer/Q1 Predictions Min                              -2.84873\n",
      "trainer/Q2 Predictions Mean                             -0.962529\n",
      "trainer/Q2 Predictions Std                               0.738026\n",
      "trainer/Q2 Predictions Max                               0.289265\n",
      "trainer/Q2 Predictions Min                              -2.87328\n",
      "trainer/Q Targets Mean                                  -0.964092\n",
      "trainer/Q Targets Std                                    0.7359\n",
      "trainer/Q Targets Max                                    0.305675\n",
      "trainer/Q Targets Min                                   -2.83453\n",
      "trainer/Log Pis Mean                                     1.90685\n",
      "trainer/Log Pis Std                                      1.36162\n",
      "trainer/Log Pis Max                                      4.59239\n",
      "trainer/Log Pis Min                                     -3.53385\n",
      "trainer/Policy mu Mean                                   0.00709555\n",
      "trainer/Policy mu Std                                    0.364493\n",
      "trainer/Policy mu Max                                    2.17999\n",
      "trainer/Policy mu Min                                   -2.60592\n",
      "trainer/Policy log std Mean                             -2.2737\n",
      "trainer/Policy log std Std                               0.615195\n",
      "trainer/Policy log std Max                               0.270077\n",
      "trainer/Policy log std Min                              -3.27106\n",
      "trainer/Alpha                                            0.02277\n",
      "trainer/Alpha Loss                                      -0.352227\n",
      "exploration/num steps total                          10000\n",
      "exploration/num paths total                            500\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0947039\n",
      "exploration/Rewards Std                                  0.120274\n",
      "exploration/Rewards Max                                  0.112146\n",
      "exploration/Rewards Min                                 -0.846928\n",
      "exploration/Returns Mean                                -1.89408\n",
      "exploration/Returns Std                                  1.38296\n",
      "exploration/Returns Max                                  0.0430853\n",
      "exploration/Returns Min                                 -3.70816\n",
      "exploration/Actions Mean                                -0.00382948\n",
      "exploration/Actions Std                                  0.176811\n",
      "exploration/Actions Max                                  0.772166\n",
      "exploration/Actions Min                                 -0.997529\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.89408\n",
      "exploration/env_infos/final/reward_dist Mean             0.16376\n",
      "exploration/env_infos/final/reward_dist Std              0.206851\n",
      "exploration/env_infos/final/reward_dist Max              0.490477\n",
      "exploration/env_infos/final/reward_dist Min              2.43336e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00466212\n",
      "exploration/env_infos/initial/reward_dist Std            0.00699272\n",
      "exploration/env_infos/initial/reward_dist Max            0.0184637\n",
      "exploration/env_infos/initial/reward_dist Min            8.69771e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.178798\n",
      "exploration/env_infos/reward_dist Std                    0.263738\n",
      "exploration/env_infos/reward_dist Max                    0.983645\n",
      "exploration/env_infos/reward_dist Min                    2.43336e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.151101\n",
      "exploration/env_infos/final/reward_energy Std            0.0777439\n",
      "exploration/env_infos/final/reward_energy Max           -0.0930175\n",
      "exploration/env_infos/final/reward_energy Min           -0.302538\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.593022\n",
      "exploration/env_infos/initial/reward_energy Std          0.442085\n",
      "exploration/env_infos/initial/reward_energy Max         -0.182515\n",
      "exploration/env_infos/initial/reward_energy Min         -1.26147\n",
      "exploration/env_infos/reward_energy Mean                -0.181912\n",
      "exploration/env_infos/reward_energy Std                  0.171643\n",
      "exploration/env_infos/reward_energy Max                 -0.0249152\n",
      "exploration/env_infos/reward_energy Min                 -1.26147\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00627374\n",
      "exploration/env_infos/final/end_effector_loc Std         0.304893\n",
      "exploration/env_infos/final/end_effector_loc Max         0.459373\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.451967\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000610545\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0261442\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0386083\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0498765\n",
      "exploration/env_infos/end_effector_loc Mean              0.0122268\n",
      "exploration/env_infos/end_effector_loc Std               0.209474\n",
      "exploration/env_infos/end_effector_loc Max               0.459373\n",
      "exploration/env_infos/end_effector_loc Min              -0.451967\n",
      "evaluation/num steps total                           90000\n",
      "evaluation/num paths total                            4500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.106774\n",
      "evaluation/Rewards Std                                   0.125971\n",
      "evaluation/Rewards Max                                   0.114303\n",
      "evaluation/Rewards Min                                  -0.650242\n",
      "evaluation/Returns Mean                                 -2.13548\n",
      "evaluation/Returns Std                                   2.06934\n",
      "evaluation/Returns Max                                   0.928545\n",
      "evaluation/Returns Min                                  -8.14401\n",
      "evaluation/Actions Mean                                  0.0132518\n",
      "evaluation/Actions Std                                   0.124079\n",
      "evaluation/Actions Max                                   0.964834\n",
      "evaluation/Actions Min                                  -0.888414\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.13548\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0887672\n",
      "evaluation/env_infos/final/reward_dist Std               0.203096\n",
      "evaluation/env_infos/final/reward_dist Max               0.75537\n",
      "evaluation/env_infos/final/reward_dist Min               4.6041e-135\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00999544\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0171086\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0742526\n",
      "evaluation/env_infos/initial/reward_dist Min             7.90644e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0689504\n",
      "evaluation/env_infos/reward_dist Std                     0.160726\n",
      "evaluation/env_infos/reward_dist Max                     0.990446\n",
      "evaluation/env_infos/reward_dist Min                     4.6041e-135\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.063079\n",
      "evaluation/env_infos/final/reward_energy Std             0.0640444\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00489161\n",
      "evaluation/env_infos/final/reward_energy Min            -0.285796\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.384708\n",
      "evaluation/env_infos/initial/reward_energy Std           0.368784\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00387758\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.21321\n",
      "evaluation/env_infos/reward_energy Mean                 -0.101755\n",
      "evaluation/env_infos/reward_energy Std                   0.144181\n",
      "evaluation/env_infos/reward_energy Max                  -0.00156242\n",
      "evaluation/env_infos/reward_energy Min                  -1.21321\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.172233\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.503105\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00774807\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0171747\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0482417\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0444207\n",
      "evaluation/env_infos/end_effector_loc Mean               0.104212\n",
      "evaluation/env_infos/end_effector_loc Std                0.319528\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.168035\n",
      "time/evaluation sampling (s)                             0.79565\n",
      "time/exploration sampling (s)                            0.108044\n",
      "time/logging (s)                                         0.0199063\n",
      "time/saving (s)                                          0.0281524\n",
      "time/training (s)                                       47.9453\n",
      "time/epoch (s)                                          49.0651\n",
      "time/total (s)                                        4273.86\n",
      "Epoch                                                   89\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:24:40.283071 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 90 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00138959\n",
      "trainer/QF2 Loss                                         0.00104955\n",
      "trainer/Policy Loss                                      3.07923\n",
      "trainer/Q1 Predictions Mean                             -0.94349\n",
      "trainer/Q1 Predictions Std                               0.69706\n",
      "trainer/Q1 Predictions Max                               0.342225\n",
      "trainer/Q1 Predictions Min                              -2.86372\n",
      "trainer/Q2 Predictions Mean                             -0.941777\n",
      "trainer/Q2 Predictions Std                               0.694722\n",
      "trainer/Q2 Predictions Max                               0.328778\n",
      "trainer/Q2 Predictions Min                              -2.85238\n",
      "trainer/Q Targets Mean                                  -0.932527\n",
      "trainer/Q Targets Std                                    0.701579\n",
      "trainer/Q Targets Max                                    0.341793\n",
      "trainer/Q Targets Min                                   -2.86561\n",
      "trainer/Log Pis Mean                                     2.13725\n",
      "trainer/Log Pis Std                                      1.38376\n",
      "trainer/Log Pis Max                                      4.59206\n",
      "trainer/Log Pis Min                                     -4.30272\n",
      "trainer/Policy mu Mean                                   0.0308615\n",
      "trainer/Policy mu Std                                    0.318565\n",
      "trainer/Policy mu Max                                    2.30473\n",
      "trainer/Policy mu Min                                   -2.08266\n",
      "trainer/Policy log std Mean                             -2.40055\n",
      "trainer/Policy log std Std                               0.626112\n",
      "trainer/Policy log std Max                               0.323687\n",
      "trainer/Policy log std Min                              -3.32356\n",
      "trainer/Alpha                                            0.0209849\n",
      "trainer/Alpha Loss                                       0.53033\n",
      "exploration/num steps total                          10100\n",
      "exploration/num paths total                            505\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0813284\n",
      "exploration/Rewards Std                                  0.0826171\n",
      "exploration/Rewards Max                                  0.0944295\n",
      "exploration/Rewards Min                                 -0.338596\n",
      "exploration/Returns Mean                                -1.62657\n",
      "exploration/Returns Std                                  1.34314\n",
      "exploration/Returns Max                                  0.0704194\n",
      "exploration/Returns Min                                 -4.02027\n",
      "exploration/Actions Mean                                -0.00501085\n",
      "exploration/Actions Std                                  0.09335\n",
      "exploration/Actions Max                                  0.42086\n",
      "exploration/Actions Min                                 -0.393982\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.62657\n",
      "exploration/env_infos/final/reward_dist Mean             0.256629\n",
      "exploration/env_infos/final/reward_dist Std              0.315044\n",
      "exploration/env_infos/final/reward_dist Max              0.70254\n",
      "exploration/env_infos/final/reward_dist Min              1.93107e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00657782\n",
      "exploration/env_infos/initial/reward_dist Std            0.00690789\n",
      "exploration/env_infos/initial/reward_dist Max            0.018196\n",
      "exploration/env_infos/initial/reward_dist Min            0.000623503\n",
      "exploration/env_infos/reward_dist Mean                   0.140598\n",
      "exploration/env_infos/reward_dist Std                    0.216073\n",
      "exploration/env_infos/reward_dist Max                    0.70254\n",
      "exploration/env_infos/reward_dist Min                    1.93107e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0615932\n",
      "exploration/env_infos/final/reward_energy Std            0.0320697\n",
      "exploration/env_infos/final/reward_energy Max           -0.0171171\n",
      "exploration/env_infos/final/reward_energy Min           -0.111776\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.107364\n",
      "exploration/env_infos/initial/reward_energy Std          0.0361744\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0575427\n",
      "exploration/env_infos/initial/reward_energy Min         -0.167208\n",
      "exploration/env_infos/reward_energy Mean                -0.103562\n",
      "exploration/env_infos/reward_energy Std                  0.0821797\n",
      "exploration/env_infos/reward_energy Max                 -0.00466368\n",
      "exploration/env_infos/reward_energy Min                 -0.452005\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0250731\n",
      "exploration/env_infos/final/end_effector_loc Std         0.186103\n",
      "exploration/env_infos/final/end_effector_loc Max         0.313328\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.32085\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00106615\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00386106\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00585487\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00734334\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0101731\n",
      "exploration/env_infos/end_effector_loc Std               0.116412\n",
      "exploration/env_infos/end_effector_loc Max               0.313328\n",
      "exploration/env_infos/end_effector_loc Min              -0.32085\n",
      "evaluation/num steps total                           91000\n",
      "evaluation/num paths total                            4550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0819685\n",
      "evaluation/Rewards Std                                   0.128183\n",
      "evaluation/Rewards Max                                   0.139955\n",
      "evaluation/Rewards Min                                  -0.698998\n",
      "evaluation/Returns Mean                                 -1.63937\n",
      "evaluation/Returns Std                                   2.07696\n",
      "evaluation/Returns Max                                   1.67836\n",
      "evaluation/Returns Min                                  -8.99939\n",
      "evaluation/Actions Mean                                  0.00356114\n",
      "evaluation/Actions Std                                   0.105527\n",
      "evaluation/Actions Max                                   0.931147\n",
      "evaluation/Actions Min                                  -0.479286\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.63937\n",
      "evaluation/env_infos/final/reward_dist Mean              0.148436\n",
      "evaluation/env_infos/final/reward_dist Std               0.221453\n",
      "evaluation/env_infos/final/reward_dist Max               0.731425\n",
      "evaluation/env_infos/final/reward_dist Min               1.51724e-110\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00916013\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0308827\n",
      "evaluation/env_infos/initial/reward_dist Max             0.170933\n",
      "evaluation/env_infos/initial/reward_dist Min             8.96813e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.149614\n",
      "evaluation/env_infos/reward_dist Std                     0.249398\n",
      "evaluation/env_infos/reward_dist Max                     0.998123\n",
      "evaluation/env_infos/reward_dist Min                     1.51724e-110\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0529181\n",
      "evaluation/env_infos/final/reward_energy Std             0.0742575\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00174358\n",
      "evaluation/env_infos/final/reward_energy Min            -0.352563\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.304096\n",
      "evaluation/env_infos/initial/reward_energy Std           0.292482\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00288517\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.02949\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0889625\n",
      "evaluation/env_infos/reward_energy Std                   0.119929\n",
      "evaluation/env_infos/reward_energy Max                  -0.000818853\n",
      "evaluation/env_infos/reward_energy Min                  -1.02949\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0885417\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.436029\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00635388\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0134964\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0465574\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0239643\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0719062\n",
      "evaluation/env_infos/end_effector_loc Std                0.294084\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.185846\n",
      "time/evaluation sampling (s)                             0.805601\n",
      "time/exploration sampling (s)                            0.117606\n",
      "time/logging (s)                                         0.0219841\n",
      "time/saving (s)                                          0.0317751\n",
      "time/training (s)                                       54.2852\n",
      "time/epoch (s)                                          55.448\n",
      "time/total (s)                                        4330.26\n",
      "Epoch                                                   90\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:25:30.629312 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 91 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00155737\r\n",
      "trainer/QF2 Loss                                         0.00122399\r\n",
      "trainer/Policy Loss                                      2.99028\r\n",
      "trainer/Q1 Predictions Mean                             -0.882227\r\n",
      "trainer/Q1 Predictions Std                               0.664823\r\n",
      "trainer/Q1 Predictions Max                               0.2317\r\n",
      "trainer/Q1 Predictions Min                              -2.72783\r\n",
      "trainer/Q2 Predictions Mean                             -0.892137\r\n",
      "trainer/Q2 Predictions Std                               0.664491\r\n",
      "trainer/Q2 Predictions Max                               0.239854\r\n",
      "trainer/Q2 Predictions Min                              -2.73414\r\n",
      "trainer/Q Targets Mean                                  -0.892964\r\n",
      "trainer/Q Targets Std                                    0.667103\r\n",
      "trainer/Q Targets Max                                    0.268519\r\n",
      "trainer/Q Targets Min                                   -2.74332\r\n",
      "trainer/Log Pis Mean                                     2.10162\r\n",
      "trainer/Log Pis Std                                      1.65718\r\n",
      "trainer/Log Pis Max                                      4.53264\r\n",
      "trainer/Log Pis Min                                     -5.22658\r\n",
      "trainer/Policy mu Mean                                   0.074245\r\n",
      "trainer/Policy mu Std                                    0.291291\r\n",
      "trainer/Policy mu Max                                    1.85503\r\n",
      "trainer/Policy mu Min                                   -0.697497\r\n",
      "trainer/Policy log std Mean                             -2.38169\r\n",
      "trainer/Policy log std Std                               0.625916\r\n",
      "trainer/Policy log std Max                              -0.463717\r\n",
      "trainer/Policy log std Min                              -3.39207\r\n",
      "trainer/Alpha                                            0.0216064\r\n",
      "trainer/Alpha Loss                                       0.38986\r\n",
      "exploration/num steps total                          10200\r\n",
      "exploration/num paths total                            510\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0772382\r\n",
      "exploration/Rewards Std                                  0.10975\r\n",
      "exploration/Rewards Max                                  0.148277\r\n",
      "exploration/Rewards Min                                 -0.302148\r\n",
      "exploration/Returns Mean                                -1.54476\r\n",
      "exploration/Returns Std                                  1.84419\r\n",
      "exploration/Returns Max                                  1.11335\r\n",
      "exploration/Returns Min                                 -4.38951\r\n",
      "exploration/Actions Mean                                -0.0118864\r\n",
      "exploration/Actions Std                                  0.191239\r\n",
      "exploration/Actions Max                                  0.827822\r\n",
      "exploration/Actions Min                                 -0.562422\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.54476\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.155784\r\n",
      "exploration/env_infos/final/reward_dist Std              0.311525\r\n",
      "exploration/env_infos/final/reward_dist Max              0.778834\r\n",
      "exploration/env_infos/final/reward_dist Min              7.40153e-26\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.013673\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0194995\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0521167\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000159176\r\n",
      "exploration/env_infos/reward_dist Mean                   0.19138\r\n",
      "exploration/env_infos/reward_dist Std                    0.310156\r\n",
      "exploration/env_infos/reward_dist Max                    0.982153\r\n",
      "exploration/env_infos/reward_dist Min                    7.40153e-26\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.118241\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0879846\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0151065\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.243601\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.42216\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.250602\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0848604\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.839817\r\n",
      "exploration/env_infos/reward_energy Mean                -0.22241\r\n",
      "exploration/env_infos/reward_energy Std                  0.154793\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0151065\r\n",
      "exploration/env_infos/reward_energy Min                 -0.839817\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.110006\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.235253\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.252749\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.377818\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00052324\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0173494\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0413911\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0237053\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0309831\r\n",
      "exploration/env_infos/end_effector_loc Std               0.195879\r\n",
      "exploration/env_infos/end_effector_loc Max               0.353334\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.382667\r\n",
      "evaluation/num steps total                           92000\r\n",
      "evaluation/num paths total                            4600\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.070914\r\n",
      "evaluation/Rewards Std                                   0.103355\r\n",
      "evaluation/Rewards Max                                   0.152981\r\n",
      "evaluation/Rewards Min                                  -0.581136\r\n",
      "evaluation/Returns Mean                                 -1.41828\r\n",
      "evaluation/Returns Std                                   1.62647\r\n",
      "evaluation/Returns Max                                   1.97874\r\n",
      "evaluation/Returns Min                                  -5.21306\r\n",
      "evaluation/Actions Mean                                  0.0106078\r\n",
      "evaluation/Actions Std                                   0.105921\r\n",
      "evaluation/Actions Max                                   0.946853\r\n",
      "evaluation/Actions Min                                  -0.765728\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.41828\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.117691\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.245389\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.909787\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.08127e-117\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0111395\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0223954\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.119325\r\n",
      "evaluation/env_infos/initial/reward_dist Min             6.69587e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.107696\r\n",
      "evaluation/env_infos/reward_dist Std                     0.21965\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998496\r\n",
      "evaluation/env_infos/reward_dist Min                     2.08127e-117\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0442769\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.036735\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00173046\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.16045\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.295204\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.315371\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00525747\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.28398\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0832165\r\n",
      "evaluation/env_infos/reward_energy Std                   0.125453\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000342415\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.28398\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.104465\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.415466\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00412171\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.014706\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0473427\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0382864\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0715915\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.284129\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.168625\r\n",
      "time/evaluation sampling (s)                             1.02598\r\n",
      "time/exploration sampling (s)                            0.11026\r\n",
      "time/logging (s)                                         0.0192959\r\n",
      "time/saving (s)                                          0.0278728\r\n",
      "time/training (s)                                       47.989\r\n",
      "time/epoch (s)                                          49.341\r\n",
      "time/total (s)                                        4380.6\r\n",
      "Epoch                                                   91\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:26:21.716500 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 92 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00118749\n",
      "trainer/QF2 Loss                                         0.00123146\n",
      "trainer/Policy Loss                                      2.96542\n",
      "trainer/Q1 Predictions Mean                             -0.893798\n",
      "trainer/Q1 Predictions Std                               0.736205\n",
      "trainer/Q1 Predictions Max                               0.410477\n",
      "trainer/Q1 Predictions Min                              -2.67128\n",
      "trainer/Q2 Predictions Mean                             -0.896374\n",
      "trainer/Q2 Predictions Std                               0.732417\n",
      "trainer/Q2 Predictions Max                               0.420352\n",
      "trainer/Q2 Predictions Min                              -2.66976\n",
      "trainer/Q Targets Mean                                  -0.897346\n",
      "trainer/Q Targets Std                                    0.732749\n",
      "trainer/Q Targets Max                                    0.405905\n",
      "trainer/Q Targets Min                                   -2.65349\n",
      "trainer/Log Pis Mean                                     2.0764\n",
      "trainer/Log Pis Std                                      1.42461\n",
      "trainer/Log Pis Max                                      4.56269\n",
      "trainer/Log Pis Min                                     -3.06148\n",
      "trainer/Policy mu Mean                                   0.00512352\n",
      "trainer/Policy mu Std                                    0.258315\n",
      "trainer/Policy mu Max                                    1.71204\n",
      "trainer/Policy mu Min                                   -1.4678\n",
      "trainer/Policy log std Mean                             -2.36662\n",
      "trainer/Policy log std Std                               0.575383\n",
      "trainer/Policy log std Max                              -0.490131\n",
      "trainer/Policy log std Min                              -3.31314\n",
      "trainer/Alpha                                            0.0221906\n",
      "trainer/Alpha Loss                                       0.291032\n",
      "exploration/num steps total                          10300\n",
      "exploration/num paths total                            515\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.104673\n",
      "exploration/Rewards Std                                  0.0600008\n",
      "exploration/Rewards Max                                  0.0346121\n",
      "exploration/Rewards Min                                 -0.2769\n",
      "exploration/Returns Mean                                -2.09347\n",
      "exploration/Returns Std                                  0.669276\n",
      "exploration/Returns Max                                 -0.974121\n",
      "exploration/Returns Min                                 -2.90418\n",
      "exploration/Actions Mean                                -0.00450965\n",
      "exploration/Actions Std                                  0.113568\n",
      "exploration/Actions Max                                  0.271622\n",
      "exploration/Actions Min                                 -0.403834\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.09347\n",
      "exploration/env_infos/final/reward_dist Mean             0.0532754\n",
      "exploration/env_infos/final/reward_dist Std              0.0926449\n",
      "exploration/env_infos/final/reward_dist Max              0.237242\n",
      "exploration/env_infos/final/reward_dist Min              2.33737e-09\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000127546\n",
      "exploration/env_infos/initial/reward_dist Std            0.00013497\n",
      "exploration/env_infos/initial/reward_dist Max            0.00034114\n",
      "exploration/env_infos/initial/reward_dist Min            1.7169e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0595518\n",
      "exploration/env_infos/reward_dist Std                    0.15724\n",
      "exploration/env_infos/reward_dist Max                    0.931356\n",
      "exploration/env_infos/reward_dist Min                    2.33737e-09\n",
      "exploration/env_infos/final/reward_energy Mean          -0.118696\n",
      "exploration/env_infos/final/reward_energy Std            0.0417644\n",
      "exploration/env_infos/final/reward_energy Max           -0.0443805\n",
      "exploration/env_infos/final/reward_energy Min           -0.160877\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.154978\n",
      "exploration/env_infos/initial/reward_energy Std          0.114629\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0749311\n",
      "exploration/env_infos/initial/reward_energy Min         -0.381691\n",
      "exploration/env_infos/reward_energy Mean                -0.133456\n",
      "exploration/env_infos/reward_energy Std                  0.0895856\n",
      "exploration/env_infos/reward_energy Max                 -0.00647227\n",
      "exploration/env_infos/reward_energy Min                 -0.416097\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0301638\n",
      "exploration/env_infos/final/end_effector_loc Std         0.212756\n",
      "exploration/env_infos/final/end_effector_loc Max         0.352241\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.436703\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00300139\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00611873\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0135811\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00413459\n",
      "exploration/env_infos/end_effector_loc Mean              0.0442568\n",
      "exploration/env_infos/end_effector_loc Std               0.141556\n",
      "exploration/env_infos/end_effector_loc Max               0.365634\n",
      "exploration/env_infos/end_effector_loc Min              -0.436703\n",
      "evaluation/num steps total                           93000\n",
      "evaluation/num paths total                            4650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0764474\n",
      "evaluation/Rewards Std                                   0.111985\n",
      "evaluation/Rewards Max                                   0.144317\n",
      "evaluation/Rewards Min                                  -0.857598\n",
      "evaluation/Returns Mean                                 -1.52895\n",
      "evaluation/Returns Std                                   1.88496\n",
      "evaluation/Returns Max                                   1.8855\n",
      "evaluation/Returns Min                                  -6.72172\n",
      "evaluation/Actions Mean                                 -0.00824461\n",
      "evaluation/Actions Std                                   0.119888\n",
      "evaluation/Actions Max                                   0.855378\n",
      "evaluation/Actions Min                                  -0.834431\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.52895\n",
      "evaluation/env_infos/final/reward_dist Mean              0.132171\n",
      "evaluation/env_infos/final/reward_dist Std               0.254182\n",
      "evaluation/env_infos/final/reward_dist Max               0.948833\n",
      "evaluation/env_infos/final/reward_dist Min               4.37801e-169\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00681101\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0140823\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0852811\n",
      "evaluation/env_infos/initial/reward_dist Min             3.17278e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.146717\n",
      "evaluation/env_infos/reward_dist Std                     0.245252\n",
      "evaluation/env_infos/reward_dist Max                     0.999745\n",
      "evaluation/env_infos/reward_dist Min                     4.37801e-169\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0623801\n",
      "evaluation/env_infos/final/reward_energy Std             0.0783741\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00365879\n",
      "evaluation/env_infos/final/reward_energy Min            -0.319185\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.233987\n",
      "evaluation/env_infos/initial/reward_energy Std           0.261756\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00917855\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.13048\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0915213\n",
      "evaluation/env_infos/reward_energy Std                   0.143199\n",
      "evaluation/env_infos/reward_energy Max                  -0.000895207\n",
      "evaluation/env_infos/reward_energy Min                  -1.13048\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00843145\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.461089\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000524884\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0124019\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0427689\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0417216\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00104593\n",
      "evaluation/env_infos/end_effector_loc Std                0.302028\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.163699\n",
      "time/evaluation sampling (s)                             0.808151\n",
      "time/exploration sampling (s)                            0.108981\n",
      "time/logging (s)                                         0.0186985\n",
      "time/saving (s)                                          0.0323459\n",
      "time/training (s)                                       49.0849\n",
      "time/epoch (s)                                          50.2167\n",
      "time/total (s)                                        4431.68\n",
      "Epoch                                                   92\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:27:11.137749 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 93 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00185674\n",
      "trainer/QF2 Loss                                         0.00111891\n",
      "trainer/Policy Loss                                      2.95915\n",
      "trainer/Q1 Predictions Mean                             -0.921569\n",
      "trainer/Q1 Predictions Std                               0.665205\n",
      "trainer/Q1 Predictions Max                               0.128227\n",
      "trainer/Q1 Predictions Min                              -2.79663\n",
      "trainer/Q2 Predictions Mean                             -0.913196\n",
      "trainer/Q2 Predictions Std                               0.658143\n",
      "trainer/Q2 Predictions Max                               0.162464\n",
      "trainer/Q2 Predictions Min                              -2.74798\n",
      "trainer/Q Targets Mean                                  -0.91146\n",
      "trainer/Q Targets Std                                    0.655314\n",
      "trainer/Q Targets Max                                    0.125212\n",
      "trainer/Q Targets Min                                   -2.76451\n",
      "trainer/Log Pis Mean                                     2.04866\n",
      "trainer/Log Pis Std                                      1.27157\n",
      "trainer/Log Pis Max                                      4.15264\n",
      "trainer/Log Pis Min                                     -3.52376\n",
      "trainer/Policy mu Mean                                  -0.00250596\n",
      "trainer/Policy mu Std                                    0.275928\n",
      "trainer/Policy mu Max                                    1.8406\n",
      "trainer/Policy mu Min                                   -1.10811\n",
      "trainer/Policy log std Mean                             -2.32798\n",
      "trainer/Policy log std Std                               0.56127\n",
      "trainer/Policy log std Max                              -0.443603\n",
      "trainer/Policy log std Min                              -3.23137\n",
      "trainer/Alpha                                            0.0231679\n",
      "trainer/Alpha Loss                                       0.183149\n",
      "exploration/num steps total                          10400\n",
      "exploration/num paths total                            520\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0529393\n",
      "exploration/Rewards Std                                  0.0677322\n",
      "exploration/Rewards Max                                  0.0976119\n",
      "exploration/Rewards Min                                 -0.182319\n",
      "exploration/Returns Mean                                -1.05879\n",
      "exploration/Returns Std                                  1.04181\n",
      "exploration/Returns Max                                  0.762286\n",
      "exploration/Returns Min                                 -1.99724\n",
      "exploration/Actions Mean                                -0.00704765\n",
      "exploration/Actions Std                                  0.07012\n",
      "exploration/Actions Max                                  0.19904\n",
      "exploration/Actions Min                                 -0.218893\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.05879\n",
      "exploration/env_infos/final/reward_dist Mean             0.10907\n",
      "exploration/env_infos/final/reward_dist Std              0.146137\n",
      "exploration/env_infos/final/reward_dist Max              0.370133\n",
      "exploration/env_infos/final/reward_dist Min              1.39452e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0091366\n",
      "exploration/env_infos/initial/reward_dist Std            0.0100494\n",
      "exploration/env_infos/initial/reward_dist Max            0.0271672\n",
      "exploration/env_infos/initial/reward_dist Min            1.04988e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.159242\n",
      "exploration/env_infos/reward_dist Std                    0.222376\n",
      "exploration/env_infos/reward_dist Max                    0.632337\n",
      "exploration/env_infos/reward_dist Min                    1.39452e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0881258\n",
      "exploration/env_infos/final/reward_energy Std            0.0229605\n",
      "exploration/env_infos/final/reward_energy Max           -0.0497275\n",
      "exploration/env_infos/final/reward_energy Min           -0.119988\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.108064\n",
      "exploration/env_infos/initial/reward_energy Std          0.0413111\n",
      "exploration/env_infos/initial/reward_energy Max         -0.029289\n",
      "exploration/env_infos/initial/reward_energy Min         -0.146996\n",
      "exploration/env_infos/reward_energy Mean                -0.0868022\n",
      "exploration/env_infos/reward_energy Std                  0.0489731\n",
      "exploration/env_infos/reward_energy Max                 -0.0107702\n",
      "exploration/env_infos/reward_energy Min                 -0.219964\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0102117\n",
      "exploration/env_infos/final/end_effector_loc Std         0.218268\n",
      "exploration/env_infos/final/end_effector_loc Max         0.293352\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.322861\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00085268\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00400043\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00658049\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00591238\n",
      "exploration/env_infos/end_effector_loc Mean              0.0219036\n",
      "exploration/env_infos/end_effector_loc Std               0.136729\n",
      "exploration/env_infos/end_effector_loc Max               0.328358\n",
      "exploration/env_infos/end_effector_loc Min              -0.322861\n",
      "evaluation/num steps total                           94000\n",
      "evaluation/num paths total                            4700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.110778\n",
      "evaluation/Rewards Std                                   0.139213\n",
      "evaluation/Rewards Max                                   0.136203\n",
      "evaluation/Rewards Min                                  -0.670013\n",
      "evaluation/Returns Mean                                 -2.21556\n",
      "evaluation/Returns Std                                   2.29663\n",
      "evaluation/Returns Max                                   1.32321\n",
      "evaluation/Returns Min                                  -7.20042\n",
      "evaluation/Actions Mean                                  0.0164403\n",
      "evaluation/Actions Std                                   0.171958\n",
      "evaluation/Actions Max                                   0.922562\n",
      "evaluation/Actions Min                                  -0.872397\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.21556\n",
      "evaluation/env_infos/final/reward_dist Mean              0.09538\n",
      "evaluation/env_infos/final/reward_dist Std               0.187605\n",
      "evaluation/env_infos/final/reward_dist Max               0.828118\n",
      "evaluation/env_infos/final/reward_dist Min               4.46701e-186\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0070456\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0135536\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0770799\n",
      "evaluation/env_infos/initial/reward_dist Min             3.15451e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.0859936\n",
      "evaluation/env_infos/reward_dist Std                     0.188354\n",
      "evaluation/env_infos/reward_dist Max                     0.987683\n",
      "evaluation/env_infos/reward_dist Min                     4.46701e-186\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0940797\n",
      "evaluation/env_infos/final/reward_energy Std             0.123841\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00655633\n",
      "evaluation/env_infos/final/reward_energy Min            -0.578653\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.387373\n",
      "evaluation/env_infos/initial/reward_energy Std           0.365447\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0150175\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.12409\n",
      "evaluation/env_infos/reward_energy Mean                 -0.142584\n",
      "evaluation/env_infos/reward_energy Std                   0.198367\n",
      "evaluation/env_infos/reward_energy Max                  -0.00240072\n",
      "evaluation/env_infos/reward_energy Min                  -1.1263\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.234069\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.527499\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00902068\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0165269\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0461206\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0253228\n",
      "evaluation/env_infos/end_effector_loc Mean               0.13353\n",
      "evaluation/env_infos/end_effector_loc Std                0.353199\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.164654\n",
      "time/evaluation sampling (s)                             0.799456\n",
      "time/exploration sampling (s)                            0.10723\n",
      "time/logging (s)                                         0.0191604\n",
      "time/saving (s)                                          0.0281289\n",
      "time/training (s)                                       47.4826\n",
      "time/epoch (s)                                          48.6012\n",
      "time/total (s)                                        4481.1\n",
      "Epoch                                                   93\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:28:04.315426 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 94 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00137394\n",
      "trainer/QF2 Loss                                         0.00146785\n",
      "trainer/Policy Loss                                      2.72376\n",
      "trainer/Q1 Predictions Mean                             -0.884395\n",
      "trainer/Q1 Predictions Std                               0.761799\n",
      "trainer/Q1 Predictions Max                               0.457388\n",
      "trainer/Q1 Predictions Min                              -3.17995\n",
      "trainer/Q2 Predictions Mean                             -0.89461\n",
      "trainer/Q2 Predictions Std                               0.759378\n",
      "trainer/Q2 Predictions Max                               0.459428\n",
      "trainer/Q2 Predictions Min                              -3.21291\n",
      "trainer/Q Targets Mean                                  -0.893207\n",
      "trainer/Q Targets Std                                    0.752449\n",
      "trainer/Q Targets Max                                    0.478338\n",
      "trainer/Q Targets Min                                   -3.27688\n",
      "trainer/Log Pis Mean                                     1.84303\n",
      "trainer/Log Pis Std                                      1.40584\n",
      "trainer/Log Pis Max                                      4.21566\n",
      "trainer/Log Pis Min                                     -3.738\n",
      "trainer/Policy mu Mean                                  -0.0253332\n",
      "trainer/Policy mu Std                                    0.264951\n",
      "trainer/Policy mu Max                                    1.51103\n",
      "trainer/Policy mu Min                                   -1.9673\n",
      "trainer/Policy log std Mean                             -2.28724\n",
      "trainer/Policy log std Std                               0.585274\n",
      "trainer/Policy log std Max                              -0.628159\n",
      "trainer/Policy log std Min                              -3.20692\n",
      "trainer/Alpha                                            0.0238466\n",
      "trainer/Alpha Loss                                      -0.586429\n",
      "exploration/num steps total                          10500\n",
      "exploration/num paths total                            525\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.153503\n",
      "exploration/Rewards Std                                  0.132331\n",
      "exploration/Rewards Max                                  0.0693541\n",
      "exploration/Rewards Min                                 -0.532834\n",
      "exploration/Returns Mean                                -3.07006\n",
      "exploration/Returns Std                                  2.19759\n",
      "exploration/Returns Max                                 -0.55696\n",
      "exploration/Returns Min                                 -7.0501\n",
      "exploration/Actions Mean                                 0.0040852\n",
      "exploration/Actions Std                                  0.270729\n",
      "exploration/Actions Max                                  0.852178\n",
      "exploration/Actions Min                                 -0.737585\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.07006\n",
      "exploration/env_infos/final/reward_dist Mean             0.0407403\n",
      "exploration/env_infos/final/reward_dist Std              0.0814733\n",
      "exploration/env_infos/final/reward_dist Max              0.203687\n",
      "exploration/env_infos/final/reward_dist Min              1.70728e-102\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00079953\n",
      "exploration/env_infos/initial/reward_dist Std            0.00148007\n",
      "exploration/env_infos/initial/reward_dist Max            0.00375622\n",
      "exploration/env_infos/initial/reward_dist Min            1.93269e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0757797\n",
      "exploration/env_infos/reward_dist Std                    0.203904\n",
      "exploration/env_infos/reward_dist Max                    0.824823\n",
      "exploration/env_infos/reward_dist Min                    1.70728e-102\n",
      "exploration/env_infos/final/reward_energy Mean          -0.176807\n",
      "exploration/env_infos/final/reward_energy Std            0.160702\n",
      "exploration/env_infos/final/reward_energy Max           -0.0709055\n",
      "exploration/env_infos/final/reward_energy Min           -0.496308\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.448066\n",
      "exploration/env_infos/initial/reward_energy Std          0.357896\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0861836\n",
      "exploration/env_infos/initial/reward_energy Min         -1.12705\n",
      "exploration/env_infos/reward_energy Mean                -0.308519\n",
      "exploration/env_infos/reward_energy Std                  0.226798\n",
      "exploration/env_infos/reward_energy Max                 -0.0208634\n",
      "exploration/env_infos/reward_energy Min                 -1.12705\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0655098\n",
      "exploration/env_infos/final/end_effector_loc Std         0.57411\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00378475\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0199184\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0426089\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0368793\n",
      "exploration/env_infos/end_effector_loc Mean              0.0124565\n",
      "exploration/env_infos/end_effector_loc Std               0.394701\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           95000\n",
      "evaluation/num paths total                            4750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0834907\n",
      "evaluation/Rewards Std                                   0.113796\n",
      "evaluation/Rewards Max                                   0.136977\n",
      "evaluation/Rewards Min                                  -0.522586\n",
      "evaluation/Returns Mean                                 -1.66981\n",
      "evaluation/Returns Std                                   1.98191\n",
      "evaluation/Returns Max                                   1.51975\n",
      "evaluation/Returns Min                                  -7.32656\n",
      "evaluation/Actions Mean                                 -0.0086389\n",
      "evaluation/Actions Std                                   0.129557\n",
      "evaluation/Actions Max                                   0.950012\n",
      "evaluation/Actions Min                                  -0.809477\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.66981\n",
      "evaluation/env_infos/final/reward_dist Mean              0.108922\n",
      "evaluation/env_infos/final/reward_dist Std               0.199785\n",
      "evaluation/env_infos/final/reward_dist Max               0.7814\n",
      "evaluation/env_infos/final/reward_dist Min               9.8265e-149\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00773283\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111658\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0437789\n",
      "evaluation/env_infos/initial/reward_dist Min             9.16724e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.11506\n",
      "evaluation/env_infos/reward_dist Std                     0.204772\n",
      "evaluation/env_infos/reward_dist Max                     0.98845\n",
      "evaluation/env_infos/reward_dist Min                     9.8265e-149\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0662707\n",
      "evaluation/env_infos/final/reward_energy Std             0.085154\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00810135\n",
      "evaluation/env_infos/final/reward_energy Min            -0.479413\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.282275\n",
      "evaluation/env_infos/initial/reward_energy Std           0.283108\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0130157\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.2297\n",
      "evaluation/env_infos/reward_energy Mean                 -0.108322\n",
      "evaluation/env_infos/reward_energy Std                   0.148275\n",
      "evaluation/env_infos/reward_energy Max                  -0.00568139\n",
      "evaluation/env_infos/reward_energy Min                  -1.2297\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0390126\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.465292\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00343393\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0137111\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0475006\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0354939\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00111212\n",
      "evaluation/env_infos/end_effector_loc Std                0.293634\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.161739\n",
      "time/evaluation sampling (s)                             0.794548\n",
      "time/exploration sampling (s)                            0.109213\n",
      "time/logging (s)                                         0.0219241\n",
      "time/saving (s)                                          0.0317928\n",
      "time/training (s)                                       51.1925\n",
      "time/epoch (s)                                          52.3117\n",
      "time/total (s)                                        4534.28\n",
      "Epoch                                                   94\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:28:56.732586 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 95 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00231526\n",
      "trainer/QF2 Loss                                         0.00790368\n",
      "trainer/Policy Loss                                      2.7964\n",
      "trainer/Q1 Predictions Mean                             -0.85589\n",
      "trainer/Q1 Predictions Std                               0.726283\n",
      "trainer/Q1 Predictions Max                               0.52332\n",
      "trainer/Q1 Predictions Min                              -2.76352\n",
      "trainer/Q2 Predictions Mean                             -0.848067\n",
      "trainer/Q2 Predictions Std                               0.733897\n",
      "trainer/Q2 Predictions Max                               0.535434\n",
      "trainer/Q2 Predictions Min                              -2.80457\n",
      "trainer/Q Targets Mean                                  -0.863016\n",
      "trainer/Q Targets Std                                    0.733453\n",
      "trainer/Q Targets Max                                    0.532478\n",
      "trainer/Q Targets Min                                   -2.74763\n",
      "trainer/Log Pis Mean                                     1.95271\n",
      "trainer/Log Pis Std                                      1.41415\n",
      "trainer/Log Pis Max                                      4.54762\n",
      "trainer/Log Pis Min                                     -5.18705\n",
      "trainer/Policy mu Mean                                  -0.0103381\n",
      "trainer/Policy mu Std                                    0.295583\n",
      "trainer/Policy mu Max                                    2.10061\n",
      "trainer/Policy mu Min                                   -1.57966\n",
      "trainer/Policy log std Mean                             -2.30224\n",
      "trainer/Policy log std Std                               0.590643\n",
      "trainer/Policy log std Max                              -0.315042\n",
      "trainer/Policy log std Min                              -3.26006\n",
      "trainer/Alpha                                            0.0205272\n",
      "trainer/Alpha Loss                                      -0.183748\n",
      "exploration/num steps total                          10600\n",
      "exploration/num paths total                            530\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.147817\n",
      "exploration/Rewards Std                                  0.142495\n",
      "exploration/Rewards Max                                  0.000754307\n",
      "exploration/Rewards Min                                 -0.598485\n",
      "exploration/Returns Mean                                -2.95634\n",
      "exploration/Returns Std                                  2.21917\n",
      "exploration/Returns Max                                 -1.34887\n",
      "exploration/Returns Min                                 -7.05916\n",
      "exploration/Actions Mean                                 0.015694\n",
      "exploration/Actions Std                                  0.162217\n",
      "exploration/Actions Max                                  0.606836\n",
      "exploration/Actions Min                                 -0.557419\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.95634\n",
      "exploration/env_infos/final/reward_dist Mean             0.0976462\n",
      "exploration/env_infos/final/reward_dist Std              0.195292\n",
      "exploration/env_infos/final/reward_dist Max              0.488231\n",
      "exploration/env_infos/final/reward_dist Min              7.08347e-66\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00509397\n",
      "exploration/env_infos/initial/reward_dist Std            0.00771626\n",
      "exploration/env_infos/initial/reward_dist Max            0.0201317\n",
      "exploration/env_infos/initial/reward_dist Min            0.000150055\n",
      "exploration/env_infos/reward_dist Mean                   0.0749105\n",
      "exploration/env_infos/reward_dist Std                    0.206961\n",
      "exploration/env_infos/reward_dist Max                    0.916951\n",
      "exploration/env_infos/reward_dist Min                    7.08347e-66\n",
      "exploration/env_infos/final/reward_energy Mean          -0.196639\n",
      "exploration/env_infos/final/reward_energy Std            0.164521\n",
      "exploration/env_infos/final/reward_energy Max           -0.0502748\n",
      "exploration/env_infos/final/reward_energy Min           -0.486905\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.176096\n",
      "exploration/env_infos/initial/reward_energy Std          0.127792\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0227829\n",
      "exploration/env_infos/initial/reward_energy Min         -0.390915\n",
      "exploration/env_infos/reward_energy Mean                -0.170608\n",
      "exploration/env_infos/reward_energy Std                  0.154965\n",
      "exploration/env_infos/reward_energy Max                 -0.0112669\n",
      "exploration/env_infos/reward_energy Min                 -0.641359\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.134351\n",
      "exploration/env_infos/final/end_effector_loc Std         0.529344\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.940925\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00288518\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00713102\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00626462\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0185146\n",
      "exploration/env_infos/end_effector_loc Mean              0.0723707\n",
      "exploration/env_infos/end_effector_loc Std               0.302962\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -0.940925\n",
      "evaluation/num steps total                           96000\n",
      "evaluation/num paths total                            4800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0715678\n",
      "evaluation/Rewards Std                                   0.139915\n",
      "evaluation/Rewards Max                                   0.117372\n",
      "evaluation/Rewards Min                                  -1.01717\n",
      "evaluation/Returns Mean                                 -1.43136\n",
      "evaluation/Returns Std                                   2.31855\n",
      "evaluation/Returns Max                                   1.29625\n",
      "evaluation/Returns Min                                  -8.05166\n",
      "evaluation/Actions Mean                                  0.0085261\n",
      "evaluation/Actions Std                                   0.102722\n",
      "evaluation/Actions Max                                   0.894474\n",
      "evaluation/Actions Min                                  -0.727094\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.43136\n",
      "evaluation/env_infos/final/reward_dist Mean              0.168554\n",
      "evaluation/env_infos/final/reward_dist Std               0.278492\n",
      "evaluation/env_infos/final/reward_dist Max               0.919047\n",
      "evaluation/env_infos/final/reward_dist Min               7.76765e-132\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00559147\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00976281\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0419018\n",
      "evaluation/env_infos/initial/reward_dist Min             1.19132e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.131671\n",
      "evaluation/env_infos/reward_dist Std                     0.242071\n",
      "evaluation/env_infos/reward_dist Max                     0.995397\n",
      "evaluation/env_infos/reward_dist Min                     7.76765e-132\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0496041\n",
      "evaluation/env_infos/final/reward_energy Std             0.0463362\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00443008\n",
      "evaluation/env_infos/final/reward_energy Min            -0.218229\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.193633\n",
      "evaluation/env_infos/initial/reward_energy Std           0.213729\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00366374\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.922362\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0752589\n",
      "evaluation/env_infos/reward_energy Std                   0.12484\n",
      "evaluation/env_infos/reward_energy Max                  -0.000802218\n",
      "evaluation/env_infos/reward_energy Min                  -1.02317\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.102162\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.406075\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00296852\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00975474\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0412231\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0363547\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0634895\n",
      "evaluation/env_infos/end_effector_loc Std                0.270142\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.177933\n",
      "time/evaluation sampling (s)                             1.10766\n",
      "time/exploration sampling (s)                            0.122548\n",
      "time/logging (s)                                         0.0202963\n",
      "time/saving (s)                                          0.0298196\n",
      "time/training (s)                                       49.6589\n",
      "time/epoch (s)                                          51.1171\n",
      "time/total (s)                                        4586.69\n",
      "Epoch                                                   95\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:29:46.988558 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 96 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00131578\n",
      "trainer/QF2 Loss                                         0.00132818\n",
      "trainer/Policy Loss                                      2.93933\n",
      "trainer/Q1 Predictions Mean                             -0.922044\n",
      "trainer/Q1 Predictions Std                               0.776813\n",
      "trainer/Q1 Predictions Max                               0.641354\n",
      "trainer/Q1 Predictions Min                              -2.91383\n",
      "trainer/Q2 Predictions Mean                             -0.923631\n",
      "trainer/Q2 Predictions Std                               0.779019\n",
      "trainer/Q2 Predictions Max                               0.651148\n",
      "trainer/Q2 Predictions Min                              -2.90705\n",
      "trainer/Q Targets Mean                                  -0.917187\n",
      "trainer/Q Targets Std                                    0.770928\n",
      "trainer/Q Targets Max                                    0.621931\n",
      "trainer/Q Targets Min                                   -2.90219\n",
      "trainer/Log Pis Mean                                     2.0304\n",
      "trainer/Log Pis Std                                      1.32935\n",
      "trainer/Log Pis Max                                      4.52002\n",
      "trainer/Log Pis Min                                     -3.34216\n",
      "trainer/Policy mu Mean                                   0.0180264\n",
      "trainer/Policy mu Std                                    0.291882\n",
      "trainer/Policy mu Max                                    1.80716\n",
      "trainer/Policy mu Min                                   -1.56982\n",
      "trainer/Policy log std Mean                             -2.323\n",
      "trainer/Policy log std Std                               0.600016\n",
      "trainer/Policy log std Max                              -0.401626\n",
      "trainer/Policy log std Min                              -3.23578\n",
      "trainer/Alpha                                            0.0212752\n",
      "trainer/Alpha Loss                                       0.116959\n",
      "exploration/num steps total                          10700\n",
      "exploration/num paths total                            535\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.129922\n",
      "exploration/Rewards Std                                  0.225307\n",
      "exploration/Rewards Max                                  0.113854\n",
      "exploration/Rewards Min                                 -0.882341\n",
      "exploration/Returns Mean                                -2.59844\n",
      "exploration/Returns Std                                  3.18992\n",
      "exploration/Returns Max                                  1.18206\n",
      "exploration/Returns Min                                 -6.89648\n",
      "exploration/Actions Mean                                 0.0414542\n",
      "exploration/Actions Std                                  0.210721\n",
      "exploration/Actions Max                                  0.868306\n",
      "exploration/Actions Min                                 -0.839029\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.59844\n",
      "exploration/env_infos/final/reward_dist Mean             0.16373\n",
      "exploration/env_infos/final/reward_dist Std              0.207102\n",
      "exploration/env_infos/final/reward_dist Max              0.491178\n",
      "exploration/env_infos/final/reward_dist Min              1.98233e-102\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00597222\n",
      "exploration/env_infos/initial/reward_dist Std            0.0105579\n",
      "exploration/env_infos/initial/reward_dist Max            0.0270747\n",
      "exploration/env_infos/initial/reward_dist Min            3.92565e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.20259\n",
      "exploration/env_infos/reward_dist Std                    0.262011\n",
      "exploration/env_infos/reward_dist Max                    0.987263\n",
      "exploration/env_infos/reward_dist Min                    1.98233e-102\n",
      "exploration/env_infos/final/reward_energy Mean          -0.257202\n",
      "exploration/env_infos/final/reward_energy Std            0.179373\n",
      "exploration/env_infos/final/reward_energy Max           -0.0434279\n",
      "exploration/env_infos/final/reward_energy Min           -0.481982\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.333807\n",
      "exploration/env_infos/initial/reward_energy Std          0.264396\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0574228\n",
      "exploration/env_infos/initial/reward_energy Min         -0.740221\n",
      "exploration/env_infos/reward_energy Mean                -0.238015\n",
      "exploration/env_infos/reward_energy Std                  0.18866\n",
      "exploration/env_infos/reward_energy Max                 -0.0154203\n",
      "exploration/env_infos/reward_energy Min                 -0.902439\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.289371\n",
      "exploration/env_infos/final/end_effector_loc Std         0.591762\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00817039\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0126456\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0366833\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00491468\n",
      "exploration/env_infos/end_effector_loc Mean              0.125867\n",
      "exploration/env_infos/end_effector_loc Std               0.367848\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           97000\n",
      "evaluation/num paths total                            4850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0972119\n",
      "evaluation/Rewards Std                                   0.155679\n",
      "evaluation/Rewards Max                                   0.163095\n",
      "evaluation/Rewards Min                                  -0.856138\n",
      "evaluation/Returns Mean                                 -1.94424\n",
      "evaluation/Returns Std                                   2.63089\n",
      "evaluation/Returns Max                                   1.86901\n",
      "evaluation/Returns Min                                 -12.2661\n",
      "evaluation/Actions Mean                                 -0.00382059\n",
      "evaluation/Actions Std                                   0.12051\n",
      "evaluation/Actions Max                                   0.767667\n",
      "evaluation/Actions Min                                  -0.910412\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.94424\n",
      "evaluation/env_infos/final/reward_dist Mean              0.12571\n",
      "evaluation/env_infos/final/reward_dist Std               0.230966\n",
      "evaluation/env_infos/final/reward_dist Max               0.964303\n",
      "evaluation/env_infos/final/reward_dist Min               7.78749e-196\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0066442\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00977109\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0342505\n",
      "evaluation/env_infos/initial/reward_dist Min             2.43467e-08\n",
      "evaluation/env_infos/reward_dist Mean                    0.135512\n",
      "evaluation/env_infos/reward_dist Std                     0.241338\n",
      "evaluation/env_infos/reward_dist Max                     0.991999\n",
      "evaluation/env_infos/reward_dist Min                     7.78749e-196\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.06622\n",
      "evaluation/env_infos/final/reward_energy Std             0.0528545\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00973352\n",
      "evaluation/env_infos/final/reward_energy Min            -0.267017\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.256781\n",
      "evaluation/env_infos/initial/reward_energy Std           0.25809\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0187785\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.855477\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0947033\n",
      "evaluation/env_infos/reward_energy Std                   0.141795\n",
      "evaluation/env_infos/reward_energy Max                  -0.00503899\n",
      "evaluation/env_infos/reward_energy Min                  -0.983874\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0366219\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.478639\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00177486\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127489\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0383834\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0402791\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0258698\n",
      "evaluation/env_infos/end_effector_loc Std                0.324326\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.170541\n",
      "time/evaluation sampling (s)                             0.803708\n",
      "time/exploration sampling (s)                            0.114141\n",
      "time/logging (s)                                         0.0289963\n",
      "time/saving (s)                                          0.0320992\n",
      "time/training (s)                                       48.1876\n",
      "time/epoch (s)                                          49.3371\n",
      "time/total (s)                                        4636.95\n",
      "Epoch                                                   96\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:30:38.320914 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 97 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00783171\r\n",
      "trainer/QF2 Loss                                         0.00423103\r\n",
      "trainer/Policy Loss                                      3.15305\r\n",
      "trainer/Q1 Predictions Mean                             -0.980192\r\n",
      "trainer/Q1 Predictions Std                               0.738425\r\n",
      "trainer/Q1 Predictions Max                               0.699493\r\n",
      "trainer/Q1 Predictions Min                              -2.71464\r\n",
      "trainer/Q2 Predictions Mean                             -0.984018\r\n",
      "trainer/Q2 Predictions Std                               0.738662\r\n",
      "trainer/Q2 Predictions Max                               0.682448\r\n",
      "trainer/Q2 Predictions Min                              -2.7029\r\n",
      "trainer/Q Targets Mean                                  -0.980287\r\n",
      "trainer/Q Targets Std                                    0.749652\r\n",
      "trainer/Q Targets Max                                    0.68113\r\n",
      "trainer/Q Targets Min                                   -2.75165\r\n",
      "trainer/Log Pis Mean                                     2.18393\r\n",
      "trainer/Log Pis Std                                      1.35643\r\n",
      "trainer/Log Pis Max                                      4.80683\r\n",
      "trainer/Log Pis Min                                     -4.01102\r\n",
      "trainer/Policy mu Mean                                  -0.0232448\r\n",
      "trainer/Policy mu Std                                    0.339304\r\n",
      "trainer/Policy mu Max                                    1.86857\r\n",
      "trainer/Policy mu Min                                   -2.26521\r\n",
      "trainer/Policy log std Mean                             -2.40262\r\n",
      "trainer/Policy log std Std                               0.632892\r\n",
      "trainer/Policy log std Max                              -0.12686\r\n",
      "trainer/Policy log std Min                              -3.52829\r\n",
      "trainer/Alpha                                            0.019126\r\n",
      "trainer/Alpha Loss                                       0.72774\r\n",
      "exploration/num steps total                          10800\r\n",
      "exploration/num paths total                            540\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.182083\r\n",
      "exploration/Rewards Std                                  0.160501\r\n",
      "exploration/Rewards Max                                  0.0708443\r\n",
      "exploration/Rewards Min                                 -0.561134\r\n",
      "exploration/Returns Mean                                -3.64166\r\n",
      "exploration/Returns Std                                  2.94169\r\n",
      "exploration/Returns Max                                 -0.47552\r\n",
      "exploration/Returns Min                                 -8.93036\r\n",
      "exploration/Actions Mean                                -0.00714552\r\n",
      "exploration/Actions Std                                  0.139167\r\n",
      "exploration/Actions Max                                  0.345612\r\n",
      "exploration/Actions Min                                 -0.862035\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -3.64166\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0303075\r\n",
      "exploration/env_infos/final/reward_dist Std              0.060615\r\n",
      "exploration/env_infos/final/reward_dist Max              0.151537\r\n",
      "exploration/env_infos/final/reward_dist Min              1.5332e-79\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00678001\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0090961\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.024542\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.19714e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.114178\r\n",
      "exploration/env_infos/reward_dist Std                    0.238414\r\n",
      "exploration/env_infos/reward_dist Max                    0.952309\r\n",
      "exploration/env_infos/reward_dist Min                    1.5332e-79\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.150709\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0561851\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0996373\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.256435\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.3544\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.273606\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0565226\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.842916\r\n",
      "exploration/env_infos/reward_energy Mean                -0.132542\r\n",
      "exploration/env_infos/reward_energy Std                  0.145842\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00688678\r\n",
      "exploration/env_infos/reward_energy Min                 -0.87187\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.026158\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.409595\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.604009\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000157131\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0158288\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0172806\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0367919\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0177991\r\n",
      "exploration/env_infos/end_effector_loc Std               0.307397\r\n",
      "exploration/env_infos/end_effector_loc Max               0.604009\r\n",
      "exploration/env_infos/end_effector_loc Min              -1\r\n",
      "evaluation/num steps total                           98000\r\n",
      "evaluation/num paths total                            4900\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.100997\r\n",
      "evaluation/Rewards Std                                   0.142497\r\n",
      "evaluation/Rewards Max                                   0.150915\r\n",
      "evaluation/Rewards Min                                  -0.856248\r\n",
      "evaluation/Returns Mean                                 -2.01995\r\n",
      "evaluation/Returns Std                                   2.44451\r\n",
      "evaluation/Returns Max                                   1.37234\r\n",
      "evaluation/Returns Min                                 -11.7817\r\n",
      "evaluation/Actions Mean                                 -0.00815475\r\n",
      "evaluation/Actions Std                                   0.108736\r\n",
      "evaluation/Actions Max                                   0.747571\r\n",
      "evaluation/Actions Min                                  -0.966193\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.01995\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.142813\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.267155\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.907751\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.4134e-188\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00470645\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00785526\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0356963\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.14638e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.106217\r\n",
      "evaluation/env_infos/reward_dist Std                     0.213115\r\n",
      "evaluation/env_infos/reward_dist Max                     0.994828\r\n",
      "evaluation/env_infos/reward_dist Min                     6.4134e-188\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.057602\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0429524\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0180413\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.191039\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.220845\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.216875\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0279236\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.16404\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0878557\r\n",
      "evaluation/env_infos/reward_energy Std                   0.126733\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00395181\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.16404\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0620434\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.453693\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00120044\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0108774\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.03246\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0483096\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0318466\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.301499\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.17812\r\n",
      "time/evaluation sampling (s)                             1.05459\r\n",
      "time/exploration sampling (s)                            0.116467\r\n",
      "time/logging (s)                                         0.0186902\r\n",
      "time/saving (s)                                          0.0262009\r\n",
      "time/training (s)                                       48.6997\r\n",
      "time/epoch (s)                                          50.0938\r\n",
      "time/total (s)                                        4688.27\r\n",
      "Epoch                                                   97\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:31:28.432383 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 98 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00268127\n",
      "trainer/QF2 Loss                                         0.00151219\n",
      "trainer/Policy Loss                                      2.95474\n",
      "trainer/Q1 Predictions Mean                             -0.994282\n",
      "trainer/Q1 Predictions Std                               0.82879\n",
      "trainer/Q1 Predictions Max                               0.581696\n",
      "trainer/Q1 Predictions Min                              -2.7871\n",
      "trainer/Q2 Predictions Mean                             -0.99788\n",
      "trainer/Q2 Predictions Std                               0.833241\n",
      "trainer/Q2 Predictions Max                               0.569908\n",
      "trainer/Q2 Predictions Min                              -2.76343\n",
      "trainer/Q Targets Mean                                  -0.999654\n",
      "trainer/Q Targets Std                                    0.826999\n",
      "trainer/Q Targets Max                                    0.577633\n",
      "trainer/Q Targets Min                                   -2.73351\n",
      "trainer/Log Pis Mean                                     1.96384\n",
      "trainer/Log Pis Std                                      1.40254\n",
      "trainer/Log Pis Max                                      4.75492\n",
      "trainer/Log Pis Min                                     -3.53964\n",
      "trainer/Policy mu Mean                                   0.0331376\n",
      "trainer/Policy mu Std                                    0.355688\n",
      "trainer/Policy mu Max                                    1.72737\n",
      "trainer/Policy mu Min                                   -1.68515\n",
      "trainer/Policy log std Mean                             -2.3353\n",
      "trainer/Policy log std Std                               0.610447\n",
      "trainer/Policy log std Max                              -0.457112\n",
      "trainer/Policy log std Min                              -3.46541\n",
      "trainer/Alpha                                            0.0197113\n",
      "trainer/Alpha Loss                                      -0.142016\n",
      "exploration/num steps total                          10900\n",
      "exploration/num paths total                            545\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.05752\n",
      "exploration/Rewards Std                                  0.0666755\n",
      "exploration/Rewards Max                                  0.0811329\n",
      "exploration/Rewards Min                                 -0.273549\n",
      "exploration/Returns Mean                                -1.1504\n",
      "exploration/Returns Std                                  0.600603\n",
      "exploration/Returns Max                                 -0.544001\n",
      "exploration/Returns Min                                 -1.88502\n",
      "exploration/Actions Mean                                -0.0100711\n",
      "exploration/Actions Std                                  0.151578\n",
      "exploration/Actions Max                                  0.784074\n",
      "exploration/Actions Min                                 -0.47495\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.1504\n",
      "exploration/env_infos/final/reward_dist Mean             0.263527\n",
      "exploration/env_infos/final/reward_dist Std              0.363426\n",
      "exploration/env_infos/final/reward_dist Max              0.927065\n",
      "exploration/env_infos/final/reward_dist Min              7.38914e-91\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000356342\n",
      "exploration/env_infos/initial/reward_dist Std            0.000374757\n",
      "exploration/env_infos/initial/reward_dist Max            0.00101285\n",
      "exploration/env_infos/initial/reward_dist Min            1.34154e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.17383\n",
      "exploration/env_infos/reward_dist Std                    0.260839\n",
      "exploration/env_infos/reward_dist Max                    0.966665\n",
      "exploration/env_infos/reward_dist Min                    7.38914e-91\n",
      "exploration/env_infos/final/reward_energy Mean          -0.109036\n",
      "exploration/env_infos/final/reward_energy Std            0.0344661\n",
      "exploration/env_infos/final/reward_energy Max           -0.0682663\n",
      "exploration/env_infos/final/reward_energy Min           -0.169501\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.263992\n",
      "exploration/env_infos/initial/reward_energy Std          0.167406\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0656605\n",
      "exploration/env_infos/initial/reward_energy Min         -0.539828\n",
      "exploration/env_infos/reward_energy Mean                -0.165324\n",
      "exploration/env_infos/reward_energy Std                  0.137196\n",
      "exploration/env_infos/reward_energy Max                 -0.0139697\n",
      "exploration/env_infos/reward_energy Min                 -0.78409\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00344937\n",
      "exploration/env_infos/final/end_effector_loc Std         0.354874\n",
      "exploration/env_infos/final/end_effector_loc Max         0.663876\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.627335\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00488461\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00991394\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0266588\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00897899\n",
      "exploration/env_infos/end_effector_loc Mean              0.0346945\n",
      "exploration/env_infos/end_effector_loc Std               0.244332\n",
      "exploration/env_infos/end_effector_loc Max               0.684725\n",
      "exploration/env_infos/end_effector_loc Min              -0.627335\n",
      "evaluation/num steps total                           99000\n",
      "evaluation/num paths total                            4950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0718889\n",
      "evaluation/Rewards Std                                   0.120173\n",
      "evaluation/Rewards Max                                   0.146834\n",
      "evaluation/Rewards Min                                  -0.660084\n",
      "evaluation/Returns Mean                                 -1.43778\n",
      "evaluation/Returns Std                                   2.09551\n",
      "evaluation/Returns Max                                   1.34097\n",
      "evaluation/Returns Min                                  -9.5749\n",
      "evaluation/Actions Mean                                  0.000575788\n",
      "evaluation/Actions Std                                   0.0896741\n",
      "evaluation/Actions Max                                   0.541784\n",
      "evaluation/Actions Min                                  -0.830116\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.43778\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0771326\n",
      "evaluation/env_infos/final/reward_dist Std               0.160585\n",
      "evaluation/env_infos/final/reward_dist Max               0.763185\n",
      "evaluation/env_infos/final/reward_dist Min               1.23355e-184\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00372513\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00758229\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0365293\n",
      "evaluation/env_infos/initial/reward_dist Min             8.3023e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.133947\n",
      "evaluation/env_infos/reward_dist Std                     0.235754\n",
      "evaluation/env_infos/reward_dist Max                     0.989706\n",
      "evaluation/env_infos/reward_dist Min                     1.23355e-184\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0559789\n",
      "evaluation/env_infos/final/reward_energy Std             0.0604744\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00250543\n",
      "evaluation/env_infos/final/reward_energy Min            -0.362685\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.20515\n",
      "evaluation/env_infos/initial/reward_energy Std           0.172136\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0149578\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.918668\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0821636\n",
      "evaluation/env_infos/reward_energy Std                   0.0966058\n",
      "evaluation/env_infos/reward_energy Max                  -0.000275975\n",
      "evaluation/env_infos/reward_energy Min                  -0.918668\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0701308\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.367352\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00104135\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00941075\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.024622\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0415058\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0412694\n",
      "evaluation/env_infos/end_effector_loc Std                0.24926\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.163019\n",
      "time/evaluation sampling (s)                             0.806203\n",
      "time/exploration sampling (s)                            0.107392\n",
      "time/logging (s)                                         0.0190888\n",
      "time/saving (s)                                          0.0275867\n",
      "time/training (s)                                       48.1107\n",
      "time/epoch (s)                                          49.234\n",
      "time/total (s)                                        4738.38\n",
      "Epoch                                                   98\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:32:20.103413 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10] Epoch 99 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.00268416\n",
      "trainer/QF2 Loss                                          0.00253302\n",
      "trainer/Policy Loss                                       2.7318\n",
      "trainer/Q1 Predictions Mean                              -0.820049\n",
      "trainer/Q1 Predictions Std                                0.803267\n",
      "trainer/Q1 Predictions Max                                0.698312\n",
      "trainer/Q1 Predictions Min                               -3.26602\n",
      "trainer/Q2 Predictions Mean                              -0.800996\n",
      "trainer/Q2 Predictions Std                                0.803651\n",
      "trainer/Q2 Predictions Max                                0.718287\n",
      "trainer/Q2 Predictions Min                               -3.22357\n",
      "trainer/Q Targets Mean                                   -0.816935\n",
      "trainer/Q Targets Std                                     0.801399\n",
      "trainer/Q Targets Max                                     0.710483\n",
      "trainer/Q Targets Min                                    -3.14408\n",
      "trainer/Log Pis Mean                                      1.93874\n",
      "trainer/Log Pis Std                                       1.23729\n",
      "trainer/Log Pis Max                                       4.39753\n",
      "trainer/Log Pis Min                                      -2.85807\n",
      "trainer/Policy mu Mean                                    0.0292972\n",
      "trainer/Policy mu Std                                     0.307466\n",
      "trainer/Policy mu Max                                     1.78451\n",
      "trainer/Policy mu Min                                    -1.96423\n",
      "trainer/Policy log std Mean                              -2.27489\n",
      "trainer/Policy log std Std                                0.544085\n",
      "trainer/Policy log std Max                               -0.385812\n",
      "trainer/Policy log std Min                               -3.2221\n",
      "trainer/Alpha                                             0.0203271\n",
      "trainer/Alpha Loss                                       -0.238707\n",
      "exploration/num steps total                           11000\n",
      "exploration/num paths total                             550\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0774127\n",
      "exploration/Rewards Std                                   0.0627395\n",
      "exploration/Rewards Max                                   0.0682729\n",
      "exploration/Rewards Min                                  -0.274201\n",
      "exploration/Returns Mean                                 -1.54825\n",
      "exploration/Returns Std                                   0.830987\n",
      "exploration/Returns Max                                  -0.0680042\n",
      "exploration/Returns Min                                  -2.4026\n",
      "exploration/Actions Mean                                 -0.00308488\n",
      "exploration/Actions Std                                   0.167443\n",
      "exploration/Actions Max                                   0.506663\n",
      "exploration/Actions Min                                  -0.710245\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.54825\n",
      "exploration/env_infos/final/reward_dist Mean              0.0113315\n",
      "exploration/env_infos/final/reward_dist Std               0.00919469\n",
      "exploration/env_infos/final/reward_dist Max               0.0211395\n",
      "exploration/env_infos/final/reward_dist Min               0.000135572\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00300653\n",
      "exploration/env_infos/initial/reward_dist Std             0.00364358\n",
      "exploration/env_infos/initial/reward_dist Max             0.00905904\n",
      "exploration/env_infos/initial/reward_dist Min             4.75212e-06\n",
      "exploration/env_infos/reward_dist Mean                    0.072311\n",
      "exploration/env_infos/reward_dist Std                     0.144048\n",
      "exploration/env_infos/reward_dist Max                     0.651046\n",
      "exploration/env_infos/reward_dist Min                     5.12634e-07\n",
      "exploration/env_infos/final/reward_energy Mean           -0.100702\n",
      "exploration/env_infos/final/reward_energy Std             0.0241137\n",
      "exploration/env_infos/final/reward_energy Max            -0.074628\n",
      "exploration/env_infos/final/reward_energy Min            -0.131592\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.217343\n",
      "exploration/env_infos/initial/reward_energy Std           0.22485\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0272663\n",
      "exploration/env_infos/initial/reward_energy Min          -0.575322\n",
      "exploration/env_infos/reward_energy Mean                 -0.171939\n",
      "exploration/env_infos/reward_energy Std                   0.162882\n",
      "exploration/env_infos/reward_energy Max                  -0.00651771\n",
      "exploration/env_infos/reward_energy Min                  -0.872442\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.0869226\n",
      "exploration/env_infos/final/end_effector_loc Std          0.203478\n",
      "exploration/env_infos/final/end_effector_loc Max          0.330311\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.352772\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.000791372\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.0110281\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0228807\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0174344\n",
      "exploration/env_infos/end_effector_loc Mean               0.0709315\n",
      "exploration/env_infos/end_effector_loc Std                0.149791\n",
      "exploration/env_infos/end_effector_loc Max                0.391133\n",
      "exploration/env_infos/end_effector_loc Min               -0.352772\n",
      "evaluation/num steps total                           100000\n",
      "evaluation/num paths total                             5000\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0730096\n",
      "evaluation/Rewards Std                                    0.0855364\n",
      "evaluation/Rewards Max                                    0.126913\n",
      "evaluation/Rewards Min                                   -0.440863\n",
      "evaluation/Returns Mean                                  -1.46019\n",
      "evaluation/Returns Std                                    1.40061\n",
      "evaluation/Returns Max                                    1.54156\n",
      "evaluation/Returns Min                                   -4.27774\n",
      "evaluation/Actions Mean                                  -0.00521921\n",
      "evaluation/Actions Std                                    0.101147\n",
      "evaluation/Actions Max                                    0.55563\n",
      "evaluation/Actions Min                                   -0.622318\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -1.46019\n",
      "evaluation/env_infos/final/reward_dist Mean               0.0956878\n",
      "evaluation/env_infos/final/reward_dist Std                0.185441\n",
      "evaluation/env_infos/final/reward_dist Max                0.887308\n",
      "evaluation/env_infos/final/reward_dist Min                1.0842e-182\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00888606\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0162984\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0726787\n",
      "evaluation/env_infos/initial/reward_dist Min              3.89294e-07\n",
      "evaluation/env_infos/reward_dist Mean                     0.146276\n",
      "evaluation/env_infos/reward_dist Std                      0.242002\n",
      "evaluation/env_infos/reward_dist Max                      0.991663\n",
      "evaluation/env_infos/reward_dist Min                      1.0842e-182\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0558949\n",
      "evaluation/env_infos/final/reward_energy Std              0.0569803\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00469141\n",
      "evaluation/env_infos/final/reward_energy Min             -0.337237\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.236529\n",
      "evaluation/env_infos/initial/reward_energy Std            0.19341\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0146476\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.671439\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0906806\n",
      "evaluation/env_infos/reward_energy Std                    0.110874\n",
      "evaluation/env_infos/reward_energy Max                   -0.00102272\n",
      "evaluation/env_infos/reward_energy Min                   -0.671439\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0395392\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.419718\n",
      "evaluation/env_infos/final/end_effector_loc Max           1\n",
      "evaluation/env_infos/final/end_effector_loc Min          -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       -0.00173927\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0106615\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0202387\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0311159\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.0214229\n",
      "evaluation/env_infos/end_effector_loc Std                 0.270936\n",
      "evaluation/env_infos/end_effector_loc Max                 1\n",
      "evaluation/env_infos/end_effector_loc Min                -1\n",
      "time/data storing (s)                                     0.169477\n",
      "time/evaluation sampling (s)                              0.985601\n",
      "time/exploration sampling (s)                             0.110448\n",
      "time/logging (s)                                          0.0202526\n",
      "time/saving (s)                                           0.0303354\n",
      "time/training (s)                                        49.3966\n",
      "time/epoch (s)                                           50.7127\n",
      "time/total (s)                                         4790.05\n",
      "Epoch                                                    99\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[22229]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21454778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a214d5740). One of the two will be used. Which one is undefined.\n",
      "objc[22229]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21454700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a214d5768). One of the two will be used. Which one is undefined.\n",
      "objc[22229]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a214547a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a214d57b8). One of the two will be used. Which one is undefined.\n",
      "objc[22229]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21454818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a214d5830). One of the two will be used. Which one is undefined.\n",
      "using sparse reward if specified\n",
      "2021-05-25 14:32:26.419410 PDT | Variant:\n",
      "2021-05-25 14:32:26.419824 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"pointreacherobs\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 20,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 20,\n",
      "    \"num_train_loops_per_epoch\": 5\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.97,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 2000,\n",
      "    \"latent_dim\": 6,\n",
      "    \"approx_irl\": true,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": true,\n",
      "    \"n_sampled_latents\": 100,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": true,\n",
      "    \"sparse_reward\": null,\n",
      "    \"fixed_ratio\": null\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": true,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"horizon\": 20\n",
      "  }\n",
      "}\n",
      "pointreacher\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "2021-05-25 14:33:04.253775 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_14_32_26_0000--s-10] Epoch 0 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   2000\n",
      "trainer/QF1 Loss                                        0.859269\n",
      "trainer/QF2 Loss                                        0.857957\n",
      "trainer/Policy Loss                                    -1.37896\n",
      "trainer/Q1 Predictions Mean                            -0.00107763\n",
      "trainer/Q1 Predictions Std                              0.000652063\n",
      "trainer/Q1 Predictions Max                              0.000429391\n",
      "trainer/Q1 Predictions Min                             -0.00218645\n",
      "trainer/Q2 Predictions Mean                            -0.000210815\n",
      "trainer/Q2 Predictions Std                              0.000665921\n",
      "trainer/Q2 Predictions Max                              0.00126331\n",
      "trainer/Q2 Predictions Min                             -0.0016474\n",
      "trainer/Q Targets Mean                                  0.755954\n",
      "trainer/Q Targets Std                                   0.534951\n",
      "trainer/Q Targets Max                                   1.67429\n",
      "trainer/Q Targets Min                                  -1.28457\n",
      "trainer/Log Pis Mean                                   -1.38022\n",
      "trainer/Log Pis Std                                     0.289181\n",
      "trainer/Log Pis Max                                    -0.579982\n",
      "trainer/Log Pis Min                                    -2.60851\n",
      "trainer/Policy mu Mean                                  0.000976768\n",
      "trainer/Policy mu Std                                   0.000575161\n",
      "trainer/Policy mu Max                                   0.00214173\n",
      "trainer/Policy mu Min                                   8.66313e-05\n",
      "trainer/Policy log std Mean                             6.60632e-05\n",
      "trainer/Policy log std Std                              0.000661669\n",
      "trainer/Policy log std Max                              0.000923157\n",
      "trainer/Policy log std Min                             -0.00128984\n",
      "trainer/Alpha                                           0.997005\n",
      "trainer/Alpha Loss                                     -0\n",
      "exploration/num steps total                          1100\n",
      "exploration/num paths total                            55\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.904943\n",
      "exploration/Rewards Std                                 0.289573\n",
      "exploration/Rewards Max                                -0.277748\n",
      "exploration/Rewards Min                                -1.39334\n",
      "exploration/Returns Mean                              -18.0989\n",
      "exploration/Returns Std                                 1.64085\n",
      "exploration/Returns Max                               -15.205\n",
      "exploration/Returns Min                               -20.2709\n",
      "exploration/Actions Mean                                0.0676481\n",
      "exploration/Actions Std                                 0.594111\n",
      "exploration/Actions Max                                 0.981492\n",
      "exploration/Actions Min                                -0.954064\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -18.0989\n",
      "exploration/env_infos/final/reward_dist Mean            2.80154e-92\n",
      "exploration/env_infos/final/reward_dist Std             5.60308e-92\n",
      "exploration/env_infos/final/reward_dist Max             1.40077e-91\n",
      "exploration/env_infos/final/reward_dist Min             1.76833e-126\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000640217\n",
      "exploration/env_infos/initial/reward_dist Std           0.000767775\n",
      "exploration/env_infos/initial/reward_dist Max           0.00166986\n",
      "exploration/env_infos/initial/reward_dist Min           9.7246e-07\n",
      "exploration/env_infos/reward_dist Mean                  4.62869e-05\n",
      "exploration/env_infos/reward_dist Std                   0.000255889\n",
      "exploration/env_infos/reward_dist Max                   0.00166986\n",
      "exploration/env_infos/reward_dist Min                   1.76833e-126\n",
      "exploration/env_infos/final/reward_energy Mean         -0.806333\n",
      "exploration/env_infos/final/reward_energy Std           0.303188\n",
      "exploration/env_infos/final/reward_energy Max          -0.206839\n",
      "exploration/env_infos/final/reward_energy Min          -1.04414\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.905761\n",
      "exploration/env_infos/initial/reward_energy Std         0.130221\n",
      "exploration/env_infos/initial/reward_energy Max        -0.758943\n",
      "exploration/env_infos/initial/reward_energy Min        -1.11347\n",
      "exploration/env_infos/reward_energy Mean               -0.798439\n",
      "exploration/env_infos/reward_energy Std                 0.278539\n",
      "exploration/env_infos/reward_energy Max                -0.182801\n",
      "exploration/env_infos/reward_energy Min                -1.25718\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.441648\n",
      "exploration/env_infos/final/end_effector_loc Std        0.680331\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.0227556\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0229975\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0473177\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0354054\n",
      "exploration/env_infos/end_effector_loc Mean             0.289894\n",
      "exploration/env_infos/end_effector_loc Std              0.493542\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           1000\n",
      "evaluation/num paths total                             50\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0724238\n",
      "evaluation/Rewards Std                                  0.0456905\n",
      "evaluation/Rewards Max                                  0.0273882\n",
      "evaluation/Rewards Min                                 -0.145049\n",
      "evaluation/Returns Mean                                -1.44848\n",
      "evaluation/Returns Std                                  0.912568\n",
      "evaluation/Returns Max                                  0.437927\n",
      "evaluation/Returns Min                                 -2.87246\n",
      "evaluation/Actions Mean                                 0.000980846\n",
      "evaluation/Actions Std                                  0.000595694\n",
      "evaluation/Actions Max                                  0.00203524\n",
      "evaluation/Actions Min                                  0.000153851\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.44848\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00345347\n",
      "evaluation/env_infos/final/reward_dist Std              0.00716752\n",
      "evaluation/env_infos/final/reward_dist Max              0.0294951\n",
      "evaluation/env_infos/final/reward_dist Min              3.19905e-07\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00264807\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00510559\n",
      "evaluation/env_infos/initial/reward_dist Max            0.020763\n",
      "evaluation/env_infos/initial/reward_dist Min            1.16923e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.00287612\n",
      "evaluation/env_infos/reward_dist Std                    0.00567184\n",
      "evaluation/env_infos/reward_dist Max                    0.0294951\n",
      "evaluation/env_infos/reward_dist Min                    3.19905e-07\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.00164662\n",
      "evaluation/env_infos/final/reward_energy Std            0.000274058\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0011215\n",
      "evaluation/env_infos/final/reward_energy Min           -0.00206738\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.00155927\n",
      "evaluation/env_infos/initial/reward_energy Std          0.000240541\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00113108\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.00193048\n",
      "evaluation/env_infos/reward_energy Mean                -0.00160215\n",
      "evaluation/env_infos/reward_energy Std                  0.000258701\n",
      "evaluation/env_infos/reward_energy Max                 -0.0011215\n",
      "evaluation/env_infos/reward_energy Min                 -0.00206738\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.010226\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.0061497\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.0203188\n",
      "evaluation/env_infos/final/end_effector_loc Min         0.00180004\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      4.80138e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std       2.83926e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Max       9.60617e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Min       7.69257e-06\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00373622\n",
      "evaluation/env_infos/end_effector_loc Std               0.00432289\n",
      "evaluation/env_infos/end_effector_loc Max               0.0203188\n",
      "evaluation/env_infos/end_effector_loc Min               7.69257e-06\n",
      "time/data storing (s)                                   5.01789\n",
      "time/evaluation sampling (s)                            0.997978\n",
      "time/exploration sampling (s)                           0.118743\n",
      "time/logging (s)                                        0.014568\n",
      "time/saving (s)                                         0.252481\n",
      "time/training (s)                                      28.9505\n",
      "time/epoch (s)                                         35.3522\n",
      "time/total (s)                                         40.7737\n",
      "Epoch                                                   0\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\r\n",
      "  \"relabeled_latents\": np.array(self.relabeled_latents),\r\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 100 --env pointreacherobs\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 1 --env pointreacherobs\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env pointreacherobs\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env pointreacherobs --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d014c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
