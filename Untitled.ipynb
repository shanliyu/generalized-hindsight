{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe6289d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-antdirectionnewsparse-SAC-5e-1000s-disc0.99-horizon1000/gher-antdirectionnewsparse-SAC-5e-1000s-disc0.99-horizon1000_2021_05_24_22_28_26_0000--s-10/progress.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f14f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[15504]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2037d778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a203fe740). One of the two will be used. Which one is undefined.\n",
      "objc[15504]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2037d700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a203fe768). One of the two will be used. Which one is undefined.\n",
      "objc[15504]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2037d7a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a203fe7b8). One of the two will be used. Which one is undefined.\n",
      "objc[15504]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2037d818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a203fe830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 10:24:53.694438 PDT | Variant:\n",
      "2021-05-25 10:24:53.694854 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"antdirectionnewsparse\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 1,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"type\": \"360\"\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"use_xy\": false,\n",
      "    \"contact_forces\": false\n",
      "  }\n",
      "}\n",
      "antdirectionnewsparse\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 10:25:52.001442 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  2000\n",
      "trainer/QF1 Loss                                      22.946\n",
      "trainer/QF2 Loss                                      22.9294\n",
      "trainer/Policy Loss                                   -5.32959\n",
      "trainer/Q1 Predictions Mean                            0.00261926\n",
      "trainer/Q1 Predictions Std                             0.00363781\n",
      "trainer/Q1 Predictions Max                             0.0115209\n",
      "trainer/Q1 Predictions Min                            -0.0101821\n",
      "trainer/Q2 Predictions Mean                            0.00433422\n",
      "trainer/Q2 Predictions Std                             0.00328976\n",
      "trainer/Q2 Predictions Max                             0.0167003\n",
      "trainer/Q2 Predictions Min                            -0.00425816\n",
      "trainer/Q Targets Mean                                 4.73022\n",
      "trainer/Q Targets Std                                  0.771815\n",
      "trainer/Q Targets Max                                  7.34868\n",
      "trainer/Q Targets Min                                  2.72291\n",
      "trainer/Log Pis Mean                                  -5.32833\n",
      "trainer/Log Pis Std                                    0.609291\n",
      "trainer/Log Pis Max                                   -3.45657\n",
      "trainer/Log Pis Min                                   -7.65128\n",
      "trainer/Policy mu Mean                                -0.000311433\n",
      "trainer/Policy mu Std                                  0.00209994\n",
      "trainer/Policy mu Max                                  0.0058089\n",
      "trainer/Policy mu Min                                 -0.00728771\n",
      "trainer/Policy log std Mean                           -6.28851e-05\n",
      "trainer/Policy log std Std                             0.00222649\n",
      "trainer/Policy log std Max                             0.00915091\n",
      "trainer/Policy log std Min                            -0.00840893\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                           17\n",
      "exploration/path length Mean                          90.9091\n",
      "exploration/path length Std                           93.571\n",
      "exploration/path length Max                          344\n",
      "exploration/path length Min                           18\n",
      "exploration/Rewards Mean                              -0.435322\n",
      "exploration/Rewards Std                                0.602718\n",
      "exploration/Rewards Max                                2.5889\n",
      "exploration/Rewards Min                               -1.89965\n",
      "exploration/Returns Mean                             -39.5747\n",
      "exploration/Returns Std                               41.2604\n",
      "exploration/Returns Max                               -4.61274\n",
      "exploration/Returns Min                             -129.239\n",
      "exploration/Actions Mean                               0.0046145\n",
      "exploration/Actions Std                                0.62243\n",
      "exploration/Actions Max                                0.999663\n",
      "exploration/Actions Min                               -0.999062\n",
      "exploration/Num Paths                                 11\n",
      "exploration/Average Returns                          -39.5747\n",
      "exploration/env_infos/final/reward_forward Mean       -0.395515\n",
      "exploration/env_infos/final/reward_forward Std         1.11989\n",
      "exploration/env_infos/final/reward_forward Max         1.57674\n",
      "exploration/env_infos/final/reward_forward Min        -2.39917\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0818824\n",
      "exploration/env_infos/initial/reward_forward Std       0.159423\n",
      "exploration/env_infos/initial/reward_forward Max       0.0975211\n",
      "exploration/env_infos/initial/reward_forward Min      -0.41539\n",
      "exploration/env_infos/reward_forward Mean              0.0971095\n",
      "exploration/env_infos/reward_forward Std               0.905224\n",
      "exploration/env_infos/reward_forward Max               3.17502\n",
      "exploration/env_infos/reward_forward Min              -2.58717\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.53577\n",
      "exploration/env_infos/final/reward_ctrl Std            0.451785\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.636608\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.37664\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.41591\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.388531\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.57685\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.95256\n",
      "exploration/env_infos/reward_ctrl Mean                -1.54976\n",
      "exploration/env_infos/reward_ctrl Std                  0.438063\n",
      "exploration/env_infos/reward_ctrl Max                 -0.477861\n",
      "exploration/env_infos/reward_ctrl Min                 -2.89965\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.468836\n",
      "exploration/env_infos/final/torso_velocity Std         1.23118\n",
      "exploration/env_infos/final/torso_velocity Max         2.85086\n",
      "exploration/env_infos/final/torso_velocity Min        -2.39917\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0809147\n",
      "exploration/env_infos/initial/torso_velocity Std       0.290305\n",
      "exploration/env_infos/initial/torso_velocity Max       0.750508\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.41539\n",
      "exploration/env_infos/torso_velocity Mean              0.0768778\n",
      "exploration/env_infos/torso_velocity Std               0.877765\n",
      "exploration/env_infos/torso_velocity Max               3.97972\n",
      "exploration/env_infos/torso_velocity Min              -2.58717\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.00143\n",
      "evaluation/Rewards Std                                 0.0218896\n",
      "evaluation/Rewards Max                                 2.39282\n",
      "evaluation/Rewards Min                                 0.999983\n",
      "evaluation/Returns Mean                             1001.43\n",
      "evaluation/Returns Std                                 1.64994\n",
      "evaluation/Returns Max                              1006.21\n",
      "evaluation/Returns Min                               999.994\n",
      "evaluation/Actions Mean                               -0.000137594\n",
      "evaluation/Actions Std                                 0.00115534\n",
      "evaluation/Actions Max                                 0.00360492\n",
      "evaluation/Actions Min                                -0.00367134\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1001.43\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000312019\n",
      "evaluation/env_infos/final/reward_forward Std          0.000358591\n",
      "evaluation/env_infos/final/reward_forward Max          0.00109986\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000660512\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0346382\n",
      "evaluation/env_infos/initial/reward_forward Std        0.106539\n",
      "evaluation/env_infos/initial/reward_forward Max        0.241381\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.279468\n",
      "evaluation/env_infos/reward_forward Mean               0.0022557\n",
      "evaluation/env_infos/reward_forward Std                0.0433711\n",
      "evaluation/env_infos/reward_forward Max                1.09405\n",
      "evaluation/env_infos/reward_forward Min               -1.26008\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -5.39701e-06\n",
      "evaluation/env_infos/final/reward_ctrl Std             6.26989e-07\n",
      "evaluation/env_infos/final/reward_ctrl Max            -4.74359e-06\n",
      "evaluation/env_infos/final/reward_ctrl Min            -6.71496e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -6.34072e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           4.95771e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -5.46803e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -7.20333e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -5.41493e-06\n",
      "evaluation/env_infos/reward_ctrl Std                   7.44184e-07\n",
      "evaluation/env_infos/reward_ctrl Max                  -2.55525e-06\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.67815e-05\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000315675\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000659196\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00257464\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000660512\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.157506\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.219636\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.654757\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.279468\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000627939\n",
      "evaluation/env_infos/torso_velocity Std                0.053504\n",
      "evaluation/env_infos/torso_velocity Max                1.16395\n",
      "evaluation/env_infos/torso_velocity Min               -2.14941\n",
      "time/data storing (s)                                  0.0183775\n",
      "time/evaluation sampling (s)                          50.3564\n",
      "time/exploration sampling (s)                          2.01676\n",
      "time/logging (s)                                       0.279635\n",
      "time/saving (s)                                        0.0688003\n",
      "time/training (s)                                      3.62625\n",
      "time/epoch (s)                                        56.3662\n",
      "time/total (s)                                        61.0745\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:26:49.070311 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  3000\n",
      "trainer/QF1 Loss                                       0.606542\n",
      "trainer/QF2 Loss                                       0.606524\n",
      "trainer/Policy Loss                                   -9.41089\n",
      "trainer/Q1 Predictions Mean                            3.96736\n",
      "trainer/Q1 Predictions Std                             0.390265\n",
      "trainer/Q1 Predictions Max                             5.04277\n",
      "trainer/Q1 Predictions Min                             2.48488\n",
      "trainer/Q2 Predictions Mean                            3.9624\n",
      "trainer/Q2 Predictions Std                             0.388732\n",
      "trainer/Q2 Predictions Max                             5.0606\n",
      "trainer/Q2 Predictions Min                             2.52054\n",
      "trainer/Q Targets Mean                                 4.02841\n",
      "trainer/Q Targets Std                                  0.740426\n",
      "trainer/Q Targets Max                                  6.75989\n",
      "trainer/Q Targets Min                                 -1.37664\n",
      "trainer/Log Pis Mean                                  -5.47178\n",
      "trainer/Log Pis Std                                    0.438132\n",
      "trainer/Log Pis Max                                   -4.60066\n",
      "trainer/Log Pis Min                                   -9.37588\n",
      "trainer/Policy mu Mean                                -6.78969e-06\n",
      "trainer/Policy mu Std                                  0.0250522\n",
      "trainer/Policy mu Max                                  0.116637\n",
      "trainer/Policy mu Min                                 -0.0710389\n",
      "trainer/Policy log std Mean                           -0.10286\n",
      "trainer/Policy log std Std                             0.0209839\n",
      "trainer/Policy log std Max                            -0.0505825\n",
      "trainer/Policy log std Min                            -0.175214\n",
      "trainer/Alpha                                          0.738558\n",
      "trainer/Alpha Loss                                    -4.04226\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                           26\n",
      "exploration/path length Mean                         111.111\n",
      "exploration/path length Std                          149.161\n",
      "exploration/path length Max                          523\n",
      "exploration/path length Min                           16\n",
      "exploration/Rewards Mean                              -0.400784\n",
      "exploration/Rewards Std                                0.467987\n",
      "exploration/Rewards Max                                2.89302\n",
      "exploration/Rewards Min                               -1.78684\n",
      "exploration/Returns Mean                             -44.5316\n",
      "exploration/Returns Std                               60.8203\n",
      "exploration/Returns Max                               -6.8531\n",
      "exploration/Returns Min                             -212.038\n",
      "exploration/Actions Mean                              -0.00783071\n",
      "exploration/Actions Std                                0.600648\n",
      "exploration/Actions Max                                0.998586\n",
      "exploration/Actions Min                               -0.998832\n",
      "exploration/Num Paths                                  9\n",
      "exploration/Average Returns                          -44.5316\n",
      "exploration/env_infos/final/reward_forward Mean       -0.352974\n",
      "exploration/env_infos/final/reward_forward Std         1.16497\n",
      "exploration/env_infos/final/reward_forward Max         1.45881\n",
      "exploration/env_infos/final/reward_forward Min        -2.82235\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0843298\n",
      "exploration/env_infos/initial/reward_forward Std       0.191085\n",
      "exploration/env_infos/initial/reward_forward Max       0.298051\n",
      "exploration/env_infos/initial/reward_forward Min      -0.334575\n",
      "exploration/env_infos/reward_forward Mean             -0.04571\n",
      "exploration/env_infos/reward_forward Std               0.611672\n",
      "exploration/env_infos/reward_forward Max               2.17909\n",
      "exploration/env_infos/reward_forward Min              -2.83749\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.29347\n",
      "exploration/env_infos/final/reward_ctrl Std            0.335409\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.819668\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.85889\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.53383\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.182571\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.20589\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.75982\n",
      "exploration/env_infos/reward_ctrl Mean                -1.44336\n",
      "exploration/env_infos/reward_ctrl Std                  0.41207\n",
      "exploration/env_infos/reward_ctrl Max                 -0.422239\n",
      "exploration/env_infos/reward_ctrl Min                 -2.78684\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.279339\n",
      "exploration/env_infos/final/torso_velocity Std         1.20318\n",
      "exploration/env_infos/final/torso_velocity Max         2.37685\n",
      "exploration/env_infos/final/torso_velocity Min        -2.82235\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.112733\n",
      "exploration/env_infos/initial/torso_velocity Std       0.265205\n",
      "exploration/env_infos/initial/torso_velocity Max       0.504986\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.422856\n",
      "exploration/env_infos/torso_velocity Mean             -0.0163697\n",
      "exploration/env_infos/torso_velocity Std               0.669962\n",
      "exploration/env_infos/torso_velocity Max               3.60766\n",
      "exploration/env_infos/torso_velocity Min              -2.83749\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.998808\n",
      "evaluation/Rewards Std                                 0.0187674\n",
      "evaluation/Rewards Max                                 2.42227\n",
      "evaluation/Rewards Min                                 0.995599\n",
      "evaluation/Returns Mean                              998.808\n",
      "evaluation/Returns Std                                 0.703589\n",
      "evaluation/Returns Max                              1000.87\n",
      "evaluation/Returns Min                               998.095\n",
      "evaluation/Actions Mean                               -0.00120439\n",
      "evaluation/Actions Std                                 0.0212433\n",
      "evaluation/Actions Max                                 0.0646575\n",
      "evaluation/Actions Min                                -0.0455115\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           998.808\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.60225e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.30351e-07\n",
      "evaluation/env_infos/final/reward_forward Max          2.24301e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.80174e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0355255\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0901958\n",
      "evaluation/env_infos/initial/reward_forward Max        0.211949\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.116692\n",
      "evaluation/env_infos/reward_forward Mean               0.000970685\n",
      "evaluation/env_infos/reward_forward Std                0.0512219\n",
      "evaluation/env_infos/reward_forward Max                1.36977\n",
      "evaluation/env_infos/reward_forward Min               -1.08007\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00180807\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000247392\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00134118\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00204786\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00126574\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000216994\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000964923\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00169954\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00181091\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000247066\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000946125\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00457778\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.15327e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          4.0025e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          8.23411e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.4674e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.137063\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.205487\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.513861\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.234168\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00101473\n",
      "evaluation/env_infos/torso_velocity Std                0.0537056\n",
      "evaluation/env_infos/torso_velocity Max                1.36977\n",
      "evaluation/env_infos/torso_velocity Min               -1.76539\n",
      "time/data storing (s)                                  0.0172376\n",
      "time/evaluation sampling (s)                          50.8696\n",
      "time/exploration sampling (s)                          1.8933\n",
      "time/logging (s)                                       0.27176\n",
      "time/saving (s)                                        0.0268071\n",
      "time/training (s)                                      3.70823\n",
      "time/epoch (s)                                        56.7869\n",
      "time/total (s)                                       118.135\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:27:44.128090 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                       0.360205\n",
      "trainer/QF2 Loss                                       0.366803\n",
      "trainer/Policy Loss                                   -9.01923\n",
      "trainer/Q1 Predictions Mean                            3.58762\n",
      "trainer/Q1 Predictions Std                             0.328466\n",
      "trainer/Q1 Predictions Max                             4.6006\n",
      "trainer/Q1 Predictions Min                             2.38658\n",
      "trainer/Q2 Predictions Mean                            3.58133\n",
      "trainer/Q2 Predictions Std                             0.33548\n",
      "trainer/Q2 Predictions Max                             4.65731\n",
      "trainer/Q2 Predictions Min                             2.40657\n",
      "trainer/Q Targets Mean                                 3.73994\n",
      "trainer/Q Targets Std                                  0.549587\n",
      "trainer/Q Targets Max                                  6.46728\n",
      "trainer/Q Targets Min                                 -0.324779\n",
      "trainer/Log Pis Mean                                  -5.45396\n",
      "trainer/Log Pis Std                                    0.350168\n",
      "trainer/Log Pis Max                                   -4.52936\n",
      "trainer/Log Pis Min                                   -7.76873\n",
      "trainer/Policy mu Mean                                -0.00781815\n",
      "trainer/Policy mu Std                                  0.0255865\n",
      "trainer/Policy mu Max                                  0.154751\n",
      "trainer/Policy mu Min                                 -0.102777\n",
      "trainer/Policy log std Mean                           -0.122764\n",
      "trainer/Policy log std Std                             0.0148932\n",
      "trainer/Policy log std Max                            -0.0606899\n",
      "trainer/Policy log std Min                            -0.21261\n",
      "trainer/Alpha                                          0.547088\n",
      "trainer/Alpha Loss                                    -8.07431\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                           28\n",
      "exploration/path length Mean                         500\n",
      "exploration/path length Std                          443\n",
      "exploration/path length Max                          943\n",
      "exploration/path length Min                           57\n",
      "exploration/Rewards Mean                              -0.372951\n",
      "exploration/Rewards Std                                0.439135\n",
      "exploration/Rewards Max                                1.03478\n",
      "exploration/Rewards Min                               -1.74829\n",
      "exploration/Returns Mean                            -186.475\n",
      "exploration/Returns Std                              167.449\n",
      "exploration/Returns Max                              -19.027\n",
      "exploration/Returns Min                             -353.924\n",
      "exploration/Actions Mean                              -0.00413804\n",
      "exploration/Actions Std                                0.593206\n",
      "exploration/Actions Max                                0.998229\n",
      "exploration/Actions Min                               -0.996405\n",
      "exploration/Num Paths                                  2\n",
      "exploration/Average Returns                         -186.475\n",
      "exploration/env_infos/final/reward_forward Mean       -0.789552\n",
      "exploration/env_infos/final/reward_forward Std         0.848785\n",
      "exploration/env_infos/final/reward_forward Max         0.0592329\n",
      "exploration/env_infos/final/reward_forward Min        -1.63834\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0814731\n",
      "exploration/env_infos/initial/reward_forward Std       0.242425\n",
      "exploration/env_infos/initial/reward_forward Max       0.323898\n",
      "exploration/env_infos/initial/reward_forward Min      -0.160952\n",
      "exploration/env_infos/reward_forward Mean             -0.0120297\n",
      "exploration/env_infos/reward_forward Std               0.383437\n",
      "exploration/env_infos/reward_forward Max               1.00984\n",
      "exploration/env_infos/reward_forward Min              -1.63834\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.37739\n",
      "exploration/env_infos/final/reward_ctrl Std            0.0408099\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.33658\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.4182\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.65824\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.871096\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.787147\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.52934\n",
      "exploration/env_infos/reward_ctrl Mean                -1.40764\n",
      "exploration/env_infos/reward_ctrl Std                  0.422008\n",
      "exploration/env_infos/reward_ctrl Max                 -0.224481\n",
      "exploration/env_infos/reward_ctrl Min                 -2.74829\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0328022\n",
      "exploration/env_infos/final/torso_velocity Std         0.796553\n",
      "exploration/env_infos/final/torso_velocity Max         0.81515\n",
      "exploration/env_infos/final/torso_velocity Min        -1.63834\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.192908\n",
      "exploration/env_infos/initial/torso_velocity Std       0.211891\n",
      "exploration/env_infos/initial/torso_velocity Max       0.479942\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.160952\n",
      "exploration/env_infos/torso_velocity Mean              0.0108432\n",
      "exploration/env_infos/torso_velocity Std               0.405458\n",
      "exploration/env_infos/torso_velocity Max               2.1777\n",
      "exploration/env_infos/torso_velocity Min              -2.42281\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.998732\n",
      "evaluation/Rewards Std                                 0.0249507\n",
      "evaluation/Rewards Max                                 2.44904\n",
      "evaluation/Rewards Min                                 0.996024\n",
      "evaluation/Returns Mean                              998.732\n",
      "evaluation/Returns Std                                 1.32515\n",
      "evaluation/Returns Max                              1004.01\n",
      "evaluation/Returns Min                               997.716\n",
      "evaluation/Actions Mean                               -0.00639897\n",
      "evaluation/Actions Std                                 0.0212947\n",
      "evaluation/Actions Max                                 0.0456837\n",
      "evaluation/Actions Min                                -0.0518553\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           998.732\n",
      "evaluation/env_infos/final/reward_forward Mean         2.58422e-05\n",
      "evaluation/env_infos/final/reward_forward Std          0.000156115\n",
      "evaluation/env_infos/final/reward_forward Max          0.000780033\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00013243\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0310224\n",
      "evaluation/env_infos/initial/reward_forward Std        0.122867\n",
      "evaluation/env_infos/initial/reward_forward Max        0.299604\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.28928\n",
      "evaluation/env_infos/reward_forward Mean              -0.00234073\n",
      "evaluation/env_infos/reward_forward Std                0.0442357\n",
      "evaluation/env_infos/reward_forward Max                1.03476\n",
      "evaluation/env_infos/reward_forward Min               -1.50949\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00197819\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000207011\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00157024\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00232892\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00161193\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000136903\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00134476\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0019431\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00197765\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000209496\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00114296\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0039764\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         9.40503e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          0.00010606\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000780033\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000330413\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.145656\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.242578\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.755381\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.28928\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00348627\n",
      "evaluation/env_infos/torso_velocity Std                0.0538398\n",
      "evaluation/env_infos/torso_velocity Max                1.03476\n",
      "evaluation/env_infos/torso_velocity Min               -2.20918\n",
      "time/data storing (s)                                  0.0149298\n",
      "time/evaluation sampling (s)                          49.1814\n",
      "time/exploration sampling (s)                          1.93628\n",
      "time/logging (s)                                       0.275844\n",
      "time/saving (s)                                        0.0267672\n",
      "time/training (s)                                      3.43627\n",
      "time/epoch (s)                                        54.8715\n",
      "time/total (s)                                       173.197\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:28:39.409369 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   5000\n",
      "trainer/QF1 Loss                                        0.371607\n",
      "trainer/QF2 Loss                                        0.368643\n",
      "trainer/Policy Loss                                    -9.07059\n",
      "trainer/Q1 Predictions Mean                             3.61325\n",
      "trainer/Q1 Predictions Std                              0.296767\n",
      "trainer/Q1 Predictions Max                              4.55949\n",
      "trainer/Q1 Predictions Min                              2.50059\n",
      "trainer/Q2 Predictions Mean                             3.62659\n",
      "trainer/Q2 Predictions Std                              0.297647\n",
      "trainer/Q2 Predictions Max                              4.43497\n",
      "trainer/Q2 Predictions Min                              2.42618\n",
      "trainer/Q Targets Mean                                  3.68909\n",
      "trainer/Q Targets Std                                   0.629181\n",
      "trainer/Q Targets Max                                   6.24423\n",
      "trainer/Q Targets Min                                  -0.419588\n",
      "trainer/Log Pis Mean                                   -5.47697\n",
      "trainer/Log Pis Std                                     0.427494\n",
      "trainer/Log Pis Max                                    -4.62317\n",
      "trainer/Log Pis Min                                    -9.53484\n",
      "trainer/Policy mu Mean                                 -0.0113808\n",
      "trainer/Policy mu Std                                   0.0332386\n",
      "trainer/Policy mu Max                                   0.135594\n",
      "trainer/Policy mu Min                                  -0.142924\n",
      "trainer/Policy log std Mean                            -0.140862\n",
      "trainer/Policy log std Std                              0.0188178\n",
      "trainer/Policy log std Max                             -0.0704517\n",
      "trainer/Policy log std Min                             -0.215702\n",
      "trainer/Alpha                                           0.405279\n",
      "trainer/Alpha Loss                                    -12.1317\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                            30\n",
      "exploration/path length Mean                          500\n",
      "exploration/path length Std                           461\n",
      "exploration/path length Max                           961\n",
      "exploration/path length Min                            39\n",
      "exploration/Rewards Mean                               -0.332221\n",
      "exploration/Rewards Std                                 0.430967\n",
      "exploration/Rewards Max                                 1.58069\n",
      "exploration/Rewards Min                                -1.69202\n",
      "exploration/Returns Mean                             -166.11\n",
      "exploration/Returns Std                               153.614\n",
      "exploration/Returns Max                               -12.4969\n",
      "exploration/Returns Min                              -319.724\n",
      "exploration/Actions Mean                               -0.00251528\n",
      "exploration/Actions Std                                 0.585622\n",
      "exploration/Actions Max                                 0.996782\n",
      "exploration/Actions Min                                -0.995335\n",
      "exploration/Num Paths                                   2\n",
      "exploration/Average Returns                          -166.11\n",
      "exploration/env_infos/final/reward_forward Mean         0.507931\n",
      "exploration/env_infos/final/reward_forward Std          1.16427\n",
      "exploration/env_infos/final/reward_forward Max          1.6722\n",
      "exploration/env_infos/final/reward_forward Min         -0.656336\n",
      "exploration/env_infos/initial/reward_forward Mean       0.165879\n",
      "exploration/env_infos/initial/reward_forward Std        0.1906\n",
      "exploration/env_infos/initial/reward_forward Max        0.356479\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0247208\n",
      "exploration/env_infos/reward_forward Mean              -0.00766838\n",
      "exploration/env_infos/reward_forward Std                0.410228\n",
      "exploration/env_infos/reward_forward Max                2.37215\n",
      "exploration/env_infos/reward_forward Min               -1.45037\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.31304\n",
      "exploration/env_infos/final/reward_ctrl Std             0.503332\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.809711\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.81637\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.40547\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.0872137\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.31825\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.49268\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.37184\n",
      "exploration/env_infos/reward_ctrl Std                   0.404715\n",
      "exploration/env_infos/reward_ctrl Max                  -0.32943\n",
      "exploration/env_infos/reward_ctrl Min                  -2.69202\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.325156\n",
      "exploration/env_infos/final/torso_velocity Std          0.754637\n",
      "exploration/env_infos/final/torso_velocity Max          1.6722\n",
      "exploration/env_infos/final/torso_velocity Min         -0.656336\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.176351\n",
      "exploration/env_infos/initial/torso_velocity Std        0.303801\n",
      "exploration/env_infos/initial/torso_velocity Max        0.559097\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.259612\n",
      "exploration/env_infos/torso_velocity Mean               0.0102447\n",
      "exploration/env_infos/torso_velocity Std                0.384042\n",
      "exploration/env_infos/torso_velocity Max                3.20139\n",
      "exploration/env_infos/torso_velocity Min               -2.89334\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.998007\n",
      "evaluation/Rewards Std                                  0.0263147\n",
      "evaluation/Rewards Max                                  2.4141\n",
      "evaluation/Rewards Min                                  0.994239\n",
      "evaluation/Returns Mean                               998.007\n",
      "evaluation/Returns Std                                  1.90108\n",
      "evaluation/Returns Max                               1003.81\n",
      "evaluation/Returns Min                                995.388\n",
      "evaluation/Actions Mean                                -0.0105852\n",
      "evaluation/Actions Std                                  0.0267141\n",
      "evaluation/Actions Max                                  0.0399429\n",
      "evaluation/Actions Min                                 -0.0655534\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            998.007\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000100683\n",
      "evaluation/env_infos/final/reward_forward Std           0.000523139\n",
      "evaluation/env_infos/final/reward_forward Max           0.00265589\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00015107\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0127757\n",
      "evaluation/env_infos/initial/reward_forward Std         0.115911\n",
      "evaluation/env_infos/initial/reward_forward Max         0.162588\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.195974\n",
      "evaluation/env_infos/reward_forward Mean                0.00100653\n",
      "evaluation/env_infos/reward_forward Std                 0.0478308\n",
      "evaluation/env_infos/reward_forward Max                 1.09674\n",
      "evaluation/env_infos/reward_forward Min                -1.48288\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00332176\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000557678\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00260253\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00461706\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00330316\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000559152\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00245025\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00425841\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00330277\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000579204\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00197345\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0057613\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.62709e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000336944\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00265589\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00108266\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.128671\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.225013\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.544437\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.235771\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000811902\n",
      "evaluation/env_infos/torso_velocity Std                 0.0529041\n",
      "evaluation/env_infos/torso_velocity Max                 1.24046\n",
      "evaluation/env_infos/torso_velocity Min                -1.73329\n",
      "time/data storing (s)                                   0.0149866\n",
      "time/evaluation sampling (s)                           48.7667\n",
      "time/exploration sampling (s)                           1.99749\n",
      "time/logging (s)                                        0.305568\n",
      "time/saving (s)                                         0.0308998\n",
      "time/training (s)                                       3.99608\n",
      "time/epoch (s)                                         55.1117\n",
      "time/total (s)                                        228.508\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:29:35.200713 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   6000\n",
      "trainer/QF1 Loss                                        0.402287\n",
      "trainer/QF2 Loss                                        0.383144\n",
      "trainer/Policy Loss                                    -9.112\n",
      "trainer/Q1 Predictions Mean                             3.68166\n",
      "trainer/Q1 Predictions Std                              0.288223\n",
      "trainer/Q1 Predictions Max                              4.49122\n",
      "trainer/Q1 Predictions Min                              2.92403\n",
      "trainer/Q2 Predictions Mean                             3.68445\n",
      "trainer/Q2 Predictions Std                              0.287815\n",
      "trainer/Q2 Predictions Max                              4.62446\n",
      "trainer/Q2 Predictions Min                              2.73789\n",
      "trainer/Q Targets Mean                                  3.7325\n",
      "trainer/Q Targets Std                                   0.679184\n",
      "trainer/Q Targets Max                                   6.93174\n",
      "trainer/Q Targets Min                                  -0.920379\n",
      "trainer/Log Pis Mean                                   -5.44333\n",
      "trainer/Log Pis Std                                     0.387289\n",
      "trainer/Log Pis Max                                    -4.52185\n",
      "trainer/Log Pis Min                                    -7.28996\n",
      "trainer/Policy mu Mean                                 -0.0202597\n",
      "trainer/Policy mu Std                                   0.0570464\n",
      "trainer/Policy mu Max                                   0.158459\n",
      "trainer/Policy mu Min                                  -0.25169\n",
      "trainer/Policy log std Mean                            -0.132945\n",
      "trainer/Policy log std Std                              0.0160952\n",
      "trainer/Policy log std Max                             -0.0932732\n",
      "trainer/Policy log std Min                             -0.207179\n",
      "trainer/Alpha                                           0.300227\n",
      "trainer/Alpha Loss                                    -16.1349\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                            40\n",
      "exploration/path length Mean                          100\n",
      "exploration/path length Std                            87.2055\n",
      "exploration/path length Max                           337\n",
      "exploration/path length Min                            17\n",
      "exploration/Rewards Mean                               -0.281422\n",
      "exploration/Rewards Std                                 0.558312\n",
      "exploration/Rewards Max                                 2.9086\n",
      "exploration/Rewards Min                                -1.81567\n",
      "exploration/Returns Mean                              -28.1422\n",
      "exploration/Returns Std                                31.6638\n",
      "exploration/Returns Max                                 7.93669\n",
      "exploration/Returns Min                              -114.698\n",
      "exploration/Actions Mean                               -0.0256338\n",
      "exploration/Actions Std                                 0.587125\n",
      "exploration/Actions Max                                 0.995734\n",
      "exploration/Actions Min                                -0.998942\n",
      "exploration/Num Paths                                  10\n",
      "exploration/Average Returns                           -28.1422\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0643964\n",
      "exploration/env_infos/final/reward_forward Std          1.16625\n",
      "exploration/env_infos/final/reward_forward Max          2.21745\n",
      "exploration/env_infos/final/reward_forward Min         -1.71145\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0345051\n",
      "exploration/env_infos/initial/reward_forward Std        0.168347\n",
      "exploration/env_infos/initial/reward_forward Max        0.217505\n",
      "exploration/env_infos/initial/reward_forward Min       -0.300126\n",
      "exploration/env_infos/reward_forward Mean               0.127907\n",
      "exploration/env_infos/reward_forward Std                0.77627\n",
      "exploration/env_infos/reward_forward Max                2.64447\n",
      "exploration/env_infos/reward_forward Min               -2.18675\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.43828\n",
      "exploration/env_infos/final/reward_ctrl Std             0.495388\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.697208\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.0034\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.34448\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.311116\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.892219\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.93271\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.38149\n",
      "exploration/env_infos/reward_ctrl Std                   0.404287\n",
      "exploration/env_infos/reward_ctrl Max                  -0.315324\n",
      "exploration/env_infos/reward_ctrl Min                  -2.81567\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.424337\n",
      "exploration/env_infos/final/torso_velocity Std          1.43934\n",
      "exploration/env_infos/final/torso_velocity Max          2.79744\n",
      "exploration/env_infos/final/torso_velocity Min         -2.10092\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.171009\n",
      "exploration/env_infos/initial/torso_velocity Std        0.242503\n",
      "exploration/env_infos/initial/torso_velocity Max        0.691015\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.300126\n",
      "exploration/env_infos/torso_velocity Mean               0.0618487\n",
      "exploration/env_infos/torso_velocity Std                0.758488\n",
      "exploration/env_infos/torso_velocity Max                3.59508\n",
      "exploration/env_infos/torso_velocity Min               -2.35663\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.991797\n",
      "evaluation/Rewards Std                                  0.026503\n",
      "evaluation/Rewards Max                                  2.39607\n",
      "evaluation/Rewards Min                                  0.963831\n",
      "evaluation/Returns Mean                               991.797\n",
      "evaluation/Returns Std                                  2.08117\n",
      "evaluation/Returns Max                                998.398\n",
      "evaluation/Returns Min                                988.368\n",
      "evaluation/Actions Mean                                -0.0165376\n",
      "evaluation/Actions Std                                  0.046131\n",
      "evaluation/Actions Max                                  0.095343\n",
      "evaluation/Actions Min                                 -0.201739\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            991.797\n",
      "evaluation/env_infos/final/reward_forward Mean         -6.48914e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.85083e-06\n",
      "evaluation/env_infos/final/reward_forward Max           5.58847e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -1.17089e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.013998\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124553\n",
      "evaluation/env_infos/initial/reward_forward Max         0.201383\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.287535\n",
      "evaluation/env_infos/reward_forward Mean               -7.98269e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0535508\n",
      "evaluation/env_infos/reward_forward Max                 1.22884\n",
      "evaluation/env_infos/reward_forward Min                -1.43468\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00955983\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00142039\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00804991\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0126674\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00897681\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000991924\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00757956\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0109396\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00960626\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00158367\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00612077\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.036169\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -6.54486e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           5.04702e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           2.35499e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -2.36745e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.150064\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.225791\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.762377\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.287535\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000934674\n",
      "evaluation/env_infos/torso_velocity Std                 0.0547097\n",
      "evaluation/env_infos/torso_velocity Max                 1.22884\n",
      "evaluation/env_infos/torso_velocity Min                -1.8519\n",
      "time/data storing (s)                                   0.0172952\n",
      "time/evaluation sampling (s)                           49.7293\n",
      "time/exploration sampling (s)                           1.89152\n",
      "time/logging (s)                                        0.278079\n",
      "time/saving (s)                                         0.0279566\n",
      "time/training (s)                                       3.57892\n",
      "time/epoch (s)                                         55.5231\n",
      "time/total (s)                                        284.271\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:27.697112 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   7000\n",
      "trainer/QF1 Loss                                        0.221621\n",
      "trainer/QF2 Loss                                        0.209144\n",
      "trainer/Policy Loss                                    -9.09589\n",
      "trainer/Q1 Predictions Mean                             3.65336\n",
      "trainer/Q1 Predictions Std                              0.327089\n",
      "trainer/Q1 Predictions Max                              5.17096\n",
      "trainer/Q1 Predictions Min                              2.85603\n",
      "trainer/Q2 Predictions Mean                             3.65715\n",
      "trainer/Q2 Predictions Std                              0.341538\n",
      "trainer/Q2 Predictions Max                              5.3743\n",
      "trainer/Q2 Predictions Min                              2.95015\n",
      "trainer/Q Targets Mean                                  3.77585\n",
      "trainer/Q Targets Std                                   0.511668\n",
      "trainer/Q Targets Max                                   6.47686\n",
      "trainer/Q Targets Min                                   2.62794\n",
      "trainer/Log Pis Mean                                   -5.45867\n",
      "trainer/Log Pis Std                                     0.606326\n",
      "trainer/Log Pis Max                                    -4.00132\n",
      "trainer/Log Pis Min                                    -9.88238\n",
      "trainer/Policy mu Mean                                 -0.0284535\n",
      "trainer/Policy mu Std                                   0.0945644\n",
      "trainer/Policy mu Max                                   0.261152\n",
      "trainer/Policy mu Min                                  -0.580878\n",
      "trainer/Policy log std Mean                            -0.133299\n",
      "trainer/Policy log std Std                              0.0189152\n",
      "trainer/Policy log std Max                             -0.0870391\n",
      "trainer/Policy log std Min                             -0.237341\n",
      "trainer/Alpha                                           0.222456\n",
      "trainer/Alpha Loss                                    -20.1884\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                            42\n",
      "exploration/path length Mean                          500\n",
      "exploration/path length Std                           443\n",
      "exploration/path length Max                           943\n",
      "exploration/path length Min                            57\n",
      "exploration/Rewards Mean                               -0.358604\n",
      "exploration/Rewards Std                                 0.453401\n",
      "exploration/Rewards Max                                 1.36623\n",
      "exploration/Rewards Min                                -2.07353\n",
      "exploration/Returns Mean                             -179.302\n",
      "exploration/Returns Std                               156.125\n",
      "exploration/Returns Max                               -23.1765\n",
      "exploration/Returns Min                              -335.427\n",
      "exploration/Actions Mean                               -0.0166429\n",
      "exploration/Actions Std                                 0.593811\n",
      "exploration/Actions Max                                 0.996267\n",
      "exploration/Actions Min                                -0.998074\n",
      "exploration/Num Paths                                   2\n",
      "exploration/Average Returns                          -179.302\n",
      "exploration/env_infos/final/reward_forward Mean         0.349272\n",
      "exploration/env_infos/final/reward_forward Std          0.127422\n",
      "exploration/env_infos/final/reward_forward Max          0.476694\n",
      "exploration/env_infos/final/reward_forward Min          0.22185\n",
      "exploration/env_infos/initial/reward_forward Mean       0.183299\n",
      "exploration/env_infos/initial/reward_forward Std        0.0184853\n",
      "exploration/env_infos/initial/reward_forward Max        0.201784\n",
      "exploration/env_infos/initial/reward_forward Min        0.164813\n",
      "exploration/env_infos/reward_forward Mean               0.0639577\n",
      "exploration/env_infos/reward_forward Std                0.572232\n",
      "exploration/env_infos/reward_forward Max                2.4896\n",
      "exploration/env_infos/reward_forward Min               -1.41921\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.01155\n",
      "exploration/env_infos/final/reward_ctrl Std             0.0970775\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.914473\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.10863\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.27046\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.0407788\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.22968\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.31123\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.41155\n",
      "exploration/env_infos/reward_ctrl Std                   0.42085\n",
      "exploration/env_infos/reward_ctrl Max                  -0.240669\n",
      "exploration/env_infos/reward_ctrl Min                  -3.07353\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.558905\n",
      "exploration/env_infos/final/torso_velocity Std          0.616584\n",
      "exploration/env_infos/final/torso_velocity Max          1.86683\n",
      "exploration/env_infos/final/torso_velocity Min          0.0121702\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.24887\n",
      "exploration/env_infos/initial/torso_velocity Std        0.166528\n",
      "exploration/env_infos/initial/torso_velocity Max        0.525795\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0336804\n",
      "exploration/env_infos/torso_velocity Mean              -0.00316633\n",
      "exploration/env_infos/torso_velocity Std                0.507407\n",
      "exploration/env_infos/torso_velocity Max                2.75422\n",
      "exploration/env_infos/torso_velocity Min               -2.56137\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.985123\n",
      "evaluation/Rewards Std                                  0.0213754\n",
      "evaluation/Rewards Max                                  2.20886\n",
      "evaluation/Rewards Min                                  0.730694\n",
      "evaluation/Returns Mean                               985.123\n",
      "evaluation/Returns Std                                  2.98391\n",
      "evaluation/Returns Max                                988.657\n",
      "evaluation/Returns Min                                977.998\n",
      "evaluation/Actions Mean                                -0.0237353\n",
      "evaluation/Actions Std                                  0.0574785\n",
      "evaluation/Actions Max                                  0.186304\n",
      "evaluation/Actions Min                                 -0.474752\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            985.123\n",
      "evaluation/env_infos/final/reward_forward Mean          5.55279e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.6081e-07\n",
      "evaluation/env_infos/final/reward_forward Max           6.91896e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.58449e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.028553\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0866772\n",
      "evaluation/env_infos/initial/reward_forward Max         0.102369\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.288209\n",
      "evaluation/env_infos/reward_forward Mean               -0.00351224\n",
      "evaluation/env_infos/reward_forward Std                 0.0512872\n",
      "evaluation/env_infos/reward_forward Max                 0.715497\n",
      "evaluation/env_infos/reward_forward Min                -1.34734\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0151179\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00322443\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0116814\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0224275\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0145644\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00394058\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0100906\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0231439\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0154686\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0084715\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00655622\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.269306\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          6.59737e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.91445e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.14939e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.63863e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.130896\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.223661\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.636787\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.288209\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00242834\n",
      "evaluation/env_infos/torso_velocity Std                 0.0519119\n",
      "evaluation/env_infos/torso_velocity Max                 1.15827\n",
      "evaluation/env_infos/torso_velocity Min                -1.74857\n",
      "time/data storing (s)                                   0.0156136\n",
      "time/evaluation sampling (s)                           46.3399\n",
      "time/exploration sampling (s)                           2.02711\n",
      "time/logging (s)                                        0.281815\n",
      "time/saving (s)                                         0.027766\n",
      "time/training (s)                                       3.592\n",
      "time/epoch (s)                                         52.2842\n",
      "time/total (s)                                        336.771\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:31:22.190322 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   8000\n",
      "trainer/QF1 Loss                                        0.314229\n",
      "trainer/QF2 Loss                                        0.302574\n",
      "trainer/Policy Loss                                    -9.02304\n",
      "trainer/Q1 Predictions Mean                             3.66675\n",
      "trainer/Q1 Predictions Std                              0.308303\n",
      "trainer/Q1 Predictions Max                              5.1721\n",
      "trainer/Q1 Predictions Min                              2.6906\n",
      "trainer/Q2 Predictions Mean                             3.61107\n",
      "trainer/Q2 Predictions Std                              0.325644\n",
      "trainer/Q2 Predictions Max                              5.2233\n",
      "trainer/Q2 Predictions Min                              2.54973\n",
      "trainer/Q Targets Mean                                  3.66661\n",
      "trainer/Q Targets Std                                   0.67492\n",
      "trainer/Q Targets Max                                   6.9872\n",
      "trainer/Q Targets Min                                  -0.794723\n",
      "trainer/Log Pis Mean                                   -5.43707\n",
      "trainer/Log Pis Std                                     0.561768\n",
      "trainer/Log Pis Max                                    -3.98195\n",
      "trainer/Log Pis Min                                   -10.062\n",
      "trainer/Policy mu Mean                                 -0.0303478\n",
      "trainer/Policy mu Std                                   0.10688\n",
      "trainer/Policy mu Max                                   0.375994\n",
      "trainer/Policy mu Min                                  -0.555145\n",
      "trainer/Policy log std Mean                            -0.155271\n",
      "trainer/Policy log std Std                              0.0242673\n",
      "trainer/Policy log std Max                             -0.0697164\n",
      "trainer/Policy log std Min                             -0.281619\n",
      "trainer/Alpha                                           0.164897\n",
      "trainer/Alpha Loss                                    -24.1793\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                            43\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.304031\n",
      "exploration/Rewards Std                                 0.456544\n",
      "exploration/Rewards Max                                 1.49165\n",
      "exploration/Rewards Min                                -1.63852\n",
      "exploration/Returns Mean                             -304.031\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -304.031\n",
      "exploration/Returns Min                              -304.031\n",
      "exploration/Actions Mean                               -0.0228873\n",
      "exploration/Actions Std                                 0.581405\n",
      "exploration/Actions Max                                 0.995112\n",
      "exploration/Actions Min                                -0.995053\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -304.031\n",
      "exploration/env_infos/final/reward_forward Mean        -0.532301\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.532301\n",
      "exploration/env_infos/final/reward_forward Min         -0.532301\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0187557\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0187557\n",
      "exploration/env_infos/initial/reward_forward Min        0.0187557\n",
      "exploration/env_infos/reward_forward Mean              -0.0524543\n",
      "exploration/env_infos/reward_forward Std                0.4066\n",
      "exploration/env_infos/reward_forward Max                1.14828\n",
      "exploration/env_infos/reward_forward Min               -1.83081\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.96161\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -1.96161\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.96161\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -2.32447\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -2.32447\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.32447\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.35422\n",
      "exploration/env_infos/reward_ctrl Std                   0.420406\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0935119\n",
      "exploration/env_infos/reward_ctrl Min                  -2.65188\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.348302\n",
      "exploration/env_infos/final/torso_velocity Std          0.131704\n",
      "exploration/env_infos/final/torso_velocity Max         -0.231257\n",
      "exploration/env_infos/final/torso_velocity Min         -0.532301\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.135018\n",
      "exploration/env_infos/initial/torso_velocity Std        0.141688\n",
      "exploration/env_infos/initial/torso_velocity Max        0.334484\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0187557\n",
      "exploration/env_infos/torso_velocity Mean              -0.0258224\n",
      "exploration/env_infos/torso_velocity Std                0.373607\n",
      "exploration/env_infos/torso_velocity Max                2.57406\n",
      "exploration/env_infos/torso_velocity Min               -2.20525\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.984037\n",
      "evaluation/Rewards Std                                  0.0237064\n",
      "evaluation/Rewards Max                                  2.33516\n",
      "evaluation/Rewards Min                                  0.703186\n",
      "evaluation/Returns Mean                               984.037\n",
      "evaluation/Returns Std                                  2.13537\n",
      "evaluation/Returns Max                                989.852\n",
      "evaluation/Returns Min                                981.178\n",
      "evaluation/Actions Mean                                -0.0342791\n",
      "evaluation/Actions Std                                  0.0546808\n",
      "evaluation/Actions Max                                  0.279455\n",
      "evaluation/Actions Min                                 -0.53299\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            984.037\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00165872\n",
      "evaluation/env_infos/final/reward_forward Std           0.00822673\n",
      "evaluation/env_infos/final/reward_forward Max           0.000487743\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0419586\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0415104\n",
      "evaluation/env_infos/initial/reward_forward Std         0.125001\n",
      "evaluation/env_infos/initial/reward_forward Max         0.238231\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.310529\n",
      "evaluation/env_infos/reward_forward Mean               -0.000789985\n",
      "evaluation/env_infos/reward_forward Std                 0.0469921\n",
      "evaluation/env_infos/reward_forward Max                 1.04829\n",
      "evaluation/env_infos/reward_forward Min                -1.70851\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0161793\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00162634\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0128574\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0186471\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.014198\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00239463\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0102258\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0202218\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0166602\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00905586\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00894136\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.296814\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000732245\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00512864\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00197903\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0419586\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.133812\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.253302\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.61694\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.310529\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00156111\n",
      "evaluation/env_infos/torso_velocity Std                 0.0495662\n",
      "evaluation/env_infos/torso_velocity Max                 1.04829\n",
      "evaluation/env_infos/torso_velocity Min                -1.88016\n",
      "time/data storing (s)                                   0.0150738\n",
      "time/evaluation sampling (s)                           48.1821\n",
      "time/exploration sampling (s)                           1.98249\n",
      "time/logging (s)                                        0.272695\n",
      "time/saving (s)                                         0.0262913\n",
      "time/training (s)                                       3.78096\n",
      "time/epoch (s)                                         54.2596\n",
      "time/total (s)                                        391.255\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:32:17.234906 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   9000\n",
      "trainer/QF1 Loss                                        0.257678\n",
      "trainer/QF2 Loss                                        0.231392\n",
      "trainer/Policy Loss                                    -8.80985\n",
      "trainer/Q1 Predictions Mean                             3.56858\n",
      "trainer/Q1 Predictions Std                              0.354346\n",
      "trainer/Q1 Predictions Max                              5.88693\n",
      "trainer/Q1 Predictions Min                              2.50439\n",
      "trainer/Q2 Predictions Mean                             3.53768\n",
      "trainer/Q2 Predictions Std                              0.398111\n",
      "trainer/Q2 Predictions Max                              5.86132\n",
      "trainer/Q2 Predictions Min                              2.47642\n",
      "trainer/Q Targets Mean                                  3.69824\n",
      "trainer/Q Targets Std                                   0.621954\n",
      "trainer/Q Targets Max                                   7.72151\n",
      "trainer/Q Targets Min                                   2.28013\n",
      "trainer/Log Pis Mean                                   -5.29072\n",
      "trainer/Log Pis Std                                     0.63513\n",
      "trainer/Log Pis Max                                    -3.27\n",
      "trainer/Log Pis Min                                    -8.13879\n",
      "trainer/Policy mu Mean                                  0.0208486\n",
      "trainer/Policy mu Std                                   0.147495\n",
      "trainer/Policy mu Max                                   0.654932\n",
      "trainer/Policy mu Min                                  -0.513452\n",
      "trainer/Policy log std Mean                            -0.212497\n",
      "trainer/Policy log std Std                              0.0542292\n",
      "trainer/Policy log std Max                             -0.124873\n",
      "trainer/Policy log std Min                             -0.545487\n",
      "trainer/Alpha                                           0.122412\n",
      "trainer/Alpha Loss                                    -27.8758\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                            59\n",
      "exploration/path length Mean                           62.5\n",
      "exploration/path length Std                            42.9287\n",
      "exploration/path length Max                           153\n",
      "exploration/path length Min                            14\n",
      "exploration/Rewards Mean                               -0.21643\n",
      "exploration/Rewards Std                                 0.52384\n",
      "exploration/Rewards Max                                 2.02484\n",
      "exploration/Rewards Min                                -1.61838\n",
      "exploration/Returns Mean                              -13.5269\n",
      "exploration/Returns Std                                10.4532\n",
      "exploration/Returns Max                                -0.85678\n",
      "exploration/Returns Min                               -40.6516\n",
      "exploration/Actions Mean                                0.0212915\n",
      "exploration/Actions Std                                 0.570425\n",
      "exploration/Actions Max                                 0.994923\n",
      "exploration/Actions Min                                -0.993371\n",
      "exploration/Num Paths                                  16\n",
      "exploration/Average Returns                           -13.5269\n",
      "exploration/env_infos/final/reward_forward Mean         0.019845\n",
      "exploration/env_infos/final/reward_forward Std          1.02553\n",
      "exploration/env_infos/final/reward_forward Max          1.97646\n",
      "exploration/env_infos/final/reward_forward Min         -2.37169\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0476197\n",
      "exploration/env_infos/initial/reward_forward Std        0.201681\n",
      "exploration/env_infos/initial/reward_forward Max        0.276643\n",
      "exploration/env_infos/initial/reward_forward Min       -0.578331\n",
      "exploration/env_infos/reward_forward Mean              -0.0468801\n",
      "exploration/env_infos/reward_forward Std                0.790114\n",
      "exploration/env_infos/reward_forward Max                2.92207\n",
      "exploration/env_infos/reward_forward Min               -2.37169\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.29066\n",
      "exploration/env_infos/final/reward_ctrl Std             0.37686\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.765889\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.05962\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.38139\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.300151\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.667486\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.02286\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.30335\n",
      "exploration/env_infos/reward_ctrl Std                   0.410146\n",
      "exploration/env_infos/reward_ctrl Max                  -0.209858\n",
      "exploration/env_infos/reward_ctrl Min                  -2.61838\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.444099\n",
      "exploration/env_infos/final/torso_velocity Std          1.17544\n",
      "exploration/env_infos/final/torso_velocity Max          3.56561\n",
      "exploration/env_infos/final/torso_velocity Min         -2.37169\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.100737\n",
      "exploration/env_infos/initial/torso_velocity Std        0.301984\n",
      "exploration/env_infos/initial/torso_velocity Max        0.746047\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.578331\n",
      "exploration/env_infos/torso_velocity Mean               0.0475794\n",
      "exploration/env_infos/torso_velocity Std                0.8308\n",
      "exploration/env_infos/torso_velocity Max                4.35868\n",
      "exploration/env_infos/torso_velocity Min               -2.75453\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.969019\n",
      "evaluation/Rewards Std                                  0.0462153\n",
      "evaluation/Rewards Max                                  2.83456\n",
      "evaluation/Rewards Min                                  0.640964\n",
      "evaluation/Returns Mean                               969.019\n",
      "evaluation/Returns Std                                 14.6626\n",
      "evaluation/Returns Max                               1028.71\n",
      "evaluation/Returns Min                                962.544\n",
      "evaluation/Actions Mean                                -0.0186908\n",
      "evaluation/Actions Std                                  0.0925107\n",
      "evaluation/Actions Max                                  0.409873\n",
      "evaluation/Actions Min                                 -0.529335\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            969.019\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000232415\n",
      "evaluation/env_infos/final/reward_forward Std           0.057182\n",
      "evaluation/env_infos/final/reward_forward Max           0.205054\n",
      "evaluation/env_infos/final/reward_forward Min          -0.199246\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0413987\n",
      "evaluation/env_infos/initial/reward_forward Std         0.108145\n",
      "evaluation/env_infos/initial/reward_forward Max         0.225487\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.256417\n",
      "evaluation/env_infos/reward_forward Mean                0.00201227\n",
      "evaluation/env_infos/reward_forward Std                 0.115508\n",
      "evaluation/env_infos/reward_forward Max                 1.20803\n",
      "evaluation/env_infos/reward_forward Min                -1.88612\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0349419\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00150916\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0311085\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0372159\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0279435\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00753926\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0203082\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0567356\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0356303\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00728471\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0176843\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.359036\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00134413\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0353392\n",
      "evaluation/env_infos/final/torso_velocity Max           0.205054\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.199246\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.126907\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.219629\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.724475\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.256417\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00138794\n",
      "evaluation/env_infos/torso_velocity Std                 0.0854096\n",
      "evaluation/env_infos/torso_velocity Max                 1.20803\n",
      "evaluation/env_infos/torso_velocity Min                -1.92294\n",
      "time/data storing (s)                                   0.0181262\n",
      "time/evaluation sampling (s)                           48.3423\n",
      "time/exploration sampling (s)                           1.83827\n",
      "time/logging (s)                                        0.278388\n",
      "time/saving (s)                                         0.0288162\n",
      "time/training (s)                                       4.3154\n",
      "time/epoch (s)                                         54.8213\n",
      "time/total (s)                                        446.305\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:33:10.517156 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.349883\n",
      "trainer/QF2 Loss                                        0.352419\n",
      "trainer/Policy Loss                                    -8.39415\n",
      "trainer/Q1 Predictions Mean                             3.42134\n",
      "trainer/Q1 Predictions Std                              0.443943\n",
      "trainer/Q1 Predictions Max                              5.90547\n",
      "trainer/Q1 Predictions Min                              2.33824\n",
      "trainer/Q2 Predictions Mean                             3.43701\n",
      "trainer/Q2 Predictions Std                              0.47094\n",
      "trainer/Q2 Predictions Max                              5.827\n",
      "trainer/Q2 Predictions Min                              2.34889\n",
      "trainer/Q Targets Mean                                  3.61395\n",
      "trainer/Q Targets Std                                   0.755659\n",
      "trainer/Q Targets Max                                   8.20213\n",
      "trainer/Q Targets Min                                  -0.108628\n",
      "trainer/Log Pis Mean                                   -4.94132\n",
      "trainer/Log Pis Std                                     0.902321\n",
      "trainer/Log Pis Max                                    -2.48223\n",
      "trainer/Log Pis Min                                    -8.04894\n",
      "trainer/Policy mu Mean                                 -0.00491656\n",
      "trainer/Policy mu Std                                   0.196265\n",
      "trainer/Policy mu Max                                   0.830542\n",
      "trainer/Policy mu Min                                  -0.831553\n",
      "trainer/Policy log std Mean                            -0.373549\n",
      "trainer/Policy log std Std                              0.120603\n",
      "trainer/Policy log std Max                             -0.147991\n",
      "trainer/Policy log std Min                             -0.862397\n",
      "trainer/Alpha                                           0.0913216\n",
      "trainer/Alpha Loss                                    -30.9359\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            67\n",
      "exploration/path length Mean                          125\n",
      "exploration/path length Std                           161.556\n",
      "exploration/path length Max                           526\n",
      "exploration/path length Min                            15\n",
      "exploration/Rewards Mean                               -0.00761758\n",
      "exploration/Rewards Std                                 0.521642\n",
      "exploration/Rewards Max                                 3.08238\n",
      "exploration/Rewards Min                                -1.38404\n",
      "exploration/Returns Mean                               -0.952197\n",
      "exploration/Returns Std                                 3.10205\n",
      "exploration/Returns Max                                 3.29693\n",
      "exploration/Returns Min                                -7.2277\n",
      "exploration/Actions Mean                                0.00346605\n",
      "exploration/Actions Std                                 0.531144\n",
      "exploration/Actions Max                                 0.995364\n",
      "exploration/Actions Min                                -0.990604\n",
      "exploration/Num Paths                                   8\n",
      "exploration/Average Returns                            -0.952197\n",
      "exploration/env_infos/final/reward_forward Mean         0.0883616\n",
      "exploration/env_infos/final/reward_forward Std          0.425777\n",
      "exploration/env_infos/final/reward_forward Max          1.09196\n",
      "exploration/env_infos/final/reward_forward Min         -0.439071\n",
      "exploration/env_infos/initial/reward_forward Mean       0.063635\n",
      "exploration/env_infos/initial/reward_forward Std        0.113031\n",
      "exploration/env_infos/initial/reward_forward Max        0.249032\n",
      "exploration/env_infos/initial/reward_forward Min       -0.118255\n",
      "exploration/env_infos/reward_forward Mean              -0.0881416\n",
      "exploration/env_infos/reward_forward Std                0.667194\n",
      "exploration/env_infos/reward_forward Max                2.37919\n",
      "exploration/env_infos/reward_forward Min               -2.82616\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.09874\n",
      "exploration/env_infos/final/reward_ctrl Std             0.343954\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.522644\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.55996\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.07318\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.433999\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.279553\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.65\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.1285\n",
      "exploration/env_infos/reward_ctrl Std                   0.377221\n",
      "exploration/env_infos/reward_ctrl Max                  -0.133902\n",
      "exploration/env_infos/reward_ctrl Min                  -2.47988\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.65077\n",
      "exploration/env_infos/final/torso_velocity Std          0.822348\n",
      "exploration/env_infos/final/torso_velocity Max          3.06315\n",
      "exploration/env_infos/final/torso_velocity Min         -0.516348\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.203078\n",
      "exploration/env_infos/initial/torso_velocity Std        0.236715\n",
      "exploration/env_infos/initial/torso_velocity Max        0.594817\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.209315\n",
      "exploration/env_infos/torso_velocity Mean              -0.0355714\n",
      "exploration/env_infos/torso_velocity Std                0.649521\n",
      "exploration/env_infos/torso_velocity Max                3.26294\n",
      "exploration/env_infos/torso_velocity Min               -2.82616\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.955986\n",
      "evaluation/Rewards Std                                  0.0253807\n",
      "evaluation/Rewards Max                                  2.43765\n",
      "evaluation/Rewards Min                                  0.721917\n",
      "evaluation/Returns Mean                               955.986\n",
      "evaluation/Returns Std                                  9.93094\n",
      "evaluation/Returns Max                                971.552\n",
      "evaluation/Returns Min                                938.38\n",
      "evaluation/Actions Mean                                -0.00242458\n",
      "evaluation/Actions Std                                  0.106007\n",
      "evaluation/Actions Max                                  0.436962\n",
      "evaluation/Actions Min                                 -0.531794\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            955.986\n",
      "evaluation/env_infos/final/reward_forward Mean          7.12313e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.43334e-06\n",
      "evaluation/env_infos/final/reward_forward Max           1.73902e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -9.5483e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0324907\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0921489\n",
      "evaluation/env_infos/initial/reward_forward Max         0.136347\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.193912\n",
      "evaluation/env_infos/reward_forward Mean               -0.00447401\n",
      "evaluation/env_infos/reward_forward Std                 0.0676282\n",
      "evaluation/env_infos/reward_forward Max                 0.873419\n",
      "evaluation/env_infos/reward_forward Min                -1.70711\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0444427\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0103088\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0328478\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0631516\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.055407\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.014216\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0344417\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0968385\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0449732\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0122288\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0234079\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.278083\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.72233e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           2.91726e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           1.73902e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.3593e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.133936\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.230082\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.626779\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.262438\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00310199\n",
      "evaluation/env_infos/torso_velocity Std                 0.0596079\n",
      "evaluation/env_infos/torso_velocity Max                 1.19611\n",
      "evaluation/env_infos/torso_velocity Min                -2.05693\n",
      "time/data storing (s)                                   0.0174526\n",
      "time/evaluation sampling (s)                           46.7055\n",
      "time/exploration sampling (s)                           1.92456\n",
      "time/logging (s)                                        0.269214\n",
      "time/saving (s)                                         0.0296992\n",
      "time/training (s)                                       4.08792\n",
      "time/epoch (s)                                         53.0343\n",
      "time/total (s)                                        499.577\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:06.234873 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  11000\n",
      "trainer/QF1 Loss                                        0.352693\n",
      "trainer/QF2 Loss                                        0.347967\n",
      "trainer/Policy Loss                                    -6.34144\n",
      "trainer/Q1 Predictions Mean                             3.39027\n",
      "trainer/Q1 Predictions Std                              0.486507\n",
      "trainer/Q1 Predictions Max                              4.89862\n",
      "trainer/Q1 Predictions Min                              1.7528\n",
      "trainer/Q2 Predictions Mean                             3.39143\n",
      "trainer/Q2 Predictions Std                              0.443038\n",
      "trainer/Q2 Predictions Max                              4.87118\n",
      "trainer/Q2 Predictions Min                              2.00957\n",
      "trainer/Q Targets Mean                                  3.4206\n",
      "trainer/Q Targets Std                                   0.771771\n",
      "trainer/Q Targets Max                                   5.53775\n",
      "trainer/Q Targets Min                                  -0.816375\n",
      "trainer/Log Pis Mean                                   -2.65963\n",
      "trainer/Log Pis Std                                     1.58084\n",
      "trainer/Log Pis Max                                     1.47672\n",
      "trainer/Log Pis Min                                    -8.36716\n",
      "trainer/Policy mu Mean                                 -0.00345547\n",
      "trainer/Policy mu Std                                   0.239321\n",
      "trainer/Policy mu Max                                   1.30353\n",
      "trainer/Policy mu Min                                  -1.12753\n",
      "trainer/Policy log std Mean                            -0.853317\n",
      "trainer/Policy log std Std                              0.266814\n",
      "trainer/Policy log std Max                             -0.323792\n",
      "trainer/Policy log std Min                             -1.56753\n",
      "trainer/Alpha                                           0.0695943\n",
      "trainer/Alpha Loss                                    -28.3821\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            71\n",
      "exploration/path length Mean                          250\n",
      "exploration/path length Std                           299.883\n",
      "exploration/path length Max                           764\n",
      "exploration/path length Min                            16\n",
      "exploration/Rewards Mean                                0.369081\n",
      "exploration/Rewards Std                                 0.314746\n",
      "exploration/Rewards Max                                 2.02772\n",
      "exploration/Rewards Min                                -0.559494\n",
      "exploration/Returns Mean                               92.2702\n",
      "exploration/Returns Std                               110.139\n",
      "exploration/Returns Max                               280.327\n",
      "exploration/Returns Min                                 3.86671\n",
      "exploration/Actions Mean                               -0.0111593\n",
      "exploration/Actions Std                                 0.411224\n",
      "exploration/Actions Max                                 0.971445\n",
      "exploration/Actions Min                                -0.986502\n",
      "exploration/Num Paths                                   4\n",
      "exploration/Average Returns                            92.2702\n",
      "exploration/env_infos/final/reward_forward Mean         0.575067\n",
      "exploration/env_infos/final/reward_forward Std          0.513372\n",
      "exploration/env_infos/final/reward_forward Max          1.33167\n",
      "exploration/env_infos/final/reward_forward Min          0.0413152\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0616485\n",
      "exploration/env_infos/initial/reward_forward Std        0.171489\n",
      "exploration/env_infos/initial/reward_forward Max        0.307152\n",
      "exploration/env_infos/initial/reward_forward Min       -0.148834\n",
      "exploration/env_infos/reward_forward Mean              -0.00241924\n",
      "exploration/env_infos/reward_forward Std                0.431256\n",
      "exploration/env_infos/reward_forward Max                1.62958\n",
      "exploration/env_infos/reward_forward Min               -1.39156\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.83662\n",
      "exploration/env_infos/final/reward_ctrl Std             0.211713\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.544257\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.14275\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.911538\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.267806\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.673789\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.34984\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.676919\n",
      "exploration/env_infos/reward_ctrl Std                   0.270696\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0807195\n",
      "exploration/env_infos/reward_ctrl Min                  -1.82361\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.567232\n",
      "exploration/env_infos/final/torso_velocity Std          0.745174\n",
      "exploration/env_infos/final/torso_velocity Max          2.19648\n",
      "exploration/env_infos/final/torso_velocity Min         -0.371205\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.244703\n",
      "exploration/env_infos/initial/torso_velocity Std        0.222901\n",
      "exploration/env_infos/initial/torso_velocity Max        0.601478\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.148834\n",
      "exploration/env_infos/torso_velocity Mean               0.00317154\n",
      "exploration/env_infos/torso_velocity Std                0.450184\n",
      "exploration/env_infos/torso_velocity Max                3.3168\n",
      "exploration/env_infos/torso_velocity Min               -2.15897\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.966421\n",
      "evaluation/Rewards Std                                  0.114168\n",
      "evaluation/Rewards Max                                  2.80369\n",
      "evaluation/Rewards Min                                  0.572154\n",
      "evaluation/Returns Mean                               966.421\n",
      "evaluation/Returns Std                                 31.2305\n",
      "evaluation/Returns Max                               1037.48\n",
      "evaluation/Returns Min                                935.651\n",
      "evaluation/Actions Mean                                -0.0516555\n",
      "evaluation/Actions Std                                  0.107625\n",
      "evaluation/Actions Max                                  0.643813\n",
      "evaluation/Actions Min                                 -0.713367\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            966.421\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0334652\n",
      "evaluation/env_infos/final/reward_forward Std           0.303698\n",
      "evaluation/env_infos/final/reward_forward Max           0.499537\n",
      "evaluation/env_infos/final/reward_forward Min          -0.811316\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00848573\n",
      "evaluation/env_infos/initial/reward_forward Std         0.115545\n",
      "evaluation/env_infos/initial/reward_forward Max         0.154741\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.214416\n",
      "evaluation/env_infos/reward_forward Mean               -0.0169982\n",
      "evaluation/env_infos/reward_forward Std                 0.267676\n",
      "evaluation/env_infos/reward_forward Max                 1.30303\n",
      "evaluation/env_infos/reward_forward Min                -1.81804\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.048341\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0120882\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0175894\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0773558\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0369476\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0103918\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0230847\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0734902\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0570061\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0254806\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.013657\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.427846\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0367619\n",
      "evaluation/env_infos/final/torso_velocity Std           0.297403\n",
      "evaluation/env_infos/final/torso_velocity Max           0.717423\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.928458\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135925\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244095\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.610949\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.377476\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0148728\n",
      "evaluation/env_infos/torso_velocity Std                 0.26997\n",
      "evaluation/env_infos/torso_velocity Max                 1.96004\n",
      "evaluation/env_infos/torso_velocity Min                -1.84354\n",
      "time/data storing (s)                                   0.0162019\n",
      "time/evaluation sampling (s)                           47.5281\n",
      "time/exploration sampling (s)                           1.90919\n",
      "time/logging (s)                                        0.299586\n",
      "time/saving (s)                                         0.0352908\n",
      "time/training (s)                                       5.71628\n",
      "time/epoch (s)                                         55.5047\n",
      "time/total (s)                                        555.325\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:59.930467 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 10 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.215878\n",
      "trainer/QF2 Loss                                        0.192199\n",
      "trainer/Policy Loss                                    -3.78472\n",
      "trainer/Q1 Predictions Mean                             3.62399\n",
      "trainer/Q1 Predictions Std                              0.558928\n",
      "trainer/Q1 Predictions Max                              5.21975\n",
      "trainer/Q1 Predictions Min                              1.77923\n",
      "trainer/Q2 Predictions Mean                             3.63444\n",
      "trainer/Q2 Predictions Std                              0.60472\n",
      "trainer/Q2 Predictions Max                              5.64152\n",
      "trainer/Q2 Predictions Min                              1.51761\n",
      "trainer/Q Targets Mean                                  3.61092\n",
      "trainer/Q Targets Std                                   0.770388\n",
      "trainer/Q Targets Max                                   6.65008\n",
      "trainer/Q Targets Min                                  -1.0034\n",
      "trainer/Log Pis Mean                                    0.39302\n",
      "trainer/Log Pis Std                                     1.91565\n",
      "trainer/Log Pis Max                                     4.61608\n",
      "trainer/Log Pis Min                                    -5.62366\n",
      "trainer/Policy mu Mean                                 -0.0234698\n",
      "trainer/Policy mu Std                                   0.191757\n",
      "trainer/Policy mu Max                                   0.908153\n",
      "trainer/Policy mu Min                                  -1.1868\n",
      "trainer/Policy log std Mean                            -1.35806\n",
      "trainer/Policy log std Std                              0.319369\n",
      "trainer/Policy log std Max                             -0.496717\n",
      "trainer/Policy log std Min                             -2.1242\n",
      "trainer/Alpha                                           0.0556006\n",
      "trainer/Alpha Loss                                    -21.9656\n",
      "exploration/num steps total                         12000\n",
      "exploration/num paths total                            72\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.720513\n",
      "exploration/Rewards Std                                 0.251557\n",
      "exploration/Rewards Max                                 2.587\n",
      "exploration/Rewards Min                                -0.136134\n",
      "exploration/Returns Mean                              720.513\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               720.513\n",
      "exploration/Returns Min                               720.513\n",
      "exploration/Actions Mean                               -0.0319534\n",
      "exploration/Actions Std                                 0.283886\n",
      "exploration/Actions Max                                 0.905207\n",
      "exploration/Actions Min                                -0.963321\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           720.513\n",
      "exploration/env_infos/final/reward_forward Mean        -0.471251\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.471251\n",
      "exploration/env_infos/final/reward_forward Min         -0.471251\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0324961\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0324961\n",
      "exploration/env_infos/initial/reward_forward Min        0.0324961\n",
      "exploration/env_infos/reward_forward Mean              -0.00696577\n",
      "exploration/env_infos/reward_forward Std                0.468272\n",
      "exploration/env_infos/reward_forward Max                1.37853\n",
      "exploration/env_infos/reward_forward Min               -1.69239\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.534913\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.534913\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.534913\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.726958\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.726958\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.726958\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.326449\n",
      "exploration/env_infos/reward_ctrl Std                   0.15861\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0457677\n",
      "exploration/env_infos/reward_ctrl Min                  -1.13613\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.328372\n",
      "exploration/env_infos/final/torso_velocity Std          0.121933\n",
      "exploration/env_infos/final/torso_velocity Max         -0.173322\n",
      "exploration/env_infos/final/torso_velocity Min         -0.471251\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.179111\n",
      "exploration/env_infos/initial/torso_velocity Std        0.165761\n",
      "exploration/env_infos/initial/torso_velocity Max        0.410826\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0324961\n",
      "exploration/env_infos/torso_velocity Mean               0.00440372\n",
      "exploration/env_infos/torso_velocity Std                0.464254\n",
      "exploration/env_infos/torso_velocity Max                1.73107\n",
      "exploration/env_infos/torso_velocity Min               -1.69239\n",
      "evaluation/num steps total                         275000\n",
      "evaluation/num paths total                            275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.926768\n",
      "evaluation/Rewards Std                                  0.0704853\n",
      "evaluation/Rewards Max                                  2.3597\n",
      "evaluation/Rewards Min                                  0.778333\n",
      "evaluation/Returns Mean                               926.768\n",
      "evaluation/Returns Std                                 26.1957\n",
      "evaluation/Returns Max                                995.91\n",
      "evaluation/Returns Min                                901.219\n",
      "evaluation/Actions Mean                                -0.0600359\n",
      "evaluation/Actions Std                                  0.135648\n",
      "evaluation/Actions Max                                  0.511157\n",
      "evaluation/Actions Min                                 -0.603202\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            926.768\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0215659\n",
      "evaluation/env_infos/final/reward_forward Std           0.0952522\n",
      "evaluation/env_infos/final/reward_forward Max           0.225078\n",
      "evaluation/env_infos/final/reward_forward Min          -0.180616\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0616924\n",
      "evaluation/env_infos/initial/reward_forward Std         0.127194\n",
      "evaluation/env_infos/initial/reward_forward Max         0.161244\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.275544\n",
      "evaluation/env_infos/reward_forward Mean               -0.00268484\n",
      "evaluation/env_infos/reward_forward Std                 0.195863\n",
      "evaluation/env_infos/reward_forward Max                 1.31254\n",
      "evaluation/env_infos/reward_forward Min                -1.2294\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0927063\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0169106\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0539719\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.119484\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0502014\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0142895\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.028386\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.080296\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0880183\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0176444\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0189782\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.221667\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00600181\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0658692\n",
      "evaluation/env_infos/final/torso_velocity Max           0.225078\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.180616\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.127336\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.255053\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.696897\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.275544\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000631481\n",
      "evaluation/env_infos/torso_velocity Std                 0.138266\n",
      "evaluation/env_infos/torso_velocity Max                 1.52596\n",
      "evaluation/env_infos/torso_velocity Min                -2.0674\n",
      "time/data storing (s)                                   0.0145912\n",
      "time/evaluation sampling (s)                           47.2138\n",
      "time/exploration sampling (s)                           1.90523\n",
      "time/logging (s)                                        0.271436\n",
      "time/saving (s)                                         0.0272231\n",
      "time/training (s)                                       3.9795\n",
      "time/epoch (s)                                         53.4118\n",
      "time/total (s)                                        608.992\n",
      "Epoch                                                  10\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:35:54.543593 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 11 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  13000\n",
      "trainer/QF1 Loss                                        0.243459\n",
      "trainer/QF2 Loss                                        0.237247\n",
      "trainer/Policy Loss                                    -2.57451\n",
      "trainer/Q1 Predictions Mean                             3.6087\n",
      "trainer/Q1 Predictions Std                              0.581998\n",
      "trainer/Q1 Predictions Max                              5.07759\n",
      "trainer/Q1 Predictions Min                              2.33904\n",
      "trainer/Q2 Predictions Mean                             3.6397\n",
      "trainer/Q2 Predictions Std                              0.598441\n",
      "trainer/Q2 Predictions Max                              5.19104\n",
      "trainer/Q2 Predictions Min                              2.43091\n",
      "trainer/Q Targets Mean                                  3.67773\n",
      "trainer/Q Targets Std                                   0.763095\n",
      "trainer/Q Targets Max                                   7.20911\n",
      "trainer/Q Targets Min                                  -0.426959\n",
      "trainer/Log Pis Mean                                    1.84695\n",
      "trainer/Log Pis Std                                     2.15464\n",
      "trainer/Log Pis Max                                     5.49473\n",
      "trainer/Log Pis Min                                    -6.75053\n",
      "trainer/Policy mu Mean                                  0.0050273\n",
      "trainer/Policy mu Std                                   0.163882\n",
      "trainer/Policy mu Max                                   1.01827\n",
      "trainer/Policy mu Min                                  -0.793157\n",
      "trainer/Policy log std Mean                            -1.56881\n",
      "trainer/Policy log std Std                              0.259378\n",
      "trainer/Policy log std Max                             -0.599536\n",
      "trainer/Policy log std Min                             -2.07753\n",
      "trainer/Alpha                                           0.0468529\n",
      "trainer/Alpha Loss                                    -18.8233\n",
      "exploration/num steps total                         13000\n",
      "exploration/num paths total                            73\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.862512\n",
      "exploration/Rewards Std                                 0.249739\n",
      "exploration/Rewards Max                                 2.30107\n",
      "exploration/Rewards Min                                 0.146662\n",
      "exploration/Returns Mean                              862.512\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               862.512\n",
      "exploration/Returns Min                               862.512\n",
      "exploration/Actions Mean                                0.00500763\n",
      "exploration/Actions Std                                 0.228138\n",
      "exploration/Actions Max                                 0.885198\n",
      "exploration/Actions Min                                -0.901981\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           862.512\n",
      "exploration/env_infos/final/reward_forward Mean        -0.180787\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.180787\n",
      "exploration/env_infos/final/reward_forward Min         -0.180787\n",
      "exploration/env_infos/initial/reward_forward Mean       0.00749848\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.00749848\n",
      "exploration/env_infos/initial/reward_forward Min        0.00749848\n",
      "exploration/env_infos/reward_forward Mean               0.0520703\n",
      "exploration/env_infos/reward_forward Std                0.45317\n",
      "exploration/env_infos/reward_forward Max                2.07712\n",
      "exploration/env_infos/reward_forward Min               -1.21278\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.287324\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.287324\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.287324\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.163129\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.163129\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.163129\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.208288\n",
      "exploration/env_infos/reward_ctrl Std                   0.114275\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0282673\n",
      "exploration/env_infos/reward_ctrl Min                  -0.853338\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0311867\n",
      "exploration/env_infos/final/torso_velocity Std          0.225028\n",
      "exploration/env_infos/final/torso_velocity Max          0.342738\n",
      "exploration/env_infos/final/torso_velocity Min         -0.180787\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.149277\n",
      "exploration/env_infos/initial/torso_velocity Std        0.272194\n",
      "exploration/env_infos/initial/torso_velocity Max        0.530099\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0897657\n",
      "exploration/env_infos/torso_velocity Mean               0.00280963\n",
      "exploration/env_infos/torso_velocity Std                0.475657\n",
      "exploration/env_infos/torso_velocity Max                2.10983\n",
      "exploration/env_infos/torso_velocity Min               -1.62837\n",
      "evaluation/num steps total                         300000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.986195\n",
      "evaluation/Rewards Std                                  0.0258546\n",
      "evaluation/Rewards Max                                  2.28232\n",
      "evaluation/Rewards Min                                  0.655436\n",
      "evaluation/Returns Mean                               986.195\n",
      "evaluation/Returns Std                                  3.72361\n",
      "evaluation/Returns Max                               1000.34\n",
      "evaluation/Returns Min                                980.364\n",
      "evaluation/Actions Mean                                 0.0260267\n",
      "evaluation/Actions Std                                  0.0557579\n",
      "evaluation/Actions Max                                  0.48598\n",
      "evaluation/Actions Min                                 -0.557848\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            986.195\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00115915\n",
      "evaluation/env_infos/final/reward_forward Std           0.00329866\n",
      "evaluation/env_infos/final/reward_forward Max           0.000102558\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0127725\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0507181\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124131\n",
      "evaluation/env_infos/initial/reward_forward Max         0.32988\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.139938\n",
      "evaluation/env_infos/reward_forward Mean                0.00209979\n",
      "evaluation/env_infos/reward_forward Std                 0.0667948\n",
      "evaluation/env_infos/reward_forward Max                 1.67952\n",
      "evaluation/env_infos/reward_forward Min                -0.953756\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0146682\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00329014\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0074879\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0259166\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0287018\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0102685\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0104413\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0480664\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0151453\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00774719\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00159981\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.344564\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000289378\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00479232\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0336208\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0127725\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.155094\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.228972\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.591313\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.364614\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00161576\n",
      "evaluation/env_infos/torso_velocity Std                 0.0678817\n",
      "evaluation/env_infos/torso_velocity Max                 1.67952\n",
      "evaluation/env_infos/torso_velocity Min                -2.09854\n",
      "time/data storing (s)                                   0.0148379\n",
      "time/evaluation sampling (s)                           48.1294\n",
      "time/exploration sampling (s)                           1.99618\n",
      "time/logging (s)                                        0.273341\n",
      "time/saving (s)                                         0.0270561\n",
      "time/training (s)                                       3.926\n",
      "time/epoch (s)                                         54.3668\n",
      "time/total (s)                                        663.607\n",
      "Epoch                                                  11\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 10:36:47.687325 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 12 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.300468\n",
      "trainer/QF2 Loss                                        0.296651\n",
      "trainer/Policy Loss                                    -1.52853\n",
      "trainer/Q1 Predictions Mean                             4.11503\n",
      "trainer/Q1 Predictions Std                              0.73327\n",
      "trainer/Q1 Predictions Max                              6.66688\n",
      "trainer/Q1 Predictions Min                              2.37793\n",
      "trainer/Q2 Predictions Mean                             4.0977\n",
      "trainer/Q2 Predictions Std                              0.736257\n",
      "trainer/Q2 Predictions Max                              6.75353\n",
      "trainer/Q2 Predictions Min                              2.56236\n",
      "trainer/Q Targets Mean                                  4.0605\n",
      "trainer/Q Targets Std                                   0.954234\n",
      "trainer/Q Targets Max                                   8.24084\n",
      "trainer/Q Targets Min                                  -0.794723\n",
      "trainer/Log Pis Mean                                    3.36817\n",
      "trainer/Log Pis Std                                     2.22502\n",
      "trainer/Log Pis Max                                     7.39966\n",
      "trainer/Log Pis Min                                    -7.47284\n",
      "trainer/Policy mu Mean                                 -0.0220242\n",
      "trainer/Policy mu Std                                   0.134838\n",
      "trainer/Policy mu Max                                   0.941569\n",
      "trainer/Policy mu Min                                  -0.769819\n",
      "trainer/Policy log std Mean                            -1.82436\n",
      "trainer/Policy log std Std                              0.246232\n",
      "trainer/Policy log std Max                             -0.86002\n",
      "trainer/Policy log std Min                             -2.42762\n",
      "trainer/Alpha                                           0.0409187\n",
      "trainer/Alpha Loss                                    -14.7984\n",
      "exploration/num steps total                         14000\n",
      "exploration/num paths total                            74\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.914988\n",
      "exploration/Rewards Std                                 0.151693\n",
      "exploration/Rewards Max                                 1.95695\n",
      "exploration/Rewards Min                                 0.552427\n",
      "exploration/Returns Mean                              914.988\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               914.988\n",
      "exploration/Returns Min                               914.988\n",
      "exploration/Actions Mean                               -0.0142025\n",
      "exploration/Actions Std                                 0.173921\n",
      "exploration/Actions Max                                 0.655557\n",
      "exploration/Actions Min                                -0.747532\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           914.988\n",
      "exploration/env_infos/final/reward_forward Mean        -0.219848\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.219848\n",
      "exploration/env_infos/final/reward_forward Min         -0.219848\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0483947\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0483947\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0483947\n",
      "exploration/env_infos/reward_forward Mean               0.0203978\n",
      "exploration/env_infos/reward_forward Std                0.377764\n",
      "exploration/env_infos/reward_forward Max                1.61244\n",
      "exploration/env_infos/reward_forward Min               -0.946183\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.180902\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.180902\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.180902\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.132742\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.132742\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.132742\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.1218\n",
      "exploration/env_infos/reward_ctrl Std                   0.0648566\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0135593\n",
      "exploration/env_infos/reward_ctrl Min                  -0.447573\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0301109\n",
      "exploration/env_infos/final/torso_velocity Std          0.224866\n",
      "exploration/env_infos/final/torso_velocity Max          0.325348\n",
      "exploration/env_infos/final/torso_velocity Min         -0.219848\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.164383\n",
      "exploration/env_infos/initial/torso_velocity Std        0.226641\n",
      "exploration/env_infos/initial/torso_velocity Max        0.478361\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0483947\n",
      "exploration/env_infos/torso_velocity Mean               0.00824177\n",
      "exploration/env_infos/torso_velocity Std                0.382253\n",
      "exploration/env_infos/torso_velocity Max                2.0507\n",
      "exploration/env_infos/torso_velocity Min               -1.25689\n",
      "evaluation/num steps total                         325000\n",
      "evaluation/num paths total                            325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.985334\n",
      "evaluation/Rewards Std                                  0.0213404\n",
      "evaluation/Rewards Max                                  2.2779\n",
      "evaluation/Rewards Min                                  0.815548\n",
      "evaluation/Returns Mean                               985.334\n",
      "evaluation/Returns Std                                  2.59865\n",
      "evaluation/Returns Max                                990.052\n",
      "evaluation/Returns Min                                980.815\n",
      "evaluation/Actions Mean                                -0.0328888\n",
      "evaluation/Actions Std                                  0.052962\n",
      "evaluation/Actions Max                                  0.239739\n",
      "evaluation/Actions Min                                 -0.465661\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            985.334\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000110973\n",
      "evaluation/env_infos/final/reward_forward Std           0.000543651\n",
      "evaluation/env_infos/final/reward_forward Max           0.00277431\n",
      "evaluation/env_infos/final/reward_forward Min          -4.10622e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00719238\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122251\n",
      "evaluation/env_infos/initial/reward_forward Max         0.259278\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.197411\n",
      "evaluation/env_infos/reward_forward Mean                0.00157888\n",
      "evaluation/env_infos/reward_forward Std                 0.0580156\n",
      "evaluation/env_infos/reward_forward Max                 1.61537\n",
      "evaluation/env_infos/reward_forward Min                -1.16446\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0152543\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00236734\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0118033\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0196578\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0213592\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00289406\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0158125\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.026607\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0155466\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00552782\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0027446\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.184452\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.10406e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000339731\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00277431\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000571466\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.131038\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.225908\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.64102\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.231146\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00159961\n",
      "evaluation/env_infos/torso_velocity Std                 0.0557915\n",
      "evaluation/env_infos/torso_velocity Max                 1.61537\n",
      "evaluation/env_infos/torso_velocity Min                -1.81442\n",
      "time/data storing (s)                                   0.0167512\n",
      "time/evaluation sampling (s)                           45.3756\n",
      "time/exploration sampling (s)                           2.2438\n",
      "time/logging (s)                                        0.322074\n",
      "time/saving (s)                                         0.0287124\n",
      "time/training (s)                                       4.93237\n",
      "time/epoch (s)                                         52.9193\n",
      "time/total (s)                                        716.799\n",
      "Epoch                                                  12\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:37:42.277171 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 13 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  15000\n",
      "trainer/QF1 Loss                                        0.185209\n",
      "trainer/QF2 Loss                                        0.181179\n",
      "trainer/Policy Loss                                    -1.03071\n",
      "trainer/Q1 Predictions Mean                             4.45719\n",
      "trainer/Q1 Predictions Std                              0.725891\n",
      "trainer/Q1 Predictions Max                              6.18492\n",
      "trainer/Q1 Predictions Min                              1.75962\n",
      "trainer/Q2 Predictions Mean                             4.43984\n",
      "trainer/Q2 Predictions Std                              0.731336\n",
      "trainer/Q2 Predictions Max                              6.00977\n",
      "trainer/Q2 Predictions Min                              1.81452\n",
      "trainer/Q Targets Mean                                  4.4691\n",
      "trainer/Q Targets Std                                   0.803048\n",
      "trainer/Q Targets Max                                   6.48117\n",
      "trainer/Q Targets Min                                  -0.858889\n",
      "trainer/Log Pis Mean                                    4.26323\n",
      "trainer/Log Pis Std                                     1.9543\n",
      "trainer/Log Pis Max                                     8.23921\n",
      "trainer/Log Pis Min                                    -2.79993\n",
      "trainer/Policy mu Mean                                  0.0198238\n",
      "trainer/Policy mu Std                                   0.130431\n",
      "trainer/Policy mu Max                                   1.12759\n",
      "trainer/Policy mu Min                                  -0.615862\n",
      "trainer/Policy log std Mean                            -1.90505\n",
      "trainer/Policy log std Std                              0.216125\n",
      "trainer/Policy log std Max                             -0.990436\n",
      "trainer/Policy log std Min                             -2.3052\n",
      "trainer/Alpha                                           0.0366923\n",
      "trainer/Alpha Loss                                    -12.3469\n",
      "exploration/num steps total                         15000\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.922072\n",
      "exploration/Rewards Std                                 0.110292\n",
      "exploration/Rewards Max                                 1.8072\n",
      "exploration/Rewards Min                                 0.643184\n",
      "exploration/Returns Mean                              922.072\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               922.072\n",
      "exploration/Returns Min                               922.072\n",
      "exploration/Actions Mean                                0.00573235\n",
      "exploration/Actions Std                                 0.156641\n",
      "exploration/Actions Max                                 0.687473\n",
      "exploration/Actions Min                                -0.589236\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           922.072\n",
      "exploration/env_infos/final/reward_forward Mean        -0.607085\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.607085\n",
      "exploration/env_infos/final/reward_forward Min         -0.607085\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0302671\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0302671\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0302671\n",
      "exploration/env_infos/reward_forward Mean               0.0611408\n",
      "exploration/env_infos/reward_forward Std                0.391528\n",
      "exploration/env_infos/reward_forward Max                1.36513\n",
      "exploration/env_infos/reward_forward Min               -0.959643\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.126824\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.126824\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.126824\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.22027\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.22027\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.22027\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0982765\n",
      "exploration/env_infos/reward_ctrl Std                   0.0526541\n",
      "exploration/env_infos/reward_ctrl Max                  -0.009272\n",
      "exploration/env_infos/reward_ctrl Min                  -0.356816\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.178295\n",
      "exploration/env_infos/final/torso_velocity Std          0.341166\n",
      "exploration/env_infos/final/torso_velocity Max          0.227661\n",
      "exploration/env_infos/final/torso_velocity Min         -0.607085\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.1393\n",
      "exploration/env_infos/initial/torso_velocity Std        0.201578\n",
      "exploration/env_infos/initial/torso_velocity Max        0.422542\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0302671\n",
      "exploration/env_infos/torso_velocity Mean               0.0147313\n",
      "exploration/env_infos/torso_velocity Std                0.364402\n",
      "exploration/env_infos/torso_velocity Max                1.36513\n",
      "exploration/env_infos/torso_velocity Min               -1.3043\n",
      "evaluation/num steps total                         350000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.989537\n",
      "evaluation/Rewards Std                                  0.0245172\n",
      "evaluation/Rewards Max                                  2.55377\n",
      "evaluation/Rewards Min                                  0.872554\n",
      "evaluation/Returns Mean                               989.537\n",
      "evaluation/Returns Std                                  2.95296\n",
      "evaluation/Returns Max                                995.458\n",
      "evaluation/Returns Min                                982.806\n",
      "evaluation/Actions Mean                                 0.017543\n",
      "evaluation/Actions Std                                  0.0507965\n",
      "evaluation/Actions Max                                  0.301704\n",
      "evaluation/Actions Min                                 -0.420771\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            989.537\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0158729\n",
      "evaluation/env_infos/final/reward_forward Std           0.15556\n",
      "evaluation/env_infos/final/reward_forward Max           0.649281\n",
      "evaluation/env_infos/final/reward_forward Min          -0.407157\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00407104\n",
      "evaluation/env_infos/initial/reward_forward Std         0.135322\n",
      "evaluation/env_infos/initial/reward_forward Max         0.248514\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.258861\n",
      "evaluation/env_infos/reward_forward Mean                0.0273263\n",
      "evaluation/env_infos/reward_forward Std                 0.165314\n",
      "evaluation/env_infos/reward_forward Max                 1.52137\n",
      "evaluation/env_infos/reward_forward Min                -1.61311\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0109538\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00295752\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00822658\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0237274\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0191531\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00666971\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0101994\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0321908\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0115521\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0049176\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00330546\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.127446\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00552744\n",
      "evaluation/env_infos/final/torso_velocity Std           0.126031\n",
      "evaluation/env_infos/final/torso_velocity Max           0.649281\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.449693\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.129023\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.234818\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.64937\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.258861\n",
      "evaluation/env_infos/torso_velocity Mean                0.00761219\n",
      "evaluation/env_infos/torso_velocity Std                 0.135618\n",
      "evaluation/env_infos/torso_velocity Max                 1.52137\n",
      "evaluation/env_infos/torso_velocity Min                -1.8944\n",
      "time/data storing (s)                                   0.0154787\n",
      "time/evaluation sampling (s)                           47.9986\n",
      "time/exploration sampling (s)                           1.9326\n",
      "time/logging (s)                                        0.281245\n",
      "time/saving (s)                                         0.0275115\n",
      "time/training (s)                                       3.97222\n",
      "time/epoch (s)                                         54.2277\n",
      "time/total (s)                                        771.347\n",
      "Epoch                                                  13\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:38:35.702259 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 14 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.130802\n",
      "trainer/QF2 Loss                                        0.124571\n",
      "trainer/Policy Loss                                    -0.838889\n",
      "trainer/Q1 Predictions Mean                             4.8411\n",
      "trainer/Q1 Predictions Std                              0.782695\n",
      "trainer/Q1 Predictions Max                              6.33688\n",
      "trainer/Q1 Predictions Min                              2.8736\n",
      "trainer/Q2 Predictions Mean                             4.91726\n",
      "trainer/Q2 Predictions Std                              0.801821\n",
      "trainer/Q2 Predictions Max                              6.54669\n",
      "trainer/Q2 Predictions Min                              2.93423\n",
      "trainer/Q Targets Mean                                  4.8445\n",
      "trainer/Q Targets Std                                   0.80284\n",
      "trainer/Q Targets Max                                   6.65752\n",
      "trainer/Q Targets Min                                   2.85324\n",
      "trainer/Log Pis Mean                                    4.8208\n",
      "trainer/Log Pis Std                                     2.30192\n",
      "trainer/Log Pis Max                                     8.84233\n",
      "trainer/Log Pis Min                                    -3.36251\n",
      "trainer/Policy mu Mean                                  0.00101184\n",
      "trainer/Policy mu Std                                   0.112643\n",
      "trainer/Policy mu Max                                   0.67307\n",
      "trainer/Policy mu Min                                  -0.54107\n",
      "trainer/Policy log std Mean                            -2.00882\n",
      "trainer/Policy log std Std                              0.190214\n",
      "trainer/Policy log std Max                             -1.07875\n",
      "trainer/Policy log std Min                             -2.45207\n",
      "trainer/Alpha                                           0.0334032\n",
      "trainer/Alpha Loss                                    -10.8036\n",
      "exploration/num steps total                         16000\n",
      "exploration/num paths total                            76\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.975519\n",
      "exploration/Rewards Std                                 0.189857\n",
      "exploration/Rewards Max                                 2.43754\n",
      "exploration/Rewards Min                                 0.707243\n",
      "exploration/Returns Mean                              975.519\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               975.519\n",
      "exploration/Returns Min                               975.519\n",
      "exploration/Actions Mean                                0.013375\n",
      "exploration/Actions Std                                 0.14183\n",
      "exploration/Actions Max                                 0.536046\n",
      "exploration/Actions Min                                -0.529589\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           975.519\n",
      "exploration/env_infos/final/reward_forward Mean        -0.237179\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.237179\n",
      "exploration/env_infos/final/reward_forward Min         -0.237179\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.1217\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.1217\n",
      "exploration/env_infos/initial/reward_forward Min       -0.1217\n",
      "exploration/env_infos/reward_forward Mean               0.0347399\n",
      "exploration/env_infos/reward_forward Std                0.273248\n",
      "exploration/env_infos/reward_forward Max                1.39564\n",
      "exploration/env_infos/reward_forward Min               -1.19432\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.125972\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.125972\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.125972\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0511505\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0511505\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0511505\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0811781\n",
      "exploration/env_infos/reward_ctrl Std                   0.0427342\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00873468\n",
      "exploration/env_infos/reward_ctrl Min                  -0.292757\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.268492\n",
      "exploration/env_infos/final/torso_velocity Std          0.422998\n",
      "exploration/env_infos/final/torso_velocity Max          0.233205\n",
      "exploration/env_infos/final/torso_velocity Min         -0.801503\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.188337\n",
      "exploration/env_infos/initial/torso_velocity Std        0.27152\n",
      "exploration/env_infos/initial/torso_velocity Max        0.53955\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.1217\n",
      "exploration/env_infos/torso_velocity Mean              -0.00095164\n",
      "exploration/env_infos/torso_velocity Std                0.330352\n",
      "exploration/env_infos/torso_velocity Max                1.39564\n",
      "exploration/env_infos/torso_velocity Min               -1.36185\n",
      "evaluation/num steps total                         375000\n",
      "evaluation/num paths total                            375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.985021\n",
      "evaluation/Rewards Std                                  0.0338144\n",
      "evaluation/Rewards Max                                  2.74863\n",
      "evaluation/Rewards Min                                  0.812136\n",
      "evaluation/Returns Mean                               985.021\n",
      "evaluation/Returns Std                                  2.79721\n",
      "evaluation/Returns Max                                991.589\n",
      "evaluation/Returns Min                                976.449\n",
      "evaluation/Actions Mean                                 0.0117721\n",
      "evaluation/Actions Std                                  0.0630146\n",
      "evaluation/Actions Max                                  0.330055\n",
      "evaluation/Actions Min                                 -0.497803\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            985.021\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.9418e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.45403e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.67803e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.13928e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0349362\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0897274\n",
      "evaluation/env_infos/initial/reward_forward Max         0.191505\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.120284\n",
      "evaluation/env_infos/reward_forward Mean                0.00330436\n",
      "evaluation/env_infos/reward_forward Std                 0.0522433\n",
      "evaluation/env_infos/reward_forward Max                 1.77491\n",
      "evaluation/env_infos/reward_forward Min                -1.28353\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0159693\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00159391\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.014685\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0230681\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0134189\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00298189\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00837414\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0208537\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0164377\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00586036\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00771773\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.187864\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.16863e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.56356e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.77391e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.13928e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.149566\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.240436\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.614612\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.257483\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00158827\n",
      "evaluation/env_infos/torso_velocity Std                 0.0568573\n",
      "evaluation/env_infos/torso_velocity Max                 1.77491\n",
      "evaluation/env_infos/torso_velocity Min                -1.85948\n",
      "time/data storing (s)                                   0.0150403\n",
      "time/evaluation sampling (s)                           46.684\n",
      "time/exploration sampling (s)                           1.96855\n",
      "time/logging (s)                                        0.280103\n",
      "time/saving (s)                                         0.0263092\n",
      "time/training (s)                                       4.17045\n",
      "time/epoch (s)                                         53.1444\n",
      "time/total (s)                                        824.77\n",
      "Epoch                                                  14\n",
      "-------------------------------------------------  ----------------\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 100 --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 1 --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env antdirectionnewsparse --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456ec7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
