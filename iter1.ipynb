{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154f0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46609d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[2432]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1c46b778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1c4ec740). One of the two will be used. Which one is undefined.\n",
      "objc[2432]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1c46b700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1c4ec768). One of the two will be used. Which one is undefined.\n",
      "objc[2432]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1c46b7a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1c4ec7b8). One of the two will be used. Which one is undefined.\n",
      "objc[2432]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1c46b818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1c4ec830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 19:10:24.923682 PDT | Variant:\n",
      "2021-05-25 19:10:24.924193 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"antdirectionnewsparse\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 300,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 1,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"type\": \"360\"\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"use_xy\": false,\n",
      "    \"contact_forces\": false\n",
      "  }\n",
      "}\n",
      "antdirectionnewsparse\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 19:13:52.776013 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  2000\n",
      "trainer/QF1 Loss                                      23.2101\n",
      "trainer/QF2 Loss                                      23.1957\n",
      "trainer/Policy Loss                                   -5.32882\n",
      "trainer/Q1 Predictions Mean                            0.002654\n",
      "trainer/Q1 Predictions Std                             0.00352299\n",
      "trainer/Q1 Predictions Max                             0.0109309\n",
      "trainer/Q1 Predictions Min                            -0.0077538\n",
      "trainer/Q2 Predictions Mean                            0.00409907\n",
      "trainer/Q2 Predictions Std                             0.00338497\n",
      "trainer/Q2 Predictions Max                             0.0158646\n",
      "trainer/Q2 Predictions Min                            -0.0040882\n",
      "trainer/Q Targets Mean                                 4.73125\n",
      "trainer/Q Targets Std                                  0.922068\n",
      "trainer/Q Targets Max                                  8.58693\n",
      "trainer/Q Targets Min                                 -1.00047\n",
      "trainer/Log Pis Mean                                  -5.32764\n",
      "trainer/Log Pis Std                                    0.609224\n",
      "trainer/Log Pis Max                                   -3.47445\n",
      "trainer/Log Pis Min                                   -7.64087\n",
      "trainer/Policy mu Mean                                -0.000252314\n",
      "trainer/Policy mu Std                                  0.00207742\n",
      "trainer/Policy mu Max                                  0.00616373\n",
      "trainer/Policy mu Min                                 -0.00709288\n",
      "trainer/Policy log std Mean                           -3.87293e-05\n",
      "trainer/Policy log std Std                             0.00219357\n",
      "trainer/Policy log std Max                             0.00970214\n",
      "trainer/Policy log std Min                            -0.00665891\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                           14\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.486789\n",
      "exploration/Rewards Std                                0.505592\n",
      "exploration/Rewards Max                                2.10966\n",
      "exploration/Rewards Min                               -1.90001\n",
      "exploration/Returns Mean                            -486.789\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -486.789\n",
      "exploration/Returns Min                             -486.789\n",
      "exploration/Actions Mean                               0.00464901\n",
      "exploration/Actions Std                                0.62244\n",
      "exploration/Actions Max                                0.999664\n",
      "exploration/Actions Min                               -0.999062\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -486.789\n",
      "exploration/env_infos/final/reward_forward Mean        0.603093\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.603093\n",
      "exploration/env_infos/final/reward_forward Min         0.603093\n",
      "exploration/env_infos/initial/reward_forward Mean      0.189068\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.189068\n",
      "exploration/env_infos/initial/reward_forward Min       0.189068\n",
      "exploration/env_infos/reward_forward Mean             -0.0201438\n",
      "exploration/env_infos/reward_forward Std               0.467217\n",
      "exploration/env_infos/reward_forward Max               1.89643\n",
      "exploration/env_infos/reward_forward Min              -2.2633\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.34172\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.34172\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.34172\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.75345\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.75345\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.75345\n",
      "exploration/env_infos/reward_ctrl Mean                -1.54981\n",
      "exploration/env_infos/reward_ctrl Std                  0.438094\n",
      "exploration/env_infos/reward_ctrl Max                 -0.478097\n",
      "exploration/env_infos/reward_ctrl Min                 -2.90001\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.00423088\n",
      "exploration/env_infos/final/torso_velocity Std         0.423936\n",
      "exploration/env_infos/final/torso_velocity Max         0.603093\n",
      "exploration/env_infos/final/torso_velocity Min        -0.319822\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.176624\n",
      "exploration/env_infos/initial/torso_velocity Std       0.416636\n",
      "exploration/env_infos/initial/torso_velocity Max       0.680562\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.339757\n",
      "exploration/env_infos/torso_velocity Mean              0.0191929\n",
      "exploration/env_infos/torso_velocity Std               0.465564\n",
      "exploration/env_infos/torso_velocity Max               2.46643\n",
      "exploration/env_infos/torso_velocity Min              -2.2633\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.00075\n",
      "evaluation/Rewards Std                                 0.0144579\n",
      "evaluation/Rewards Max                                 2.0018\n",
      "evaluation/Rewards Min                                 0.999983\n",
      "evaluation/Returns Mean                             1000.75\n",
      "evaluation/Returns Std                                 0.642567\n",
      "evaluation/Returns Max                              1001.77\n",
      "evaluation/Returns Min                               999.994\n",
      "evaluation/Actions Mean                               -0.000137569\n",
      "evaluation/Actions Std                                 0.00114683\n",
      "evaluation/Actions Max                                 0.00346976\n",
      "evaluation/Actions Min                                -0.00447905\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1000.75\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000331287\n",
      "evaluation/env_infos/final/reward_forward Std          0.000386865\n",
      "evaluation/env_infos/final/reward_forward Max          0.00114029\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000109555\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0294231\n",
      "evaluation/env_infos/initial/reward_forward Std        0.127246\n",
      "evaluation/env_infos/initial/reward_forward Max        0.192246\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.210905\n",
      "evaluation/env_infos/reward_forward Mean               0.00215038\n",
      "evaluation/env_infos/reward_forward Std                0.0446932\n",
      "evaluation/env_infos/reward_forward Max                1.15621\n",
      "evaluation/env_infos/reward_forward Min               -1.32485\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -5.30988e-06\n",
      "evaluation/env_infos/final/reward_ctrl Std             6.25347e-07\n",
      "evaluation/env_infos/final/reward_ctrl Max            -4.31243e-06\n",
      "evaluation/env_infos/final/reward_ctrl Min            -6.64618e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -6.40871e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           4.97117e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -5.57393e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -7.30387e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -5.33658e-06\n",
      "evaluation/env_infos/reward_ctrl Std                   7.79679e-07\n",
      "evaluation/env_infos/reward_ctrl Max                  -3.20538e-06\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.71268e-05\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000245719\n",
      "evaluation/env_infos/final/torso_velocity Std          0.00049244\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00202069\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000121812\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.126558\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.239458\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.590864\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.31396\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000954153\n",
      "evaluation/env_infos/torso_velocity Std                0.0496868\n",
      "evaluation/env_infos/torso_velocity Max                1.15621\n",
      "evaluation/env_infos/torso_velocity Min               -2.16232\n",
      "time/data storing (s)                                  0.0609484\n",
      "time/evaluation sampling (s)                         178.229\n",
      "time/exploration sampling (s)                          7.80692\n",
      "time/logging (s)                                       1.33134\n",
      "time/saving (s)                                        0.426052\n",
      "time/training (s)                                     18.8109\n",
      "time/epoch (s)                                       206.665\n",
      "time/total (s)                                       212.016\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:16:04.133965 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  3000\n",
      "trainer/QF1 Loss                                       0.891577\n",
      "trainer/QF2 Loss                                       0.872106\n",
      "trainer/Policy Loss                                   -9.45289\n",
      "trainer/Q1 Predictions Mean                            4.02009\n",
      "trainer/Q1 Predictions Std                             0.366896\n",
      "trainer/Q1 Predictions Max                             4.96062\n",
      "trainer/Q1 Predictions Min                             2.89914\n",
      "trainer/Q2 Predictions Mean                            4.00955\n",
      "trainer/Q2 Predictions Std                             0.356586\n",
      "trainer/Q2 Predictions Max                             4.8696\n",
      "trainer/Q2 Predictions Min                             2.9338\n",
      "trainer/Q Targets Mean                                 3.97322\n",
      "trainer/Q Targets Std                                  0.850076\n",
      "trainer/Q Targets Max                                  6.98097\n",
      "trainer/Q Targets Min                                 -1.2366\n",
      "trainer/Log Pis Mean                                  -5.46558\n",
      "trainer/Log Pis Std                                    0.436939\n",
      "trainer/Log Pis Max                                   -4.50292\n",
      "trainer/Log Pis Min                                   -9.2674\n",
      "trainer/Policy mu Mean                                 0.00689497\n",
      "trainer/Policy mu Std                                  0.0256551\n",
      "trainer/Policy mu Max                                  0.0907521\n",
      "trainer/Policy mu Min                                 -0.101843\n",
      "trainer/Policy log std Mean                           -0.108037\n",
      "trainer/Policy log std Std                             0.0223251\n",
      "trainer/Policy log std Max                            -0.0566616\n",
      "trainer/Policy log std Min                            -0.216261\n",
      "trainer/Alpha                                          0.738529\n",
      "trainer/Alpha Loss                                    -4.04092\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                           28\n",
      "exploration/path length Mean                          71.4286\n",
      "exploration/path length Std                           58.3312\n",
      "exploration/path length Max                          188\n",
      "exploration/path length Min                           13\n",
      "exploration/Rewards Mean                              -0.368238\n",
      "exploration/Rewards Std                                0.515178\n",
      "exploration/Rewards Max                                2.36459\n",
      "exploration/Rewards Min                               -1.81222\n",
      "exploration/Returns Mean                             -26.3027\n",
      "exploration/Returns Std                               21.0402\n",
      "exploration/Returns Max                               -4.61011\n",
      "exploration/Returns Min                              -72.5586\n",
      "exploration/Actions Mean                              -0.00331888\n",
      "exploration/Actions Std                                0.599887\n",
      "exploration/Actions Max                                0.998125\n",
      "exploration/Actions Min                               -0.999006\n",
      "exploration/Num Paths                                 14\n",
      "exploration/Average Returns                          -26.3027\n",
      "exploration/env_infos/final/reward_forward Mean        0.696565\n",
      "exploration/env_infos/final/reward_forward Std         0.833408\n",
      "exploration/env_infos/final/reward_forward Max         2.3686\n",
      "exploration/env_infos/final/reward_forward Min        -0.909013\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0992336\n",
      "exploration/env_infos/initial/reward_forward Std       0.185191\n",
      "exploration/env_infos/initial/reward_forward Max       0.415744\n",
      "exploration/env_infos/initial/reward_forward Min      -0.236347\n",
      "exploration/env_infos/reward_forward Mean             -0.0337696\n",
      "exploration/env_infos/reward_forward Std               0.865071\n",
      "exploration/env_infos/reward_forward Max               3.60913\n",
      "exploration/env_infos/reward_forward Min              -2.40995\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.61592\n",
      "exploration/env_infos/final/reward_ctrl Std            0.320111\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.910348\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.2366\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.36221\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.382885\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.422681\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.00077\n",
      "exploration/env_infos/reward_ctrl Mean                -1.4395\n",
      "exploration/env_infos/reward_ctrl Std                  0.412295\n",
      "exploration/env_infos/reward_ctrl Max                 -0.422681\n",
      "exploration/env_infos/reward_ctrl Min                 -2.81222\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.756625\n",
      "exploration/env_infos/final/torso_velocity Std         1.24402\n",
      "exploration/env_infos/final/torso_velocity Max         5.10339\n",
      "exploration/env_infos/final/torso_velocity Min        -1.23923\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.193938\n",
      "exploration/env_infos/initial/torso_velocity Std       0.233372\n",
      "exploration/env_infos/initial/torso_velocity Max       0.687556\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.236347\n",
      "exploration/env_infos/torso_velocity Mean              0.00119781\n",
      "exploration/env_infos/torso_velocity Std               0.90966\n",
      "exploration/env_infos/torso_velocity Max               5.10339\n",
      "exploration/env_infos/torso_velocity Min              -3.48\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.999009\n",
      "evaluation/Rewards Std                                 0.0253015\n",
      "evaluation/Rewards Max                                 2.37401\n",
      "evaluation/Rewards Min                                 0.995808\n",
      "evaluation/Returns Mean                              999.009\n",
      "evaluation/Returns Std                                 1.44527\n",
      "evaluation/Returns Max                              1002.38\n",
      "evaluation/Returns Min                               997.288\n",
      "evaluation/Actions Mean                                0.00716488\n",
      "evaluation/Actions Std                                 0.0219606\n",
      "evaluation/Actions Max                                 0.0649693\n",
      "evaluation/Actions Min                                -0.0281148\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           999.009\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000116947\n",
      "evaluation/env_infos/final/reward_forward Std          0.000495695\n",
      "evaluation/env_infos/final/reward_forward Max          0.00228312\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000400171\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00556799\n",
      "evaluation/env_infos/initial/reward_forward Std        0.119222\n",
      "evaluation/env_infos/initial/reward_forward Max        0.237456\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.223158\n",
      "evaluation/env_infos/reward_forward Mean               0.00503562\n",
      "evaluation/env_infos/reward_forward Std                0.0506982\n",
      "evaluation/env_infos/reward_forward Max                1.30463\n",
      "evaluation/env_infos/reward_forward Min               -0.908845\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00214195\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000535365\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00137516\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00274599\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00136759\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000329616\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000762963\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0018847\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00213441\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000538443\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000762963\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00419232\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         5.58546e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000339159\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00228312\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000564497\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.148933\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.236715\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.582916\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.223158\n",
      "evaluation/env_infos/torso_velocity Mean              -0.001655\n",
      "evaluation/env_infos/torso_velocity Std                0.0579078\n",
      "evaluation/env_infos/torso_velocity Max                1.30463\n",
      "evaluation/env_infos/torso_velocity Min               -1.84325\n",
      "time/data storing (s)                                  0.0187246\n",
      "time/evaluation sampling (s)                         123.268\n",
      "time/exploration sampling (s)                          1.84213\n",
      "time/logging (s)                                       0.294173\n",
      "time/saving (s)                                        0.0288357\n",
      "time/training (s)                                      3.30327\n",
      "time/epoch (s)                                       128.755\n",
      "time/total (s)                                       342.312\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:16:54.726157 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                       0.603775\n",
      "trainer/QF2 Loss                                       0.61448\n",
      "trainer/Policy Loss                                   -9.01318\n",
      "trainer/Q1 Predictions Mean                            3.57897\n",
      "trainer/Q1 Predictions Std                             0.351777\n",
      "trainer/Q1 Predictions Max                             4.64875\n",
      "trainer/Q1 Predictions Min                             2.42722\n",
      "trainer/Q2 Predictions Mean                            3.57502\n",
      "trainer/Q2 Predictions Std                             0.359045\n",
      "trainer/Q2 Predictions Max                             4.64025\n",
      "trainer/Q2 Predictions Min                             2.52155\n",
      "trainer/Q Targets Mean                                 3.67756\n",
      "trainer/Q Targets Std                                  0.773049\n",
      "trainer/Q Targets Max                                  6.11884\n",
      "trainer/Q Targets Min                                 -1.0429\n",
      "trainer/Log Pis Mean                                  -5.46537\n",
      "trainer/Log Pis Std                                    0.345783\n",
      "trainer/Log Pis Max                                   -4.66182\n",
      "trainer/Log Pis Min                                   -7.79494\n",
      "trainer/Policy mu Mean                                -0.00709728\n",
      "trainer/Policy mu Std                                  0.037767\n",
      "trainer/Policy mu Max                                  0.120683\n",
      "trainer/Policy mu Min                                 -0.177747\n",
      "trainer/Policy log std Mean                           -0.124599\n",
      "trainer/Policy log std Std                             0.0175449\n",
      "trainer/Policy log std Max                            -0.078226\n",
      "trainer/Policy log std Min                            -0.212656\n",
      "trainer/Alpha                                          0.547067\n",
      "trainer/Alpha Loss                                    -8.08166\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                           29\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.360413\n",
      "exploration/Rewards Std                                0.461905\n",
      "exploration/Rewards Max                                1.27138\n",
      "exploration/Rewards Min                               -1.73997\n",
      "exploration/Returns Mean                            -360.413\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -360.413\n",
      "exploration/Returns Min                             -360.413\n",
      "exploration/Actions Mean                              -0.00380229\n",
      "exploration/Actions Std                                0.594065\n",
      "exploration/Actions Max                                0.997991\n",
      "exploration/Actions Min                               -0.996392\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -360.413\n",
      "exploration/env_infos/final/reward_forward Mean        0.0502047\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0502047\n",
      "exploration/env_infos/final/reward_forward Min         0.0502047\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0437547\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0437547\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0437547\n",
      "exploration/env_infos/reward_forward Mean              0.012586\n",
      "exploration/env_infos/reward_forward Std               0.405693\n",
      "exploration/env_infos/reward_forward Max               1.82904\n",
      "exploration/env_infos/reward_forward Min              -1.0855\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.42003\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.42003\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.42003\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.807696\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.807696\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.807696\n",
      "exploration/env_infos/reward_ctrl Mean                -1.41171\n",
      "exploration/env_infos/reward_ctrl Std                  0.424361\n",
      "exploration/env_infos/reward_ctrl Max                 -0.211134\n",
      "exploration/env_infos/reward_ctrl Min                 -2.73997\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0330233\n",
      "exploration/env_infos/final/torso_velocity Std         0.0828329\n",
      "exploration/env_infos/final/torso_velocity Max         0.0502047\n",
      "exploration/env_infos/final/torso_velocity Min        -0.146029\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.260042\n",
      "exploration/env_infos/initial/torso_velocity Std       0.334759\n",
      "exploration/env_infos/initial/torso_velocity Max       0.726386\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0437547\n",
      "exploration/env_infos/torso_velocity Mean              0.0139501\n",
      "exploration/env_infos/torso_velocity Std               0.428892\n",
      "exploration/env_infos/torso_velocity Max               2.45293\n",
      "exploration/env_infos/torso_velocity Min              -3.17036\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.996506\n",
      "evaluation/Rewards Std                                 0.0196091\n",
      "evaluation/Rewards Max                                 2.19973\n",
      "evaluation/Rewards Min                                 0.98997\n",
      "evaluation/Returns Mean                              996.506\n",
      "evaluation/Returns Std                                 0.887528\n",
      "evaluation/Returns Max                               998.142\n",
      "evaluation/Returns Min                               994.288\n",
      "evaluation/Actions Mean                               -0.00328104\n",
      "evaluation/Actions Std                                 0.0317426\n",
      "evaluation/Actions Max                                 0.0563597\n",
      "evaluation/Actions Min                                -0.112632\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           996.506\n",
      "evaluation/env_infos/final/reward_forward Mean         7.06864e-06\n",
      "evaluation/env_infos/final/reward_forward Std          3.38535e-05\n",
      "evaluation/env_infos/final/reward_forward Max          0.000172906\n",
      "evaluation/env_infos/final/reward_forward Min         -4.95725e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00681335\n",
      "evaluation/env_infos/initial/reward_forward Std        0.132697\n",
      "evaluation/env_infos/initial/reward_forward Max        0.309335\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.272547\n",
      "evaluation/env_infos/reward_forward Mean              -0.000225767\n",
      "evaluation/env_infos/reward_forward Std                0.0370411\n",
      "evaluation/env_infos/reward_forward Max                0.956277\n",
      "evaluation/env_infos/reward_forward Min               -1.08475\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00405175\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00109562\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00240366\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00643764\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00425026\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00162063\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00233881\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00737082\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00407343\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00109786\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00220564\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0100298\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         6.33264e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000180681\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00124099\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000938871\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.121676\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.235777\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.587284\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.272547\n",
      "evaluation/env_infos/torso_velocity Mean              -0.0027165\n",
      "evaluation/env_infos/torso_velocity Std                0.0495391\n",
      "evaluation/env_infos/torso_velocity Max                0.956277\n",
      "evaluation/env_infos/torso_velocity Min               -1.79608\n",
      "time/data storing (s)                                  0.0146168\n",
      "time/evaluation sampling (s)                          44.9117\n",
      "time/exploration sampling (s)                          1.92461\n",
      "time/logging (s)                                       0.279425\n",
      "time/saving (s)                                        0.0313188\n",
      "time/training (s)                                      3.21751\n",
      "time/epoch (s)                                        50.3792\n",
      "time/total (s)                                       392.89\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:17:44.538238 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   5000\n",
      "trainer/QF1 Loss                                        0.42945\n",
      "trainer/QF2 Loss                                        0.431706\n",
      "trainer/Policy Loss                                    -8.98867\n",
      "trainer/Q1 Predictions Mean                             3.57832\n",
      "trainer/Q1 Predictions Std                              0.28528\n",
      "trainer/Q1 Predictions Max                              5.13395\n",
      "trainer/Q1 Predictions Min                              2.62374\n",
      "trainer/Q2 Predictions Mean                             3.56389\n",
      "trainer/Q2 Predictions Std                              0.284645\n",
      "trainer/Q2 Predictions Max                              5.34834\n",
      "trainer/Q2 Predictions Min                              2.62221\n",
      "trainer/Q Targets Mean                                  3.67084\n",
      "trainer/Q Targets Std                                   0.676596\n",
      "trainer/Q Targets Max                                   5.60503\n",
      "trainer/Q Targets Min                                  -1.0429\n",
      "trainer/Log Pis Mean                                   -5.44948\n",
      "trainer/Log Pis Std                                     0.441912\n",
      "trainer/Log Pis Max                                    -4.62526\n",
      "trainer/Log Pis Min                                    -9.78371\n",
      "trainer/Policy mu Mean                                 -0.0149106\n",
      "trainer/Policy mu Std                                   0.038533\n",
      "trainer/Policy mu Max                                   0.0945983\n",
      "trainer/Policy mu Min                                  -0.175962\n",
      "trainer/Policy log std Mean                            -0.144997\n",
      "trainer/Policy log std Std                              0.0191173\n",
      "trainer/Policy log std Max                             -0.0963135\n",
      "trainer/Policy log std Min                             -0.240731\n",
      "trainer/Alpha                                           0.405267\n",
      "trainer/Alpha Loss                                    -12.1073\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                            30\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.342886\n",
      "exploration/Rewards Std                                 0.41672\n",
      "exploration/Rewards Max                                 0.977813\n",
      "exploration/Rewards Min                                -1.60384\n",
      "exploration/Returns Mean                             -342.886\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -342.886\n",
      "exploration/Returns Min                              -342.886\n",
      "exploration/Actions Mean                               -0.00452691\n",
      "exploration/Actions Std                                 0.583836\n",
      "exploration/Actions Max                                 0.996904\n",
      "exploration/Actions Min                                -0.994776\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -342.886\n",
      "exploration/env_infos/final/reward_forward Mean        -0.198586\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.198586\n",
      "exploration/env_infos/final/reward_forward Min         -0.198586\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.133775\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.133775\n",
      "exploration/env_infos/initial/reward_forward Min       -0.133775\n",
      "exploration/env_infos/reward_forward Mean               0.00073045\n",
      "exploration/env_infos/reward_forward Std                0.44699\n",
      "exploration/env_infos/reward_forward Max                1.40789\n",
      "exploration/env_infos/reward_forward Min               -1.53568\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.794814\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.794814\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.794814\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.29191\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.29191\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.29191\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.36354\n",
      "exploration/env_infos/reward_ctrl Std                   0.404549\n",
      "exploration/env_infos/reward_ctrl Max                  -0.306468\n",
      "exploration/env_infos/reward_ctrl Min                  -2.67006\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0125176\n",
      "exploration/env_infos/final/torso_velocity Std          0.215187\n",
      "exploration/env_infos/final/torso_velocity Max          0.307897\n",
      "exploration/env_infos/final/torso_velocity Min         -0.198586\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.107046\n",
      "exploration/env_infos/initial/torso_velocity Std        0.25645\n",
      "exploration/env_infos/initial/torso_velocity Max        0.462305\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.133775\n",
      "exploration/env_infos/torso_velocity Mean              -0.0133001\n",
      "exploration/env_infos/torso_velocity Std                0.394677\n",
      "exploration/env_infos/torso_velocity Max                1.46161\n",
      "exploration/env_infos/torso_velocity Min               -2.62606\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.996419\n",
      "evaluation/Rewards Std                                  0.025966\n",
      "evaluation/Rewards Max                                  2.53673\n",
      "evaluation/Rewards Min                                  0.984837\n",
      "evaluation/Returns Mean                               996.419\n",
      "evaluation/Returns Std                                  1.63835\n",
      "evaluation/Returns Max                               1001.25\n",
      "evaluation/Returns Min                                994.397\n",
      "evaluation/Actions Mean                                -0.0132144\n",
      "evaluation/Actions Std                                  0.0309652\n",
      "evaluation/Actions Max                                  0.0702539\n",
      "evaluation/Actions Min                                 -0.128225\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            996.419\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.87106e-05\n",
      "evaluation/env_infos/final/reward_forward Std           8.90736e-05\n",
      "evaluation/env_infos/final/reward_forward Max           7.54988e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000454928\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0208099\n",
      "evaluation/env_infos/initial/reward_forward Std         0.110026\n",
      "evaluation/env_infos/initial/reward_forward Max         0.263027\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.162837\n",
      "evaluation/env_infos/reward_forward Mean                0.00521295\n",
      "evaluation/env_infos/reward_forward Std                 0.0574209\n",
      "evaluation/env_infos/reward_forward Max                 1.52961\n",
      "evaluation/env_infos/reward_forward Min                -0.926134\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00451828\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000914092\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00255337\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00557813\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00414975\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00143956\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00191651\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00574154\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00453387\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000945911\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00191651\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0151632\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.39317e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000151579\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000380679\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00116358\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.128281\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.226668\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.592919\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.352477\n",
      "evaluation/env_infos/torso_velocity Mean                0.0016468\n",
      "evaluation/env_infos/torso_velocity Std                 0.0597595\n",
      "evaluation/env_infos/torso_velocity Max                 1.52961\n",
      "evaluation/env_infos/torso_velocity Min                -1.78787\n",
      "time/data storing (s)                                   0.0153486\n",
      "time/evaluation sampling (s)                           44.3021\n",
      "time/exploration sampling (s)                           1.83434\n",
      "time/logging (s)                                        0.273162\n",
      "time/saving (s)                                         0.0258337\n",
      "time/training (s)                                       3.15192\n",
      "time/epoch (s)                                         49.6027\n",
      "time/total (s)                                        442.696\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:18:35.489688 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   6000\n",
      "trainer/QF1 Loss                                        0.258392\n",
      "trainer/QF2 Loss                                        0.254445\n",
      "trainer/Policy Loss                                    -9.03569\n",
      "trainer/Q1 Predictions Mean                             3.66707\n",
      "trainer/Q1 Predictions Std                              0.312059\n",
      "trainer/Q1 Predictions Max                              5.35775\n",
      "trainer/Q1 Predictions Min                              3.00261\n",
      "trainer/Q2 Predictions Mean                             3.62815\n",
      "trainer/Q2 Predictions Std                              0.317601\n",
      "trainer/Q2 Predictions Max                              5.43158\n",
      "trainer/Q2 Predictions Min                              2.84324\n",
      "trainer/Q Targets Mean                                  3.75367\n",
      "trainer/Q Targets Std                                   0.553115\n",
      "trainer/Q Targets Max                                   7.19727\n",
      "trainer/Q Targets Min                                   2.29612\n",
      "trainer/Log Pis Mean                                   -5.42165\n",
      "trainer/Log Pis Std                                     0.420369\n",
      "trainer/Log Pis Max                                    -4.54829\n",
      "trainer/Log Pis Min                                    -7.40804\n",
      "trainer/Policy mu Mean                                 -0.0227821\n",
      "trainer/Policy mu Std                                   0.0655493\n",
      "trainer/Policy mu Max                                   0.303228\n",
      "trainer/Policy mu Min                                  -0.443585\n",
      "trainer/Policy log std Mean                            -0.140385\n",
      "trainer/Policy log std Std                              0.022741\n",
      "trainer/Policy log std Max                             -0.0755909\n",
      "trainer/Policy log std Min                             -0.310041\n",
      "trainer/Alpha                                           0.300216\n",
      "trainer/Alpha Loss                                    -16.1094\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                            38\n",
      "exploration/path length Mean                          125\n",
      "exploration/path length Std                           222.254\n",
      "exploration/path length Max                           711\n",
      "exploration/path length Min                            13\n",
      "exploration/Rewards Mean                               -0.314232\n",
      "exploration/Rewards Std                                 0.461599\n",
      "exploration/Rewards Max                                 1.8695\n",
      "exploration/Rewards Min                                -1.85909\n",
      "exploration/Returns Mean                              -39.279\n",
      "exploration/Returns Std                                73.0892\n",
      "exploration/Returns Max                                 0.720653\n",
      "exploration/Returns Min                              -231.8\n",
      "exploration/Actions Mean                               -0.0260599\n",
      "exploration/Actions Std                                 0.585237\n",
      "exploration/Actions Max                                 0.996348\n",
      "exploration/Actions Min                                -0.999044\n",
      "exploration/Num Paths                                   8\n",
      "exploration/Average Returns                           -39.279\n",
      "exploration/env_infos/final/reward_forward Mean         0.229029\n",
      "exploration/env_infos/final/reward_forward Std          1.30063\n",
      "exploration/env_infos/final/reward_forward Max          2.60288\n",
      "exploration/env_infos/final/reward_forward Min         -1.48971\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0150624\n",
      "exploration/env_infos/initial/reward_forward Std        0.187962\n",
      "exploration/env_infos/initial/reward_forward Max        0.334341\n",
      "exploration/env_infos/initial/reward_forward Min       -0.30153\n",
      "exploration/env_infos/reward_forward Mean               0.0343391\n",
      "exploration/env_infos/reward_forward Std                0.529601\n",
      "exploration/env_infos/reward_forward Max                2.60288\n",
      "exploration/env_infos/reward_forward Min               -1.77078\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.40158\n",
      "exploration/env_infos/final/reward_ctrl Std             0.475823\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.785723\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.14843\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.2786\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.14539\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.05526\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.50101\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.37273\n",
      "exploration/env_infos/reward_ctrl Std                   0.405197\n",
      "exploration/env_infos/reward_ctrl Max                  -0.319192\n",
      "exploration/env_infos/reward_ctrl Min                  -2.85909\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.681323\n",
      "exploration/env_infos/final/torso_velocity Std          1.14947\n",
      "exploration/env_infos/final/torso_velocity Max          3.19541\n",
      "exploration/env_infos/final/torso_velocity Min         -1.48971\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.120874\n",
      "exploration/env_infos/initial/torso_velocity Std        0.294029\n",
      "exploration/env_infos/initial/torso_velocity Max        0.57756\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.350609\n",
      "exploration/env_infos/torso_velocity Mean               0.0431365\n",
      "exploration/env_infos/torso_velocity Std                0.579675\n",
      "exploration/env_infos/torso_velocity Max                3.4565\n",
      "exploration/env_infos/torso_velocity Min               -2.26536\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.996114\n",
      "evaluation/Rewards Std                                  0.0184557\n",
      "evaluation/Rewards Max                                  2.23979\n",
      "evaluation/Rewards Min                                  0.969194\n",
      "evaluation/Returns Mean                               996.114\n",
      "evaluation/Returns Std                                  1.16765\n",
      "evaluation/Returns Max                                998.69\n",
      "evaluation/Returns Min                                994.012\n",
      "evaluation/Actions Mean                                -0.0181003\n",
      "evaluation/Actions Std                                  0.0286277\n",
      "evaluation/Actions Max                                  0.0900301\n",
      "evaluation/Actions Min                                 -0.176109\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            996.114\n",
      "evaluation/env_infos/final/reward_forward Mean          4.32849e-06\n",
      "evaluation/env_infos/final/reward_forward Std           2.00977e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000102755\n",
      "evaluation/env_infos/final/reward_forward Min          -6.72927e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00742406\n",
      "evaluation/env_infos/initial/reward_forward Std         0.148028\n",
      "evaluation/env_infos/initial/reward_forward Max         0.263396\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.244125\n",
      "evaluation/env_infos/reward_forward Mean                0.0015325\n",
      "evaluation/env_infos/reward_forward Std                 0.0412684\n",
      "evaluation/env_infos/reward_forward Max                 0.989239\n",
      "evaluation/env_infos/reward_forward Min                -1.11221\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00456617\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000667246\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00375146\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00629717\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00618542\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00133154\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00514113\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0103633\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00458866\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000772025\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00367787\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0308059\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.38174e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000106004\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000920054\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.89004e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.130265\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.234293\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.561063\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.244125\n",
      "evaluation/env_infos/torso_velocity Mean                0.000214812\n",
      "evaluation/env_infos/torso_velocity Std                 0.0509598\n",
      "evaluation/env_infos/torso_velocity Max                 1.55461\n",
      "evaluation/env_infos/torso_velocity Min                -2.08137\n",
      "time/data storing (s)                                   0.0158349\n",
      "time/evaluation sampling (s)                           45.2548\n",
      "time/exploration sampling (s)                           1.8447\n",
      "time/logging (s)                                        0.276755\n",
      "time/saving (s)                                         0.0269849\n",
      "time/training (s)                                       3.3359\n",
      "time/epoch (s)                                         50.755\n",
      "time/total (s)                                        493.651\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:19:27.700565 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   7000\n",
      "trainer/QF1 Loss                                        0.241509\n",
      "trainer/QF2 Loss                                        0.220251\n",
      "trainer/Policy Loss                                    -9.09673\n",
      "trainer/Q1 Predictions Mean                             3.7207\n",
      "trainer/Q1 Predictions Std                              0.330041\n",
      "trainer/Q1 Predictions Max                              4.58195\n",
      "trainer/Q1 Predictions Min                              2.90458\n",
      "trainer/Q2 Predictions Mean                             3.65881\n",
      "trainer/Q2 Predictions Std                              0.34411\n",
      "trainer/Q2 Predictions Max                              4.49171\n",
      "trainer/Q2 Predictions Min                              2.55832\n",
      "trainer/Q Targets Mean                                  3.75637\n",
      "trainer/Q Targets Std                                   0.528594\n",
      "trainer/Q Targets Max                                   6.00769\n",
      "trainer/Q Targets Min                                   1.51214\n",
      "trainer/Log Pis Mean                                   -5.44863\n",
      "trainer/Log Pis Std                                     0.519635\n",
      "trainer/Log Pis Max                                    -4.32836\n",
      "trainer/Log Pis Min                                    -9.99192\n",
      "trainer/Policy mu Mean                                 -0.00994594\n",
      "trainer/Policy mu Std                                   0.0650867\n",
      "trainer/Policy mu Max                                   0.398967\n",
      "trainer/Policy mu Min                                  -0.344718\n",
      "trainer/Policy log std Mean                            -0.146621\n",
      "trainer/Policy log std Std                              0.021374\n",
      "trainer/Policy log std Max                             -0.0910845\n",
      "trainer/Policy log std Min                             -0.236124\n",
      "trainer/Alpha                                           0.222481\n",
      "trainer/Alpha Loss                                    -20.1718\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                            49\n",
      "exploration/path length Mean                           90.9091\n",
      "exploration/path length Std                            76.754\n",
      "exploration/path length Max                           305\n",
      "exploration/path length Min                            19\n",
      "exploration/Rewards Mean                               -0.277224\n",
      "exploration/Rewards Std                                 0.568042\n",
      "exploration/Rewards Max                                 2.81677\n",
      "exploration/Rewards Min                                -1.98091\n",
      "exploration/Returns Mean                              -25.2022\n",
      "exploration/Returns Std                                27.9959\n",
      "exploration/Returns Max                                 6.73402\n",
      "exploration/Returns Min                              -101.356\n",
      "exploration/Actions Mean                               -0.00836854\n",
      "exploration/Actions Std                                 0.589016\n",
      "exploration/Actions Max                                 0.994819\n",
      "exploration/Actions Min                                -0.996512\n",
      "exploration/Num Paths                                  11\n",
      "exploration/Average Returns                           -25.2022\n",
      "exploration/env_infos/final/reward_forward Mean         0.588667\n",
      "exploration/env_infos/final/reward_forward Std          0.834754\n",
      "exploration/env_infos/final/reward_forward Max          2.22967\n",
      "exploration/env_infos/final/reward_forward Min         -0.226739\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.00958746\n",
      "exploration/env_infos/initial/reward_forward Std        0.251162\n",
      "exploration/env_infos/initial/reward_forward Max        0.634358\n",
      "exploration/env_infos/initial/reward_forward Min       -0.256927\n",
      "exploration/env_infos/reward_forward Mean               0.055879\n",
      "exploration/env_infos/reward_forward Std                0.749345\n",
      "exploration/env_infos/reward_forward Max                2.6929\n",
      "exploration/env_infos/reward_forward Min               -2.40438\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.25097\n",
      "exploration/env_infos/final/reward_ctrl Std             0.398246\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.666376\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.1146\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.60309\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.44958\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.04477\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.44701\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.38804\n",
      "exploration/env_infos/reward_ctrl Std                   0.419654\n",
      "exploration/env_infos/reward_ctrl Max                  -0.286552\n",
      "exploration/env_infos/reward_ctrl Min                  -2.98091\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.780529\n",
      "exploration/env_infos/final/torso_velocity Std          1.11233\n",
      "exploration/env_infos/final/torso_velocity Max          3.6685\n",
      "exploration/env_infos/final/torso_velocity Min         -1.44318\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.16638\n",
      "exploration/env_infos/initial/torso_velocity Std        0.306941\n",
      "exploration/env_infos/initial/torso_velocity Max        0.701886\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.356098\n",
      "exploration/env_infos/torso_velocity Mean               0.0457675\n",
      "exploration/env_infos/torso_velocity Std                0.763336\n",
      "exploration/env_infos/torso_velocity Max                4.33821\n",
      "exploration/env_infos/torso_velocity Min               -2.49065\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.990023\n",
      "evaluation/Rewards Std                                  0.0241017\n",
      "evaluation/Rewards Max                                  2.42127\n",
      "evaluation/Rewards Min                                  0.976735\n",
      "evaluation/Returns Mean                               990.023\n",
      "evaluation/Returns Std                                  2.39918\n",
      "evaluation/Returns Max                                998.603\n",
      "evaluation/Returns Min                                987.64\n",
      "evaluation/Actions Mean                                 0.000237679\n",
      "evaluation/Actions Std                                  0.052032\n",
      "evaluation/Actions Max                                  0.152656\n",
      "evaluation/Actions Min                                 -0.136674\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            990.023\n",
      "evaluation/env_infos/final/reward_forward Mean         -3.42579e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000114037\n",
      "evaluation/env_infos/final/reward_forward Max           9.23459e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00048987\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0356539\n",
      "evaluation/env_infos/initial/reward_forward Std         0.1236\n",
      "evaluation/env_infos/initial/reward_forward Max         0.313122\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.136076\n",
      "evaluation/env_infos/reward_forward Mean               -0.00148822\n",
      "evaluation/env_infos/reward_forward Std                 0.044729\n",
      "evaluation/env_infos/reward_forward Max                 1.04678\n",
      "evaluation/env_infos/reward_forward Min                -1.57592\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0108385\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00142568\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00875919\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0123673\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00838775\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0022484\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00575937\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0139063\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0108295\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00146667\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00403856\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0232645\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -7.2135e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000119154\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000523473\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00048987\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.168393\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.219712\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.589465\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.192019\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00270094\n",
      "evaluation/env_infos/torso_velocity Std                 0.050635\n",
      "evaluation/env_infos/torso_velocity Max                 1.04678\n",
      "evaluation/env_infos/torso_velocity Min                -1.764\n",
      "time/data storing (s)                                   0.0181648\n",
      "time/evaluation sampling (s)                           45.9361\n",
      "time/exploration sampling (s)                           1.90139\n",
      "time/logging (s)                                        0.287167\n",
      "time/saving (s)                                         0.0283528\n",
      "time/training (s)                                       3.84246\n",
      "time/epoch (s)                                         52.0136\n",
      "time/total (s)                                        545.873\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:20:20.469377 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   8000\n",
      "trainer/QF1 Loss                                        0.31019\n",
      "trainer/QF2 Loss                                        0.292933\n",
      "trainer/Policy Loss                                    -9.02877\n",
      "trainer/Q1 Predictions Mean                             3.7209\n",
      "trainer/Q1 Predictions Std                              0.351693\n",
      "trainer/Q1 Predictions Max                              4.76289\n",
      "trainer/Q1 Predictions Min                              2.58387\n",
      "trainer/Q2 Predictions Mean                             3.73764\n",
      "trainer/Q2 Predictions Std                              0.371048\n",
      "trainer/Q2 Predictions Max                              5.16292\n",
      "trainer/Q2 Predictions Min                              2.5422\n",
      "trainer/Q Targets Mean                                  3.73693\n",
      "trainer/Q Targets Std                                   0.650734\n",
      "trainer/Q Targets Max                                   7.35311\n",
      "trainer/Q Targets Min                                  -0.408045\n",
      "trainer/Log Pis Mean                                   -5.32724\n",
      "trainer/Log Pis Std                                     0.680582\n",
      "trainer/Log Pis Max                                    -3.59941\n",
      "trainer/Log Pis Min                                   -10.6715\n",
      "trainer/Policy mu Mean                                  0.0153462\n",
      "trainer/Policy mu Std                                   0.135934\n",
      "trainer/Policy mu Max                                   0.50944\n",
      "trainer/Policy mu Min                                  -0.416099\n",
      "trainer/Policy log std Mean                            -0.236407\n",
      "trainer/Policy log std Std                              0.0561954\n",
      "trainer/Policy log std Max                             -0.128211\n",
      "trainer/Policy log std Min                             -0.561845\n",
      "trainer/Alpha                                           0.164986\n",
      "trainer/Alpha Loss                                    -23.9746\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                            56\n",
      "exploration/path length Mean                          142.857\n",
      "exploration/path length Std                           247.707\n",
      "exploration/path length Max                           747\n",
      "exploration/path length Min                            16\n",
      "exploration/Rewards Mean                               -0.20632\n",
      "exploration/Rewards Std                                 0.453975\n",
      "exploration/Rewards Max                                 2.24801\n",
      "exploration/Rewards Min                                -1.41149\n",
      "exploration/Returns Mean                              -29.4742\n",
      "exploration/Returns Std                                54.4409\n",
      "exploration/Returns Max                                 2.94965\n",
      "exploration/Returns Min                              -162.18\n",
      "exploration/Actions Mean                                0.0067929\n",
      "exploration/Actions Std                                 0.560052\n",
      "exploration/Actions Max                                 0.994202\n",
      "exploration/Actions Min                                -0.99232\n",
      "exploration/Num Paths                                   7\n",
      "exploration/Average Returns                           -29.4742\n",
      "exploration/env_infos/final/reward_forward Mean         0.944765\n",
      "exploration/env_infos/final/reward_forward Std          1.53651\n",
      "exploration/env_infos/final/reward_forward Max          3.38513\n",
      "exploration/env_infos/final/reward_forward Min         -0.806013\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0792154\n",
      "exploration/env_infos/initial/reward_forward Std        0.134146\n",
      "exploration/env_infos/initial/reward_forward Max        0.0689463\n",
      "exploration/env_infos/initial/reward_forward Min       -0.336492\n",
      "exploration/env_infos/reward_forward Mean               0.0574941\n",
      "exploration/env_infos/reward_forward Std                0.546545\n",
      "exploration/env_infos/reward_forward Max                3.49967\n",
      "exploration/env_infos/reward_forward Min               -1.68219\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.37645\n",
      "exploration/env_infos/final/reward_ctrl Std             0.477717\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.539749\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.85626\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.17989\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.440014\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.755753\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.08161\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.25482\n",
      "exploration/env_infos/reward_ctrl Std                   0.396832\n",
      "exploration/env_infos/reward_ctrl Max                  -0.259165\n",
      "exploration/env_infos/reward_ctrl Min                  -2.41149\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.999555\n",
      "exploration/env_infos/final/torso_velocity Std          1.30419\n",
      "exploration/env_infos/final/torso_velocity Max          3.38513\n",
      "exploration/env_infos/final/torso_velocity Min         -0.806013\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.157583\n",
      "exploration/env_infos/initial/torso_velocity Std        0.275977\n",
      "exploration/env_infos/initial/torso_velocity Max        0.822254\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.336492\n",
      "exploration/env_infos/torso_velocity Mean               0.0191213\n",
      "exploration/env_infos/torso_velocity Std                0.562696\n",
      "exploration/env_infos/torso_velocity Max                4.04802\n",
      "exploration/env_infos/torso_velocity Min               -2.92515\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.964862\n",
      "evaluation/Rewards Std                                  0.0297154\n",
      "evaluation/Rewards Max                                  2.77787\n",
      "evaluation/Rewards Min                                  0.818892\n",
      "evaluation/Returns Mean                               964.862\n",
      "evaluation/Returns Std                                 11.2511\n",
      "evaluation/Returns Max                                980.785\n",
      "evaluation/Returns Min                                942.974\n",
      "evaluation/Actions Mean                                 0.0150527\n",
      "evaluation/Actions Std                                  0.0936625\n",
      "evaluation/Actions Max                                  0.411366\n",
      "evaluation/Actions Min                                 -0.179456\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            964.862\n",
      "evaluation/env_infos/final/reward_forward Mean          2.02985e-06\n",
      "evaluation/env_infos/final/reward_forward Std           5.97978e-06\n",
      "evaluation/env_infos/final/reward_forward Max           2.02769e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -8.92325e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0133687\n",
      "evaluation/env_infos/initial/reward_forward Std         0.130615\n",
      "evaluation/env_infos/initial/reward_forward Max         0.255067\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.293264\n",
      "evaluation/env_infos/reward_forward Mean               -0.00274578\n",
      "evaluation/env_infos/reward_forward Std                 0.0548147\n",
      "evaluation/env_infos/reward_forward Max                 1.08184\n",
      "evaluation/env_infos/reward_forward Min                -1.47767\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.036148\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0115164\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.019308\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0569401\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0330976\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0100947\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0220146\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0561283\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.035997\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0121439\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0103876\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.181108\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -7.38533e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           7.21421e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           2.02769e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -3.43659e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.137255\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.239423\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.612209\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.293264\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0029766\n",
      "evaluation/env_infos/torso_velocity Std                 0.056257\n",
      "evaluation/env_infos/torso_velocity Max                 1.08184\n",
      "evaluation/env_infos/torso_velocity Min                -1.85765\n",
      "time/data storing (s)                                   0.016813\n",
      "time/evaluation sampling (s)                           46.621\n",
      "time/exploration sampling (s)                           1.94239\n",
      "time/logging (s)                                        0.299168\n",
      "time/saving (s)                                         0.0295737\n",
      "time/training (s)                                       3.64216\n",
      "time/epoch (s)                                         52.5511\n",
      "time/total (s)                                        598.654\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:21:17.435448 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   9000\n",
      "trainer/QF1 Loss                                        0.394394\n",
      "trainer/QF2 Loss                                        0.322329\n",
      "trainer/Policy Loss                                    -8.47066\n",
      "trainer/Q1 Predictions Mean                             3.50088\n",
      "trainer/Q1 Predictions Std                              0.345598\n",
      "trainer/Q1 Predictions Max                              4.71385\n",
      "trainer/Q1 Predictions Min                              2.56522\n",
      "trainer/Q2 Predictions Mean                             3.56465\n",
      "trainer/Q2 Predictions Std                              0.400335\n",
      "trainer/Q2 Predictions Max                              4.93118\n",
      "trainer/Q2 Predictions Min                              2.40388\n",
      "trainer/Q Targets Mean                                  3.67113\n",
      "trainer/Q Targets Std                                   0.703641\n",
      "trainer/Q Targets Max                                   7.31571\n",
      "trainer/Q Targets Min                                  -0.41638\n",
      "trainer/Log Pis Mean                                   -4.89991\n",
      "trainer/Log Pis Std                                     0.883574\n",
      "trainer/Log Pis Max                                    -0.987568\n",
      "trainer/Log Pis Min                                    -8.64333\n",
      "trainer/Policy mu Mean                                  0.0113396\n",
      "trainer/Policy mu Std                                   0.172371\n",
      "trainer/Policy mu Max                                   0.7745\n",
      "trainer/Policy mu Min                                  -0.881195\n",
      "trainer/Policy log std Mean                            -0.367373\n",
      "trainer/Policy log std Std                              0.100459\n",
      "trainer/Policy log std Max                             -0.191217\n",
      "trainer/Policy log std Min                             -0.995497\n",
      "trainer/Alpha                                           0.122806\n",
      "trainer/Alpha Loss                                    -27.0155\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                            58\n",
      "exploration/path length Mean                          500\n",
      "exploration/path length Std                           472\n",
      "exploration/path length Max                           972\n",
      "exploration/path length Min                            28\n",
      "exploration/Rewards Mean                               -0.0448408\n",
      "exploration/Rewards Std                                 0.453869\n",
      "exploration/Rewards Max                                 2.73737\n",
      "exploration/Rewards Min                                -1.44768\n",
      "exploration/Returns Mean                              -22.4204\n",
      "exploration/Returns Std                                34.5227\n",
      "exploration/Returns Max                                12.1023\n",
      "exploration/Returns Min                               -56.9431\n",
      "exploration/Actions Mean                                0.0160696\n",
      "exploration/Actions Std                                 0.528083\n",
      "exploration/Actions Max                                 0.991863\n",
      "exploration/Actions Min                                -0.986434\n",
      "exploration/Num Paths                                   2\n",
      "exploration/Average Returns                           -22.4204\n",
      "exploration/env_infos/final/reward_forward Mean         0.210655\n",
      "exploration/env_infos/final/reward_forward Std          0.217426\n",
      "exploration/env_infos/final/reward_forward Max          0.428081\n",
      "exploration/env_infos/final/reward_forward Min         -0.00677149\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.143728\n",
      "exploration/env_infos/initial/reward_forward Std        0.0480725\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0956553\n",
      "exploration/env_infos/initial/reward_forward Min       -0.1918\n",
      "exploration/env_infos/reward_forward Mean               0.0564546\n",
      "exploration/env_infos/reward_forward Std                0.487443\n",
      "exploration/env_infos/reward_forward Max                2.45757\n",
      "exploration/env_infos/reward_forward Min               -1.66287\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.25562\n",
      "exploration/env_infos/final/reward_ctrl Std             0.105951\n",
      "exploration/env_infos/final/reward_ctrl Max            -1.14967\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.36157\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.781502\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.154771\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.626732\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.936273\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.11652\n",
      "exploration/env_infos/reward_ctrl Std                   0.36557\n",
      "exploration/env_infos/reward_ctrl Max                  -0.161076\n",
      "exploration/env_infos/reward_ctrl Min                  -2.44768\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.277066\n",
      "exploration/env_infos/final/torso_velocity Std          0.710762\n",
      "exploration/env_infos/final/torso_velocity Max          1.76941\n",
      "exploration/env_infos/final/torso_velocity Min         -0.341332\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.134614\n",
      "exploration/env_infos/initial/torso_velocity Std        0.27394\n",
      "exploration/env_infos/initial/torso_velocity Max        0.544966\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.1918\n",
      "exploration/env_infos/torso_velocity Mean              -0.0178264\n",
      "exploration/env_infos/torso_velocity Std                0.499162\n",
      "exploration/env_infos/torso_velocity Max                3.18733\n",
      "exploration/env_infos/torso_velocity Min               -3.08847\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.912439\n",
      "evaluation/Rewards Std                                  0.0330614\n",
      "evaluation/Rewards Max                                  2.37786\n",
      "evaluation/Rewards Min                                  0.681476\n",
      "evaluation/Returns Mean                               912.439\n",
      "evaluation/Returns Std                                  8.7319\n",
      "evaluation/Returns Max                                924.639\n",
      "evaluation/Returns Min                                897.182\n",
      "evaluation/Actions Mean                                 0.0431417\n",
      "evaluation/Actions Std                                  0.142506\n",
      "evaluation/Actions Max                                  0.50139\n",
      "evaluation/Actions Min                                 -0.542564\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            912.439\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.57848e-08\n",
      "evaluation/env_infos/final/reward_forward Std           1.78305e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.51792e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -2.54663e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0312432\n",
      "evaluation/env_infos/initial/reward_forward Std         0.119246\n",
      "evaluation/env_infos/initial/reward_forward Max         0.17345\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.283954\n",
      "evaluation/env_infos/reward_forward Mean               -0.002664\n",
      "evaluation/env_infos/reward_forward Std                 0.0730871\n",
      "evaluation/env_infos/reward_forward Max                 1.45441\n",
      "evaluation/env_infos/reward_forward Min                -1.32159\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0886022\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00997361\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.075476\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.105522\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0459062\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0153093\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0239478\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0947255\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0886766\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0136099\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0126557\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.318524\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.16832e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           2.29759e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.91338e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.72522e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.129744\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.242323\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.62259\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.283954\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00275627\n",
      "evaluation/env_infos/torso_velocity Std                 0.0695366\n",
      "evaluation/env_infos/torso_velocity Max                 1.46548\n",
      "evaluation/env_infos/torso_velocity Min                -1.86897\n",
      "time/data storing (s)                                   0.0153455\n",
      "time/evaluation sampling (s)                           50.4181\n",
      "time/exploration sampling (s)                           1.86479\n",
      "time/logging (s)                                        0.276766\n",
      "time/saving (s)                                         0.0271444\n",
      "time/training (s)                                       4.10607\n",
      "time/epoch (s)                                         56.7082\n",
      "time/total (s)                                        655.598\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:22:10.538657 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.331368\n",
      "trainer/QF2 Loss                                        0.280654\n",
      "trainer/Policy Loss                                    -7.40516\n",
      "trainer/Q1 Predictions Mean                             3.56669\n",
      "trainer/Q1 Predictions Std                              0.467331\n",
      "trainer/Q1 Predictions Max                              4.93774\n",
      "trainer/Q1 Predictions Min                              1.61233\n",
      "trainer/Q2 Predictions Mean                             3.58312\n",
      "trainer/Q2 Predictions Std                              0.542977\n",
      "trainer/Q2 Predictions Max                              5.2874\n",
      "trainer/Q2 Predictions Min                              1.54541\n",
      "trainer/Q Targets Mean                                  3.58433\n",
      "trainer/Q Targets Std                                   0.76339\n",
      "trainer/Q Targets Max                                   6.18412\n",
      "trainer/Q Targets Min                                  -1.0429\n",
      "trainer/Log Pis Mean                                   -3.60389\n",
      "trainer/Log Pis Std                                     1.45137\n",
      "trainer/Log Pis Max                                    -0.503521\n",
      "trainer/Log Pis Min                                    -8.65318\n",
      "trainer/Policy mu Mean                                  0.0098514\n",
      "trainer/Policy mu Std                                   0.182556\n",
      "trainer/Policy mu Max                                   0.985333\n",
      "trainer/Policy mu Min                                  -0.748419\n",
      "trainer/Policy log std Mean                            -0.729438\n",
      "trainer/Policy log std Std                              0.145452\n",
      "trainer/Policy log std Max                             -0.393278\n",
      "trainer/Policy log std Min                             -1.2591\n",
      "trainer/Alpha                                           0.092636\n",
      "trainer/Alpha Loss                                    -27.5755\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            62\n",
      "exploration/path length Mean                          250\n",
      "exploration/path length Std                           164.835\n",
      "exploration/path length Max                           518\n",
      "exploration/path length Min                            69\n",
      "exploration/Rewards Mean                                0.366475\n",
      "exploration/Rewards Std                                 0.452368\n",
      "exploration/Rewards Max                                 2.74368\n",
      "exploration/Rewards Min                                -0.5695\n",
      "exploration/Returns Mean                               91.6188\n",
      "exploration/Returns Std                                68.4028\n",
      "exploration/Returns Max                               207.321\n",
      "exploration/Returns Min                                29.4375\n",
      "exploration/Actions Mean                                0.0189422\n",
      "exploration/Actions Std                                 0.428534\n",
      "exploration/Actions Max                                 0.97102\n",
      "exploration/Actions Min                                -0.968626\n",
      "exploration/Num Paths                                   4\n",
      "exploration/Average Returns                            91.6188\n",
      "exploration/env_infos/final/reward_forward Mean        -0.26944\n",
      "exploration/env_infos/final/reward_forward Std          0.911343\n",
      "exploration/env_infos/final/reward_forward Max          1.04208\n",
      "exploration/env_infos/final/reward_forward Min         -1.32514\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0745779\n",
      "exploration/env_infos/initial/reward_forward Std        0.158082\n",
      "exploration/env_infos/initial/reward_forward Max        0.1387\n",
      "exploration/env_infos/initial/reward_forward Min       -0.247343\n",
      "exploration/env_infos/reward_forward Mean               0.219032\n",
      "exploration/env_infos/reward_forward Std                0.718957\n",
      "exploration/env_infos/reward_forward Max                2.16088\n",
      "exploration/env_infos/reward_forward Min               -2.12036\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.604897\n",
      "exploration/env_infos/final/reward_ctrl Std             0.106519\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.439836\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.729981\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.795113\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.0803542\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.687821\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.892957\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.736002\n",
      "exploration/env_infos/reward_ctrl Std                   0.283403\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0670074\n",
      "exploration/env_infos/reward_ctrl Min                  -1.6648\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.255423\n",
      "exploration/env_infos/final/torso_velocity Std          1.11958\n",
      "exploration/env_infos/final/torso_velocity Max          2.74163\n",
      "exploration/env_infos/final/torso_velocity Min         -1.32514\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.133202\n",
      "exploration/env_infos/initial/torso_velocity Std        0.303474\n",
      "exploration/env_infos/initial/torso_velocity Max        0.679858\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.318088\n",
      "exploration/env_infos/torso_velocity Mean               0.0563427\n",
      "exploration/env_infos/torso_velocity Std                0.806146\n",
      "exploration/env_infos/torso_velocity Max                3.40258\n",
      "exploration/env_infos/torso_velocity Min               -2.43188\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.94606\n",
      "evaluation/Rewards Std                                  0.0248961\n",
      "evaluation/Rewards Max                                  2.07548\n",
      "evaluation/Rewards Min                                  0.54761\n",
      "evaluation/Returns Mean                               946.06\n",
      "evaluation/Returns Std                                 12.575\n",
      "evaluation/Returns Max                                962.742\n",
      "evaluation/Returns Min                                926.617\n",
      "evaluation/Actions Mean                                 0.042744\n",
      "evaluation/Actions Std                                  0.109013\n",
      "evaluation/Actions Max                                  0.475978\n",
      "evaluation/Actions Min                                 -0.522632\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            946.06\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.26507e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.47965e-07\n",
      "evaluation/env_infos/final/reward_forward Max           1.04395e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -1.09613e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0038464\n",
      "evaluation/env_infos/initial/reward_forward Std         0.104774\n",
      "evaluation/env_infos/initial/reward_forward Max         0.205215\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.251755\n",
      "evaluation/env_infos/reward_forward Mean               -0.000653565\n",
      "evaluation/env_infos/reward_forward Std                 0.0624881\n",
      "evaluation/env_infos/reward_forward Max                 1.31825\n",
      "evaluation/env_infos/reward_forward Min                -1.07543\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0541419\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0129777\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0358\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0732393\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0403515\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00747944\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0222856\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0552393\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0548439\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0159694\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.016428\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.45239\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -7.47071e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.83098e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.04395e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.09613e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152029\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.245086\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.670352\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.2536\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00213695\n",
      "evaluation/env_infos/torso_velocity Std                 0.0760757\n",
      "evaluation/env_infos/torso_velocity Max                 1.46447\n",
      "evaluation/env_infos/torso_velocity Min                -2.11162\n",
      "time/data storing (s)                                   0.019246\n",
      "time/evaluation sampling (s)                           46.3869\n",
      "time/exploration sampling (s)                           2.04261\n",
      "time/logging (s)                                        0.295755\n",
      "time/saving (s)                                         0.0294259\n",
      "time/training (s)                                       4.11821\n",
      "time/epoch (s)                                         52.8922\n",
      "time/total (s)                                        708.72\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:23:04.720363 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  11000\n",
      "trainer/QF1 Loss                                        0.265339\n",
      "trainer/QF2 Loss                                        0.21084\n",
      "trainer/Policy Loss                                    -4.82458\n",
      "trainer/Q1 Predictions Mean                             3.46938\n",
      "trainer/Q1 Predictions Std                              0.501323\n",
      "trainer/Q1 Predictions Max                              5.15989\n",
      "trainer/Q1 Predictions Min                              2.12114\n",
      "trainer/Q2 Predictions Mean                             3.4982\n",
      "trainer/Q2 Predictions Std                              0.582611\n",
      "trainer/Q2 Predictions Max                              5.59862\n",
      "trainer/Q2 Predictions Min                              1.78746\n",
      "trainer/Q Targets Mean                                  3.69471\n",
      "trainer/Q Targets Std                                   0.699745\n",
      "trainer/Q Targets Max                                   8.00835\n",
      "trainer/Q Targets Min                                   1.97239\n",
      "trainer/Log Pis Mean                                   -0.762125\n",
      "trainer/Log Pis Std                                     1.79142\n",
      "trainer/Log Pis Max                                     3.02956\n",
      "trainer/Log Pis Min                                    -6.81717\n",
      "trainer/Policy mu Mean                                  0.0447091\n",
      "trainer/Policy mu Std                                   0.160229\n",
      "trainer/Policy mu Max                                   0.860759\n",
      "trainer/Policy mu Min                                  -0.643701\n",
      "trainer/Policy log std Mean                            -1.21601\n",
      "trainer/Policy log std Std                              0.164239\n",
      "trainer/Policy log std Max                             -0.706203\n",
      "trainer/Policy log std Min                             -1.75179\n",
      "trainer/Alpha                                           0.0726199\n",
      "trainer/Alpha Loss                                    -22.9596\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            63\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.700912\n",
      "exploration/Rewards Std                                 0.253161\n",
      "exploration/Rewards Max                                 2.6063\n",
      "exploration/Rewards Min                                 0.0155349\n",
      "exploration/Returns Mean                              700.912\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               700.912\n",
      "exploration/Returns Min                               700.912\n",
      "exploration/Actions Mean                                0.0589504\n",
      "exploration/Actions Std                                 0.28843\n",
      "exploration/Actions Max                                 0.938814\n",
      "exploration/Actions Min                                -0.888498\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           700.912\n",
      "exploration/env_infos/final/reward_forward Mean        -1.15124\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -1.15124\n",
      "exploration/env_infos/final/reward_forward Min         -1.15124\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.00610067\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.00610067\n",
      "exploration/env_infos/initial/reward_forward Min       -0.00610067\n",
      "exploration/env_infos/reward_forward Mean               0.165607\n",
      "exploration/env_infos/reward_forward Std                0.477199\n",
      "exploration/env_infos/reward_forward Max                1.6431\n",
      "exploration/env_infos/reward_forward Min               -1.17724\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.332409\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.332409\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.332409\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.310492\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.310492\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.310492\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.346669\n",
      "exploration/env_infos/reward_ctrl Std                   0.156702\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0280409\n",
      "exploration/env_infos/reward_ctrl Min                  -1.15603\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.364213\n",
      "exploration/env_infos/final/torso_velocity Std          0.745147\n",
      "exploration/env_infos/final/torso_velocity Max          0.636184\n",
      "exploration/env_infos/final/torso_velocity Min         -1.15124\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.108713\n",
      "exploration/env_infos/initial/torso_velocity Std        0.107231\n",
      "exploration/env_infos/initial/torso_velocity Max        0.251917\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.00610067\n",
      "exploration/env_infos/torso_velocity Mean               0.00651102\n",
      "exploration/env_infos/torso_velocity Std                0.531183\n",
      "exploration/env_infos/torso_velocity Max                2.57008\n",
      "exploration/env_infos/torso_velocity Min               -2.2843\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.947205\n",
      "evaluation/Rewards Std                                  0.0282326\n",
      "evaluation/Rewards Max                                  2.48716\n",
      "evaluation/Rewards Min                                  0.743379\n",
      "evaluation/Returns Mean                               947.205\n",
      "evaluation/Returns Std                                  5.31544\n",
      "evaluation/Returns Max                                959.711\n",
      "evaluation/Returns Min                                939.83\n",
      "evaluation/Actions Mean                                 0.0658063\n",
      "evaluation/Actions Std                                  0.0958793\n",
      "evaluation/Actions Max                                  0.477738\n",
      "evaluation/Actions Min                                 -0.431127\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            947.205\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.12826e-07\n",
      "evaluation/env_infos/final/reward_forward Std           5.92665e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.30699e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.0509e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.017037\n",
      "evaluation/env_infos/initial/reward_forward Std         0.144316\n",
      "evaluation/env_infos/initial/reward_forward Max         0.245935\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.337166\n",
      "evaluation/env_infos/reward_forward Mean               -0.00116112\n",
      "evaluation/env_infos/reward_forward Std                 0.0548936\n",
      "evaluation/env_infos/reward_forward Max                 1.29706\n",
      "evaluation/env_infos/reward_forward Min                -0.730212\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.053879\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00466322\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0470148\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.059991\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0652645\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0110128\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0460437\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0828195\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0540932\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00744687\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0268622\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.331879\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -9.20022e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           4.08358e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.30699e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.0509e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.155347\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.232161\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.602236\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.337166\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00128163\n",
      "evaluation/env_infos/torso_velocity Std                 0.0616716\n",
      "evaluation/env_infos/torso_velocity Max                 1.88467\n",
      "evaluation/env_infos/torso_velocity Min                -1.8243\n",
      "time/data storing (s)                                   0.0151911\n",
      "time/evaluation sampling (s)                           47.9063\n",
      "time/exploration sampling (s)                           1.8959\n",
      "time/logging (s)                                        0.284862\n",
      "time/saving (s)                                         0.0290187\n",
      "time/training (s)                                       3.78105\n",
      "time/epoch (s)                                         53.9123\n",
      "time/total (s)                                        762.891\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:23:57.964387 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 10 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.206575\n",
      "trainer/QF2 Loss                                        0.186282\n",
      "trainer/Policy Loss                                    -3.58782\n",
      "trainer/Q1 Predictions Mean                             3.99312\n",
      "trainer/Q1 Predictions Std                              0.650996\n",
      "trainer/Q1 Predictions Max                              5.48767\n",
      "trainer/Q1 Predictions Min                              2.58037\n",
      "trainer/Q2 Predictions Mean                             4.00994\n",
      "trainer/Q2 Predictions Std                              0.67489\n",
      "trainer/Q2 Predictions Max                              5.52238\n",
      "trainer/Q2 Predictions Min                              2.26003\n",
      "trainer/Q Targets Mean                                  3.97145\n",
      "trainer/Q Targets Std                                   0.818654\n",
      "trainer/Q Targets Max                                   6.69103\n",
      "trainer/Q Targets Min                                  -0.933615\n",
      "trainer/Log Pis Mean                                    1.13662\n",
      "trainer/Log Pis Std                                     2.02556\n",
      "trainer/Log Pis Max                                     5.16457\n",
      "trainer/Log Pis Min                                    -5.95021\n",
      "trainer/Policy mu Mean                                  0.0271712\n",
      "trainer/Policy mu Std                                   0.0946863\n",
      "trainer/Policy mu Max                                   0.585848\n",
      "trainer/Policy mu Min                                  -0.433738\n",
      "trainer/Policy log std Mean                            -1.49636\n",
      "trainer/Policy log std Std                              0.154979\n",
      "trainer/Policy log std Max                             -0.972245\n",
      "trainer/Policy log std Min                             -1.91532\n",
      "trainer/Alpha                                           0.059891\n",
      "trainer/Alpha Loss                                    -19.3097\n",
      "exploration/num steps total                         12000\n",
      "exploration/num paths total                            64\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.838827\n",
      "exploration/Rewards Std                                 0.164043\n",
      "exploration/Rewards Max                                 1.93739\n",
      "exploration/Rewards Min                                 0.338613\n",
      "exploration/Returns Mean                              838.827\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               838.827\n",
      "exploration/Returns Min                               838.827\n",
      "exploration/Actions Mean                                0.0273152\n",
      "exploration/Actions Std                                 0.216928\n",
      "exploration/Actions Max                                 0.84044\n",
      "exploration/Actions Min                                -0.713909\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           838.827\n",
      "exploration/env_infos/final/reward_forward Mean         0.694013\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.694013\n",
      "exploration/env_infos/final/reward_forward Min          0.694013\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0589462\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0589462\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0589462\n",
      "exploration/env_infos/reward_forward Mean               0.0478457\n",
      "exploration/env_infos/reward_forward Std                0.378942\n",
      "exploration/env_infos/reward_forward Max                1.10916\n",
      "exploration/env_infos/reward_forward Min               -1.12021\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.143593\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.143593\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.143593\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.532924\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.532924\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.532924\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.191216\n",
      "exploration/env_infos/reward_ctrl Std                   0.0986629\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0258185\n",
      "exploration/env_infos/reward_ctrl Min                  -0.661387\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.079064\n",
      "exploration/env_infos/final/torso_velocity Std          0.484749\n",
      "exploration/env_infos/final/torso_velocity Max          0.694013\n",
      "exploration/env_infos/final/torso_velocity Min         -0.490806\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.170104\n",
      "exploration/env_infos/initial/torso_velocity Std        0.257339\n",
      "exploration/env_infos/initial/torso_velocity Max        0.529552\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0589462\n",
      "exploration/env_infos/torso_velocity Mean               0.0487113\n",
      "exploration/env_infos/torso_velocity Std                0.451541\n",
      "exploration/env_infos/torso_velocity Max                2.17932\n",
      "exploration/env_infos/torso_velocity Min               -1.5437\n",
      "evaluation/num steps total                         275000\n",
      "evaluation/num paths total                            275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.990544\n",
      "evaluation/Rewards Std                                  0.0201272\n",
      "evaluation/Rewards Max                                  2.57161\n",
      "evaluation/Rewards Min                                  0.89392\n",
      "evaluation/Returns Mean                               990.544\n",
      "evaluation/Returns Std                                  2.26076\n",
      "evaluation/Returns Max                                995.4\n",
      "evaluation/Returns Min                                986.035\n",
      "evaluation/Actions Mean                                 0.0312712\n",
      "evaluation/Actions Std                                  0.039365\n",
      "evaluation/Actions Max                                  0.307042\n",
      "evaluation/Actions Min                                 -0.25269\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            990.544\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.01378e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.8586e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.86775e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.72233e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0324985\n",
      "evaluation/env_infos/initial/reward_forward Std         0.102326\n",
      "evaluation/env_infos/initial/reward_forward Max         0.198811\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.214062\n",
      "evaluation/env_infos/reward_forward Mean                0.00210327\n",
      "evaluation/env_infos/reward_forward Std                 0.0515125\n",
      "evaluation/env_infos/reward_forward Max                 1.25563\n",
      "evaluation/env_infos/reward_forward Min                -0.863432\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00980726\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00206036\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00702793\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.014311\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0173502\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00501229\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00617798\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0241003\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.01011\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0040098\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00581743\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.10608\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          6.8348e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.40766e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.06148e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.62459e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.143041\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.221933\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.673881\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.237033\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00107606\n",
      "evaluation/env_infos/torso_velocity Std                 0.0635785\n",
      "evaluation/env_infos/torso_velocity Max                 1.69828\n",
      "evaluation/env_infos/torso_velocity Min                -1.82844\n",
      "time/data storing (s)                                   0.0146771\n",
      "time/evaluation sampling (s)                           46.837\n",
      "time/exploration sampling (s)                           1.9273\n",
      "time/logging (s)                                        0.284368\n",
      "time/saving (s)                                         0.0292948\n",
      "time/training (s)                                       3.90108\n",
      "time/epoch (s)                                         52.9938\n",
      "time/total (s)                                        816.135\n",
      "Epoch                                                  10\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:24:52.371520 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 11 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  13000\n",
      "trainer/QF1 Loss                                        0.2247\n",
      "trainer/QF2 Loss                                        0.202953\n",
      "trainer/Policy Loss                                    -2.98279\n",
      "trainer/Q1 Predictions Mean                             4.47928\n",
      "trainer/Q1 Predictions Std                              0.651975\n",
      "trainer/Q1 Predictions Max                              5.93676\n",
      "trainer/Q1 Predictions Min                              2.99677\n",
      "trainer/Q2 Predictions Mean                             4.48649\n",
      "trainer/Q2 Predictions Std                              0.695854\n",
      "trainer/Q2 Predictions Max                              6.04291\n",
      "trainer/Q2 Predictions Min                              3.04955\n",
      "trainer/Q Targets Mean                                  4.26136\n",
      "trainer/Q Targets Std                                   0.776646\n",
      "trainer/Q Targets Max                                   6.74695\n",
      "trainer/Q Targets Min                                   0.214277\n",
      "trainer/Log Pis Mean                                    2.36885\n",
      "trainer/Log Pis Std                                     2.15542\n",
      "trainer/Log Pis Max                                     6.45721\n",
      "trainer/Log Pis Min                                    -6.54004\n",
      "trainer/Policy mu Mean                                  0.0137046\n",
      "trainer/Policy mu Std                                   0.099059\n",
      "trainer/Policy mu Max                                   0.43099\n",
      "trainer/Policy mu Min                                  -0.38644\n",
      "trainer/Policy log std Mean                            -1.66315\n",
      "trainer/Policy log std Std                              0.137742\n",
      "trainer/Policy log std Max                             -1.15952\n",
      "trainer/Policy log std Min                             -2.12145\n",
      "trainer/Alpha                                           0.0509523\n",
      "trainer/Alpha Loss                                    -16.7545\n",
      "exploration/num steps total                         13000\n",
      "exploration/num paths total                            65\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.886158\n",
      "exploration/Rewards Std                                 0.123586\n",
      "exploration/Rewards Max                                 1.82435\n",
      "exploration/Rewards Min                                 0.494334\n",
      "exploration/Returns Mean                              886.158\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               886.158\n",
      "exploration/Returns Min                               886.158\n",
      "exploration/Actions Mean                                0.0129925\n",
      "exploration/Actions Std                                 0.184765\n",
      "exploration/Actions Max                                 0.601216\n",
      "exploration/Actions Min                                -0.668637\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           886.158\n",
      "exploration/env_infos/final/reward_forward Mean        -0.343993\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.343993\n",
      "exploration/env_infos/final/reward_forward Min         -0.343993\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0103364\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0103364\n",
      "exploration/env_infos/initial/reward_forward Min        0.0103364\n",
      "exploration/env_infos/reward_forward Mean              -0.0157461\n",
      "exploration/env_infos/reward_forward Std                0.314727\n",
      "exploration/env_infos/reward_forward Max                0.86463\n",
      "exploration/env_infos/reward_forward Min               -1.48816\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.167458\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.167458\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.167458\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.140174\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.140174\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.140174\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.137228\n",
      "exploration/env_infos/reward_ctrl Std                   0.0660163\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0116626\n",
      "exploration/env_infos/reward_ctrl Min                  -0.595952\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.196818\n",
      "exploration/env_infos/final/torso_velocity Std          0.233469\n",
      "exploration/env_infos/final/torso_velocity Max          0.132731\n",
      "exploration/env_infos/final/torso_velocity Min         -0.379193\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.235064\n",
      "exploration/env_infos/initial/torso_velocity Std        0.239584\n",
      "exploration/env_infos/initial/torso_velocity Max        0.567027\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0103364\n",
      "exploration/env_infos/torso_velocity Mean              -0.00289787\n",
      "exploration/env_infos/torso_velocity Std                0.328938\n",
      "exploration/env_infos/torso_velocity Max                1.20352\n",
      "exploration/env_infos/torso_velocity Min               -1.48816\n",
      "evaluation/num steps total                         300000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.985231\n",
      "evaluation/Rewards Std                                  0.0249106\n",
      "evaluation/Rewards Max                                  2.29583\n",
      "evaluation/Rewards Min                                  0.91641\n",
      "evaluation/Returns Mean                               985.231\n",
      "evaluation/Returns Std                                  3.48259\n",
      "evaluation/Returns Max                                998.099\n",
      "evaluation/Returns Min                                980.429\n",
      "evaluation/Actions Mean                                 0.0227444\n",
      "evaluation/Actions Std                                  0.0606255\n",
      "evaluation/Actions Max                                  0.236903\n",
      "evaluation/Actions Min                                 -0.227308\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            985.231\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00496815\n",
      "evaluation/env_infos/final/reward_forward Std           0.0400088\n",
      "evaluation/env_infos/final/reward_forward Max           0.133258\n",
      "evaluation/env_infos/final/reward_forward Min          -0.115656\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0389052\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122566\n",
      "evaluation/env_infos/initial/reward_forward Max         0.233234\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.2765\n",
      "evaluation/env_infos/reward_forward Mean                0.00339617\n",
      "evaluation/env_infos/reward_forward Std                 0.0701125\n",
      "evaluation/env_infos/reward_forward Max                 1.83018\n",
      "evaluation/env_infos/reward_forward Min                -0.702396\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0165665\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00159539\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0138007\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0196883\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0206891\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00217369\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0160283\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0254653\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.016771\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00262762\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0106433\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0835898\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00634389\n",
      "evaluation/env_infos/final/torso_velocity Std           0.090979\n",
      "evaluation/env_infos/final/torso_velocity Max           0.295582\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.460544\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.121042\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.235258\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.586201\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.2765\n",
      "evaluation/env_infos/torso_velocity Mean                0.000607198\n",
      "evaluation/env_infos/torso_velocity Std                 0.0757519\n",
      "evaluation/env_infos/torso_velocity Max                 1.83018\n",
      "evaluation/env_infos/torso_velocity Min                -1.86066\n",
      "time/data storing (s)                                   0.0157814\n",
      "time/evaluation sampling (s)                           47.4644\n",
      "time/exploration sampling (s)                           2.07458\n",
      "time/logging (s)                                        0.296407\n",
      "time/saving (s)                                         0.0317448\n",
      "time/training (s)                                       4.27428\n",
      "time/epoch (s)                                         54.1572\n",
      "time/total (s)                                        870.554\n",
      "Epoch                                                  11\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:25:42.992159 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 12 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.204241\n",
      "trainer/QF2 Loss                                        0.196053\n",
      "trainer/Policy Loss                                    -3.12722\n",
      "trainer/Q1 Predictions Mean                             4.72272\n",
      "trainer/Q1 Predictions Std                              0.789886\n",
      "trainer/Q1 Predictions Max                              7.15609\n",
      "trainer/Q1 Predictions Min                              2.23232\n",
      "trainer/Q2 Predictions Mean                             4.75578\n",
      "trainer/Q2 Predictions Std                              0.786896\n",
      "trainer/Q2 Predictions Max                              7.49618\n",
      "trainer/Q2 Predictions Min                              1.90722\n",
      "trainer/Q Targets Mean                                  4.77589\n",
      "trainer/Q Targets Std                                   0.90364\n",
      "trainer/Q Targets Max                                   9.03655\n",
      "trainer/Q Targets Min                                  -0.608529\n",
      "trainer/Log Pis Mean                                    2.37746\n",
      "trainer/Log Pis Std                                     2.17725\n",
      "trainer/Log Pis Max                                     7.07489\n",
      "trainer/Log Pis Min                                    -5.83572\n",
      "trainer/Policy mu Mean                                  0.018166\n",
      "trainer/Policy mu Std                                   0.106796\n",
      "trainer/Policy mu Max                                   0.818904\n",
      "trainer/Policy mu Min                                  -0.492886\n",
      "trainer/Policy log std Mean                            -1.7005\n",
      "trainer/Policy log std Std                              0.149067\n",
      "trainer/Policy log std Max                             -0.954221\n",
      "trainer/Policy log std Min                             -2.14996\n",
      "trainer/Alpha                                           0.0438848\n",
      "trainer/Alpha Loss                                    -17.5686\n",
      "exploration/num steps total                         14000\n",
      "exploration/num paths total                            66\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.87788\n",
      "exploration/Rewards Std                                 0.166917\n",
      "exploration/Rewards Max                                 2.32096\n",
      "exploration/Rewards Min                                 0.330401\n",
      "exploration/Returns Mean                              877.88\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               877.88\n",
      "exploration/Returns Min                               877.88\n",
      "exploration/Actions Mean                                0.0201617\n",
      "exploration/Actions Std                                 0.193878\n",
      "exploration/Actions Max                                 0.773604\n",
      "exploration/Actions Min                                -0.698921\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           877.88\n",
      "exploration/env_infos/final/reward_forward Mean         0.0294863\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0294863\n",
      "exploration/env_infos/final/reward_forward Min          0.0294863\n",
      "exploration/env_infos/initial/reward_forward Mean       0.222647\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.222647\n",
      "exploration/env_infos/initial/reward_forward Min        0.222647\n",
      "exploration/env_infos/reward_forward Mean               0.0101625\n",
      "exploration/env_infos/reward_forward Std                0.39987\n",
      "exploration/env_infos/reward_forward Max                1.29137\n",
      "exploration/env_infos/reward_forward Min               -1.27855\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.107174\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.107174\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.107174\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.198603\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.198603\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.198603\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.15198\n",
      "exploration/env_infos/reward_ctrl Std                   0.0810251\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0203605\n",
      "exploration/env_infos/reward_ctrl Min                  -0.669599\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.273051\n",
      "exploration/env_infos/final/torso_velocity Std          0.213105\n",
      "exploration/env_infos/final/torso_velocity Max          0.548547\n",
      "exploration/env_infos/final/torso_velocity Min          0.0294863\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.154472\n",
      "exploration/env_infos/initial/torso_velocity Std        0.217152\n",
      "exploration/env_infos/initial/torso_velocity Max        0.379705\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.138934\n",
      "exploration/env_infos/torso_velocity Mean               0.0106571\n",
      "exploration/env_infos/torso_velocity Std                0.439287\n",
      "exploration/env_infos/torso_velocity Max                1.52643\n",
      "exploration/env_infos/torso_velocity Min               -1.65041\n",
      "evaluation/num steps total                         325000\n",
      "evaluation/num paths total                            325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.979337\n",
      "evaluation/Rewards Std                                  0.030296\n",
      "evaluation/Rewards Max                                  2.71851\n",
      "evaluation/Rewards Min                                  0.830195\n",
      "evaluation/Returns Mean                               979.337\n",
      "evaluation/Returns Std                                  1.74021\n",
      "evaluation/Returns Max                                984.018\n",
      "evaluation/Returns Min                                976.963\n",
      "evaluation/Actions Mean                                 0.0114659\n",
      "evaluation/Actions Std                                  0.0727794\n",
      "evaluation/Actions Max                                  0.487908\n",
      "evaluation/Actions Min                                 -0.370163\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            979.337\n",
      "evaluation/env_infos/final/reward_forward Mean         -4.76841e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.07788e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.41092e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.76493e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0176437\n",
      "evaluation/env_infos/initial/reward_forward Std         0.099638\n",
      "evaluation/env_infos/initial/reward_forward Max         0.250698\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.111571\n",
      "evaluation/env_infos/reward_forward Mean                0.00382718\n",
      "evaluation/env_infos/reward_forward Std                 0.055504\n",
      "evaluation/env_infos/reward_forward Max                 0.987768\n",
      "evaluation/env_infos/reward_forward Min                -0.906051\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0212507\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00102456\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0187917\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0228578\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0308321\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00371201\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0215915\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0383681\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0217132\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00537258\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0105212\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.169805\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.46053e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.67621e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.41092e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.76493e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.168134\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.231553\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.656678\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.192487\n",
      "evaluation/env_infos/torso_velocity Mean                0.000404201\n",
      "evaluation/env_infos/torso_velocity Std                 0.0657607\n",
      "evaluation/env_infos/torso_velocity Max                 1.8095\n",
      "evaluation/env_infos/torso_velocity Min                -1.93163\n",
      "time/data storing (s)                                   0.0147109\n",
      "time/evaluation sampling (s)                           44.529\n",
      "time/exploration sampling (s)                           1.82716\n",
      "time/logging (s)                                        0.282245\n",
      "time/saving (s)                                         0.0283498\n",
      "time/training (s)                                       3.65188\n",
      "time/epoch (s)                                         50.3334\n",
      "time/total (s)                                        921.161\n",
      "Epoch                                                  12\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:26:34.719394 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 13 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  15000\n",
      "trainer/QF1 Loss                                        0.30642\n",
      "trainer/QF2 Loss                                        0.315299\n",
      "trainer/Policy Loss                                    -2.47802\n",
      "trainer/Q1 Predictions Mean                             5.08557\n",
      "trainer/Q1 Predictions Std                              0.865665\n",
      "trainer/Q1 Predictions Max                              6.84673\n",
      "trainer/Q1 Predictions Min                              1.74149\n",
      "trainer/Q2 Predictions Mean                             5.11153\n",
      "trainer/Q2 Predictions Std                              0.873438\n",
      "trainer/Q2 Predictions Max                              6.74668\n",
      "trainer/Q2 Predictions Min                              1.53926\n",
      "trainer/Q Targets Mean                                  5.0405\n",
      "trainer/Q Targets Std                                   1.05697\n",
      "trainer/Q Targets Max                                   8.1725\n",
      "trainer/Q Targets Min                                  -0.871689\n",
      "trainer/Log Pis Mean                                    3.41409\n",
      "trainer/Log Pis Std                                     2.04899\n",
      "trainer/Log Pis Max                                     8.59865\n",
      "trainer/Log Pis Min                                    -2.42107\n",
      "trainer/Policy mu Mean                                  0.0134961\n",
      "trainer/Policy mu Std                                   0.103802\n",
      "trainer/Policy mu Max                                   0.842431\n",
      "trainer/Policy mu Min                                  -0.501305\n",
      "trainer/Policy log std Mean                            -1.79958\n",
      "trainer/Policy log std Std                              0.150807\n",
      "trainer/Policy log std Max                             -1.10644\n",
      "trainer/Policy log std Min                             -2.19931\n",
      "trainer/Alpha                                           0.0381411\n",
      "trainer/Alpha Loss                                    -14.9735\n",
      "exploration/num steps total                         15000\n",
      "exploration/num paths total                            67\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.926871\n",
      "exploration/Rewards Std                                 0.170281\n",
      "exploration/Rewards Max                                 2.34984\n",
      "exploration/Rewards Min                                 0.476467\n",
      "exploration/Returns Mean                              926.871\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               926.871\n",
      "exploration/Returns Min                               926.871\n",
      "exploration/Actions Mean                                0.00692675\n",
      "exploration/Actions Std                                 0.16835\n",
      "exploration/Actions Max                                 0.624633\n",
      "exploration/Actions Min                                -0.640983\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           926.871\n",
      "exploration/env_infos/final/reward_forward Mean        -0.213257\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.213257\n",
      "exploration/env_infos/final/reward_forward Min         -0.213257\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0485243\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0485243\n",
      "exploration/env_infos/initial/reward_forward Min        0.0485243\n",
      "exploration/env_infos/reward_forward Mean               0.0066303\n",
      "exploration/env_infos/reward_forward Std                0.359792\n",
      "exploration/env_infos/reward_forward Max                0.99827\n",
      "exploration/env_infos/reward_forward Min               -1.13034\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.137506\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.137506\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.137506\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.239389\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.239389\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.239389\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.113559\n",
      "exploration/env_infos/reward_ctrl Std                   0.0627964\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00886272\n",
      "exploration/env_infos/reward_ctrl Min                  -0.523533\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.180593\n",
      "exploration/env_infos/final/torso_velocity Std          0.0915026\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0558222\n",
      "exploration/env_infos/final/torso_velocity Min         -0.272699\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0523394\n",
      "exploration/env_infos/initial/torso_velocity Std        0.171331\n",
      "exploration/env_infos/initial/torso_velocity Max        0.264058\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.155564\n",
      "exploration/env_infos/torso_velocity Mean              -0.00374739\n",
      "exploration/env_infos/torso_velocity Std                0.396771\n",
      "exploration/env_infos/torso_velocity Max                1.52115\n",
      "exploration/env_infos/torso_velocity Min               -1.5465\n",
      "evaluation/num steps total                         350000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.984958\n",
      "evaluation/Rewards Std                                  0.0369176\n",
      "evaluation/Rewards Max                                  2.97797\n",
      "evaluation/Rewards Min                                  0.838471\n",
      "evaluation/Returns Mean                               984.958\n",
      "evaluation/Returns Std                                  4.37098\n",
      "evaluation/Returns Max                                995.668\n",
      "evaluation/Returns Min                                978.041\n",
      "evaluation/Actions Mean                                 0.00935078\n",
      "evaluation/Actions Std                                  0.0631838\n",
      "evaluation/Actions Max                                  0.424451\n",
      "evaluation/Actions Min                                 -0.338619\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            984.958\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000118983\n",
      "evaluation/env_infos/final/reward_forward Std           0.00038565\n",
      "evaluation/env_infos/final/reward_forward Max           0.00189375\n",
      "evaluation/env_infos/final/reward_forward Min          -3.41214e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0496513\n",
      "evaluation/env_infos/initial/reward_forward Std         0.120957\n",
      "evaluation/env_infos/initial/reward_forward Max         0.304134\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.141149\n",
      "evaluation/env_infos/reward_forward Mean                0.00255026\n",
      "evaluation/env_infos/reward_forward Std                 0.049852\n",
      "evaluation/env_infos/reward_forward Max                 1.36228\n",
      "evaluation/env_infos/reward_forward Min                -1.08177\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0152107\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00369807\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00973598\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0218648\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0212258\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00325583\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0130071\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0258914\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0163185\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0065677\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00335153\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.238308\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.000105597\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000646584\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00384082\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00211902\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.161873\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.206902\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.569954\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.23437\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0010936\n",
      "evaluation/env_infos/torso_velocity Std                 0.0699882\n",
      "evaluation/env_infos/torso_velocity Max                 1.8395\n",
      "evaluation/env_infos/torso_velocity Min                -1.83231\n",
      "time/data storing (s)                                   0.0146515\n",
      "time/evaluation sampling (s)                           45.5438\n",
      "time/exploration sampling (s)                           1.90499\n",
      "time/logging (s)                                        0.280773\n",
      "time/saving (s)                                         0.0281534\n",
      "time/training (s)                                       3.68268\n",
      "time/epoch (s)                                         51.4551\n",
      "time/total (s)                                        972.886\n",
      "Epoch                                                  13\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:27:26.749525 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 14 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.191935\n",
      "trainer/QF2 Loss                                        0.138866\n",
      "trainer/Policy Loss                                    -2.68274\n",
      "trainer/Q1 Predictions Mean                             5.56519\n",
      "trainer/Q1 Predictions Std                              0.807454\n",
      "trainer/Q1 Predictions Max                              7.21985\n",
      "trainer/Q1 Predictions Min                              2.79413\n",
      "trainer/Q2 Predictions Mean                             5.55844\n",
      "trainer/Q2 Predictions Std                              0.829975\n",
      "trainer/Q2 Predictions Max                              6.85447\n",
      "trainer/Q2 Predictions Min                              1.83004\n",
      "trainer/Q Targets Mean                                  5.54611\n",
      "trainer/Q Targets Std                                   0.88746\n",
      "trainer/Q Targets Max                                   7.09419\n",
      "trainer/Q Targets Min                                  -1.2366\n",
      "trainer/Log Pis Mean                                    3.54665\n",
      "trainer/Log Pis Std                                     2.35094\n",
      "trainer/Log Pis Max                                     8.46664\n",
      "trainer/Log Pis Min                                    -6.36245\n",
      "trainer/Policy mu Mean                                  0.00622631\n",
      "trainer/Policy mu Std                                   0.0997078\n",
      "trainer/Policy mu Max                                   0.651372\n",
      "trainer/Policy mu Min                                  -0.374303\n",
      "trainer/Policy log std Mean                            -1.84396\n",
      "trainer/Policy log std Std                              0.155282\n",
      "trainer/Policy log std Max                             -1.24186\n",
      "trainer/Policy log std Min                             -2.40518\n",
      "trainer/Alpha                                           0.0336112\n",
      "trainer/Alpha Loss                                    -15.1042\n",
      "exploration/num steps total                         16000\n",
      "exploration/num paths total                            68\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.931467\n",
      "exploration/Rewards Std                                 0.137179\n",
      "exploration/Rewards Max                                 2.37192\n",
      "exploration/Rewards Min                                 0.630165\n",
      "exploration/Returns Mean                              931.467\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               931.467\n",
      "exploration/Returns Min                               931.467\n",
      "exploration/Actions Mean                                0.0127045\n",
      "exploration/Actions Std                                 0.153049\n",
      "exploration/Actions Max                                 0.553352\n",
      "exploration/Actions Min                                -0.616633\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           931.467\n",
      "exploration/env_infos/final/reward_forward Mean        -0.162012\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.162012\n",
      "exploration/env_infos/final/reward_forward Min         -0.162012\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.293488\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.293488\n",
      "exploration/env_infos/initial/reward_forward Min       -0.293488\n",
      "exploration/env_infos/reward_forward Mean              -0.000506615\n",
      "exploration/env_infos/reward_forward Std                0.152433\n",
      "exploration/env_infos/reward_forward Max                0.620455\n",
      "exploration/env_infos/reward_forward Min               -0.721822\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.189522\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.189522\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.189522\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0242021\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0242021\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0242021\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0943419\n",
      "exploration/env_infos/reward_ctrl Std                   0.0497083\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0102751\n",
      "exploration/env_infos/reward_ctrl Min                  -0.369835\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0816436\n",
      "exploration/env_infos/final/torso_velocity Std          0.0975083\n",
      "exploration/env_infos/final/torso_velocity Max          0.0555843\n",
      "exploration/env_infos/final/torso_velocity Min         -0.162012\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0255071\n",
      "exploration/env_infos/initial/torso_velocity Std        0.328934\n",
      "exploration/env_infos/initial/torso_velocity Max        0.478225\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.293488\n",
      "exploration/env_infos/torso_velocity Mean              -0.014548\n",
      "exploration/env_infos/torso_velocity Std                0.20498\n",
      "exploration/env_infos/torso_velocity Max                1.42042\n",
      "exploration/env_infos/torso_velocity Min               -1.42855\n",
      "evaluation/num steps total                         375000\n",
      "evaluation/num paths total                            375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.987911\n",
      "evaluation/Rewards Std                                  0.0187051\n",
      "evaluation/Rewards Max                                  1.97742\n",
      "evaluation/Rewards Min                                  0.797423\n",
      "evaluation/Returns Mean                               987.911\n",
      "evaluation/Returns Std                                  2.19619\n",
      "evaluation/Returns Max                                994.582\n",
      "evaluation/Returns Min                                984.125\n",
      "evaluation/Actions Mean                                -0.00144516\n",
      "evaluation/Actions Std                                  0.0565862\n",
      "evaluation/Actions Max                                  0.413931\n",
      "evaluation/Actions Min                                 -0.355063\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            987.911\n",
      "evaluation/env_infos/final/reward_forward Mean          1.04149e-06\n",
      "evaluation/env_infos/final/reward_forward Std           5.56248e-06\n",
      "evaluation/env_infos/final/reward_forward Max           2.82738e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -5.2293e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00338252\n",
      "evaluation/env_infos/initial/reward_forward Std         0.123985\n",
      "evaluation/env_infos/initial/reward_forward Max         0.288199\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.17855\n",
      "evaluation/env_infos/reward_forward Mean                0.00324573\n",
      "evaluation/env_infos/reward_forward Std                 0.0649228\n",
      "evaluation/env_infos/reward_forward Max                 1.77778\n",
      "evaluation/env_infos/reward_forward Min                -0.811009\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0128708\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00228282\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00933998\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0177845\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0156417\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00301199\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0106436\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0228891\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0128163\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00533153\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00503777\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.202577\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.4174e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           9.96667e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           8.23968e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.00262e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135391\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.22709\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.645285\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.288207\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00117802\n",
      "evaluation/env_infos/torso_velocity Std                 0.0694343\n",
      "evaluation/env_infos/torso_velocity Max                 1.77778\n",
      "evaluation/env_infos/torso_velocity Min                -1.81589\n",
      "time/data storing (s)                                   0.0148896\n",
      "time/evaluation sampling (s)                           45.2045\n",
      "time/exploration sampling (s)                           2.12577\n",
      "time/logging (s)                                        0.286358\n",
      "time/saving (s)                                         0.0274753\n",
      "time/training (s)                                       4.10044\n",
      "time/epoch (s)                                         51.7594\n",
      "time/total (s)                                       1024.92\n",
      "Epoch                                                  14\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:28:18.604293 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 15 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  17000\n",
      "trainer/QF1 Loss                                        0.169845\n",
      "trainer/QF2 Loss                                        0.157199\n",
      "trainer/Policy Loss                                    -1.36531\n",
      "trainer/Q1 Predictions Mean                             5.79155\n",
      "trainer/Q1 Predictions Std                              0.847722\n",
      "trainer/Q1 Predictions Max                              7.14329\n",
      "trainer/Q1 Predictions Min                              3.67565\n",
      "trainer/Q2 Predictions Mean                             5.81345\n",
      "trainer/Q2 Predictions Std                              0.860039\n",
      "trainer/Q2 Predictions Max                              7.32133\n",
      "trainer/Q2 Predictions Min                              3.88598\n",
      "trainer/Q Targets Mean                                  6.01101\n",
      "trainer/Q Targets Std                                   0.857817\n",
      "trainer/Q Targets Max                                   9.68913\n",
      "trainer/Q Targets Min                                   3.92065\n",
      "trainer/Log Pis Mean                                    5.11583\n",
      "trainer/Log Pis Std                                     2.2515\n",
      "trainer/Log Pis Max                                    10.775\n",
      "trainer/Log Pis Min                                    -5.50775\n",
      "trainer/Policy mu Mean                                  0.00684511\n",
      "trainer/Policy mu Std                                   0.114929\n",
      "trainer/Policy mu Max                                   0.520714\n",
      "trainer/Policy mu Min                                  -0.586657\n",
      "trainer/Policy log std Mean                            -2.01379\n",
      "trainer/Policy log std Std                              0.168119\n",
      "trainer/Policy log std Max                             -1.37865\n",
      "trainer/Policy log std Min                             -2.53117\n",
      "trainer/Alpha                                           0.0299473\n",
      "trainer/Alpha Loss                                    -10.1155\n",
      "exploration/num steps total                         17000\n",
      "exploration/num paths total                            69\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.942008\n",
      "exploration/Rewards Std                                 0.0987446\n",
      "exploration/Rewards Max                                 1.65334\n",
      "exploration/Rewards Min                                 0.602881\n",
      "exploration/Returns Mean                              942.008\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               942.008\n",
      "exploration/Returns Min                               942.008\n",
      "exploration/Actions Mean                                0.0227162\n",
      "exploration/Actions Std                                 0.140354\n",
      "exploration/Actions Max                                 0.573901\n",
      "exploration/Actions Min                                -0.641303\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           942.008\n",
      "exploration/env_infos/final/reward_forward Mean         0.0299264\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0299264\n",
      "exploration/env_infos/final/reward_forward Min          0.0299264\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0979316\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0979316\n",
      "exploration/env_infos/initial/reward_forward Min        0.0979316\n",
      "exploration/env_infos/reward_forward Mean               0.0118692\n",
      "exploration/env_infos/reward_forward Std                0.18969\n",
      "exploration/env_infos/reward_forward Max                0.693022\n",
      "exploration/env_infos/reward_forward Min               -1.25061\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.116235\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.116235\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.116235\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0874566\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0874566\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0874566\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0808607\n",
      "exploration/env_infos/reward_ctrl Std                   0.0443915\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00665448\n",
      "exploration/env_infos/reward_ctrl Min                  -0.397119\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0135694\n",
      "exploration/env_infos/final/torso_velocity Std          0.063817\n",
      "exploration/env_infos/final/torso_velocity Max          0.0331663\n",
      "exploration/env_infos/final/torso_velocity Min         -0.103801\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0974049\n",
      "exploration/env_infos/initial/torso_velocity Std        0.301111\n",
      "exploration/env_infos/initial/torso_velocity Max        0.465925\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.271642\n",
      "exploration/env_infos/torso_velocity Mean               0.000651819\n",
      "exploration/env_infos/torso_velocity Std                0.263486\n",
      "exploration/env_infos/torso_velocity Max                1.10662\n",
      "exploration/env_infos/torso_velocity Min               -1.25061\n",
      "evaluation/num steps total                         400000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.97688\n",
      "evaluation/Rewards Std                                  0.0261403\n",
      "evaluation/Rewards Max                                  2.41432\n",
      "evaluation/Rewards Min                                  0.712203\n",
      "evaluation/Returns Mean                               976.88\n",
      "evaluation/Returns Std                                  2.61504\n",
      "evaluation/Returns Max                                981.524\n",
      "evaluation/Returns Min                                971.71\n",
      "evaluation/Actions Mean                                 0.0156151\n",
      "evaluation/Actions Std                                  0.076256\n",
      "evaluation/Actions Max                                  0.417383\n",
      "evaluation/Actions Min                                 -0.508702\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            976.88\n",
      "evaluation/env_infos/final/reward_forward Mean         -3.41652e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.25229e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.42016e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.40678e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0236592\n",
      "evaluation/env_infos/initial/reward_forward Std         0.143041\n",
      "evaluation/env_infos/initial/reward_forward Max         0.330286\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.396056\n",
      "evaluation/env_infos/reward_forward Mean               -0.00124102\n",
      "evaluation/env_infos/reward_forward Std                 0.0631284\n",
      "evaluation/env_infos/reward_forward Max                 1.78024\n",
      "evaluation/env_infos/reward_forward Min                -1.30522\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0235898\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0023445\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0201826\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0279308\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0269166\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00356489\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0193174\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0346155\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0242352\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00842478\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0114505\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.287797\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.80117e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.08957e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           6.26536e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.02977e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.134405\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.240139\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.687539\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.396056\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00277191\n",
      "evaluation/env_infos/torso_velocity Std                 0.0680468\n",
      "evaluation/env_infos/torso_velocity Max                 1.78024\n",
      "evaluation/env_infos/torso_velocity Min                -2.18914\n",
      "time/data storing (s)                                   0.0149703\n",
      "time/evaluation sampling (s)                           45.1409\n",
      "time/exploration sampling (s)                           1.96278\n",
      "time/logging (s)                                        0.290652\n",
      "time/saving (s)                                         0.0284152\n",
      "time/training (s)                                       4.13499\n",
      "time/epoch (s)                                         51.5727\n",
      "time/total (s)                                       1076.78\n",
      "Epoch                                                  15\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:29:09.507558 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 16 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.168708\n",
      "trainer/QF2 Loss                                        0.186586\n",
      "trainer/Policy Loss                                    -1.80792\n",
      "trainer/Q1 Predictions Mean                             6.36904\n",
      "trainer/Q1 Predictions Std                              0.832137\n",
      "trainer/Q1 Predictions Max                              8.41603\n",
      "trainer/Q1 Predictions Min                              3.85414\n",
      "trainer/Q2 Predictions Mean                             6.34732\n",
      "trainer/Q2 Predictions Std                              0.871564\n",
      "trainer/Q2 Predictions Max                              8.48044\n",
      "trainer/Q2 Predictions Min                              3.47922\n",
      "trainer/Q Targets Mean                                  6.34618\n",
      "trainer/Q Targets Std                                   0.909554\n",
      "trainer/Q Targets Max                                   8.81639\n",
      "trainer/Q Targets Min                                   3.53027\n",
      "trainer/Log Pis Mean                                    5.29091\n",
      "trainer/Log Pis Std                                     2.17674\n",
      "trainer/Log Pis Max                                    10.6606\n",
      "trainer/Log Pis Min                                    -2.13589\n",
      "trainer/Policy mu Mean                                  0.0290302\n",
      "trainer/Policy mu Std                                   0.106163\n",
      "trainer/Policy mu Max                                   0.903745\n",
      "trainer/Policy mu Min                                  -0.451885\n",
      "trainer/Policy log std Mean                            -2.04411\n",
      "trainer/Policy log std Std                              0.156503\n",
      "trainer/Policy log std Max                             -1.53177\n",
      "trainer/Policy log std Min                             -2.46606\n",
      "trainer/Alpha                                           0.0269706\n",
      "trainer/Alpha Loss                                     -9.78532\n",
      "exploration/num steps total                         18000\n",
      "exploration/num paths total                            70\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.91621\n",
      "exploration/Rewards Std                                 0.0983557\n",
      "exploration/Rewards Max                                 1.537\n",
      "exploration/Rewards Min                                 0.625088\n",
      "exploration/Returns Mean                              916.21\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               916.21\n",
      "exploration/Returns Min                               916.21\n",
      "exploration/Actions Mean                                0.0348727\n",
      "exploration/Actions Std                                 0.156085\n",
      "exploration/Actions Max                                 0.547125\n",
      "exploration/Actions Min                                -0.634394\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           916.21\n",
      "exploration/env_infos/final/reward_forward Mean        -0.301696\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.301696\n",
      "exploration/env_infos/final/reward_forward Min         -0.301696\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0248361\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0248361\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0248361\n",
      "exploration/env_infos/reward_forward Mean              -0.0351145\n",
      "exploration/env_infos/reward_forward Std                0.254684\n",
      "exploration/env_infos/reward_forward Max                0.837291\n",
      "exploration/env_infos/reward_forward Min               -1.37944\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.140847\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.140847\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.140847\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0932585\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0932585\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0932585\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.102314\n",
      "exploration/env_infos/reward_ctrl Std                   0.0548953\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00905966\n",
      "exploration/env_infos/reward_ctrl Min                  -0.374912\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00913013\n",
      "exploration/env_infos/final/torso_velocity Std          0.216824\n",
      "exploration/env_infos/final/torso_velocity Max          0.216669\n",
      "exploration/env_infos/final/torso_velocity Min         -0.301696\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.103012\n",
      "exploration/env_infos/initial/torso_velocity Std        0.309118\n",
      "exploration/env_infos/initial/torso_velocity Max        0.528975\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.195103\n",
      "exploration/env_infos/torso_velocity Mean              -0.0055861\n",
      "exploration/env_infos/torso_velocity Std                0.308011\n",
      "exploration/env_infos/torso_velocity Max                1.15719\n",
      "exploration/env_infos/torso_velocity Min               -1.57352\n",
      "evaluation/num steps total                         425000\n",
      "evaluation/num paths total                            425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.958157\n",
      "evaluation/Rewards Std                                  0.0233921\n",
      "evaluation/Rewards Max                                  1.96671\n",
      "evaluation/Rewards Min                                  0.778427\n",
      "evaluation/Returns Mean                               958.157\n",
      "evaluation/Returns Std                                 11.4356\n",
      "evaluation/Returns Max                                982.651\n",
      "evaluation/Returns Min                                944.033\n",
      "evaluation/Actions Mean                                 0.0376486\n",
      "evaluation/Actions Std                                  0.0963418\n",
      "evaluation/Actions Max                                  0.414751\n",
      "evaluation/Actions Min                                 -0.437632\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            958.157\n",
      "evaluation/env_infos/final/reward_forward Mean          2.88268e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.23627e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.43414e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.96888e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00226672\n",
      "evaluation/env_infos/initial/reward_forward Std         0.136764\n",
      "evaluation/env_infos/initial/reward_forward Max         0.199471\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.381995\n",
      "evaluation/env_infos/reward_forward Mean                0.000952387\n",
      "evaluation/env_infos/reward_forward Std                 0.0496756\n",
      "evaluation/env_infos/reward_forward Max                 1.5257\n",
      "evaluation/env_infos/reward_forward Min                -1.11657\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0423909\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0124161\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0164815\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0587227\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0243951\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00421037\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0180923\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0330798\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0427966\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0135348\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00709298\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.221573\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -2.75282e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.40772e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.43414e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.69621e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.137628\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.234326\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.661822\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.381995\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000634212\n",
      "evaluation/env_infos/torso_velocity Std                 0.0618942\n",
      "evaluation/env_infos/torso_velocity Max                 1.5257\n",
      "evaluation/env_infos/torso_velocity Min                -1.89845\n",
      "time/data storing (s)                                   0.0148176\n",
      "time/evaluation sampling (s)                           44.3914\n",
      "time/exploration sampling (s)                           1.85764\n",
      "time/logging (s)                                        0.274335\n",
      "time/saving (s)                                         0.0273484\n",
      "time/training (s)                                       4.02913\n",
      "time/epoch (s)                                         50.5946\n",
      "time/total (s)                                       1127.67\n",
      "Epoch                                                  16\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:30:01.007001 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 17 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  19000\n",
      "trainer/QF1 Loss                                        0.247542\n",
      "trainer/QF2 Loss                                        0.20117\n",
      "trainer/Policy Loss                                    -1.73164\n",
      "trainer/Q1 Predictions Mean                             6.87707\n",
      "trainer/Q1 Predictions Std                              0.846659\n",
      "trainer/Q1 Predictions Max                              8.24576\n",
      "trainer/Q1 Predictions Min                              4.51527\n",
      "trainer/Q2 Predictions Mean                             6.78463\n",
      "trainer/Q2 Predictions Std                              0.84673\n",
      "trainer/Q2 Predictions Max                              8.30426\n",
      "trainer/Q2 Predictions Min                              4.3996\n",
      "trainer/Q Targets Mean                                  6.6795\n",
      "trainer/Q Targets Std                                   0.862601\n",
      "trainer/Q Targets Max                                   9.01819\n",
      "trainer/Q Targets Min                                   4.15501\n",
      "trainer/Log Pis Mean                                    5.8276\n",
      "trainer/Log Pis Std                                     2.3083\n",
      "trainer/Log Pis Max                                    12.4069\n",
      "trainer/Log Pis Min                                    -1.08468\n",
      "trainer/Policy mu Mean                                  0.0311268\n",
      "trainer/Policy mu Std                                   0.113412\n",
      "trainer/Policy mu Max                                   0.653054\n",
      "trainer/Policy mu Min                                  -0.650152\n",
      "trainer/Policy log std Mean                            -2.11885\n",
      "trainer/Policy log std Std                              0.148922\n",
      "trainer/Policy log std Max                             -1.43062\n",
      "trainer/Policy log std Min                             -2.68828\n",
      "trainer/Alpha                                           0.0245027\n",
      "trainer/Alpha Loss                                     -8.05548\n",
      "exploration/num steps total                         19000\n",
      "exploration/num paths total                            71\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.930571\n",
      "exploration/Rewards Std                                 0.10318\n",
      "exploration/Rewards Max                                 2.0116\n",
      "exploration/Rewards Min                                 0.715133\n",
      "exploration/Returns Mean                              930.571\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               930.571\n",
      "exploration/Returns Min                               930.571\n",
      "exploration/Actions Mean                                0.0443552\n",
      "exploration/Actions Std                                 0.141222\n",
      "exploration/Actions Max                                 0.528822\n",
      "exploration/Actions Min                                -0.473503\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           930.571\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0388031\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0388031\n",
      "exploration/env_infos/final/reward_forward Min         -0.0388031\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0129431\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0129431\n",
      "exploration/env_infos/initial/reward_forward Min        0.0129431\n",
      "exploration/env_infos/reward_forward Mean               0.0240276\n",
      "exploration/env_infos/reward_forward Std                0.257015\n",
      "exploration/env_infos/reward_forward Max                1.17726\n",
      "exploration/env_infos/reward_forward Min               -0.862512\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.105225\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.105225\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.105225\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0248053\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0248053\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0248053\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.087644\n",
      "exploration/env_infos/reward_ctrl Std                   0.044119\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00698873\n",
      "exploration/env_infos/reward_ctrl Min                  -0.284867\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0763158\n",
      "exploration/env_infos/final/torso_velocity Std          0.0653778\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0218877\n",
      "exploration/env_infos/final/torso_velocity Min         -0.168257\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.155694\n",
      "exploration/env_infos/initial/torso_velocity Std        0.258846\n",
      "exploration/env_infos/initial/torso_velocity Max        0.518991\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0648524\n",
      "exploration/env_infos/torso_velocity Mean               0.000665371\n",
      "exploration/env_infos/torso_velocity Std                0.283945\n",
      "exploration/env_infos/torso_velocity Max                1.17726\n",
      "exploration/env_infos/torso_velocity Min               -1.5588\n",
      "evaluation/num steps total                         450000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.96102\n",
      "evaluation/Rewards Std                                  0.0246541\n",
      "evaluation/Rewards Max                                  1.85504\n",
      "evaluation/Rewards Min                                  0.709427\n",
      "evaluation/Returns Mean                               961.02\n",
      "evaluation/Returns Std                                 11.1671\n",
      "evaluation/Returns Max                                979.526\n",
      "evaluation/Returns Min                                946.847\n",
      "evaluation/Actions Mean                                 0.0495366\n",
      "evaluation/Actions Std                                  0.0869017\n",
      "evaluation/Actions Max                                  0.466336\n",
      "evaluation/Actions Min                                 -0.384074\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            961.02\n",
      "evaluation/env_infos/final/reward_forward Mean          7.84811e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.84727e-06\n",
      "evaluation/env_infos/final/reward_forward Max           1.17715e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -5.6434e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00682848\n",
      "evaluation/env_infos/initial/reward_forward Std         0.110639\n",
      "evaluation/env_infos/initial/reward_forward Max         0.226733\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.238981\n",
      "evaluation/env_infos/reward_forward Mean                0.000381108\n",
      "evaluation/env_infos/reward_forward Std                 0.0402545\n",
      "evaluation/env_infos/reward_forward Max                 1.08692\n",
      "evaluation/env_infos/reward_forward Min                -0.938076\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0395102\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0111113\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0233023\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0527176\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.025259\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0114135\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0130287\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0535532\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0400231\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0128193\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00706619\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.290573\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.1563e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           2.05832e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           1.17715e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.6434e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.144847\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.237566\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.697046\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.355852\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00223256\n",
      "evaluation/env_infos/torso_velocity Std                 0.0667418\n",
      "evaluation/env_infos/torso_velocity Max                 1.62233\n",
      "evaluation/env_infos/torso_velocity Min                -2.08079\n",
      "time/data storing (s)                                   0.0153513\n",
      "time/evaluation sampling (s)                           44.8692\n",
      "time/exploration sampling (s)                           1.96214\n",
      "time/logging (s)                                        0.286599\n",
      "time/saving (s)                                         0.0284373\n",
      "time/training (s)                                       4.05817\n",
      "time/epoch (s)                                         51.2199\n",
      "time/total (s)                                       1179.18\n",
      "Epoch                                                  17\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:30:52.987631 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 18 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  20000\n",
      "trainer/QF1 Loss                                        0.233299\n",
      "trainer/QF2 Loss                                        0.221351\n",
      "trainer/Policy Loss                                    -2.08134\n",
      "trainer/Q1 Predictions Mean                             6.98419\n",
      "trainer/Q1 Predictions Std                              0.93769\n",
      "trainer/Q1 Predictions Max                              8.70096\n",
      "trainer/Q1 Predictions Min                              2.65339\n",
      "trainer/Q2 Predictions Mean                             6.9437\n",
      "trainer/Q2 Predictions Std                              0.985058\n",
      "trainer/Q2 Predictions Max                              8.53443\n",
      "trainer/Q2 Predictions Min                              2.30984\n",
      "trainer/Q Targets Mean                                  6.99716\n",
      "trainer/Q Targets Std                                   1.14245\n",
      "trainer/Q Targets Max                                   9.68914\n",
      "trainer/Q Targets Min                                  -0.730113\n",
      "trainer/Log Pis Mean                                    5.59134\n",
      "trainer/Log Pis Std                                     2.40027\n",
      "trainer/Log Pis Max                                    10.0561\n",
      "trainer/Log Pis Min                                    -4.23546\n",
      "trainer/Policy mu Mean                                  0.0265232\n",
      "trainer/Policy mu Std                                   0.116388\n",
      "trainer/Policy mu Max                                   0.73116\n",
      "trainer/Policy mu Min                                  -0.489913\n",
      "trainer/Policy log std Mean                            -2.09008\n",
      "trainer/Policy log std Std                              0.174371\n",
      "trainer/Policy log std Max                             -1.24217\n",
      "trainer/Policy log std Min                             -2.5973\n",
      "trainer/Alpha                                           0.0225038\n",
      "trainer/Alpha Loss                                     -9.13671\n",
      "exploration/num steps total                         20000\n",
      "exploration/num paths total                            72\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.945005\n",
      "exploration/Rewards Std                                 0.107254\n",
      "exploration/Rewards Max                                 1.83902\n",
      "exploration/Rewards Min                                 0.601129\n",
      "exploration/Returns Mean                              945.005\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               945.005\n",
      "exploration/Returns Min                               945.005\n",
      "exploration/Actions Mean                                0.0414675\n",
      "exploration/Actions Std                                 0.138963\n",
      "exploration/Actions Max                                 0.676783\n",
      "exploration/Actions Min                                -0.506596\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           945.005\n",
      "exploration/env_infos/final/reward_forward Mean        -0.40313\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.40313\n",
      "exploration/env_infos/final/reward_forward Min         -0.40313\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0414189\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0414189\n",
      "exploration/env_infos/initial/reward_forward Min        0.0414189\n",
      "exploration/env_infos/reward_forward Mean               0.00539927\n",
      "exploration/env_infos/reward_forward Std                0.255166\n",
      "exploration/env_infos/reward_forward Max                0.966545\n",
      "exploration/env_infos/reward_forward Min               -0.852159\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.234279\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.234279\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.234279\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0562836\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0562836\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0562836\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0841209\n",
      "exploration/env_infos/reward_ctrl Std                   0.0450073\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00647212\n",
      "exploration/env_infos/reward_ctrl Min                  -0.398871\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.172567\n",
      "exploration/env_infos/final/torso_velocity Std          0.163545\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0414508\n",
      "exploration/env_infos/final/torso_velocity Min         -0.40313\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.207407\n",
      "exploration/env_infos/initial/torso_velocity Std        0.201137\n",
      "exploration/env_infos/initial/torso_velocity Max        0.490452\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0414189\n",
      "exploration/env_infos/torso_velocity Mean               0.00959746\n",
      "exploration/env_infos/torso_velocity Std                0.268609\n",
      "exploration/env_infos/torso_velocity Max                1.15383\n",
      "exploration/env_infos/torso_velocity Min               -1.5868\n",
      "evaluation/num steps total                         475000\n",
      "evaluation/num paths total                            475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.968724\n",
      "evaluation/Rewards Std                                  0.0310729\n",
      "evaluation/Rewards Max                                  2.33839\n",
      "evaluation/Rewards Min                                  0.811218\n",
      "evaluation/Returns Mean                               968.724\n",
      "evaluation/Returns Std                                 12.8038\n",
      "evaluation/Returns Max                                988.989\n",
      "evaluation/Returns Min                                949.445\n",
      "evaluation/Actions Mean                                 0.0358807\n",
      "evaluation/Actions Std                                  0.0828143\n",
      "evaluation/Actions Max                                  0.407638\n",
      "evaluation/Actions Min                                 -0.485686\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            968.724\n",
      "evaluation/env_infos/final/reward_forward Mean         -4.55502e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.61717e-05\n",
      "evaluation/env_infos/final/reward_forward Max           5.59936e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.05711e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0228159\n",
      "evaluation/env_infos/initial/reward_forward Std         0.131437\n",
      "evaluation/env_infos/initial/reward_forward Max         0.236191\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.275288\n",
      "evaluation/env_infos/reward_forward Mean                0.000726895\n",
      "evaluation/env_infos/reward_forward Std                 0.0432456\n",
      "evaluation/env_infos/reward_forward Max                 1.28542\n",
      "evaluation/env_infos/reward_forward Min                -1.14594\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.032181\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0121746\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0142693\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0501366\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0180936\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00726976\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00853583\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0362545\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0325825\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0136047\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00677816\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.285358\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.70274e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           4.17805e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000297334\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.05711e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.143207\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.2491\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.641865\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.275288\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00139034\n",
      "evaluation/env_infos/torso_velocity Std                 0.0630966\n",
      "evaluation/env_infos/torso_velocity Max                 1.68506\n",
      "evaluation/env_infos/torso_velocity Min                -1.79702\n",
      "time/data storing (s)                                   0.014607\n",
      "time/evaluation sampling (s)                           45.3766\n",
      "time/exploration sampling (s)                           1.88161\n",
      "time/logging (s)                                        0.283363\n",
      "time/saving (s)                                         0.0280028\n",
      "time/training (s)                                       4.07928\n",
      "time/epoch (s)                                         51.6635\n",
      "time/total (s)                                       1231.16\n",
      "Epoch                                                  18\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:31:44.237122 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 19 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  21000\n",
      "trainer/QF1 Loss                                        0.197619\n",
      "trainer/QF2 Loss                                        0.194406\n",
      "trainer/Policy Loss                                    -2.48998\n",
      "trainer/Q1 Predictions Mean                             7.61591\n",
      "trainer/Q1 Predictions Std                              0.926217\n",
      "trainer/Q1 Predictions Max                              9.07563\n",
      "trainer/Q1 Predictions Min                              5.0993\n",
      "trainer/Q2 Predictions Mean                             7.6332\n",
      "trainer/Q2 Predictions Std                              0.940337\n",
      "trainer/Q2 Predictions Max                              9.02398\n",
      "trainer/Q2 Predictions Min                              5.0209\n",
      "trainer/Q Targets Mean                                  7.4526\n",
      "trainer/Q Targets Std                                   0.91172\n",
      "trainer/Q Targets Max                                  10.8117\n",
      "trainer/Q Targets Min                                   4.93493\n",
      "trainer/Log Pis Mean                                    5.761\n",
      "trainer/Log Pis Std                                     2.30278\n",
      "trainer/Log Pis Max                                    10.4182\n",
      "trainer/Log Pis Min                                    -4.47156\n",
      "trainer/Policy mu Mean                                  0.0279505\n",
      "trainer/Policy mu Std                                   0.118314\n",
      "trainer/Policy mu Max                                   0.740614\n",
      "trainer/Policy mu Min                                  -0.493543\n",
      "trainer/Policy log std Mean                            -2.10243\n",
      "trainer/Policy log std Std                              0.166772\n",
      "trainer/Policy log std Max                             -1.32847\n",
      "trainer/Policy log std Min                             -2.62375\n",
      "trainer/Alpha                                           0.0209355\n",
      "trainer/Alpha Loss                                     -8.65498\n",
      "exploration/num steps total                         21000\n",
      "exploration/num paths total                            73\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.914625\n",
      "exploration/Rewards Std                                 0.0795251\n",
      "exploration/Rewards Max                                 1.39151\n",
      "exploration/Rewards Min                                 0.590666\n",
      "exploration/Returns Mean                              914.625\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               914.625\n",
      "exploration/Returns Min                               914.625\n",
      "exploration/Actions Mean                                0.0237785\n",
      "exploration/Actions Std                                 0.157769\n",
      "exploration/Actions Max                                 0.508189\n",
      "exploration/Actions Min                                -0.513264\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           914.625\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0882946\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0882946\n",
      "exploration/env_infos/final/reward_forward Min         -0.0882946\n",
      "exploration/env_infos/initial/reward_forward Mean       0.155959\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.155959\n",
      "exploration/env_infos/initial/reward_forward Min        0.155959\n",
      "exploration/env_infos/reward_forward Mean              -0.00489517\n",
      "exploration/env_infos/reward_forward Std                0.162147\n",
      "exploration/env_infos/reward_forward Max                0.811374\n",
      "exploration/env_infos/reward_forward Min               -1.03576\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.115574\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.115574\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.115574\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0877981\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0877981\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0877981\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.101826\n",
      "exploration/env_infos/reward_ctrl Std                   0.0481074\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00873235\n",
      "exploration/env_infos/reward_ctrl Min                  -0.409334\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.114258\n",
      "exploration/env_infos/final/torso_velocity Std          0.0378857\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0866509\n",
      "exploration/env_infos/final/torso_velocity Min         -0.167828\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.132282\n",
      "exploration/env_infos/initial/torso_velocity Std        0.178868\n",
      "exploration/env_infos/initial/torso_velocity Max        0.338549\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0976635\n",
      "exploration/env_infos/torso_velocity Mean              -0.0031729\n",
      "exploration/env_infos/torso_velocity Std                0.22111\n",
      "exploration/env_infos/torso_velocity Max                1.31641\n",
      "exploration/env_infos/torso_velocity Min               -1.66272\n",
      "evaluation/num steps total                         500000\n",
      "evaluation/num paths total                            500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.94998\n",
      "evaluation/Rewards Std                                  0.018648\n",
      "evaluation/Rewards Max                                  2.00975\n",
      "evaluation/Rewards Min                                  0.522119\n",
      "evaluation/Returns Mean                               949.98\n",
      "evaluation/Returns Std                                  6.92051\n",
      "evaluation/Returns Max                                961.008\n",
      "evaluation/Returns Min                                937.244\n",
      "evaluation/Actions Mean                                 0.0337771\n",
      "evaluation/Actions Std                                  0.107073\n",
      "evaluation/Actions Max                                  0.529649\n",
      "evaluation/Actions Min                                 -0.484444\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            949.98\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.00859e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.00403e-05\n",
      "evaluation/env_infos/final/reward_forward Max           5.69338e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.11513e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0182758\n",
      "evaluation/env_infos/initial/reward_forward Std         0.103076\n",
      "evaluation/env_infos/initial/reward_forward Max         0.179442\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.182401\n",
      "evaluation/env_infos/reward_forward Mean                0.00181596\n",
      "evaluation/env_infos/reward_forward Std                 0.0436011\n",
      "evaluation/env_infos/reward_forward Max                 1.11436\n",
      "evaluation/env_infos/reward_forward Min                -1.01807\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.049823\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00690608\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0374722\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0626166\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0222102\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00386147\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0160174\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0298392\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0504221\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0141486\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00278183\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.477881\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.90993e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000126953\n",
      "evaluation/env_infos/final/torso_velocity Max           5.69338e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00106441\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.146391\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.238391\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.661232\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.2925\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00147787\n",
      "evaluation/env_infos/torso_velocity Std                 0.0628889\n",
      "evaluation/env_infos/torso_velocity Max                 1.44547\n",
      "evaluation/env_infos/torso_velocity Min                -1.77209\n",
      "time/data storing (s)                                   0.0146174\n",
      "time/evaluation sampling (s)                           44.7126\n",
      "time/exploration sampling (s)                           1.98259\n",
      "time/logging (s)                                        0.281957\n",
      "time/saving (s)                                         0.0269042\n",
      "time/training (s)                                       3.92186\n",
      "time/epoch (s)                                         50.9406\n",
      "time/total (s)                                       1282.4\n",
      "Epoch                                                  19\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:32:36.228866 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 20 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  22000\n",
      "trainer/QF1 Loss                                        0.257067\n",
      "trainer/QF2 Loss                                        0.251143\n",
      "trainer/Policy Loss                                    -2.25562\n",
      "trainer/Q1 Predictions Mean                             8.01504\n",
      "trainer/Q1 Predictions Std                              0.829357\n",
      "trainer/Q1 Predictions Max                             10.1857\n",
      "trainer/Q1 Predictions Min                              5.56132\n",
      "trainer/Q2 Predictions Mean                             8.04509\n",
      "trainer/Q2 Predictions Std                              0.852502\n",
      "trainer/Q2 Predictions Max                              9.76499\n",
      "trainer/Q2 Predictions Min                              5.38091\n",
      "trainer/Q Targets Mean                                  8.00582\n",
      "trainer/Q Targets Std                                   1.02349\n",
      "trainer/Q Targets Max                                  11.8339\n",
      "trainer/Q Targets Min                                   0.0662556\n",
      "trainer/Log Pis Mean                                    6.27673\n",
      "trainer/Log Pis Std                                     2.27598\n",
      "trainer/Log Pis Max                                    12.827\n",
      "trainer/Log Pis Min                                    -1.0621\n",
      "trainer/Policy mu Mean                                  0.0155746\n",
      "trainer/Policy mu Std                                   0.117719\n",
      "trainer/Policy mu Max                                   0.798498\n",
      "trainer/Policy mu Min                                  -0.535439\n",
      "trainer/Policy log std Mean                            -2.1867\n",
      "trainer/Policy log std Std                              0.20196\n",
      "trainer/Policy log std Max                             -1.13152\n",
      "trainer/Policy log std Min                             -2.95741\n",
      "trainer/Alpha                                           0.0194322\n",
      "trainer/Alpha Loss                                     -6.78998\n",
      "exploration/num steps total                         22000\n",
      "exploration/num paths total                            74\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.01367\n",
      "exploration/Rewards Std                                 0.206469\n",
      "exploration/Rewards Max                                 2.08132\n",
      "exploration/Rewards Min                                 0.648214\n",
      "exploration/Returns Mean                             1013.67\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1013.67\n",
      "exploration/Returns Min                              1013.67\n",
      "exploration/Actions Mean                                0.0210597\n",
      "exploration/Actions Std                                 0.122596\n",
      "exploration/Actions Max                                 0.584263\n",
      "exploration/Actions Min                                -0.452455\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1013.67\n",
      "exploration/env_infos/final/reward_forward Mean         0.0791602\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0791602\n",
      "exploration/env_infos/final/reward_forward Min          0.0791602\n",
      "exploration/env_infos/initial/reward_forward Mean       0.101378\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.101378\n",
      "exploration/env_infos/initial/reward_forward Min        0.101378\n",
      "exploration/env_infos/reward_forward Mean               0.0113703\n",
      "exploration/env_infos/reward_forward Std                0.221021\n",
      "exploration/env_infos/reward_forward Max                1.69898\n",
      "exploration/env_infos/reward_forward Min               -0.963631\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0381993\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0381993\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0381993\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0192276\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0192276\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0192276\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0618935\n",
      "exploration/env_infos/reward_ctrl Std                   0.039644\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00393472\n",
      "exploration/env_infos/reward_ctrl Min                  -0.351786\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.148073\n",
      "exploration/env_infos/final/torso_velocity Std          0.240692\n",
      "exploration/env_infos/final/torso_velocity Max          0.471212\n",
      "exploration/env_infos/final/torso_velocity Min         -0.106152\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.245275\n",
      "exploration/env_infos/initial/torso_velocity Std        0.174797\n",
      "exploration/env_infos/initial/torso_velocity Max        0.491296\n",
      "exploration/env_infos/initial/torso_velocity Min        0.101378\n",
      "exploration/env_infos/torso_velocity Mean               0.029597\n",
      "exploration/env_infos/torso_velocity Std                0.296476\n",
      "exploration/env_infos/torso_velocity Max                1.69898\n",
      "exploration/env_infos/torso_velocity Min               -1.64994\n",
      "evaluation/num steps total                         525000\n",
      "evaluation/num paths total                            525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.978107\n",
      "evaluation/Rewards Std                                  0.0312787\n",
      "evaluation/Rewards Max                                  2.76266\n",
      "evaluation/Rewards Min                                  0.6935\n",
      "evaluation/Returns Mean                               978.107\n",
      "evaluation/Returns Std                                  6.088\n",
      "evaluation/Returns Max                                989.221\n",
      "evaluation/Returns Min                                962.299\n",
      "evaluation/Actions Mean                                 0.0276489\n",
      "evaluation/Actions Std                                  0.0709389\n",
      "evaluation/Actions Max                                  0.529512\n",
      "evaluation/Actions Min                                 -0.382836\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            978.107\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.11985e-06\n",
      "evaluation/env_infos/final/reward_forward Std           2.45397e-05\n",
      "evaluation/env_infos/final/reward_forward Max           3.63963e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000125336\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0280243\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0799115\n",
      "evaluation/env_infos/initial/reward_forward Max         0.268221\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.104524\n",
      "evaluation/env_infos/reward_forward Mean                0.00224858\n",
      "evaluation/env_infos/reward_forward Std                 0.0548787\n",
      "evaluation/env_infos/reward_forward Max                 1.63132\n",
      "evaluation/env_infos/reward_forward Min                -1.07168\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0222539\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00564181\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0128807\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0379364\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.018663\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00242073\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0144486\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0241984\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0231872\n",
      "evaluation/env_infos/reward_ctrl Std                    0.011787\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00578035\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.3065\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.00612e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           1.83531e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           1.5686e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000125336\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.157771\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.224034\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.763187\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.16291\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000382665\n",
      "evaluation/env_infos/torso_velocity Std                 0.0649024\n",
      "evaluation/env_infos/torso_velocity Max                 1.63132\n",
      "evaluation/env_infos/torso_velocity Min                -1.91706\n",
      "time/data storing (s)                                   0.0151166\n",
      "time/evaluation sampling (s)                           45.3558\n",
      "time/exploration sampling (s)                           1.92931\n",
      "time/logging (s)                                        0.291663\n",
      "time/saving (s)                                         0.0281254\n",
      "time/training (s)                                       4.06744\n",
      "time/epoch (s)                                         51.6875\n",
      "time/total (s)                                       1334.41\n",
      "Epoch                                                  20\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:33:29.109016 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 21 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  23000\n",
      "trainer/QF1 Loss                                        0.201186\n",
      "trainer/QF2 Loss                                        0.202196\n",
      "trainer/Policy Loss                                    -2.51161\n",
      "trainer/Q1 Predictions Mean                             8.36952\n",
      "trainer/Q1 Predictions Std                              1.02197\n",
      "trainer/Q1 Predictions Max                             11.2471\n",
      "trainer/Q1 Predictions Min                              4.48952\n",
      "trainer/Q2 Predictions Mean                             8.46232\n",
      "trainer/Q2 Predictions Std                              0.987931\n",
      "trainer/Q2 Predictions Max                             11.5066\n",
      "trainer/Q2 Predictions Min                              5.26821\n",
      "trainer/Q Targets Mean                                  8.29744\n",
      "trainer/Q Targets Std                                   1.00464\n",
      "trainer/Q Targets Max                                  12.8795\n",
      "trainer/Q Targets Min                                   4.09213\n",
      "trainer/Log Pis Mean                                    6.44759\n",
      "trainer/Log Pis Std                                     2.25287\n",
      "trainer/Log Pis Max                                    14.1386\n",
      "trainer/Log Pis Min                                    -5.54242\n",
      "trainer/Policy mu Mean                                  0.0419044\n",
      "trainer/Policy mu Std                                   0.133035\n",
      "trainer/Policy mu Max                                   1.132\n",
      "trainer/Policy mu Min                                  -0.488033\n",
      "trainer/Policy log std Mean                            -2.19744\n",
      "trainer/Policy log std Std                              0.170109\n",
      "trainer/Policy log std Max                             -1.39417\n",
      "trainer/Policy log std Min                             -2.97441\n",
      "trainer/Alpha                                           0.018216\n",
      "trainer/Alpha Loss                                     -6.21717\n",
      "exploration/num steps total                         23000\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.897258\n",
      "exploration/Rewards Std                                 0.0835586\n",
      "exploration/Rewards Max                                 1.51377\n",
      "exploration/Rewards Min                                 0.596557\n",
      "exploration/Returns Mean                              897.258\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               897.258\n",
      "exploration/Returns Min                               897.258\n",
      "exploration/Actions Mean                                0.0826645\n",
      "exploration/Actions Std                                 0.153208\n",
      "exploration/Actions Max                                 0.605112\n",
      "exploration/Actions Min                                -0.514126\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           897.258\n",
      "exploration/env_infos/final/reward_forward Mean         0.0303312\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0303312\n",
      "exploration/env_infos/final/reward_forward Min          0.0303312\n",
      "exploration/env_infos/initial/reward_forward Mean       0.123042\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.123042\n",
      "exploration/env_infos/initial/reward_forward Min        0.123042\n",
      "exploration/env_infos/reward_forward Mean               0.0124295\n",
      "exploration/env_infos/reward_forward Std                0.152324\n",
      "exploration/env_infos/reward_forward Max                1.16742\n",
      "exploration/env_infos/reward_forward Min               -0.480012\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0582598\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0582598\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0582598\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0404569\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0404569\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0404569\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.121225\n",
      "exploration/env_infos/reward_ctrl Std                   0.0547695\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0135914\n",
      "exploration/env_infos/reward_ctrl Min                  -0.403443\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00804185\n",
      "exploration/env_infos/final/torso_velocity Std          0.030506\n",
      "exploration/env_infos/final/torso_velocity Max          0.0303312\n",
      "exploration/env_infos/final/torso_velocity Min         -0.035092\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.193785\n",
      "exploration/env_infos/initial/torso_velocity Std        0.249895\n",
      "exploration/env_infos/initial/torso_velocity Max        0.529019\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0707069\n",
      "exploration/env_infos/torso_velocity Mean               0.0178331\n",
      "exploration/env_infos/torso_velocity Std                0.225329\n",
      "exploration/env_infos/torso_velocity Max                1.47155\n",
      "exploration/env_infos/torso_velocity Min               -2.16806\n",
      "evaluation/num steps total                         550000\n",
      "evaluation/num paths total                            550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.944634\n",
      "evaluation/Rewards Std                                  0.0397781\n",
      "evaluation/Rewards Max                                  2.79184\n",
      "evaluation/Rewards Min                                  0.623573\n",
      "evaluation/Returns Mean                               944.634\n",
      "evaluation/Returns Std                                 19.1991\n",
      "evaluation/Returns Max                                972.935\n",
      "evaluation/Returns Min                                908.22\n",
      "evaluation/Actions Mean                                 0.0582218\n",
      "evaluation/Actions Std                                  0.104277\n",
      "evaluation/Actions Max                                  0.498271\n",
      "evaluation/Actions Min                                 -0.627011\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            944.634\n",
      "evaluation/env_infos/final/reward_forward Mean         -3.82677e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.27423e-07\n",
      "evaluation/env_infos/final/reward_forward Max           6.03351e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.48285e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0238407\n",
      "evaluation/env_infos/initial/reward_forward Std         0.139315\n",
      "evaluation/env_infos/initial/reward_forward Max         0.293292\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.21136\n",
      "evaluation/env_infos/reward_forward Mean                0.00298822\n",
      "evaluation/env_infos/reward_forward Std                 0.0668588\n",
      "evaluation/env_infos/reward_forward Max                 1.85204\n",
      "evaluation/env_infos/reward_forward Min                -0.78263\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0565005\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0187971\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0309747\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.091426\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0218251\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00475833\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0151991\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0316827\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0570538\n",
      "evaluation/env_infos/reward_ctrl Std                    0.021375\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00539653\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.376427\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -6.98106e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.71304e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           6.03351e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.06738e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.139817\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.249946\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.647214\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.21136\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000144779\n",
      "evaluation/env_infos/torso_velocity Std                 0.0682169\n",
      "evaluation/env_infos/torso_velocity Max                 1.85204\n",
      "evaluation/env_infos/torso_velocity Min                -2.09712\n",
      "time/data storing (s)                                   0.0148725\n",
      "time/evaluation sampling (s)                           46.2521\n",
      "time/exploration sampling (s)                           1.98042\n",
      "time/logging (s)                                        0.280264\n",
      "time/saving (s)                                         0.0278525\n",
      "time/training (s)                                       3.92926\n",
      "time/epoch (s)                                         52.4848\n",
      "time/total (s)                                       1387.27\n",
      "Epoch                                                  21\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:34:20.625238 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 22 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  24000\n",
      "trainer/QF1 Loss                                        0.181082\n",
      "trainer/QF2 Loss                                        0.180274\n",
      "trainer/Policy Loss                                    -2.54629\n",
      "trainer/Q1 Predictions Mean                             8.59193\n",
      "trainer/Q1 Predictions Std                              0.969188\n",
      "trainer/Q1 Predictions Max                             10.3807\n",
      "trainer/Q1 Predictions Min                              4.15805\n",
      "trainer/Q2 Predictions Mean                             8.58585\n",
      "trainer/Q2 Predictions Std                              0.994907\n",
      "trainer/Q2 Predictions Max                             10.3062\n",
      "trainer/Q2 Predictions Min                              4.61587\n",
      "trainer/Q Targets Mean                                  8.5882\n",
      "trainer/Q Targets Std                                   1.08292\n",
      "trainer/Q Targets Max                                  11.1708\n",
      "trainer/Q Targets Min                                   1.5477\n",
      "trainer/Log Pis Mean                                    6.58769\n",
      "trainer/Log Pis Std                                     2.29437\n",
      "trainer/Log Pis Max                                    12.1504\n",
      "trainer/Log Pis Min                                    -0.561085\n",
      "trainer/Policy mu Mean                                  0.0342092\n",
      "trainer/Policy mu Std                                   0.144026\n",
      "trainer/Policy mu Max                                   0.88862\n",
      "trainer/Policy mu Min                                  -0.561971\n",
      "trainer/Policy log std Mean                            -2.20266\n",
      "trainer/Policy log std Std                              0.16756\n",
      "trainer/Policy log std Max                             -1.42931\n",
      "trainer/Policy log std Min                             -2.71942\n",
      "trainer/Alpha                                           0.0172013\n",
      "trainer/Alpha Loss                                     -5.7372\n",
      "exploration/num steps total                         24000\n",
      "exploration/num paths total                            76\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.935941\n",
      "exploration/Rewards Std                                 0.0972681\n",
      "exploration/Rewards Max                                 1.87107\n",
      "exploration/Rewards Min                                 0.68563\n",
      "exploration/Returns Mean                              935.941\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               935.941\n",
      "exploration/Returns Min                               935.941\n",
      "exploration/Actions Mean                                0.0257106\n",
      "exploration/Actions Std                                 0.142262\n",
      "exploration/Actions Max                                 0.553574\n",
      "exploration/Actions Min                                -0.539946\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           935.941\n",
      "exploration/env_infos/final/reward_forward Mean        -0.601482\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.601482\n",
      "exploration/env_infos/final/reward_forward Min         -0.601482\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0827535\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0827535\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0827535\n",
      "exploration/env_infos/reward_forward Mean               0.0455421\n",
      "exploration/env_infos/reward_forward Std                0.288234\n",
      "exploration/env_infos/reward_forward Max                1.23345\n",
      "exploration/env_infos/reward_forward Min               -1.03194\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0712571\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0712571\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0712571\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0347955\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0347955\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0347955\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0835983\n",
      "exploration/env_infos/reward_ctrl Std                   0.0449539\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00462322\n",
      "exploration/env_infos/reward_ctrl Min                  -0.31437\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.206849\n",
      "exploration/env_infos/final/torso_velocity Std          0.296148\n",
      "exploration/env_infos/final/torso_velocity Max          0.111936\n",
      "exploration/env_infos/final/torso_velocity Min         -0.601482\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.167956\n",
      "exploration/env_infos/initial/torso_velocity Std        0.257359\n",
      "exploration/env_infos/initial/torso_velocity Max        0.521804\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0827535\n",
      "exploration/env_infos/torso_velocity Mean               0.0108186\n",
      "exploration/env_infos/torso_velocity Std                0.304385\n",
      "exploration/env_infos/torso_velocity Max                1.47933\n",
      "exploration/env_infos/torso_velocity Min               -1.64746\n",
      "evaluation/num steps total                         575000\n",
      "evaluation/num paths total                            575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.966172\n",
      "evaluation/Rewards Std                                  0.0293126\n",
      "evaluation/Rewards Max                                  2.30793\n",
      "evaluation/Rewards Min                                  0.660901\n",
      "evaluation/Returns Mean                               966.172\n",
      "evaluation/Returns Std                                 11.0064\n",
      "evaluation/Returns Max                                979.723\n",
      "evaluation/Returns Min                                944.667\n",
      "evaluation/Actions Mean                                 0.0321631\n",
      "evaluation/Actions Std                                  0.0879445\n",
      "evaluation/Actions Max                                  0.575757\n",
      "evaluation/Actions Min                                 -0.446735\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            966.172\n",
      "evaluation/env_infos/final/reward_forward Mean         -8.20825e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.8161e-06\n",
      "evaluation/env_infos/final/reward_forward Max           3.9531e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.94773e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00977656\n",
      "evaluation/env_infos/initial/reward_forward Std         0.108519\n",
      "evaluation/env_infos/initial/reward_forward Max         0.17857\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.213395\n",
      "evaluation/env_infos/reward_forward Mean               -0.00159366\n",
      "evaluation/env_infos/reward_forward Std                 0.0558988\n",
      "evaluation/env_infos/reward_forward Max                 1.0327\n",
      "evaluation/env_infos/reward_forward Min                -1.62753\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0341851\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0105894\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0210064\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0540349\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0297575\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00499755\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0202584\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0420032\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0350748\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0136007\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0140602\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.339099\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.24444e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           9.76153e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           6.94475e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.94773e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152817\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.252295\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.748101\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.31527\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00270012\n",
      "evaluation/env_infos/torso_velocity Std                 0.0717329\n",
      "evaluation/env_infos/torso_velocity Max                 1.67917\n",
      "evaluation/env_infos/torso_velocity Min                -1.87308\n",
      "time/data storing (s)                                   0.0139056\n",
      "time/evaluation sampling (s)                           44.9267\n",
      "time/exploration sampling (s)                           1.87397\n",
      "time/logging (s)                                        0.282549\n",
      "time/saving (s)                                         0.0272628\n",
      "time/training (s)                                       4.06386\n",
      "time/epoch (s)                                         51.1882\n",
      "time/total (s)                                       1438.79\n",
      "Epoch                                                  22\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:35:11.767743 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 23 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  25000\n",
      "trainer/QF1 Loss                                        0.281678\n",
      "trainer/QF2 Loss                                        0.233203\n",
      "trainer/Policy Loss                                    -3.0216\n",
      "trainer/Q1 Predictions Mean                             9.07575\n",
      "trainer/Q1 Predictions Std                              0.982922\n",
      "trainer/Q1 Predictions Max                             11.2089\n",
      "trainer/Q1 Predictions Min                              5.92784\n",
      "trainer/Q2 Predictions Mean                             9.14167\n",
      "trainer/Q2 Predictions Std                              0.986877\n",
      "trainer/Q2 Predictions Max                             10.717\n",
      "trainer/Q2 Predictions Min                              4.49713\n",
      "trainer/Q Targets Mean                                  8.98771\n",
      "trainer/Q Targets Std                                   1.08493\n",
      "trainer/Q Targets Max                                  11.4309\n",
      "trainer/Q Targets Min                                   0.346005\n",
      "trainer/Log Pis Mean                                    6.58946\n",
      "trainer/Log Pis Std                                     2.1387\n",
      "trainer/Log Pis Max                                    10.8465\n",
      "trainer/Log Pis Min                                     0.710887\n",
      "trainer/Policy mu Mean                                  0.0117427\n",
      "trainer/Policy mu Std                                   0.129666\n",
      "trainer/Policy mu Max                                   0.640394\n",
      "trainer/Policy mu Min                                  -0.691054\n",
      "trainer/Policy log std Mean                            -2.20301\n",
      "trainer/Policy log std Std                              0.192134\n",
      "trainer/Policy log std Max                             -1.37413\n",
      "trainer/Policy log std Min                             -2.78566\n",
      "trainer/Alpha                                           0.0163036\n",
      "trainer/Alpha Loss                                     -5.80558\n",
      "exploration/num steps total                         25000\n",
      "exploration/num paths total                            77\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.964727\n",
      "exploration/Rewards Std                                 0.0660612\n",
      "exploration/Rewards Max                                 1.38632\n",
      "exploration/Rewards Min                                 0.805326\n",
      "exploration/Returns Mean                              964.727\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               964.727\n",
      "exploration/Returns Min                               964.727\n",
      "exploration/Actions Mean                               -0.00189536\n",
      "exploration/Actions Std                                 0.11517\n",
      "exploration/Actions Max                                 0.47397\n",
      "exploration/Actions Min                                -0.426141\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           964.727\n",
      "exploration/env_infos/final/reward_forward Mean         0.0189111\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0189111\n",
      "exploration/env_infos/final/reward_forward Min          0.0189111\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0117893\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0117893\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0117893\n",
      "exploration/env_infos/reward_forward Mean              -0.00564225\n",
      "exploration/env_infos/reward_forward Std                0.154421\n",
      "exploration/env_infos/reward_forward Max                0.89888\n",
      "exploration/env_infos/reward_forward Min               -0.741816\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0581829\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0581829\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0581829\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0426712\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0426712\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0426712\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0530713\n",
      "exploration/env_infos/reward_ctrl Std                   0.0249123\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00699151\n",
      "exploration/env_infos/reward_ctrl Min                  -0.194674\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0555773\n",
      "exploration/env_infos/final/torso_velocity Std          0.067125\n",
      "exploration/env_infos/final/torso_velocity Max          0.149741\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00192053\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0851662\n",
      "exploration/env_infos/initial/torso_velocity Std        0.145238\n",
      "exploration/env_infos/initial/torso_velocity Max        0.290459\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0231711\n",
      "exploration/env_infos/torso_velocity Mean              -0.0142281\n",
      "exploration/env_infos/torso_velocity Std                0.163148\n",
      "exploration/env_infos/torso_velocity Max                0.89888\n",
      "exploration/env_infos/torso_velocity Min               -0.861327\n",
      "evaluation/num steps total                         600000\n",
      "evaluation/num paths total                            600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.970481\n",
      "evaluation/Rewards Std                                  0.030037\n",
      "evaluation/Rewards Max                                  2.46065\n",
      "evaluation/Rewards Min                                  0.639654\n",
      "evaluation/Returns Mean                               970.481\n",
      "evaluation/Returns Std                                  9.68743\n",
      "evaluation/Returns Max                                984.892\n",
      "evaluation/Returns Min                                949.727\n",
      "evaluation/Actions Mean                                 0.00361518\n",
      "evaluation/Actions Std                                  0.087471\n",
      "evaluation/Actions Max                                  0.458115\n",
      "evaluation/Actions Min                                 -0.607267\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            970.481\n",
      "evaluation/env_infos/final/reward_forward Mean         -9.52616e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.67299e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.11151e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.61195e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00532441\n",
      "evaluation/env_infos/initial/reward_forward Std         0.136719\n",
      "evaluation/env_infos/initial/reward_forward Max         0.214192\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.233552\n",
      "evaluation/env_infos/reward_forward Mean               -0.000688863\n",
      "evaluation/env_infos/reward_forward Std                 0.0611292\n",
      "evaluation/env_infos/reward_forward Max                 1.4863\n",
      "evaluation/env_infos/reward_forward Min                -1.18876\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0304385\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00999228\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0181945\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0525515\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.020354\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0104534\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00600106\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0401183\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.030657\n",
      "evaluation/env_infos/reward_ctrl Std                    0.011188\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00316823\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.360346\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.75337e-10\n",
      "evaluation/env_infos/final/torso_velocity Std           2.99976e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.44466e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.61195e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.117542\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.241931\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.648773\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.30526\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00308557\n",
      "evaluation/env_infos/torso_velocity Std                 0.0669878\n",
      "evaluation/env_infos/torso_velocity Max                 1.4863\n",
      "evaluation/env_infos/torso_velocity Min                -2.12976\n",
      "time/data storing (s)                                   0.0144047\n",
      "time/evaluation sampling (s)                           44.5272\n",
      "time/exploration sampling (s)                           1.97865\n",
      "time/logging (s)                                        0.293291\n",
      "time/saving (s)                                         0.0282245\n",
      "time/training (s)                                       3.97641\n",
      "time/epoch (s)                                         50.8181\n",
      "time/total (s)                                       1489.95\n",
      "Epoch                                                  23\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:36:03.641858 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 24 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  26000\n",
      "trainer/QF1 Loss                                        0.155145\n",
      "trainer/QF2 Loss                                        0.151979\n",
      "trainer/Policy Loss                                    -2.51818\n",
      "trainer/Q1 Predictions Mean                             9.50773\n",
      "trainer/Q1 Predictions Std                              0.918595\n",
      "trainer/Q1 Predictions Max                             12.2227\n",
      "trainer/Q1 Predictions Min                              6.42837\n",
      "trainer/Q2 Predictions Mean                             9.48587\n",
      "trainer/Q2 Predictions Std                              0.922556\n",
      "trainer/Q2 Predictions Max                             12.1283\n",
      "trainer/Q2 Predictions Min                              6.20738\n",
      "trainer/Q Targets Mean                                  9.42628\n",
      "trainer/Q Targets Std                                   0.935356\n",
      "trainer/Q Targets Max                                  13.6135\n",
      "trainer/Q Targets Min                                   6.08227\n",
      "trainer/Log Pis Mean                                    7.47692\n",
      "trainer/Log Pis Std                                     2.28158\n",
      "trainer/Log Pis Max                                    13.1648\n",
      "trainer/Log Pis Min                                    -1.49805\n",
      "trainer/Policy mu Mean                                  0.035869\n",
      "trainer/Policy mu Std                                   0.116196\n",
      "trainer/Policy mu Max                                   0.532433\n",
      "trainer/Policy mu Min                                  -0.581739\n",
      "trainer/Policy log std Mean                            -2.32315\n",
      "trainer/Policy log std Std                              0.171527\n",
      "trainer/Policy log std Max                             -1.65639\n",
      "trainer/Policy log std Min                             -2.88022\n",
      "trainer/Alpha                                           0.0154867\n",
      "trainer/Alpha Loss                                     -2.17984\n",
      "exploration/num steps total                         26000\n",
      "exploration/num paths total                            78\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.894823\n",
      "exploration/Rewards Std                                 0.102975\n",
      "exploration/Rewards Max                                 2.02026\n",
      "exploration/Rewards Min                                 0.64029\n",
      "exploration/Returns Mean                              894.823\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               894.823\n",
      "exploration/Returns Min                               894.823\n",
      "exploration/Actions Mean                                0.0781065\n",
      "exploration/Actions Std                                 0.163363\n",
      "exploration/Actions Max                                 0.604\n",
      "exploration/Actions Min                                -0.451911\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           894.823\n",
      "exploration/env_infos/final/reward_forward Mean        -0.173398\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.173398\n",
      "exploration/env_infos/final/reward_forward Min         -0.173398\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0906537\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0906537\n",
      "exploration/env_infos/initial/reward_forward Min        0.0906537\n",
      "exploration/env_infos/reward_forward Mean               0.0160736\n",
      "exploration/env_infos/reward_forward Std                0.129875\n",
      "exploration/env_infos/reward_forward Max                0.643675\n",
      "exploration/env_infos/reward_forward Min               -0.393018\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.153189\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.153189\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.153189\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0653561\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0653561\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0653561\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.131152\n",
      "exploration/env_infos/reward_ctrl Std                   0.0522367\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0244959\n",
      "exploration/env_infos/reward_ctrl Min                  -0.35971\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.120085\n",
      "exploration/env_infos/final/torso_velocity Std          0.0987166\n",
      "exploration/env_infos/final/torso_velocity Max          0.0183114\n",
      "exploration/env_infos/final/torso_velocity Min         -0.205168\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0963875\n",
      "exploration/env_infos/initial/torso_velocity Std        0.129718\n",
      "exploration/env_infos/initial/torso_velocity Max        0.258049\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.05954\n",
      "exploration/env_infos/torso_velocity Mean               0.0110419\n",
      "exploration/env_infos/torso_velocity Std                0.16506\n",
      "exploration/env_infos/torso_velocity Max                1.23144\n",
      "exploration/env_infos/torso_velocity Min               -1.11405\n",
      "evaluation/num steps total                         625000\n",
      "evaluation/num paths total                            625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.943804\n",
      "evaluation/Rewards Std                                  0.041836\n",
      "evaluation/Rewards Max                                  2.47574\n",
      "evaluation/Rewards Min                                  0.725865\n",
      "evaluation/Returns Mean                               943.804\n",
      "evaluation/Returns Std                                 36.369\n",
      "evaluation/Returns Max                                986.852\n",
      "evaluation/Returns Min                                877.795\n",
      "evaluation/Actions Mean                                 0.0389189\n",
      "evaluation/Actions Std                                  0.112696\n",
      "evaluation/Actions Max                                  0.545225\n",
      "evaluation/Actions Min                                 -0.410375\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            943.804\n",
      "evaluation/env_infos/final/reward_forward Mean          1.28541e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.97865e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.29641e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.33966e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.012705\n",
      "evaluation/env_infos/initial/reward_forward Std         0.117341\n",
      "evaluation/env_infos/initial/reward_forward Max         0.188746\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.19135\n",
      "evaluation/env_infos/reward_forward Mean                0.00303124\n",
      "evaluation/env_infos/reward_forward Std                 0.0564138\n",
      "evaluation/env_infos/reward_forward Max                 1.38323\n",
      "evaluation/env_infos/reward_forward Min                -0.950124\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0565906\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0369109\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0121404\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.122834\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0190233\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0132926\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00671333\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0510462\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0568601\n",
      "evaluation/env_infos/reward_ctrl Std                    0.037108\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00671333\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.29087\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -2.24548e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.28364e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.29641e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.71872e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.141214\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.209456\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.620195\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.19135\n",
      "evaluation/env_infos/torso_velocity Mean               -2.79882e-05\n",
      "evaluation/env_infos/torso_velocity Std                 0.0646028\n",
      "evaluation/env_infos/torso_velocity Max                 1.61319\n",
      "evaluation/env_infos/torso_velocity Min                -1.78662\n",
      "time/data storing (s)                                   0.0154197\n",
      "time/evaluation sampling (s)                           45.2722\n",
      "time/exploration sampling (s)                           1.95217\n",
      "time/logging (s)                                        0.280316\n",
      "time/saving (s)                                         0.0282508\n",
      "time/training (s)                                       3.9569\n",
      "time/epoch (s)                                         51.5053\n",
      "time/total (s)                                       1541.81\n",
      "Epoch                                                  24\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:36:55.154119 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 25 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  27000\n",
      "trainer/QF1 Loss                                        0.211959\n",
      "trainer/QF2 Loss                                        0.163333\n",
      "trainer/Policy Loss                                    -3.11647\n",
      "trainer/Q1 Predictions Mean                             9.99817\n",
      "trainer/Q1 Predictions Std                              0.980269\n",
      "trainer/Q1 Predictions Max                             11.7513\n",
      "trainer/Q1 Predictions Min                              6.27374\n",
      "trainer/Q2 Predictions Mean                             9.87693\n",
      "trainer/Q2 Predictions Std                              1.02826\n",
      "trainer/Q2 Predictions Max                             11.5572\n",
      "trainer/Q2 Predictions Min                              5.14587\n",
      "trainer/Q Targets Mean                                  9.78834\n",
      "trainer/Q Targets Std                                   0.980315\n",
      "trainer/Q Targets Max                                  11.7046\n",
      "trainer/Q Targets Min                                   6.58179\n",
      "trainer/Log Pis Mean                                    7.2129\n",
      "trainer/Log Pis Std                                     2.19544\n",
      "trainer/Log Pis Max                                    12.0334\n",
      "trainer/Log Pis Min                                    -0.630969\n",
      "trainer/Policy mu Mean                                 -0.0375292\n",
      "trainer/Policy mu Std                                   0.129885\n",
      "trainer/Policy mu Max                                   0.65217\n",
      "trainer/Policy mu Min                                  -0.661592\n",
      "trainer/Policy log std Mean                            -2.26421\n",
      "trainer/Policy log std Std                              0.208936\n",
      "trainer/Policy log std Max                             -1.2367\n",
      "trainer/Policy log std Min                             -2.90536\n",
      "trainer/Alpha                                           0.0148357\n",
      "trainer/Alpha Loss                                     -3.31393\n",
      "exploration/num steps total                         27000\n",
      "exploration/num paths total                            79\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.920004\n",
      "exploration/Rewards Std                                 0.0417289\n",
      "exploration/Rewards Max                                 1.20382\n",
      "exploration/Rewards Min                                 0.750455\n",
      "exploration/Returns Mean                              920.004\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               920.004\n",
      "exploration/Returns Min                               920.004\n",
      "exploration/Actions Mean                               -0.0987196\n",
      "exploration/Actions Std                                 0.109227\n",
      "exploration/Actions Max                                 0.291658\n",
      "exploration/Actions Min                                -0.518225\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           920.004\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0179604\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0179604\n",
      "exploration/env_infos/final/reward_forward Min         -0.0179604\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0569788\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0569788\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0569788\n",
      "exploration/env_infos/reward_forward Mean               0.0116132\n",
      "exploration/env_infos/reward_forward Std                0.0907057\n",
      "exploration/env_infos/reward_forward Max                0.852711\n",
      "exploration/env_infos/reward_forward Min               -0.662408\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.105625\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.105625\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.105625\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0822793\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0822793\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0822793\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0867043\n",
      "exploration/env_infos/reward_ctrl Std                   0.0327987\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0137308\n",
      "exploration/env_infos/reward_ctrl Min                  -0.249545\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0484622\n",
      "exploration/env_infos/final/torso_velocity Std          0.0541217\n",
      "exploration/env_infos/final/torso_velocity Max         -0.00291872\n",
      "exploration/env_infos/final/torso_velocity Min         -0.124508\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0891641\n",
      "exploration/env_infos/initial/torso_velocity Std        0.31357\n",
      "exploration/env_infos/initial/torso_velocity Max        0.524825\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.200354\n",
      "exploration/env_infos/torso_velocity Mean               0.0012538\n",
      "exploration/env_infos/torso_velocity Std                0.128138\n",
      "exploration/env_infos/torso_velocity Max                0.852711\n",
      "exploration/env_infos/torso_velocity Min               -0.96474\n",
      "evaluation/num steps total                         650000\n",
      "evaluation/num paths total                            650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.969232\n",
      "evaluation/Rewards Std                                  0.0278883\n",
      "evaluation/Rewards Max                                  2.36786\n",
      "evaluation/Rewards Min                                  0.622383\n",
      "evaluation/Returns Mean                               969.232\n",
      "evaluation/Returns Std                                 13.9614\n",
      "evaluation/Returns Max                                988.262\n",
      "evaluation/Returns Min                                937.27\n",
      "evaluation/Actions Mean                                -0.067211\n",
      "evaluation/Actions Std                                  0.0585239\n",
      "evaluation/Actions Max                                  0.529053\n",
      "evaluation/Actions Min                                 -0.688108\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            969.232\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.43684e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.13092e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.55068e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.01521e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0200549\n",
      "evaluation/env_infos/initial/reward_forward Std         0.128681\n",
      "evaluation/env_infos/initial/reward_forward Max         0.210922\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.351015\n",
      "evaluation/env_infos/reward_forward Mean                0.000282027\n",
      "evaluation/env_infos/reward_forward Std                 0.0558292\n",
      "evaluation/env_infos/reward_forward Max                 1.4906\n",
      "evaluation/env_infos/reward_forward Min                -0.981936\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0311234\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0138682\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0130051\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0632858\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0262658\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00951017\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00936541\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0431892\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0317695\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0159894\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00936541\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.382981\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -6.96644e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           4.0221e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.47021e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.01521e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.13066\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.248939\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.771046\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.351015\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00114824\n",
      "evaluation/env_infos/torso_velocity Std                 0.0615605\n",
      "evaluation/env_infos/torso_velocity Max                 1.4906\n",
      "evaluation/env_infos/torso_velocity Min                -2.06471\n",
      "time/data storing (s)                                   0.0151377\n",
      "time/evaluation sampling (s)                           44.7635\n",
      "time/exploration sampling (s)                           2.00648\n",
      "time/logging (s)                                        0.28728\n",
      "time/saving (s)                                         0.028522\n",
      "time/training (s)                                       4.08141\n",
      "time/epoch (s)                                         51.1824\n",
      "time/total (s)                                       1593.33\n",
      "Epoch                                                  25\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:37:46.167564 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 26 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  28000\n",
      "trainer/QF1 Loss                                        0.141681\n",
      "trainer/QF2 Loss                                        0.147512\n",
      "trainer/Policy Loss                                    -2.5458\n",
      "trainer/Q1 Predictions Mean                            10.217\n",
      "trainer/Q1 Predictions Std                              0.98799\n",
      "trainer/Q1 Predictions Max                             11.4124\n",
      "trainer/Q1 Predictions Min                              6.17856\n",
      "trainer/Q2 Predictions Mean                            10.0282\n",
      "trainer/Q2 Predictions Std                              0.981002\n",
      "trainer/Q2 Predictions Max                             11.1373\n",
      "trainer/Q2 Predictions Min                              5.72044\n",
      "trainer/Q Targets Mean                                 10.107\n",
      "trainer/Q Targets Std                                   0.984456\n",
      "trainer/Q Targets Max                                  11.8285\n",
      "trainer/Q Targets Min                                   6.14805\n",
      "trainer/Log Pis Mean                                    8.02203\n",
      "trainer/Log Pis Std                                     2.4182\n",
      "trainer/Log Pis Max                                    14.4561\n",
      "trainer/Log Pis Min                                     0.0734797\n",
      "trainer/Policy mu Mean                                  0.00386199\n",
      "trainer/Policy mu Std                                   0.137471\n",
      "trainer/Policy mu Max                                   0.930962\n",
      "trainer/Policy mu Min                                  -0.603626\n",
      "trainer/Policy log std Mean                            -2.39448\n",
      "trainer/Policy log std Std                              0.227366\n",
      "trainer/Policy log std Max                             -1.75937\n",
      "trainer/Policy log std Min                             -3.35603\n",
      "trainer/Alpha                                           0.0143043\n",
      "trainer/Alpha Loss                                      0.0935493\n",
      "exploration/num steps total                         28000\n",
      "exploration/num paths total                            80\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.917316\n",
      "exploration/Rewards Std                                 0.0679075\n",
      "exploration/Rewards Max                                 1.4398\n",
      "exploration/Rewards Min                                 0.712628\n",
      "exploration/Returns Mean                              917.316\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               917.316\n",
      "exploration/Returns Min                               917.316\n",
      "exploration/Actions Mean                                0.0393575\n",
      "exploration/Actions Std                                 0.148905\n",
      "exploration/Actions Max                                 0.570527\n",
      "exploration/Actions Min                                -0.441472\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           917.316\n",
      "exploration/env_infos/final/reward_forward Mean        -0.642104\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.642104\n",
      "exploration/env_infos/final/reward_forward Min         -0.642104\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0245551\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0245551\n",
      "exploration/env_infos/initial/reward_forward Min        0.0245551\n",
      "exploration/env_infos/reward_forward Mean               0.0300354\n",
      "exploration/env_infos/reward_forward Std                0.182142\n",
      "exploration/env_infos/reward_forward Max                0.82293\n",
      "exploration/env_infos/reward_forward Min               -0.642104\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0795419\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0795419\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0795419\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0297623\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0297623\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0297623\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0948874\n",
      "exploration/env_infos/reward_ctrl Std                   0.0446261\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0104435\n",
      "exploration/env_infos/reward_ctrl Min                  -0.287372\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.256401\n",
      "exploration/env_infos/final/torso_velocity Std          0.274376\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0268281\n",
      "exploration/env_infos/final/torso_velocity Min         -0.642104\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0890402\n",
      "exploration/env_infos/initial/torso_velocity Std        0.155916\n",
      "exploration/env_infos/initial/torso_velocity Max        0.303891\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0613259\n",
      "exploration/env_infos/torso_velocity Mean               0.0141889\n",
      "exploration/env_infos/torso_velocity Std                0.192301\n",
      "exploration/env_infos/torso_velocity Max                1.09445\n",
      "exploration/env_infos/torso_velocity Min               -1.71114\n",
      "evaluation/num steps total                         675000\n",
      "evaluation/num paths total                            675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.967477\n",
      "evaluation/Rewards Std                                  0.0348848\n",
      "evaluation/Rewards Max                                  2.42694\n",
      "evaluation/Rewards Min                                  0.74334\n",
      "evaluation/Returns Mean                               967.477\n",
      "evaluation/Returns Std                                 13.3379\n",
      "evaluation/Returns Max                                987.275\n",
      "evaluation/Returns Min                                941.383\n",
      "evaluation/Actions Mean                                -0.00172568\n",
      "evaluation/Actions Std                                  0.092233\n",
      "evaluation/Actions Max                                  0.537284\n",
      "evaluation/Actions Min                                 -0.483011\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            967.477\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.15407e-07\n",
      "evaluation/env_infos/final/reward_forward Std           5.68235e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.76684e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.21306e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.01932\n",
      "evaluation/env_infos/initial/reward_forward Std         0.147922\n",
      "evaluation/env_infos/initial/reward_forward Max         0.352059\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.258638\n",
      "evaluation/env_infos/reward_forward Mean                0.00231066\n",
      "evaluation/env_infos/reward_forward Std                 0.0585852\n",
      "evaluation/env_infos/reward_forward Max                 1.0934\n",
      "evaluation/env_infos/reward_forward Min                -1.01953\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0338235\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0135715\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0109841\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0584911\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0193636\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00696675\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00860047\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0346288\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0340397\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0145716\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00189064\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.25666\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.19724e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           4.58469e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.07351e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.21306e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.142836\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.248208\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.684557\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.271737\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000351267\n",
      "evaluation/env_infos/torso_velocity Std                 0.0655247\n",
      "evaluation/env_infos/torso_velocity Max                 1.47509\n",
      "evaluation/env_infos/torso_velocity Min                -1.83804\n",
      "time/data storing (s)                                   0.0151057\n",
      "time/evaluation sampling (s)                           44.3615\n",
      "time/exploration sampling (s)                           1.97891\n",
      "time/logging (s)                                        0.290725\n",
      "time/saving (s)                                         0.0277449\n",
      "time/training (s)                                       3.97906\n",
      "time/epoch (s)                                         50.6531\n",
      "time/total (s)                                       1644.34\n",
      "Epoch                                                  26\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:38:38.492281 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 27 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  29000\n",
      "trainer/QF1 Loss                                        0.125179\n",
      "trainer/QF2 Loss                                        0.201656\n",
      "trainer/Policy Loss                                    -3.76424\n",
      "trainer/Q1 Predictions Mean                            10.6611\n",
      "trainer/Q1 Predictions Std                              0.936518\n",
      "trainer/Q1 Predictions Max                             14.6592\n",
      "trainer/Q1 Predictions Min                              7.83988\n",
      "trainer/Q2 Predictions Mean                            10.8579\n",
      "trainer/Q2 Predictions Std                              0.950354\n",
      "trainer/Q2 Predictions Max                             14.8752\n",
      "trainer/Q2 Predictions Min                              7.59038\n",
      "trainer/Q Targets Mean                                 10.6587\n",
      "trainer/Q Targets Std                                   0.954227\n",
      "trainer/Q Targets Max                                  16.2017\n",
      "trainer/Q Targets Min                                   7.76499\n",
      "trainer/Log Pis Mean                                    7.34476\n",
      "trainer/Log Pis Std                                     2.45105\n",
      "trainer/Log Pis Max                                    13.4122\n",
      "trainer/Log Pis Min                                     0.715495\n",
      "trainer/Policy mu Mean                                 -0.0427393\n",
      "trainer/Policy mu Std                                   0.114829\n",
      "trainer/Policy mu Max                                   0.65004\n",
      "trainer/Policy mu Min                                  -0.462651\n",
      "trainer/Policy log std Mean                            -2.31931\n",
      "trainer/Policy log std Std                              0.234399\n",
      "trainer/Policy log std Max                             -1.01454\n",
      "trainer/Policy log std Min                             -2.99237\n",
      "trainer/Alpha                                           0.0138842\n",
      "trainer/Alpha Loss                                     -2.80231\n",
      "exploration/num steps total                         29000\n",
      "exploration/num paths total                            81\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.941099\n",
      "exploration/Rewards Std                                 0.0317543\n",
      "exploration/Rewards Max                                 1.22012\n",
      "exploration/Rewards Min                                 0.706119\n",
      "exploration/Returns Mean                              941.099\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               941.099\n",
      "exploration/Returns Min                               941.099\n",
      "exploration/Actions Mean                               -0.0829234\n",
      "exploration/Actions Std                                 0.09352\n",
      "exploration/Actions Max                                 0.452511\n",
      "exploration/Actions Min                                -0.535059\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           941.099\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0603264\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0603264\n",
      "exploration/env_infos/final/reward_forward Min         -0.0603264\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0427023\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0427023\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0427023\n",
      "exploration/env_infos/reward_forward Mean               0.00436387\n",
      "exploration/env_infos/reward_forward Std                0.108241\n",
      "exploration/env_infos/reward_forward Max                0.552937\n",
      "exploration/env_infos/reward_forward Min               -0.602133\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0837282\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0837282\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0837282\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0538872\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0538872\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0538872\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0624891\n",
      "exploration/env_infos/reward_ctrl Std                   0.0238184\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0134211\n",
      "exploration/env_infos/reward_ctrl Min                  -0.293881\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00430187\n",
      "exploration/env_infos/final/torso_velocity Std          0.0421805\n",
      "exploration/env_infos/final/torso_velocity Max          0.0414511\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0603264\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.189424\n",
      "exploration/env_infos/initial/torso_velocity Std        0.186052\n",
      "exploration/env_infos/initial/torso_velocity Max        0.412776\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0427023\n",
      "exploration/env_infos/torso_velocity Mean              -0.000285923\n",
      "exploration/env_infos/torso_velocity Std                0.116549\n",
      "exploration/env_infos/torso_velocity Max                0.89478\n",
      "exploration/env_infos/torso_velocity Min               -1.14518\n",
      "evaluation/num steps total                         700000\n",
      "evaluation/num paths total                            700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.969877\n",
      "evaluation/Rewards Std                                  0.0356811\n",
      "evaluation/Rewards Max                                  2.51438\n",
      "evaluation/Rewards Min                                  0.606274\n",
      "evaluation/Returns Mean                               969.877\n",
      "evaluation/Returns Std                                 15.7757\n",
      "evaluation/Returns Max                                986.04\n",
      "evaluation/Returns Min                                930.06\n",
      "evaluation/Actions Mean                                -0.0572099\n",
      "evaluation/Actions Std                                  0.0682167\n",
      "evaluation/Actions Max                                  0.451637\n",
      "evaluation/Actions Min                                 -0.621509\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            969.877\n",
      "evaluation/env_infos/final/reward_forward Mean          0.00308358\n",
      "evaluation/env_infos/final/reward_forward Std           0.010703\n",
      "evaluation/env_infos/final/reward_forward Max           0.0466128\n",
      "evaluation/env_infos/final/reward_forward Min          -8.8493e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0516264\n",
      "evaluation/env_infos/initial/reward_forward Std         0.127979\n",
      "evaluation/env_infos/initial/reward_forward Max         0.251966\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.161105\n",
      "evaluation/env_infos/reward_forward Mean                0.000890523\n",
      "evaluation/env_infos/reward_forward Std                 0.0595974\n",
      "evaluation/env_infos/reward_forward Max                 1.50394\n",
      "evaluation/env_infos/reward_forward Min                -1.09998\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0310003\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.016301\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0167661\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.071149\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0290376\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0113595\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0185224\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0521883\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.031706\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0183442\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0105411\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.393726\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000354605\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0137782\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0466128\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0997728\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.151199\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.231843\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.625621\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.265019\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00105564\n",
      "evaluation/env_infos/torso_velocity Std                 0.0744451\n",
      "evaluation/env_infos/torso_velocity Max                 1.52357\n",
      "evaluation/env_infos/torso_velocity Min                -2.17825\n",
      "time/data storing (s)                                   0.0153486\n",
      "time/evaluation sampling (s)                           45.6535\n",
      "time/exploration sampling (s)                           1.94401\n",
      "time/logging (s)                                        0.280596\n",
      "time/saving (s)                                         0.0284986\n",
      "time/training (s)                                       4.01678\n",
      "time/epoch (s)                                         51.9387\n",
      "time/total (s)                                       1696.66\n",
      "Epoch                                                  27\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:39:30.295627 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 28 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  30000\n",
      "trainer/QF1 Loss                                        0.186515\n",
      "trainer/QF2 Loss                                        0.187325\n",
      "trainer/Policy Loss                                    -3.95801\n",
      "trainer/Q1 Predictions Mean                            10.945\n",
      "trainer/Q1 Predictions Std                              0.98151\n",
      "trainer/Q1 Predictions Max                             12.3198\n",
      "trainer/Q1 Predictions Min                              7.36841\n",
      "trainer/Q2 Predictions Mean                            10.8721\n",
      "trainer/Q2 Predictions Std                              0.946055\n",
      "trainer/Q2 Predictions Max                             12.1448\n",
      "trainer/Q2 Predictions Min                              7.55126\n",
      "trainer/Q Targets Mean                                 10.9042\n",
      "trainer/Q Targets Std                                   0.978464\n",
      "trainer/Q Targets Max                                  14.2522\n",
      "trainer/Q Targets Min                                   6.8682\n",
      "trainer/Log Pis Mean                                    7.32978\n",
      "trainer/Log Pis Std                                     2.45456\n",
      "trainer/Log Pis Max                                    13.5527\n",
      "trainer/Log Pis Min                                     0.173189\n",
      "trainer/Policy mu Mean                                 -0.0116687\n",
      "trainer/Policy mu Std                                   0.133009\n",
      "trainer/Policy mu Max                                   0.639888\n",
      "trainer/Policy mu Min                                  -0.731003\n",
      "trainer/Policy log std Mean                            -2.3087\n",
      "trainer/Policy log std Std                              0.222348\n",
      "trainer/Policy log std Max                             -1.43625\n",
      "trainer/Policy log std Min                             -3.11982\n",
      "trainer/Alpha                                           0.0134766\n",
      "trainer/Alpha Loss                                     -2.88616\n",
      "exploration/num steps total                         30000\n",
      "exploration/num paths total                            82\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.9469\n",
      "exploration/Rewards Std                                 0.0690123\n",
      "exploration/Rewards Max                                 2.46065\n",
      "exploration/Rewards Min                                 0.644205\n",
      "exploration/Returns Mean                              946.9\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               946.9\n",
      "exploration/Returns Min                               946.9\n",
      "exploration/Actions Mean                               -0.0535149\n",
      "exploration/Actions Std                                 0.111491\n",
      "exploration/Actions Max                                 0.486193\n",
      "exploration/Actions Min                                -0.548493\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           946.9\n",
      "exploration/env_infos/final/reward_forward Mean         0.0722702\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0722702\n",
      "exploration/env_infos/final/reward_forward Min          0.0722702\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.113755\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.113755\n",
      "exploration/env_infos/initial/reward_forward Min       -0.113755\n",
      "exploration/env_infos/reward_forward Mean              -0.00320056\n",
      "exploration/env_infos/reward_forward Std                0.15014\n",
      "exploration/env_infos/reward_forward Max                1.44433\n",
      "exploration/env_infos/reward_forward Min               -0.442494\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0796187\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0796187\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0796187\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0144248\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0144248\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0144248\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0611761\n",
      "exploration/env_infos/reward_ctrl Std                   0.0303063\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00593181\n",
      "exploration/env_infos/reward_ctrl Min                  -0.355795\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0199627\n",
      "exploration/env_infos/final/torso_velocity Std          0.0370912\n",
      "exploration/env_infos/final/torso_velocity Max          0.0722702\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00959377\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0965562\n",
      "exploration/env_infos/initial/torso_velocity Std        0.227623\n",
      "exploration/env_infos/initial/torso_velocity Max        0.412769\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.113755\n",
      "exploration/env_infos/torso_velocity Mean              -0.000766508\n",
      "exploration/env_infos/torso_velocity Std                0.145052\n",
      "exploration/env_infos/torso_velocity Max                1.81466\n",
      "exploration/env_infos/torso_velocity Min               -1.15546\n",
      "evaluation/num steps total                         725000\n",
      "evaluation/num paths total                            725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.958656\n",
      "evaluation/Rewards Std                                  0.0320034\n",
      "evaluation/Rewards Max                                  2.56313\n",
      "evaluation/Rewards Min                                  0.605728\n",
      "evaluation/Returns Mean                               958.656\n",
      "evaluation/Returns Std                                 10.953\n",
      "evaluation/Returns Max                                976.228\n",
      "evaluation/Returns Min                                933.019\n",
      "evaluation/Actions Mean                                -0.059008\n",
      "evaluation/Actions Std                                  0.0842965\n",
      "evaluation/Actions Max                                  0.554898\n",
      "evaluation/Actions Min                                 -0.626739\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            958.656\n",
      "evaluation/env_infos/final/reward_forward Mean          3.7789e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.9568e-05\n",
      "evaluation/env_infos/final/reward_forward Max           9.96236e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -8.35774e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0114369\n",
      "evaluation/env_infos/initial/reward_forward Std         0.129136\n",
      "evaluation/env_infos/initial/reward_forward Max         0.285751\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.214649\n",
      "evaluation/env_infos/reward_forward Mean                0.00338272\n",
      "evaluation/env_infos/reward_forward Std                 0.0661927\n",
      "evaluation/env_infos/reward_forward Max                 1.71012\n",
      "evaluation/env_infos/reward_forward Min                -0.706881\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0416225\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00994302\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0289452\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0658294\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0194725\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00442502\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00977776\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0290666\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0423514\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0136249\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00442039\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.394272\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          9.08685e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           5.15822e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000401437\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.35774e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.149179\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.222359\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.694693\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.225814\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00136412\n",
      "evaluation/env_infos/torso_velocity Std                 0.0724653\n",
      "evaluation/env_infos/torso_velocity Max                 2.04235\n",
      "evaluation/env_infos/torso_velocity Min                -1.79682\n",
      "time/data storing (s)                                   0.0146134\n",
      "time/evaluation sampling (s)                           45.1377\n",
      "time/exploration sampling (s)                           1.98521\n",
      "time/logging (s)                                        0.284884\n",
      "time/saving (s)                                         0.0271022\n",
      "time/training (s)                                       3.98772\n",
      "time/epoch (s)                                         51.4372\n",
      "time/total (s)                                       1748.46\n",
      "Epoch                                                  28\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:40:22.521809 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 29 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  31000\n",
      "trainer/QF1 Loss                                        0.160211\n",
      "trainer/QF2 Loss                                        0.185929\n",
      "trainer/Policy Loss                                    -4.04005\n",
      "trainer/Q1 Predictions Mean                            11.3462\n",
      "trainer/Q1 Predictions Std                              1.00699\n",
      "trainer/Q1 Predictions Max                             13.0387\n",
      "trainer/Q1 Predictions Min                              8.07591\n",
      "trainer/Q2 Predictions Mean                            11.0662\n",
      "trainer/Q2 Predictions Std                              0.975289\n",
      "trainer/Q2 Predictions Max                             13.2146\n",
      "trainer/Q2 Predictions Min                              7.76819\n",
      "trainer/Q Targets Mean                                 11.2279\n",
      "trainer/Q Targets Std                                   0.968815\n",
      "trainer/Q Targets Max                                  13.4109\n",
      "trainer/Q Targets Min                                   7.89723\n",
      "trainer/Log Pis Mean                                    7.49093\n",
      "trainer/Log Pis Std                                     2.62705\n",
      "trainer/Log Pis Max                                    12.9902\n",
      "trainer/Log Pis Min                                    -4.72043\n",
      "trainer/Policy mu Mean                                 -0.00496865\n",
      "trainer/Policy mu Std                                   0.127817\n",
      "trainer/Policy mu Max                                   0.61155\n",
      "trainer/Policy mu Min                                  -0.731207\n",
      "trainer/Policy log std Mean                            -2.36311\n",
      "trainer/Policy log std Std                              0.196259\n",
      "trainer/Policy log std Max                             -1.49249\n",
      "trainer/Policy log std Min                             -3.02734\n",
      "trainer/Alpha                                           0.0132984\n",
      "trainer/Alpha Loss                                     -2.19913\n",
      "exploration/num steps total                         31000\n",
      "exploration/num paths total                            83\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.976222\n",
      "exploration/Rewards Std                                 0.110759\n",
      "exploration/Rewards Max                                 2.01292\n",
      "exploration/Rewards Min                                 0.698892\n",
      "exploration/Returns Mean                              976.222\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               976.222\n",
      "exploration/Returns Min                               976.222\n",
      "exploration/Actions Mean                               -0.0130523\n",
      "exploration/Actions Std                                 0.110664\n",
      "exploration/Actions Max                                 0.45792\n",
      "exploration/Actions Min                                -0.507405\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           976.222\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0410787\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0410787\n",
      "exploration/env_infos/final/reward_forward Min         -0.0410787\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0146646\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0146646\n",
      "exploration/env_infos/initial/reward_forward Min        0.0146646\n",
      "exploration/env_infos/reward_forward Mean               0.000354556\n",
      "exploration/env_infos/reward_forward Std                0.191908\n",
      "exploration/env_infos/reward_forward Max                0.845481\n",
      "exploration/env_infos/reward_forward Min               -0.806358\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0622966\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0622966\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0622966\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.057932\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.057932\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.057932\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.049668\n",
      "exploration/env_infos/reward_ctrl Std                   0.029065\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00630294\n",
      "exploration/env_infos/reward_ctrl Min                  -0.301108\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.125939\n",
      "exploration/env_infos/final/torso_velocity Std          0.166196\n",
      "exploration/env_infos/final/torso_velocity Max          0.0214475\n",
      "exploration/env_infos/final/torso_velocity Min         -0.358186\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0583688\n",
      "exploration/env_infos/initial/torso_velocity Std        0.134437\n",
      "exploration/env_infos/initial/torso_velocity Max        0.240462\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0800204\n",
      "exploration/env_infos/torso_velocity Mean               0.0138133\n",
      "exploration/env_infos/torso_velocity Std                0.254025\n",
      "exploration/env_infos/torso_velocity Max                1.45616\n",
      "exploration/env_infos/torso_velocity Min               -1.53556\n",
      "evaluation/num steps total                         750000\n",
      "evaluation/num paths total                            750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.973441\n",
      "evaluation/Rewards Std                                  0.0315815\n",
      "evaluation/Rewards Max                                  1.82902\n",
      "evaluation/Rewards Min                                  0.631227\n",
      "evaluation/Returns Mean                               973.441\n",
      "evaluation/Returns Std                                 14.6991\n",
      "evaluation/Returns Max                                988.255\n",
      "evaluation/Returns Min                                939.409\n",
      "evaluation/Actions Mean                                -0.0141702\n",
      "evaluation/Actions Std                                  0.0824199\n",
      "evaluation/Actions Max                                  0.644062\n",
      "evaluation/Actions Min                                 -0.497324\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            973.441\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.44483e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.79369e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.35351e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.10392e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0222174\n",
      "evaluation/env_infos/initial/reward_forward Std         0.123679\n",
      "evaluation/env_infos/initial/reward_forward Max         0.282955\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.23538\n",
      "evaluation/env_infos/reward_forward Mean                0.000948177\n",
      "evaluation/env_infos/reward_forward Std                 0.0688867\n",
      "evaluation/env_infos/reward_forward Max                 1.63254\n",
      "evaluation/env_infos/reward_forward Min                -0.78259\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0272536\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0146655\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0156443\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0606204\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0187767\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00504645\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0126233\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0318833\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0279753\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0175917\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00928186\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.368773\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -2.08565e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           2.97717e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           6.71896e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.10392e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.146059\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.221892\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.626424\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.23538\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0010992\n",
      "evaluation/env_infos/torso_velocity Std                 0.0714766\n",
      "evaluation/env_infos/torso_velocity Max                 1.63254\n",
      "evaluation/env_infos/torso_velocity Min                -1.7066\n",
      "time/data storing (s)                                   0.015994\n",
      "time/evaluation sampling (s)                           45.1739\n",
      "time/exploration sampling (s)                           1.94101\n",
      "time/logging (s)                                        0.28134\n",
      "time/saving (s)                                         0.0292776\n",
      "time/training (s)                                       4.40427\n",
      "time/epoch (s)                                         51.8458\n",
      "time/total (s)                                       1800.69\n",
      "Epoch                                                  29\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:41:15.248438 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 30 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  32000\n",
      "trainer/QF1 Loss                                        0.439038\n",
      "trainer/QF2 Loss                                        0.21636\n",
      "trainer/Policy Loss                                    -3.54766\n",
      "trainer/Q1 Predictions Mean                            11.3572\n",
      "trainer/Q1 Predictions Std                              1.10659\n",
      "trainer/Q1 Predictions Max                             13.3665\n",
      "trainer/Q1 Predictions Min                              3.77713\n",
      "trainer/Q2 Predictions Mean                            11.6209\n",
      "trainer/Q2 Predictions Std                              1.18903\n",
      "trainer/Q2 Predictions Max                             13.1852\n",
      "trainer/Q2 Predictions Min                              2.95931\n",
      "trainer/Q Targets Mean                                 11.6556\n",
      "trainer/Q Targets Std                                   1.32799\n",
      "trainer/Q Targets Max                                  13.3861\n",
      "trainer/Q Targets Min                                   0.270019\n",
      "trainer/Log Pis Mean                                    8.19227\n",
      "trainer/Log Pis Std                                     2.40083\n",
      "trainer/Log Pis Max                                    14.1147\n",
      "trainer/Log Pis Min                                    -0.770193\n",
      "trainer/Policy mu Mean                                 -0.028743\n",
      "trainer/Policy mu Std                                   0.126662\n",
      "trainer/Policy mu Max                                   1.19728\n",
      "trainer/Policy mu Min                                  -0.952416\n",
      "trainer/Policy log std Mean                            -2.42802\n",
      "trainer/Policy log std Std                              0.210814\n",
      "trainer/Policy log std Max                             -1.65326\n",
      "trainer/Policy log std Min                             -3.18163\n",
      "trainer/Alpha                                           0.0130781\n",
      "trainer/Alpha Loss                                      0.833888\n",
      "exploration/num steps total                         32000\n",
      "exploration/num paths total                            84\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.964345\n",
      "exploration/Rewards Std                                 0.126542\n",
      "exploration/Rewards Max                                 2.12883\n",
      "exploration/Rewards Min                                 0.611622\n",
      "exploration/Returns Mean                              964.345\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               964.345\n",
      "exploration/Returns Min                               964.345\n",
      "exploration/Actions Mean                               -0.0421836\n",
      "exploration/Actions Std                                 0.12192\n",
      "exploration/Actions Max                                 0.351431\n",
      "exploration/Actions Min                                -0.672493\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           964.345\n",
      "exploration/env_infos/final/reward_forward Mean         0.110833\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.110833\n",
      "exploration/env_infos/final/reward_forward Min          0.110833\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0568483\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0568483\n",
      "exploration/env_infos/initial/reward_forward Min        0.0568483\n",
      "exploration/env_infos/reward_forward Mean              -0.0160165\n",
      "exploration/env_infos/reward_forward Std                0.158587\n",
      "exploration/env_infos/reward_forward Max                0.691423\n",
      "exploration/env_infos/reward_forward Min               -0.932745\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0462819\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0462819\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0462819\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0542423\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0542423\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0542423\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0665761\n",
      "exploration/env_infos/reward_ctrl Std                   0.0323536\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00789334\n",
      "exploration/env_infos/reward_ctrl Min                  -0.388378\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0571115\n",
      "exploration/env_infos/final/torso_velocity Std          0.0479304\n",
      "exploration/env_infos/final/torso_velocity Max          0.110833\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00554692\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.14477\n",
      "exploration/env_infos/initial/torso_velocity Std        0.228142\n",
      "exploration/env_infos/initial/torso_velocity Max        0.457572\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0801107\n",
      "exploration/env_infos/torso_velocity Mean              -0.0134947\n",
      "exploration/env_infos/torso_velocity Std                0.178778\n",
      "exploration/env_infos/torso_velocity Max                0.691423\n",
      "exploration/env_infos/torso_velocity Min               -1.71645\n",
      "evaluation/num steps total                         775000\n",
      "evaluation/num paths total                            775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.963562\n",
      "evaluation/Rewards Std                                  0.0324637\n",
      "evaluation/Rewards Max                                  2.12266\n",
      "evaluation/Rewards Min                                  0.578034\n",
      "evaluation/Returns Mean                               963.562\n",
      "evaluation/Returns Std                                  8.82389\n",
      "evaluation/Returns Max                                984.711\n",
      "evaluation/Returns Min                                950.486\n",
      "evaluation/Actions Mean                                -0.047131\n",
      "evaluation/Actions Std                                  0.0858053\n",
      "evaluation/Actions Max                                  0.590831\n",
      "evaluation/Actions Min                                 -0.698401\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            963.562\n",
      "evaluation/env_infos/final/reward_forward Mean          1.08755e-05\n",
      "evaluation/env_infos/final/reward_forward Std           5.21325e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000266266\n",
      "evaluation/env_infos/final/reward_forward Min          -5.45551e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0263604\n",
      "evaluation/env_infos/initial/reward_forward Std         0.112269\n",
      "evaluation/env_infos/initial/reward_forward Max         0.219028\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.177961\n",
      "evaluation/env_infos/reward_forward Mean                0.001405\n",
      "evaluation/env_infos/reward_forward Std                 0.0601658\n",
      "evaluation/env_infos/reward_forward Max                 1.21861\n",
      "evaluation/env_infos/reward_forward Min                -1.63318\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0373801\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00773477\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0182161\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0489147\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0247282\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00586406\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0187171\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0423575\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0383355\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0150258\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0127044\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.57417\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.04489e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           5.66436e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000408846\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.69601e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.141809\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.235848\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.633816\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.329959\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00176133\n",
      "evaluation/env_infos/torso_velocity Std                 0.0740587\n",
      "evaluation/env_infos/torso_velocity Max                 1.21861\n",
      "evaluation/env_infos/torso_velocity Min                -1.78015\n",
      "time/data storing (s)                                   0.014514\n",
      "time/evaluation sampling (s)                           46.0819\n",
      "time/exploration sampling (s)                           1.96825\n",
      "time/logging (s)                                        0.280573\n",
      "time/saving (s)                                         0.0272557\n",
      "time/training (s)                                       3.97744\n",
      "time/epoch (s)                                         52.35\n",
      "time/total (s)                                       1853.41\n",
      "Epoch                                                  30\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:42:06.867217 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 31 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  33000\n",
      "trainer/QF1 Loss                                        0.167407\n",
      "trainer/QF2 Loss                                        0.195953\n",
      "trainer/Policy Loss                                    -4.33295\n",
      "trainer/Q1 Predictions Mean                            11.8307\n",
      "trainer/Q1 Predictions Std                              0.96064\n",
      "trainer/Q1 Predictions Max                             14.1788\n",
      "trainer/Q1 Predictions Min                              6.59622\n",
      "trainer/Q2 Predictions Mean                            11.8158\n",
      "trainer/Q2 Predictions Std                              0.976719\n",
      "trainer/Q2 Predictions Max                             13.5305\n",
      "trainer/Q2 Predictions Min                              6.6451\n",
      "trainer/Q Targets Mean                                 12.0579\n",
      "trainer/Q Targets Std                                   0.925176\n",
      "trainer/Q Targets Max                                  13.9708\n",
      "trainer/Q Targets Min                                   7.249\n",
      "trainer/Log Pis Mean                                    7.79748\n",
      "trainer/Log Pis Std                                     2.2593\n",
      "trainer/Log Pis Max                                    13.7248\n",
      "trainer/Log Pis Min                                     0.503917\n",
      "trainer/Policy mu Mean                                 -0.0227594\n",
      "trainer/Policy mu Std                                   0.140888\n",
      "trainer/Policy mu Max                                   1.25682\n",
      "trainer/Policy mu Min                                  -0.938896\n",
      "trainer/Policy log std Mean                            -2.35658\n",
      "trainer/Policy log std Std                              0.195066\n",
      "trainer/Policy log std Max                             -1.60904\n",
      "trainer/Policy log std Min                             -3.07894\n",
      "trainer/Alpha                                           0.0130204\n",
      "trainer/Alpha Loss                                     -0.87919\n",
      "exploration/num steps total                         33000\n",
      "exploration/num paths total                            85\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.952111\n",
      "exploration/Rewards Std                                 0.0485753\n",
      "exploration/Rewards Max                                 1.32581\n",
      "exploration/Rewards Min                                 0.743543\n",
      "exploration/Returns Mean                              952.111\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               952.111\n",
      "exploration/Returns Min                               952.111\n",
      "exploration/Actions Mean                               -0.0561997\n",
      "exploration/Actions Std                                 0.107059\n",
      "exploration/Actions Max                                 0.561077\n",
      "exploration/Actions Min                                -0.481844\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           952.111\n",
      "exploration/env_infos/final/reward_forward Mean        -0.161139\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.161139\n",
      "exploration/env_infos/final/reward_forward Min         -0.161139\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.263906\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.263906\n",
      "exploration/env_infos/initial/reward_forward Min       -0.263906\n",
      "exploration/env_infos/reward_forward Mean               0.0174974\n",
      "exploration/env_infos/reward_forward Std                0.10585\n",
      "exploration/env_infos/reward_forward Max                0.83556\n",
      "exploration/env_infos/reward_forward Min               -0.48864\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0413895\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0413895\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0413895\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0193114\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0193114\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0193114\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0584798\n",
      "exploration/env_infos/reward_ctrl Std                   0.0288467\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00810636\n",
      "exploration/env_infos/reward_ctrl Min                  -0.338153\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0450847\n",
      "exploration/env_infos/final/torso_velocity Std          0.0962161\n",
      "exploration/env_infos/final/torso_velocity Max          0.0744636\n",
      "exploration/env_infos/final/torso_velocity Min         -0.161139\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0805873\n",
      "exploration/env_infos/initial/torso_velocity Std        0.244936\n",
      "exploration/env_infos/initial/torso_velocity Max        0.284193\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.263906\n",
      "exploration/env_infos/torso_velocity Mean               0.00345982\n",
      "exploration/env_infos/torso_velocity Std                0.121142\n",
      "exploration/env_infos/torso_velocity Max                0.83556\n",
      "exploration/env_infos/torso_velocity Min               -1.11281\n",
      "evaluation/num steps total                         800000\n",
      "evaluation/num paths total                            800\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.964878\n",
      "evaluation/Rewards Std                                  0.0389827\n",
      "evaluation/Rewards Max                                  2.07229\n",
      "evaluation/Rewards Min                                  0.464239\n",
      "evaluation/Returns Mean                               964.878\n",
      "evaluation/Returns Std                                 24.0758\n",
      "evaluation/Returns Max                                989.711\n",
      "evaluation/Returns Min                                917.675\n",
      "evaluation/Actions Mean                                -0.0393924\n",
      "evaluation/Actions Std                                  0.0868601\n",
      "evaluation/Actions Max                                  0.778341\n",
      "evaluation/Actions Min                                 -0.530666\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            964.878\n",
      "evaluation/env_infos/final/reward_forward Mean          3.59931e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.97216e-05\n",
      "evaluation/env_infos/final/reward_forward Max           9.97073e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -9.79623e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0253856\n",
      "evaluation/env_infos/initial/reward_forward Std         0.138616\n",
      "evaluation/env_infos/initial/reward_forward Max         0.334437\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.215105\n",
      "evaluation/env_infos/reward_forward Mean                0.00156142\n",
      "evaluation/env_infos/reward_forward Std                 0.0683567\n",
      "evaluation/env_infos/reward_forward Max                 1.50068\n",
      "evaluation/env_infos/reward_forward Min                -0.834131\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0343773\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0234573\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0130905\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0836221\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0231516\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0100127\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0136461\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0517116\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0363858\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0285255\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00665517\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.535761\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          8.55454e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           1.63867e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           9.97073e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.43526e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.130281\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.217012\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.631027\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.215105\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000544756\n",
      "evaluation/env_infos/torso_velocity Std                 0.0753549\n",
      "evaluation/env_infos/torso_velocity Max                 1.62314\n",
      "evaluation/env_infos/torso_velocity Min                -1.79579\n",
      "time/data storing (s)                                   0.0145327\n",
      "time/evaluation sampling (s)                           44.6433\n",
      "time/exploration sampling (s)                           2.13266\n",
      "time/logging (s)                                        0.281037\n",
      "time/saving (s)                                         0.0285134\n",
      "time/training (s)                                       4.12673\n",
      "time/epoch (s)                                         51.2267\n",
      "time/total (s)                                       1905.03\n",
      "Epoch                                                  31\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:42:58.449173 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 32 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  34000\n",
      "trainer/QF1 Loss                                        0.130974\n",
      "trainer/QF2 Loss                                        0.178848\n",
      "trainer/Policy Loss                                    -4.50879\n",
      "trainer/Q1 Predictions Mean                            12.1923\n",
      "trainer/Q1 Predictions Std                              1.01444\n",
      "trainer/Q1 Predictions Max                             14.5504\n",
      "trainer/Q1 Predictions Min                              6.98494\n",
      "trainer/Q2 Predictions Mean                            12.1228\n",
      "trainer/Q2 Predictions Std                              1.00456\n",
      "trainer/Q2 Predictions Max                             14.0431\n",
      "trainer/Q2 Predictions Min                              7.14646\n",
      "trainer/Q Targets Mean                                 12.2878\n",
      "trainer/Q Targets Std                                   1.04741\n",
      "trainer/Q Targets Max                                  14.9628\n",
      "trainer/Q Targets Min                                   6.60507\n",
      "trainer/Log Pis Mean                                    8.04311\n",
      "trainer/Log Pis Std                                     2.72386\n",
      "trainer/Log Pis Max                                    26.5366\n",
      "trainer/Log Pis Min                                    -4.73855\n",
      "trainer/Policy mu Mean                                 -0.0271597\n",
      "trainer/Policy mu Std                                   0.148041\n",
      "trainer/Policy mu Max                                   1.44656\n",
      "trainer/Policy mu Min                                  -1.45269\n",
      "trainer/Policy log std Mean                            -2.38159\n",
      "trainer/Policy log std Std                              0.245321\n",
      "trainer/Policy log std Max                             -1.41268\n",
      "trainer/Policy log std Min                             -4.75089\n",
      "trainer/Alpha                                           0.012917\n",
      "trainer/Alpha Loss                                      0.187508\n",
      "exploration/num steps total                         34000\n",
      "exploration/num paths total                            86\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.935211\n",
      "exploration/Rewards Std                                 0.0473184\n",
      "exploration/Rewards Max                                 1.27528\n",
      "exploration/Rewards Min                                 0.393735\n",
      "exploration/Returns Mean                              935.211\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               935.211\n",
      "exploration/Returns Min                               935.211\n",
      "exploration/Actions Mean                               -0.0412142\n",
      "exploration/Actions Std                                 0.122557\n",
      "exploration/Actions Max                                 0.758694\n",
      "exploration/Actions Min                                -0.544451\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           935.211\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0699018\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0699018\n",
      "exploration/env_infos/final/reward_forward Min         -0.0699018\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0621628\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0621628\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0621628\n",
      "exploration/env_infos/reward_forward Mean               0.0444327\n",
      "exploration/env_infos/reward_forward Std                0.263162\n",
      "exploration/env_infos/reward_forward Max                0.994222\n",
      "exploration/env_infos/reward_forward Min               -0.847132\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0591075\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0591075\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0591075\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0242826\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0242826\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0242826\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0668757\n",
      "exploration/env_infos/reward_ctrl Std                   0.044198\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00760306\n",
      "exploration/env_infos/reward_ctrl Min                  -0.606265\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0759673\n",
      "exploration/env_infos/final/torso_velocity Std          0.0677997\n",
      "exploration/env_infos/final/torso_velocity Max          0.00387096\n",
      "exploration/env_infos/final/torso_velocity Min         -0.161871\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.192307\n",
      "exploration/env_infos/initial/torso_velocity Std        0.223012\n",
      "exploration/env_infos/initial/torso_velocity Max        0.480897\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0621628\n",
      "exploration/env_infos/torso_velocity Mean               0.00972225\n",
      "exploration/env_infos/torso_velocity Std                0.185342\n",
      "exploration/env_infos/torso_velocity Max                0.994222\n",
      "exploration/env_infos/torso_velocity Min               -1.31211\n",
      "evaluation/num steps total                         825000\n",
      "evaluation/num paths total                            825\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.946732\n",
      "evaluation/Rewards Std                                  0.0342498\n",
      "evaluation/Rewards Max                                  2.46743\n",
      "evaluation/Rewards Min                                  0.387767\n",
      "evaluation/Returns Mean                               946.732\n",
      "evaluation/Returns Std                                  7.23402\n",
      "evaluation/Returns Max                                958.295\n",
      "evaluation/Returns Min                                923.129\n",
      "evaluation/Actions Mean                                -0.0837739\n",
      "evaluation/Actions Std                                  0.0809333\n",
      "evaluation/Actions Max                                  0.78811\n",
      "evaluation/Actions Min                                 -0.550323\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            946.732\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.28676e-06\n",
      "evaluation/env_infos/final/reward_forward Std           7.19625e-06\n",
      "evaluation/env_infos/final/reward_forward Max           7.58995e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -3.64617e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00463869\n",
      "evaluation/env_infos/initial/reward_forward Std         0.136687\n",
      "evaluation/env_infos/initial/reward_forward Max         0.263638\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.29119\n",
      "evaluation/env_infos/reward_forward Mean                0.00345851\n",
      "evaluation/env_infos/reward_forward Std                 0.0614906\n",
      "evaluation/env_infos/reward_forward Max                 1.36723\n",
      "evaluation/env_infos/reward_forward Min                -0.818683\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0532215\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00672437\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0438078\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0753212\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0324939\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0176798\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0151176\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0740166\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0542731\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0198433\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00510986\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.612233\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.41851e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           4.39683e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           7.25784e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -3.64617e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.149227\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.228164\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.588927\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.304792\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00103572\n",
      "evaluation/env_infos/torso_velocity Std                 0.0673524\n",
      "evaluation/env_infos/torso_velocity Max                 1.40855\n",
      "evaluation/env_infos/torso_velocity Min                -1.84988\n",
      "time/data storing (s)                                   0.0153483\n",
      "time/evaluation sampling (s)                           44.5561\n",
      "time/exploration sampling (s)                           2.04601\n",
      "time/logging (s)                                        0.286842\n",
      "time/saving (s)                                         0.0275931\n",
      "time/training (s)                                       4.2606\n",
      "time/epoch (s)                                         51.1925\n",
      "time/total (s)                                       1956.62\n",
      "Epoch                                                  32\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:43:50.197918 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 33 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  35000\n",
      "trainer/QF1 Loss                                        0.172157\n",
      "trainer/QF2 Loss                                        0.177854\n",
      "trainer/Policy Loss                                    -5.71843\n",
      "trainer/Q1 Predictions Mean                            12.7358\n",
      "trainer/Q1 Predictions Std                              0.904221\n",
      "trainer/Q1 Predictions Max                             14.3746\n",
      "trainer/Q1 Predictions Min                              9.4987\n",
      "trainer/Q2 Predictions Mean                            12.8512\n",
      "trainer/Q2 Predictions Std                              0.908823\n",
      "trainer/Q2 Predictions Max                             14.2905\n",
      "trainer/Q2 Predictions Min                              8.88518\n",
      "trainer/Q Targets Mean                                 12.7534\n",
      "trainer/Q Targets Std                                   0.904732\n",
      "trainer/Q Targets Max                                  14.0192\n",
      "trainer/Q Targets Min                                   9.16454\n",
      "trainer/Log Pis Mean                                    7.33638\n",
      "trainer/Log Pis Std                                     2.1749\n",
      "trainer/Log Pis Max                                    13.523\n",
      "trainer/Log Pis Min                                     0.18201\n",
      "trainer/Policy mu Mean                                  0.0289597\n",
      "trainer/Policy mu Std                                   0.130733\n",
      "trainer/Policy mu Max                                   0.714323\n",
      "trainer/Policy mu Min                                  -0.450401\n",
      "trainer/Policy log std Mean                            -2.30239\n",
      "trainer/Policy log std Std                              0.190684\n",
      "trainer/Policy log std Max                             -1.54753\n",
      "trainer/Policy log std Min                             -2.86706\n",
      "trainer/Alpha                                           0.0128074\n",
      "trainer/Alpha Loss                                     -2.89156\n",
      "exploration/num steps total                         35000\n",
      "exploration/num paths total                            87\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.903174\n",
      "exploration/Rewards Std                                 0.0772093\n",
      "exploration/Rewards Max                                 1.42875\n",
      "exploration/Rewards Min                                 0.357541\n",
      "exploration/Returns Mean                              903.174\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               903.174\n",
      "exploration/Returns Min                               903.174\n",
      "exploration/Actions Mean                                0.0161439\n",
      "exploration/Actions Std                                 0.166239\n",
      "exploration/Actions Max                                 0.760441\n",
      "exploration/Actions Min                                -0.62435\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           903.174\n",
      "exploration/env_infos/final/reward_forward Mean        -0.117944\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.117944\n",
      "exploration/env_infos/final/reward_forward Min         -0.117944\n",
      "exploration/env_infos/initial/reward_forward Mean       0.152859\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.152859\n",
      "exploration/env_infos/initial/reward_forward Min        0.152859\n",
      "exploration/env_infos/reward_forward Mean               0.0276459\n",
      "exploration/env_infos/reward_forward Std                0.187504\n",
      "exploration/env_infos/reward_forward Max                1.15669\n",
      "exploration/env_infos/reward_forward Min               -0.656201\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.217669\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.217669\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.217669\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.151095\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.151095\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.151095\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.111585\n",
      "exploration/env_infos/reward_ctrl Std                   0.0531963\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0155964\n",
      "exploration/env_infos/reward_ctrl Min                  -0.642459\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0521348\n",
      "exploration/env_infos/final/torso_velocity Std          0.048428\n",
      "exploration/env_infos/final/torso_velocity Max         -0.00280542\n",
      "exploration/env_infos/final/torso_velocity Min         -0.117944\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.267436\n",
      "exploration/env_infos/initial/torso_velocity Std        0.145047\n",
      "exploration/env_infos/initial/torso_velocity Max        0.472075\n",
      "exploration/env_infos/initial/torso_velocity Min        0.152859\n",
      "exploration/env_infos/torso_velocity Mean               0.0149906\n",
      "exploration/env_infos/torso_velocity Std                0.151904\n",
      "exploration/env_infos/torso_velocity Max                1.15669\n",
      "exploration/env_infos/torso_velocity Min               -0.975851\n",
      "evaluation/num steps total                         850000\n",
      "evaluation/num paths total                            850\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.967646\n",
      "evaluation/Rewards Std                                  0.032384\n",
      "evaluation/Rewards Max                                  1.99231\n",
      "evaluation/Rewards Min                                  0.22863\n",
      "evaluation/Returns Mean                               967.646\n",
      "evaluation/Returns Std                                 11.3016\n",
      "evaluation/Returns Max                                982.283\n",
      "evaluation/Returns Min                                944.204\n",
      "evaluation/Actions Mean                                 0.00761945\n",
      "evaluation/Actions Std                                  0.0906078\n",
      "evaluation/Actions Max                                  0.747723\n",
      "evaluation/Actions Min                                 -0.478665\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            967.646\n",
      "evaluation/env_infos/final/reward_forward Mean          3.28622e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.54733e-05\n",
      "evaluation/env_infos/final/reward_forward Max           7.90784e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -5.44922e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00848205\n",
      "evaluation/env_infos/initial/reward_forward Std         0.136904\n",
      "evaluation/env_infos/initial/reward_forward Max         0.308842\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.187258\n",
      "evaluation/env_infos/reward_forward Mean                0.00395639\n",
      "evaluation/env_infos/reward_forward Std                 0.0631933\n",
      "evaluation/env_infos/reward_forward Max                 1.93358\n",
      "evaluation/env_infos/reward_forward Min                -0.635564\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0314506\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0112562\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0161571\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0546302\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0287853\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0118229\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0116328\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0543115\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0330713\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0260364\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00886108\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.77137\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.77821e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000135324\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0011766\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.11048e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.143252\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.218082\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.579881\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.272149\n",
      "evaluation/env_infos/torso_velocity Mean                0.000286814\n",
      "evaluation/env_infos/torso_velocity Std                 0.0635895\n",
      "evaluation/env_infos/torso_velocity Max                 1.93358\n",
      "evaluation/env_infos/torso_velocity Min                -1.85115\n",
      "time/data storing (s)                                   0.0148359\n",
      "time/evaluation sampling (s)                           44.8899\n",
      "time/exploration sampling (s)                           2.04024\n",
      "time/logging (s)                                        0.276437\n",
      "time/saving (s)                                         0.025704\n",
      "time/training (s)                                       4.07891\n",
      "time/epoch (s)                                         51.326\n",
      "time/total (s)                                       2008.36\n",
      "Epoch                                                  33\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:44:41.080512 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 34 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  36000\n",
      "trainer/QF1 Loss                                        0.152725\n",
      "trainer/QF2 Loss                                        0.443678\n",
      "trainer/Policy Loss                                    -4.95497\n",
      "trainer/Q1 Predictions Mean                            13.0627\n",
      "trainer/Q1 Predictions Std                              1.46462\n",
      "trainer/Q1 Predictions Max                             15.4345\n",
      "trainer/Q1 Predictions Min                             -3.52862\n",
      "trainer/Q2 Predictions Mean                            13.0651\n",
      "trainer/Q2 Predictions Std                              1.06774\n",
      "trainer/Q2 Predictions Max                             15.6352\n",
      "trainer/Q2 Predictions Min                              8.4927\n",
      "trainer/Q Targets Mean                                 12.9726\n",
      "trainer/Q Targets Std                                   1.34578\n",
      "trainer/Q Targets Max                                  16.4611\n",
      "trainer/Q Targets Min                                  -0.777987\n",
      "trainer/Log Pis Mean                                    8.43007\n",
      "trainer/Log Pis Std                                     2.36215\n",
      "trainer/Log Pis Max                                    14.5488\n",
      "trainer/Log Pis Min                                    -0.266312\n",
      "trainer/Policy mu Mean                                 -0.056592\n",
      "trainer/Policy mu Std                                   0.117234\n",
      "trainer/Policy mu Max                                   0.675396\n",
      "trainer/Policy mu Min                                  -0.876227\n",
      "trainer/Policy log std Mean                            -2.42584\n",
      "trainer/Policy log std Std                              0.220296\n",
      "trainer/Policy log std Max                             -1.7696\n",
      "trainer/Policy log std Min                             -3.04245\n",
      "trainer/Alpha                                           0.0126485\n",
      "trainer/Alpha Loss                                      1.87964\n",
      "exploration/num steps total                         36000\n",
      "exploration/num paths total                            88\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.943892\n",
      "exploration/Rewards Std                                 0.0517604\n",
      "exploration/Rewards Max                                 2.29125\n",
      "exploration/Rewards Min                                 0.796123\n",
      "exploration/Returns Mean                              943.892\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               943.892\n",
      "exploration/Returns Min                               943.892\n",
      "exploration/Actions Mean                               -0.0835658\n",
      "exploration/Actions Std                                 0.090245\n",
      "exploration/Actions Max                                 0.395509\n",
      "exploration/Actions Min                                -0.462972\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           943.892\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00643909\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.00643909\n",
      "exploration/env_infos/final/reward_forward Min         -0.00643909\n",
      "exploration/env_infos/initial/reward_forward Mean       0.135087\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.135087\n",
      "exploration/env_infos/initial/reward_forward Min        0.135087\n",
      "exploration/env_infos/reward_forward Mean               0.00355137\n",
      "exploration/env_infos/reward_forward Std                0.113782\n",
      "exploration/env_infos/reward_forward Max                0.938766\n",
      "exploration/env_infos/reward_forward Min               -0.737768\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.103322\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.103322\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.103322\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0732371\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0732371\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0732371\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0605096\n",
      "exploration/env_infos/reward_ctrl Std                   0.0266941\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00496857\n",
      "exploration/env_infos/reward_ctrl Min                  -0.263818\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00165747\n",
      "exploration/env_infos/final/torso_velocity Std          0.00341602\n",
      "exploration/env_infos/final/torso_velocity Max          0.00132986\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00643909\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.168636\n",
      "exploration/env_infos/initial/torso_velocity Std        0.143768\n",
      "exploration/env_infos/initial/torso_velocity Max        0.359075\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0117451\n",
      "exploration/env_infos/torso_velocity Mean               0.00576173\n",
      "exploration/env_infos/torso_velocity Std                0.124214\n",
      "exploration/env_infos/torso_velocity Max                1.54756\n",
      "exploration/env_infos/torso_velocity Min               -1.52101\n",
      "evaluation/num steps total                         875000\n",
      "evaluation/num paths total                            875\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.956995\n",
      "evaluation/Rewards Std                                  0.0222817\n",
      "evaluation/Rewards Max                                  2.1984\n",
      "evaluation/Rewards Min                                  0.622861\n",
      "evaluation/Returns Mean                               956.995\n",
      "evaluation/Returns Std                                  9.24984\n",
      "evaluation/Returns Max                                967.548\n",
      "evaluation/Returns Min                                926.811\n",
      "evaluation/Actions Mean                                -0.0786175\n",
      "evaluation/Actions Std                                  0.068893\n",
      "evaluation/Actions Max                                  0.408023\n",
      "evaluation/Actions Min                                 -0.516489\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            956.995\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.36774e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.02423e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.79883e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.79627e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00328463\n",
      "evaluation/env_infos/initial/reward_forward Std         0.119718\n",
      "evaluation/env_infos/initial/reward_forward Max         0.305944\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.232554\n",
      "evaluation/env_infos/reward_forward Mean               -0.00136206\n",
      "evaluation/env_infos/reward_forward Std                 0.0492427\n",
      "evaluation/env_infos/reward_forward Max                 1.31175\n",
      "evaluation/env_infos/reward_forward Min                -1.19653\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0432619\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00905096\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0342027\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0733379\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0300024\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0105447\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0164932\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0553058\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0437079\n",
      "evaluation/env_infos/reward_ctrl Std                    0.010776\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0111072\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.377139\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -7.98587e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.16578e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           5.79883e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.87414e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.137473\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.237647\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.558651\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.306749\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00197781\n",
      "evaluation/env_infos/torso_velocity Std                 0.0508226\n",
      "evaluation/env_infos/torso_velocity Max                 1.31175\n",
      "evaluation/env_infos/torso_velocity Min                -1.76766\n",
      "time/data storing (s)                                   0.0159354\n",
      "time/evaluation sampling (s)                           43.8452\n",
      "time/exploration sampling (s)                           2.01057\n",
      "time/logging (s)                                        0.286715\n",
      "time/saving (s)                                         0.0272777\n",
      "time/training (s)                                       4.24364\n",
      "time/epoch (s)                                         50.4293\n",
      "time/total (s)                                       2059.25\n",
      "Epoch                                                  34\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:45:32.228897 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 35 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  37000\n",
      "trainer/QF1 Loss                                        0.159755\n",
      "trainer/QF2 Loss                                        0.173363\n",
      "trainer/Policy Loss                                    -5.30253\n",
      "trainer/Q1 Predictions Mean                            13.3762\n",
      "trainer/Q1 Predictions Std                              1.01854\n",
      "trainer/Q1 Predictions Max                             17.6302\n",
      "trainer/Q1 Predictions Min                              9.13696\n",
      "trainer/Q2 Predictions Mean                            13.4795\n",
      "trainer/Q2 Predictions Std                              1.07094\n",
      "trainer/Q2 Predictions Max                             16.9887\n",
      "trainer/Q2 Predictions Min                              9.98537\n",
      "trainer/Q Targets Mean                                 13.416\n",
      "trainer/Q Targets Std                                   0.999789\n",
      "trainer/Q Targets Max                                  18.4409\n",
      "trainer/Q Targets Min                                  10.4807\n",
      "trainer/Log Pis Mean                                    8.39884\n",
      "trainer/Log Pis Std                                     2.41574\n",
      "trainer/Log Pis Max                                    16.2584\n",
      "trainer/Log Pis Min                                     0.413502\n",
      "trainer/Policy mu Mean                                  0.000412283\n",
      "trainer/Policy mu Std                                   0.153468\n",
      "trainer/Policy mu Max                                   0.747003\n",
      "trainer/Policy mu Min                                  -0.585196\n",
      "trainer/Policy log std Mean                            -2.43086\n",
      "trainer/Policy log std Std                              0.212734\n",
      "trainer/Policy log std Max                             -1.58456\n",
      "trainer/Policy log std Min                             -3.36759\n",
      "trainer/Alpha                                           0.0126328\n",
      "trainer/Alpha Loss                                      1.74354\n",
      "exploration/num steps total                         37000\n",
      "exploration/num paths total                            89\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.960762\n",
      "exploration/Rewards Std                                 0.115393\n",
      "exploration/Rewards Max                                 1.83946\n",
      "exploration/Rewards Min                                 0.6266\n",
      "exploration/Returns Mean                              960.762\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               960.762\n",
      "exploration/Returns Min                               960.762\n",
      "exploration/Actions Mean                                0.00499187\n",
      "exploration/Actions Std                                 0.132343\n",
      "exploration/Actions Max                                 0.510466\n",
      "exploration/Actions Min                                -0.650183\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           960.762\n",
      "exploration/env_infos/final/reward_forward Mean         0.0854219\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0854219\n",
      "exploration/env_infos/final/reward_forward Min          0.0854219\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0930654\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0930654\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0930654\n",
      "exploration/env_infos/reward_forward Mean              -0.00322789\n",
      "exploration/env_infos/reward_forward Std                0.134996\n",
      "exploration/env_infos/reward_forward Max                0.892456\n",
      "exploration/env_infos/reward_forward Min               -0.650828\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.032393\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.032393\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.032393\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0615911\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0615911\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0615911\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0701582\n",
      "exploration/env_infos/reward_ctrl Std                   0.0348699\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0110142\n",
      "exploration/env_infos/reward_ctrl Min                  -0.3734\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0120062\n",
      "exploration/env_infos/final/torso_velocity Std          0.0936626\n",
      "exploration/env_infos/final/torso_velocity Max          0.0854219\n",
      "exploration/env_infos/final/torso_velocity Min         -0.120183\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0383029\n",
      "exploration/env_infos/initial/torso_velocity Std        0.221002\n",
      "exploration/env_infos/initial/torso_velocity Max        0.349587\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.141613\n",
      "exploration/env_infos/torso_velocity Mean               0.00926931\n",
      "exploration/env_infos/torso_velocity Std                0.214374\n",
      "exploration/env_infos/torso_velocity Max                1.54386\n",
      "exploration/env_infos/torso_velocity Min               -1.81863\n",
      "evaluation/num steps total                         900000\n",
      "evaluation/num paths total                            900\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.945541\n",
      "evaluation/Rewards Std                                  0.0307931\n",
      "evaluation/Rewards Max                                  2.35904\n",
      "evaluation/Rewards Min                                  0.652436\n",
      "evaluation/Returns Mean                               945.541\n",
      "evaluation/Returns Std                                 13.4972\n",
      "evaluation/Returns Max                                969.633\n",
      "evaluation/Returns Min                                920.312\n",
      "evaluation/Actions Mean                                -0.00395919\n",
      "evaluation/Actions Std                                  0.117859\n",
      "evaluation/Actions Max                                  0.49877\n",
      "evaluation/Actions Min                                 -0.53499\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            945.541\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.7979e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.97401e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.79268e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.49265e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0215472\n",
      "evaluation/env_infos/initial/reward_forward Std         0.11605\n",
      "evaluation/env_infos/initial/reward_forward Max         0.199406\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.191266\n",
      "evaluation/env_infos/reward_forward Mean               -0.00127405\n",
      "evaluation/env_infos/reward_forward Std                 0.0503813\n",
      "evaluation/env_infos/reward_forward Max                 1.26255\n",
      "evaluation/env_infos/reward_forward Min                -1.3143\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0551929\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0143078\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0311892\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0798752\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0544327\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00971643\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0398321\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0795662\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0556256\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0160576\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.017435\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.347564\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.89213e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           1.4408e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           1.20022e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.49265e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152752\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.249408\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.690482\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.296705\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00150261\n",
      "evaluation/env_infos/torso_velocity Std                 0.0657016\n",
      "evaluation/env_infos/torso_velocity Max                 1.89519\n",
      "evaluation/env_infos/torso_velocity Min                -1.88372\n",
      "time/data storing (s)                                   0.0149176\n",
      "time/evaluation sampling (s)                           44.2737\n",
      "time/exploration sampling (s)                           2.00232\n",
      "time/logging (s)                                        0.279004\n",
      "time/saving (s)                                         0.0267519\n",
      "time/training (s)                                       4.12511\n",
      "time/epoch (s)                                         50.7218\n",
      "time/total (s)                                       2110.39\n",
      "Epoch                                                  35\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:46:23.535395 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 36 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  38000\n",
      "trainer/QF1 Loss                                        0.246114\n",
      "trainer/QF2 Loss                                        0.218408\n",
      "trainer/Policy Loss                                    -5.58576\n",
      "trainer/Q1 Predictions Mean                            13.4692\n",
      "trainer/Q1 Predictions Std                              1.32728\n",
      "trainer/Q1 Predictions Max                             14.9749\n",
      "trainer/Q1 Predictions Min                              2.96297\n",
      "trainer/Q2 Predictions Mean                            13.3043\n",
      "trainer/Q2 Predictions Std                              1.35655\n",
      "trainer/Q2 Predictions Max                             14.738\n",
      "trainer/Q2 Predictions Min                              1.36985\n",
      "trainer/Q Targets Mean                                 13.4763\n",
      "trainer/Q Targets Std                                   1.42121\n",
      "trainer/Q Targets Max                                  15.3075\n",
      "trainer/Q Targets Min                                  -0.843897\n",
      "trainer/Log Pis Mean                                    8.21201\n",
      "trainer/Log Pis Std                                     2.2985\n",
      "trainer/Log Pis Max                                    13.0058\n",
      "trainer/Log Pis Min                                    -0.607793\n",
      "trainer/Policy mu Mean                                 -0.037728\n",
      "trainer/Policy mu Std                                   0.146058\n",
      "trainer/Policy mu Max                                   1.2274\n",
      "trainer/Policy mu Min                                  -0.708744\n",
      "trainer/Policy log std Mean                            -2.40579\n",
      "trainer/Policy log std Std                              0.203724\n",
      "trainer/Policy log std Max                             -1.76343\n",
      "trainer/Policy log std Min                             -3.23124\n",
      "trainer/Alpha                                           0.012412\n",
      "trainer/Alpha Loss                                      0.930519\n",
      "exploration/num steps total                         38000\n",
      "exploration/num paths total                            90\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.972155\n",
      "exploration/Rewards Std                                 0.0808709\n",
      "exploration/Rewards Max                                 1.66294\n",
      "exploration/Rewards Min                                 0.78985\n",
      "exploration/Returns Mean                              972.155\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               972.155\n",
      "exploration/Returns Min                               972.155\n",
      "exploration/Actions Mean                               -0.03952\n",
      "exploration/Actions Std                                 0.106276\n",
      "exploration/Actions Max                                 0.367826\n",
      "exploration/Actions Min                                -0.569925\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           972.155\n",
      "exploration/env_infos/final/reward_forward Mean         0.113633\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.113633\n",
      "exploration/env_infos/final/reward_forward Min          0.113633\n",
      "exploration/env_infos/initial/reward_forward Mean       0.209921\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.209921\n",
      "exploration/env_infos/initial/reward_forward Min        0.209921\n",
      "exploration/env_infos/reward_forward Mean              -0.00197524\n",
      "exploration/env_infos/reward_forward Std                0.129472\n",
      "exploration/env_infos/reward_forward Max                0.654286\n",
      "exploration/env_infos/reward_forward Min               -0.42545\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.109797\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.109797\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.109797\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0344467\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0344467\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0344467\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0514259\n",
      "exploration/env_infos/reward_ctrl Std                   0.0262476\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00304031\n",
      "exploration/env_infos/reward_ctrl Min                  -0.268978\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0298949\n",
      "exploration/env_infos/final/torso_velocity Std          0.0598991\n",
      "exploration/env_infos/final/torso_velocity Max          0.113633\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0230534\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.175382\n",
      "exploration/env_infos/initial/torso_velocity Std        0.081164\n",
      "exploration/env_infos/initial/torso_velocity Max        0.252911\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0633146\n",
      "exploration/env_infos/torso_velocity Mean              -3.49545e-05\n",
      "exploration/env_infos/torso_velocity Std                0.149697\n",
      "exploration/env_infos/torso_velocity Max                1.24767\n",
      "exploration/env_infos/torso_velocity Min               -1.66232\n",
      "evaluation/num steps total                         925000\n",
      "evaluation/num paths total                            925\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.94343\n",
      "evaluation/Rewards Std                                  0.0238036\n",
      "evaluation/Rewards Max                                  2.03676\n",
      "evaluation/Rewards Min                                  0.611979\n",
      "evaluation/Returns Mean                               943.43\n",
      "evaluation/Returns Std                                 16.8708\n",
      "evaluation/Returns Max                                974.705\n",
      "evaluation/Returns Min                                923.308\n",
      "evaluation/Actions Mean                                -0.0785155\n",
      "evaluation/Actions Std                                  0.0899326\n",
      "evaluation/Actions Max                                  0.451372\n",
      "evaluation/Actions Min                                 -0.63762\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            943.43\n",
      "evaluation/env_infos/final/reward_forward Mean          4.65711e-06\n",
      "evaluation/env_infos/final/reward_forward Std           2.32411e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000118503\n",
      "evaluation/env_infos/final/reward_forward Min          -7.1004e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.026358\n",
      "evaluation/env_infos/initial/reward_forward Std         0.123565\n",
      "evaluation/env_infos/initial/reward_forward Max         0.233808\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.24473\n",
      "evaluation/env_infos/reward_forward Mean                0.00363416\n",
      "evaluation/env_infos/reward_forward Std                 0.0574825\n",
      "evaluation/env_infos/reward_forward Max                 1.5584\n",
      "evaluation/env_infos/reward_forward Min                -0.843007\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0562383\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0171838\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0251301\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.076321\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0264862\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0156504\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0121664\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0594684\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0570102\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0199553\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0121664\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.388021\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.57371e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           3.55766e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000282111\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.34776e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.138819\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.227269\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.62396\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.24473\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000282909\n",
      "evaluation/env_infos/torso_velocity Std                 0.0573116\n",
      "evaluation/env_infos/torso_velocity Max                 1.5584\n",
      "evaluation/env_infos/torso_velocity Min                -1.85733\n",
      "time/data storing (s)                                   0.0149956\n",
      "time/evaluation sampling (s)                           44.4543\n",
      "time/exploration sampling (s)                           1.969\n",
      "time/logging (s)                                        0.287132\n",
      "time/saving (s)                                         0.0266075\n",
      "time/training (s)                                       4.09347\n",
      "time/epoch (s)                                         50.8455\n",
      "time/total (s)                                       2161.7\n",
      "Epoch                                                  36\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:47:15.098580 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 37 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  39000\n",
      "trainer/QF1 Loss                                        0.143147\n",
      "trainer/QF2 Loss                                        0.137032\n",
      "trainer/Policy Loss                                    -6.93533\n",
      "trainer/Q1 Predictions Mean                            14.0806\n",
      "trainer/Q1 Predictions Std                              1.06667\n",
      "trainer/Q1 Predictions Max                             18.0907\n",
      "trainer/Q1 Predictions Min                             10.3945\n",
      "trainer/Q2 Predictions Mean                            13.9805\n",
      "trainer/Q2 Predictions Std                              1.04477\n",
      "trainer/Q2 Predictions Max                             15.8617\n",
      "trainer/Q2 Predictions Min                             10.8426\n",
      "trainer/Q Targets Mean                                 13.9714\n",
      "trainer/Q Targets Std                                   1.10686\n",
      "trainer/Q Targets Max                                  17.927\n",
      "trainer/Q Targets Min                                  10.3586\n",
      "trainer/Log Pis Mean                                    7.36698\n",
      "trainer/Log Pis Std                                     2.51296\n",
      "trainer/Log Pis Max                                    17.9834\n",
      "trainer/Log Pis Min                                    -0.633686\n",
      "trainer/Policy mu Mean                                  0.0101708\n",
      "trainer/Policy mu Std                                   0.153637\n",
      "trainer/Policy mu Max                                   1.63273\n",
      "trainer/Policy mu Min                                  -1.33781\n",
      "trainer/Policy log std Mean                            -2.31985\n",
      "trainer/Policy log std Std                              0.227434\n",
      "trainer/Policy log std Max                             -1.04826\n",
      "trainer/Policy log std Min                             -3.67345\n",
      "trainer/Alpha                                           0.0123335\n",
      "trainer/Alpha Loss                                     -2.78219\n",
      "exploration/num steps total                         39000\n",
      "exploration/num paths total                            91\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.956965\n",
      "exploration/Rewards Std                                 0.115694\n",
      "exploration/Rewards Max                                 1.83197\n",
      "exploration/Rewards Min                                 0.706131\n",
      "exploration/Returns Mean                              956.965\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               956.965\n",
      "exploration/Returns Min                               956.965\n",
      "exploration/Actions Mean                               -0.0210308\n",
      "exploration/Actions Std                                 0.13339\n",
      "exploration/Actions Max                                 0.568162\n",
      "exploration/Actions Min                                -0.488373\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           956.965\n",
      "exploration/env_infos/final/reward_forward Mean        -0.153605\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.153605\n",
      "exploration/env_infos/final/reward_forward Min         -0.153605\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0754169\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0754169\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0754169\n",
      "exploration/env_infos/reward_forward Mean              -0.0177032\n",
      "exploration/env_infos/reward_forward Std                0.249753\n",
      "exploration/env_infos/reward_forward Max                0.733665\n",
      "exploration/env_infos/reward_forward Min               -1.1703\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0604696\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0604696\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0604696\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.04339\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.04339\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.04339\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0729409\n",
      "exploration/env_infos/reward_ctrl Std                   0.0392872\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00765973\n",
      "exploration/env_infos/reward_ctrl Min                  -0.293869\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00190466\n",
      "exploration/env_infos/final/torso_velocity Std          0.119479\n",
      "exploration/env_infos/final/torso_velocity Max          0.13839\n",
      "exploration/env_infos/final/torso_velocity Min         -0.153605\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.120793\n",
      "exploration/env_infos/initial/torso_velocity Std        0.295607\n",
      "exploration/env_infos/initial/torso_velocity Max        0.538587\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.100791\n",
      "exploration/env_infos/torso_velocity Mean               0.000733149\n",
      "exploration/env_infos/torso_velocity Std                0.207958\n",
      "exploration/env_infos/torso_velocity Max                0.951415\n",
      "exploration/env_infos/torso_velocity Min               -1.59457\n",
      "evaluation/num steps total                         950000\n",
      "evaluation/num paths total                            950\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.966737\n",
      "evaluation/Rewards Std                                  0.0333142\n",
      "evaluation/Rewards Max                                  2.47888\n",
      "evaluation/Rewards Min                                  0.465519\n",
      "evaluation/Returns Mean                               966.737\n",
      "evaluation/Returns Std                                 16.1801\n",
      "evaluation/Returns Max                                991.471\n",
      "evaluation/Returns Min                                939.075\n",
      "evaluation/Actions Mean                                -0.0363826\n",
      "evaluation/Actions Std                                  0.0851329\n",
      "evaluation/Actions Max                                  0.59494\n",
      "evaluation/Actions Min                                 -0.485085\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            966.737\n",
      "evaluation/env_infos/final/reward_forward Mean         -9.98134e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.2786e-07\n",
      "evaluation/env_infos/final/reward_forward Max           6.86919e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.90865e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0333991\n",
      "evaluation/env_infos/initial/reward_forward Std         0.12811\n",
      "evaluation/env_infos/initial/reward_forward Max         0.291973\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.193143\n",
      "evaluation/env_infos/reward_forward Mean                0.00561135\n",
      "evaluation/env_infos/reward_forward Std                 0.070767\n",
      "evaluation/env_infos/reward_forward Max                 1.62905\n",
      "evaluation/env_infos/reward_forward Min                -0.686227\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0332761\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.015805\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0141693\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0611939\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0180086\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0081333\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00780464\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0335418\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0342852\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0231586\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0049007\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.534481\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -9.99511e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           3.26224e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           6.86919e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.90865e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.167854\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.228761\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.681779\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.206194\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000254891\n",
      "evaluation/env_infos/torso_velocity Std                 0.0719907\n",
      "evaluation/env_infos/torso_velocity Max                 1.62905\n",
      "evaluation/env_infos/torso_velocity Min                -2.18106\n",
      "time/data storing (s)                                   0.0144607\n",
      "time/evaluation sampling (s)                           44.9019\n",
      "time/exploration sampling (s)                           1.91811\n",
      "time/logging (s)                                        0.280385\n",
      "time/saving (s)                                         0.0262921\n",
      "time/training (s)                                       3.98442\n",
      "time/epoch (s)                                         51.1256\n",
      "time/total (s)                                       2213.26\n",
      "Epoch                                                  37\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:48:06.274637 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 38 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  40000\n",
      "trainer/QF1 Loss                                        0.131515\n",
      "trainer/QF2 Loss                                        0.166765\n",
      "trainer/Policy Loss                                    -6.53359\n",
      "trainer/Q1 Predictions Mean                            14.35\n",
      "trainer/Q1 Predictions Std                              1.01695\n",
      "trainer/Q1 Predictions Max                             16.1586\n",
      "trainer/Q1 Predictions Min                             10.904\n",
      "trainer/Q2 Predictions Mean                            14.261\n",
      "trainer/Q2 Predictions Std                              1.00038\n",
      "trainer/Q2 Predictions Max                             15.5723\n",
      "trainer/Q2 Predictions Min                             10.9232\n",
      "trainer/Q Targets Mean                                 14.4452\n",
      "trainer/Q Targets Std                                   1.02897\n",
      "trainer/Q Targets Max                                  17.4575\n",
      "trainer/Q Targets Min                                  11.3254\n",
      "trainer/Log Pis Mean                                    8.05245\n",
      "trainer/Log Pis Std                                     2.55494\n",
      "trainer/Log Pis Max                                    15.0125\n",
      "trainer/Log Pis Min                                    -2.44425\n",
      "trainer/Policy mu Mean                                 -0.0343766\n",
      "trainer/Policy mu Std                                   0.131694\n",
      "trainer/Policy mu Max                                   0.610384\n",
      "trainer/Policy mu Min                                  -0.667997\n",
      "trainer/Policy log std Mean                            -2.42247\n",
      "trainer/Policy log std Std                              0.19432\n",
      "trainer/Policy log std Max                             -1.70985\n",
      "trainer/Policy log std Min                             -3.2709\n",
      "trainer/Alpha                                           0.0124118\n",
      "trainer/Alpha Loss                                      0.230214\n",
      "exploration/num steps total                         40000\n",
      "exploration/num paths total                            92\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.937645\n",
      "exploration/Rewards Std                                 0.0546241\n",
      "exploration/Rewards Max                                 1.34418\n",
      "exploration/Rewards Min                                 0.609497\n",
      "exploration/Returns Mean                              937.645\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               937.645\n",
      "exploration/Returns Min                               937.645\n",
      "exploration/Actions Mean                               -0.0262922\n",
      "exploration/Actions Std                                 0.131056\n",
      "exploration/Actions Max                                 0.386779\n",
      "exploration/Actions Min                                -0.494909\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           937.645\n",
      "exploration/env_infos/final/reward_forward Mean        -0.192246\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.192246\n",
      "exploration/env_infos/final/reward_forward Min         -0.192246\n",
      "exploration/env_infos/initial/reward_forward Mean       0.172201\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.172201\n",
      "exploration/env_infos/initial/reward_forward Min        0.172201\n",
      "exploration/env_infos/reward_forward Mean               0.0187449\n",
      "exploration/env_infos/reward_forward Std                0.202726\n",
      "exploration/env_infos/reward_forward Max                0.879595\n",
      "exploration/env_infos/reward_forward Min               -0.953943\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0615066\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0615066\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0615066\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0910069\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0910069\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0910069\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0714683\n",
      "exploration/env_infos/reward_ctrl Std                   0.036507\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00666513\n",
      "exploration/env_infos/reward_ctrl Min                  -0.390503\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.138979\n",
      "exploration/env_infos/final/torso_velocity Std          0.0513561\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0695891\n",
      "exploration/env_infos/final/torso_velocity Min         -0.192246\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.230223\n",
      "exploration/env_infos/initial/torso_velocity Std        0.134706\n",
      "exploration/env_infos/initial/torso_velocity Max        0.416376\n",
      "exploration/env_infos/initial/torso_velocity Min        0.102091\n",
      "exploration/env_infos/torso_velocity Mean               0.00786092\n",
      "exploration/env_infos/torso_velocity Std                0.206978\n",
      "exploration/env_infos/torso_velocity Max                0.879595\n",
      "exploration/env_infos/torso_velocity Min               -1.46571\n",
      "evaluation/num steps total                         975000\n",
      "evaluation/num paths total                            975\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.9564\n",
      "evaluation/Rewards Std                                  0.0278376\n",
      "evaluation/Rewards Max                                  1.96136\n",
      "evaluation/Rewards Min                                  0.632593\n",
      "evaluation/Returns Mean                               956.4\n",
      "evaluation/Returns Std                                 18.6289\n",
      "evaluation/Returns Max                                980.811\n",
      "evaluation/Returns Min                                919.321\n",
      "evaluation/Actions Mean                                -0.0594078\n",
      "evaluation/Actions Std                                  0.0869862\n",
      "evaluation/Actions Max                                  0.411489\n",
      "evaluation/Actions Min                                 -0.561474\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            956.4\n",
      "evaluation/env_infos/final/reward_forward Mean          4.4138e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.22341e-06\n",
      "evaluation/env_infos/final/reward_forward Max           1.10831e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -8.5804e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0312203\n",
      "evaluation/env_infos/initial/reward_forward Std         0.112083\n",
      "evaluation/env_infos/initial/reward_forward Max         0.217964\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.204026\n",
      "evaluation/env_infos/reward_forward Mean                0.00203083\n",
      "evaluation/env_infos/reward_forward Std                 0.0549949\n",
      "evaluation/env_infos/reward_forward Max                 1.39386\n",
      "evaluation/env_infos/reward_forward Min                -1.16438\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0428707\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0193683\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0209075\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0806899\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.02225\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00809341\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0121904\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0435883\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0443836\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0214597\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0121904\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.367407\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.90575e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           1.93688e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           1.10831e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.5804e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.126003\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.239935\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.600787\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.228694\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00120658\n",
      "evaluation/env_infos/torso_velocity Std                 0.0632739\n",
      "evaluation/env_infos/torso_velocity Max                 1.57367\n",
      "evaluation/env_infos/torso_velocity Min                -1.63838\n",
      "time/data storing (s)                                   0.0148585\n",
      "time/evaluation sampling (s)                           44.4392\n",
      "time/exploration sampling (s)                           1.91408\n",
      "time/logging (s)                                        0.284417\n",
      "time/saving (s)                                         0.0285474\n",
      "time/training (s)                                       4.07083\n",
      "time/epoch (s)                                         50.752\n",
      "time/total (s)                                       2264.44\n",
      "Epoch                                                  38\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:48:59.007570 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 39 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 41000\n",
      "trainer/QF1 Loss                                       0.137456\n",
      "trainer/QF2 Loss                                       0.10476\n",
      "trainer/Policy Loss                                   -7.01144\n",
      "trainer/Q1 Predictions Mean                           14.6086\n",
      "trainer/Q1 Predictions Std                             1.09422\n",
      "trainer/Q1 Predictions Max                            15.9776\n",
      "trainer/Q1 Predictions Min                            10.7521\n",
      "trainer/Q2 Predictions Mean                           14.7678\n",
      "trainer/Q2 Predictions Std                             1.08389\n",
      "trainer/Q2 Predictions Max                            15.9128\n",
      "trainer/Q2 Predictions Min                            10.784\n",
      "trainer/Q Targets Mean                                14.774\n",
      "trainer/Q Targets Std                                  1.08766\n",
      "trainer/Q Targets Max                                 16.6378\n",
      "trainer/Q Targets Min                                 11.198\n",
      "trainer/Log Pis Mean                                   7.90829\n",
      "trainer/Log Pis Std                                    2.3411\n",
      "trainer/Log Pis Max                                   12.1897\n",
      "trainer/Log Pis Min                                    0.232089\n",
      "trainer/Policy mu Mean                                -0.0216028\n",
      "trainer/Policy mu Std                                  0.129062\n",
      "trainer/Policy mu Max                                  0.635603\n",
      "trainer/Policy mu Min                                 -0.915323\n",
      "trainer/Policy log std Mean                           -2.3782\n",
      "trainer/Policy log std Std                             0.186183\n",
      "trainer/Policy log std Max                            -1.6597\n",
      "trainer/Policy log std Min                            -2.97242\n",
      "trainer/Alpha                                          0.0124123\n",
      "trainer/Alpha Loss                                    -0.402508\n",
      "exploration/num steps total                        41000\n",
      "exploration/num paths total                           93\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.929681\n",
      "exploration/Rewards Std                                0.0820152\n",
      "exploration/Rewards Max                                1.89333\n",
      "exploration/Rewards Min                                0.690056\n",
      "exploration/Returns Mean                             929.681\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              929.681\n",
      "exploration/Returns Min                              929.681\n",
      "exploration/Actions Mean                               0.0143716\n",
      "exploration/Actions Std                                0.146528\n",
      "exploration/Actions Max                                0.511337\n",
      "exploration/Actions Min                               -0.544306\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          929.681\n",
      "exploration/env_infos/final/reward_forward Mean        0.00193407\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.00193407\n",
      "exploration/env_infos/final/reward_forward Min         0.00193407\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0164128\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0164128\n",
      "exploration/env_infos/initial/reward_forward Min       0.0164128\n",
      "exploration/env_infos/reward_forward Mean             -0.00887785\n",
      "exploration/env_infos/reward_forward Std               0.148405\n",
      "exploration/env_infos/reward_forward Max               0.582478\n",
      "exploration/env_infos/reward_forward Min              -0.773217\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.123622\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.123622\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.123622\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0602431\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0602431\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0602431\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0867081\n",
      "exploration/env_infos/reward_ctrl Std                  0.0385027\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0156759\n",
      "exploration/env_infos/reward_ctrl Min                 -0.309944\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.00228804\n",
      "exploration/env_infos/final/torso_velocity Std         0.00590049\n",
      "exploration/env_infos/final/torso_velocity Max         0.00968511\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00475506\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.157504\n",
      "exploration/env_infos/initial/torso_velocity Std       0.170137\n",
      "exploration/env_infos/initial/torso_velocity Max       0.39684\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0164128\n",
      "exploration/env_infos/torso_velocity Mean              0.00446485\n",
      "exploration/env_infos/torso_velocity Std               0.167314\n",
      "exploration/env_infos/torso_velocity Max               1.22798\n",
      "exploration/env_infos/torso_velocity Min              -1.61629\n",
      "evaluation/num steps total                             1e+06\n",
      "evaluation/num paths total                          1000\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.967541\n",
      "evaluation/Rewards Std                                 0.0221925\n",
      "evaluation/Rewards Max                                 2.48362\n",
      "evaluation/Rewards Min                                 0.685082\n",
      "evaluation/Returns Mean                              967.541\n",
      "evaluation/Returns Std                                 9.49907\n",
      "evaluation/Returns Max                               979.389\n",
      "evaluation/Returns Min                               945.598\n",
      "evaluation/Actions Mean                               -0.0116192\n",
      "evaluation/Actions Std                                 0.0904086\n",
      "evaluation/Actions Max                                 0.455298\n",
      "evaluation/Actions Min                                -0.426968\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           967.541\n",
      "evaluation/env_infos/final/reward_forward Mean        -4.9104e-05\n",
      "evaluation/env_infos/final/reward_forward Std          0.000203329\n",
      "evaluation/env_infos/final/reward_forward Max          9.35787e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00103248\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0364521\n",
      "evaluation/env_infos/initial/reward_forward Std        0.109515\n",
      "evaluation/env_infos/initial/reward_forward Max        0.224652\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.196356\n",
      "evaluation/env_infos/reward_forward Mean               0.00104616\n",
      "evaluation/env_infos/reward_forward Std                0.0633206\n",
      "evaluation/env_infos/reward_forward Max                1.64626\n",
      "evaluation/env_infos/reward_forward Min               -1.56099\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0330875\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0102604\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0213147\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0594195\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0241965\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0107871\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0127578\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0533816\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0332349\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0119415\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0109739\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.314918\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -3.59568e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000199407\n",
      "evaluation/env_infos/final/torso_velocity Max          3.55257e-05\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00140705\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.136566\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.240644\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.579356\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.364404\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00128137\n",
      "evaluation/env_infos/torso_velocity Std                0.0682642\n",
      "evaluation/env_infos/torso_velocity Max                1.64626\n",
      "evaluation/env_infos/torso_velocity Min               -1.71289\n",
      "time/data storing (s)                                  0.015172\n",
      "time/evaluation sampling (s)                          45.8545\n",
      "time/exploration sampling (s)                          2.0027\n",
      "time/logging (s)                                       0.280894\n",
      "time/saving (s)                                        0.0272009\n",
      "time/training (s)                                      4.08193\n",
      "time/epoch (s)                                        52.2624\n",
      "time/total (s)                                      2317.17\n",
      "Epoch                                                 39\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:49:50.402238 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 40 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 42000\n",
      "trainer/QF1 Loss                                       0.241346\n",
      "trainer/QF2 Loss                                       0.231875\n",
      "trainer/Policy Loss                                   -8.09351\n",
      "trainer/Q1 Predictions Mean                           15.0208\n",
      "trainer/Q1 Predictions Std                             1.11223\n",
      "trainer/Q1 Predictions Max                            16.2077\n",
      "trainer/Q1 Predictions Min                             8.63778\n",
      "trainer/Q2 Predictions Mean                           14.9694\n",
      "trainer/Q2 Predictions Std                             1.05597\n",
      "trainer/Q2 Predictions Max                            17.0198\n",
      "trainer/Q2 Predictions Min                             8.87555\n",
      "trainer/Q Targets Mean                                15.1451\n",
      "trainer/Q Targets Std                                  1.03641\n",
      "trainer/Q Targets Max                                 19.9194\n",
      "trainer/Q Targets Min                                 11.1418\n",
      "trainer/Log Pis Mean                                   7.10581\n",
      "trainer/Log Pis Std                                    2.26501\n",
      "trainer/Log Pis Max                                   12.1481\n",
      "trainer/Log Pis Min                                   -2.44249\n",
      "trainer/Policy mu Mean                                -0.043906\n",
      "trainer/Policy mu Std                                  0.126961\n",
      "trainer/Policy mu Max                                  0.606215\n",
      "trainer/Policy mu Min                                 -0.78212\n",
      "trainer/Policy log std Mean                           -2.29633\n",
      "trainer/Policy log std Std                             0.201718\n",
      "trainer/Policy log std Max                            -1.34461\n",
      "trainer/Policy log std Min                            -3.0652\n",
      "trainer/Alpha                                          0.0123422\n",
      "trainer/Alpha Loss                                    -3.92958\n",
      "exploration/num steps total                        42000\n",
      "exploration/num paths total                           94\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.955123\n",
      "exploration/Rewards Std                                0.0627423\n",
      "exploration/Rewards Max                                1.4709\n",
      "exploration/Rewards Min                                0.816865\n",
      "exploration/Returns Mean                             955.123\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              955.123\n",
      "exploration/Returns Min                              955.123\n",
      "exploration/Actions Mean                              -0.0618367\n",
      "exploration/Actions Std                                0.101368\n",
      "exploration/Actions Max                                0.329599\n",
      "exploration/Actions Min                               -0.469556\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          955.123\n",
      "exploration/env_infos/final/reward_forward Mean        0.100426\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.100426\n",
      "exploration/env_infos/final/reward_forward Min         0.100426\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.022014\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.022014\n",
      "exploration/env_infos/initial/reward_forward Min      -0.022014\n",
      "exploration/env_infos/reward_forward Mean              0.0142265\n",
      "exploration/env_infos/reward_forward Std               0.151102\n",
      "exploration/env_infos/reward_forward Max               0.557002\n",
      "exploration/env_infos/reward_forward Min              -1.01714\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.027881\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.027881\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.027881\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0938951\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0938951\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0938951\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0563973\n",
      "exploration/env_infos/reward_ctrl Std                  0.0275912\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00606328\n",
      "exploration/env_infos/reward_ctrl Min                 -0.183135\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0163079\n",
      "exploration/env_infos/final/torso_velocity Std         0.0612357\n",
      "exploration/env_infos/final/torso_velocity Max         0.100426\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0435798\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.13433\n",
      "exploration/env_infos/initial/torso_velocity Std       0.209817\n",
      "exploration/env_infos/initial/torso_velocity Max       0.430911\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.022014\n",
      "exploration/env_infos/torso_velocity Mean              0.00179755\n",
      "exploration/env_infos/torso_velocity Std               0.146281\n",
      "exploration/env_infos/torso_velocity Max               0.732146\n",
      "exploration/env_infos/torso_velocity Min              -1.56629\n",
      "evaluation/num steps total                             1.025e+06\n",
      "evaluation/num paths total                          1025\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.952273\n",
      "evaluation/Rewards Std                                 0.0337854\n",
      "evaluation/Rewards Max                                 2.30451\n",
      "evaluation/Rewards Min                                 0.859909\n",
      "evaluation/Returns Mean                              952.273\n",
      "evaluation/Returns Std                                23.5711\n",
      "evaluation/Returns Max                               976.161\n",
      "evaluation/Returns Min                               903.769\n",
      "evaluation/Actions Mean                               -0.0676234\n",
      "evaluation/Actions Std                                 0.0872016\n",
      "evaluation/Actions Max                                 0.289446\n",
      "evaluation/Actions Min                                -0.459099\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           952.273\n",
      "evaluation/env_infos/final/reward_forward Mean        -2.83644e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.40579e-06\n",
      "evaluation/env_infos/final/reward_forward Max          9.93963e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -1.19186e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0149964\n",
      "evaluation/env_infos/initial/reward_forward Std        0.122012\n",
      "evaluation/env_infos/initial/reward_forward Max        0.2232\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.238686\n",
      "evaluation/env_infos/reward_forward Mean               0.000199453\n",
      "evaluation/env_infos/reward_forward Std                0.0497914\n",
      "evaluation/env_infos/reward_forward Max                1.2852\n",
      "evaluation/env_infos/reward_forward Min               -1.12039\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0483478\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0238519\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0235849\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0964911\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.028692\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00520294\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0211612\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0462275\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0487081\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0237367\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0187647\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.140091\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.33703e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          1.42178e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          9.93963e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.19186e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.145604\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.249737\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.837976\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.240855\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00205925\n",
      "evaluation/env_infos/torso_velocity Std                0.0587539\n",
      "evaluation/env_infos/torso_velocity Max                1.2852\n",
      "evaluation/env_infos/torso_velocity Min               -1.75376\n",
      "time/data storing (s)                                  0.0153553\n",
      "time/evaluation sampling (s)                          44.5968\n",
      "time/exploration sampling (s)                          1.9328\n",
      "time/logging (s)                                       0.282672\n",
      "time/saving (s)                                        0.0266599\n",
      "time/training (s)                                      4.08186\n",
      "time/epoch (s)                                        50.9361\n",
      "time/total (s)                                      2368.56\n",
      "Epoch                                                 40\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:50:42.299174 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 41 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 43000\n",
      "trainer/QF1 Loss                                       0.176163\n",
      "trainer/QF2 Loss                                       0.138515\n",
      "trainer/Policy Loss                                   -7.93709\n",
      "trainer/Q1 Predictions Mean                           15.4521\n",
      "trainer/Q1 Predictions Std                             1.09138\n",
      "trainer/Q1 Predictions Max                            18.9917\n",
      "trainer/Q1 Predictions Min                            11.8983\n",
      "trainer/Q2 Predictions Mean                           15.3046\n",
      "trainer/Q2 Predictions Std                             1.0942\n",
      "trainer/Q2 Predictions Max                            18.0222\n",
      "trainer/Q2 Predictions Min                            11.8724\n",
      "trainer/Q Targets Mean                                15.3437\n",
      "trainer/Q Targets Std                                  1.15301\n",
      "trainer/Q Targets Max                                 19.1495\n",
      "trainer/Q Targets Min                                 10.3521\n",
      "trainer/Log Pis Mean                                   7.64308\n",
      "trainer/Log Pis Std                                    2.40867\n",
      "trainer/Log Pis Max                                   12.6373\n",
      "trainer/Log Pis Min                                   -4.18187\n",
      "trainer/Policy mu Mean                                -0.0417388\n",
      "trainer/Policy mu Std                                  0.136244\n",
      "trainer/Policy mu Max                                  0.839768\n",
      "trainer/Policy mu Min                                 -1.2007\n",
      "trainer/Policy log std Mean                           -2.35029\n",
      "trainer/Policy log std Std                             0.211793\n",
      "trainer/Policy log std Max                            -1.08728\n",
      "trainer/Policy log std Min                            -2.98621\n",
      "trainer/Alpha                                          0.0121994\n",
      "trainer/Alpha Loss                                    -1.57269\n",
      "exploration/num steps total                        43000\n",
      "exploration/num paths total                           95\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.936462\n",
      "exploration/Rewards Std                                0.0736788\n",
      "exploration/Rewards Max                                1.82646\n",
      "exploration/Rewards Min                                0.725686\n",
      "exploration/Returns Mean                             936.462\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              936.462\n",
      "exploration/Returns Min                              936.462\n",
      "exploration/Actions Mean                              -0.0557243\n",
      "exploration/Actions Std                                0.1271\n",
      "exploration/Actions Max                                0.339883\n",
      "exploration/Actions Min                               -0.463866\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          936.462\n",
      "exploration/env_infos/final/reward_forward Mean        0.0182091\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0182091\n",
      "exploration/env_infos/final/reward_forward Min         0.0182091\n",
      "exploration/env_infos/initial/reward_forward Mean      0.061919\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.061919\n",
      "exploration/env_infos/initial/reward_forward Min       0.061919\n",
      "exploration/env_infos/reward_forward Mean             -0.0148766\n",
      "exploration/env_infos/reward_forward Std               0.235434\n",
      "exploration/env_infos/reward_forward Max               1.017\n",
      "exploration/env_infos/reward_forward Min              -0.903942\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0518452\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0518452\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0518452\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0237086\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0237086\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0237086\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0770382\n",
      "exploration/env_infos/reward_ctrl Std                  0.0367105\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0082767\n",
      "exploration/env_infos/reward_ctrl Min                 -0.274314\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.105688\n",
      "exploration/env_infos/final/torso_velocity Std         0.0917777\n",
      "exploration/env_infos/final/torso_velocity Max         0.0182091\n",
      "exploration/env_infos/final/torso_velocity Min        -0.201131\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.222999\n",
      "exploration/env_infos/initial/torso_velocity Std       0.184371\n",
      "exploration/env_infos/initial/torso_velocity Max       0.481103\n",
      "exploration/env_infos/initial/torso_velocity Min       0.061919\n",
      "exploration/env_infos/torso_velocity Mean             -0.00696112\n",
      "exploration/env_infos/torso_velocity Std               0.222053\n",
      "exploration/env_infos/torso_velocity Max               1.16897\n",
      "exploration/env_infos/torso_velocity Min              -0.981893\n",
      "evaluation/num steps total                             1.05e+06\n",
      "evaluation/num paths total                          1050\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.950325\n",
      "evaluation/Rewards Std                                 0.0182831\n",
      "evaluation/Rewards Max                                 1.67222\n",
      "evaluation/Rewards Min                                 0.626773\n",
      "evaluation/Returns Mean                              950.325\n",
      "evaluation/Returns Std                                10.8876\n",
      "evaluation/Returns Max                               962.098\n",
      "evaluation/Returns Min                               925.745\n",
      "evaluation/Actions Mean                               -0.0618238\n",
      "evaluation/Actions Std                                 0.0931404\n",
      "evaluation/Actions Max                                 0.41894\n",
      "evaluation/Actions Min                                -0.517067\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           950.325\n",
      "evaluation/env_infos/final/reward_forward Mean         2.93504e-06\n",
      "evaluation/env_infos/final/reward_forward Std          1.45911e-05\n",
      "evaluation/env_infos/final/reward_forward Max          7.43844e-05\n",
      "evaluation/env_infos/final/reward_forward Min         -7.90241e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00694895\n",
      "evaluation/env_infos/initial/reward_forward Std        0.122198\n",
      "evaluation/env_infos/initial/reward_forward Max        0.228078\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.208793\n",
      "evaluation/env_infos/reward_forward Mean               0.00416803\n",
      "evaluation/env_infos/reward_forward Std                0.0588157\n",
      "evaluation/env_infos/reward_forward Max                1.41337\n",
      "evaluation/env_infos/reward_forward Min               -0.914726\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0493284\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0108903\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0368614\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0741843\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0228923\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00601633\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0135408\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0339539\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0499893\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0150106\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00915627\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.373227\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         3.42028e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          1.78792e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000124873\n",
      "evaluation/env_infos/final/torso_velocity Min         -7.90241e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.139355\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.253693\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.698914\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.376604\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000745209\n",
      "evaluation/env_infos/torso_velocity Std                0.061578\n",
      "evaluation/env_infos/torso_velocity Max                1.44737\n",
      "evaluation/env_infos/torso_velocity Min               -2.0737\n",
      "time/data storing (s)                                  0.0147766\n",
      "time/evaluation sampling (s)                          44.938\n",
      "time/exploration sampling (s)                          1.97384\n",
      "time/logging (s)                                       0.282429\n",
      "time/saving (s)                                        0.0266979\n",
      "time/training (s)                                      4.10312\n",
      "time/epoch (s)                                        51.3389\n",
      "time/total (s)                                      2420.46\n",
      "Epoch                                                 41\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:51:34.217740 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 42 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 44000\n",
      "trainer/QF1 Loss                                       0.120331\n",
      "trainer/QF2 Loss                                       0.0953471\n",
      "trainer/Policy Loss                                   -7.66617\n",
      "trainer/Q1 Predictions Mean                           15.5536\n",
      "trainer/Q1 Predictions Std                             1.06\n",
      "trainer/Q1 Predictions Max                            17.0619\n",
      "trainer/Q1 Predictions Min                            11.6479\n",
      "trainer/Q2 Predictions Mean                           15.6457\n",
      "trainer/Q2 Predictions Std                             1.12299\n",
      "trainer/Q2 Predictions Max                            17.3299\n",
      "trainer/Q2 Predictions Min                            11.6677\n",
      "trainer/Q Targets Mean                                15.7101\n",
      "trainer/Q Targets Std                                  1.10626\n",
      "trainer/Q Targets Max                                 18.3432\n",
      "trainer/Q Targets Min                                 12.0028\n",
      "trainer/Log Pis Mean                                   8.15131\n",
      "trainer/Log Pis Std                                    2.0982\n",
      "trainer/Log Pis Max                                   12.8634\n",
      "trainer/Log Pis Min                                    0.430827\n",
      "trainer/Policy mu Mean                                -0.0297689\n",
      "trainer/Policy mu Std                                  0.140143\n",
      "trainer/Policy mu Max                                  0.620598\n",
      "trainer/Policy mu Min                                 -0.790539\n",
      "trainer/Policy log std Mean                           -2.37558\n",
      "trainer/Policy log std Std                             0.193442\n",
      "trainer/Policy log std Max                            -1.7191\n",
      "trainer/Policy log std Min                            -3.09015\n",
      "trainer/Alpha                                          0.0121647\n",
      "trainer/Alpha Loss                                     0.667176\n",
      "exploration/num steps total                        44000\n",
      "exploration/num paths total                           96\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.969815\n",
      "exploration/Rewards Std                                0.124059\n",
      "exploration/Rewards Max                                1.90392\n",
      "exploration/Rewards Min                                0.488013\n",
      "exploration/Returns Mean                             969.815\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              969.815\n",
      "exploration/Returns Min                              969.815\n",
      "exploration/Actions Mean                              -0.0163037\n",
      "exploration/Actions Std                                0.121835\n",
      "exploration/Actions Max                                0.629335\n",
      "exploration/Actions Min                               -0.519516\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          969.815\n",
      "exploration/env_infos/final/reward_forward Mean        0.0915059\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0915059\n",
      "exploration/env_infos/final/reward_forward Min         0.0915059\n",
      "exploration/env_infos/initial/reward_forward Mean      0.00325664\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.00325664\n",
      "exploration/env_infos/initial/reward_forward Min       0.00325664\n",
      "exploration/env_infos/reward_forward Mean             -0.0216502\n",
      "exploration/env_infos/reward_forward Std               0.258539\n",
      "exploration/env_infos/reward_forward Max               0.980383\n",
      "exploration/env_infos/reward_forward Min              -1.11737\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0767631\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0767631\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0767631\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0639709\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0639709\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0639709\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0604381\n",
      "exploration/env_infos/reward_ctrl Std                  0.0436196\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00674243\n",
      "exploration/env_infos/reward_ctrl Min                 -0.511987\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0530156\n",
      "exploration/env_infos/final/torso_velocity Std         0.0604385\n",
      "exploration/env_infos/final/torso_velocity Max         0.099862\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0323212\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.13194\n",
      "exploration/env_infos/initial/torso_velocity Std       0.202081\n",
      "exploration/env_infos/initial/torso_velocity Max       0.417269\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0247061\n",
      "exploration/env_infos/torso_velocity Mean              0.00452962\n",
      "exploration/env_infos/torso_velocity Std               0.272668\n",
      "exploration/env_infos/torso_velocity Max               1.46894\n",
      "exploration/env_infos/torso_velocity Min              -1.74682\n",
      "evaluation/num steps total                             1.075e+06\n",
      "evaluation/num paths total                          1075\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.958283\n",
      "evaluation/Rewards Std                                 0.0365459\n",
      "evaluation/Rewards Max                                 2.43549\n",
      "evaluation/Rewards Min                                 0.493948\n",
      "evaluation/Returns Mean                              958.283\n",
      "evaluation/Returns Std                                15.9605\n",
      "evaluation/Returns Max                               975.939\n",
      "evaluation/Returns Min                               927.16\n",
      "evaluation/Actions Mean                               -0.0303926\n",
      "evaluation/Actions Std                                 0.0994994\n",
      "evaluation/Actions Max                                 0.526599\n",
      "evaluation/Actions Min                                -0.514013\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           958.283\n",
      "evaluation/env_infos/final/reward_forward Mean         5.27964e-05\n",
      "evaluation/env_infos/final/reward_forward Std          0.000260666\n",
      "evaluation/env_infos/final/reward_forward Max          0.00132965\n",
      "evaluation/env_infos/final/reward_forward Min         -1.72133e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0115239\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0863451\n",
      "evaluation/env_infos/initial/reward_forward Max        0.184332\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.136602\n",
      "evaluation/env_infos/reward_forward Mean               0.00219379\n",
      "evaluation/env_infos/reward_forward Std                0.0680247\n",
      "evaluation/env_infos/reward_forward Max                1.29618\n",
      "evaluation/env_infos/reward_forward Min               -1.02554\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0421857\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0170721\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0217728\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0732275\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0237363\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00997513\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00980049\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0455272\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0432953\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0214469\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00856492\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.560049\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         3.90438e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000241843\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00164531\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000147399\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.138817\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.230467\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.620807\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.285066\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000979195\n",
      "evaluation/env_infos/torso_velocity Std                0.0735305\n",
      "evaluation/env_infos/torso_velocity Max                1.42367\n",
      "evaluation/env_infos/torso_velocity Min               -1.79993\n",
      "time/data storing (s)                                  0.015309\n",
      "time/evaluation sampling (s)                          45.1114\n",
      "time/exploration sampling (s)                          1.98305\n",
      "time/logging (s)                                       0.280681\n",
      "time/saving (s)                                        0.0270689\n",
      "time/training (s)                                      4.04051\n",
      "time/epoch (s)                                        51.458\n",
      "time/total (s)                                      2472.37\n",
      "Epoch                                                 42\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:52:32.860226 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 43 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 45000\n",
      "trainer/QF1 Loss                                       0.230459\n",
      "trainer/QF2 Loss                                       0.213544\n",
      "trainer/Policy Loss                                   -8.0624\n",
      "trainer/Q1 Predictions Mean                           15.8872\n",
      "trainer/Q1 Predictions Std                             1.53305\n",
      "trainer/Q1 Predictions Max                            17.4726\n",
      "trainer/Q1 Predictions Min                             3.18812\n",
      "trainer/Q2 Predictions Mean                           15.9158\n",
      "trainer/Q2 Predictions Std                             1.46251\n",
      "trainer/Q2 Predictions Max                            17.5435\n",
      "trainer/Q2 Predictions Min                             4.93592\n",
      "trainer/Q Targets Mean                                15.9775\n",
      "trainer/Q Targets Std                                  1.59957\n",
      "trainer/Q Targets Max                                 18.1652\n",
      "trainer/Q Targets Min                                  0.00420529\n",
      "trainer/Log Pis Mean                                   8.11615\n",
      "trainer/Log Pis Std                                    2.38159\n",
      "trainer/Log Pis Max                                   14.2504\n",
      "trainer/Log Pis Min                                    1.15063\n",
      "trainer/Policy mu Mean                                 0.0190696\n",
      "trainer/Policy mu Std                                  0.119185\n",
      "trainer/Policy mu Max                                  0.954805\n",
      "trainer/Policy mu Min                                 -0.784134\n",
      "trainer/Policy log std Mean                           -2.43033\n",
      "trainer/Policy log std Std                             0.180402\n",
      "trainer/Policy log std Max                            -1.48252\n",
      "trainer/Policy log std Min                            -3.21053\n",
      "trainer/Alpha                                          0.01196\n",
      "trainer/Alpha Loss                                     0.514165\n",
      "exploration/num steps total                        45000\n",
      "exploration/num paths total                           97\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.904766\n",
      "exploration/Rewards Std                                0.0571576\n",
      "exploration/Rewards Max                                1.41085\n",
      "exploration/Rewards Min                                0.731236\n",
      "exploration/Returns Mean                             904.766\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              904.766\n",
      "exploration/Returns Min                              904.766\n",
      "exploration/Actions Mean                               0.0347479\n",
      "exploration/Actions Std                                0.153353\n",
      "exploration/Actions Max                                0.469803\n",
      "exploration/Actions Min                               -0.554189\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          904.766\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0172662\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0172662\n",
      "exploration/env_infos/final/reward_forward Min        -0.0172662\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0665873\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0665873\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0665873\n",
      "exploration/env_infos/reward_forward Mean              0.0127\n",
      "exploration/env_infos/reward_forward Std               0.146295\n",
      "exploration/env_infos/reward_forward Max               0.779582\n",
      "exploration/env_infos/reward_forward Min              -1.18506\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.118171\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.118171\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.118171\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.038463\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.038463\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.038463\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0988985\n",
      "exploration/env_infos/reward_ctrl Std                  0.0464397\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00704496\n",
      "exploration/env_infos/reward_ctrl Min                 -0.268764\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.026659\n",
      "exploration/env_infos/final/torso_velocity Std         0.0426357\n",
      "exploration/env_infos/final/torso_velocity Max         0.0843937\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0172662\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.215188\n",
      "exploration/env_infos/initial/torso_velocity Std       0.330819\n",
      "exploration/env_infos/initial/torso_velocity Max       0.679516\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0665873\n",
      "exploration/env_infos/torso_velocity Mean              0.00147089\n",
      "exploration/env_infos/torso_velocity Std               0.165845\n",
      "exploration/env_infos/torso_velocity Max               1.31018\n",
      "exploration/env_infos/torso_velocity Min              -1.3\n",
      "evaluation/num steps total                             1.1e+06\n",
      "evaluation/num paths total                          1100\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.975056\n",
      "evaluation/Rewards Std                                 0.0365476\n",
      "evaluation/Rewards Max                                 2.4266\n",
      "evaluation/Rewards Min                                 0.595342\n",
      "evaluation/Returns Mean                              975.056\n",
      "evaluation/Returns Std                                16.0366\n",
      "evaluation/Returns Max                               997.756\n",
      "evaluation/Returns Min                               942.501\n",
      "evaluation/Actions Mean                                0.00257357\n",
      "evaluation/Actions Std                                 0.0817265\n",
      "evaluation/Actions Max                                 0.572365\n",
      "evaluation/Actions Min                                -0.575164\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           975.056\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000197397\n",
      "evaluation/env_infos/final/reward_forward Std          0.000965724\n",
      "evaluation/env_infos/final/reward_forward Max          0.0049284\n",
      "evaluation/env_infos/final/reward_forward Min         -1.32432e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00225379\n",
      "evaluation/env_infos/initial/reward_forward Std        0.107318\n",
      "evaluation/env_infos/initial/reward_forward Max        0.260377\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.150321\n",
      "evaluation/env_infos/reward_forward Mean               0.00152817\n",
      "evaluation/env_infos/reward_forward Std                0.0703019\n",
      "evaluation/env_infos/reward_forward Max                1.42731\n",
      "evaluation/env_infos/reward_forward Min               -1.05119\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0260994\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0161439\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00426047\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0571221\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0176225\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00811707\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00850647\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0460202\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0267434\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0206565\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0017457\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.404658\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         3.11093e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000760097\n",
      "evaluation/env_infos/final/torso_velocity Max          0.0049284\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00406341\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.11402\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.249106\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.673949\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.403361\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00087685\n",
      "evaluation/env_infos/torso_velocity Std                0.073659\n",
      "evaluation/env_infos/torso_velocity Max                1.42731\n",
      "evaluation/env_infos/torso_velocity Min               -1.81408\n",
      "time/data storing (s)                                  0.0189663\n",
      "time/evaluation sampling (s)                          46.1465\n",
      "time/exploration sampling (s)                          2.6799\n",
      "time/logging (s)                                       0.355374\n",
      "time/saving (s)                                        0.0326831\n",
      "time/training (s)                                      9.01718\n",
      "time/epoch (s)                                        58.2506\n",
      "time/total (s)                                      2531.09\n",
      "Epoch                                                 43\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:53:30.897834 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 44 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 46000\n",
      "trainer/QF1 Loss                                       0.114011\n",
      "trainer/QF2 Loss                                       0.121695\n",
      "trainer/Policy Loss                                   -8.60262\n",
      "trainer/Q1 Predictions Mean                           16.3899\n",
      "trainer/Q1 Predictions Std                             1.518\n",
      "trainer/Q1 Predictions Max                            17.9553\n",
      "trainer/Q1 Predictions Min                             0.421211\n",
      "trainer/Q2 Predictions Mean                           16.362\n",
      "trainer/Q2 Predictions Std                             1.50431\n",
      "trainer/Q2 Predictions Max                            17.7608\n",
      "trainer/Q2 Predictions Min                            -0.031906\n",
      "trainer/Q Targets Mean                                16.321\n",
      "trainer/Q Targets Std                                  1.532\n",
      "trainer/Q Targets Max                                 18.0258\n",
      "trainer/Q Targets Min                                 -0.871689\n",
      "trainer/Log Pis Mean                                   8.01445\n",
      "trainer/Log Pis Std                                    2.25133\n",
      "trainer/Log Pis Max                                   13.4195\n",
      "trainer/Log Pis Min                                   -1.8784\n",
      "trainer/Policy mu Mean                                -0.024655\n",
      "trainer/Policy mu Std                                  0.153119\n",
      "trainer/Policy mu Max                                  1.83123\n",
      "trainer/Policy mu Min                                 -0.695631\n",
      "trainer/Policy log std Mean                           -2.40413\n",
      "trainer/Policy log std Std                             0.173895\n",
      "trainer/Policy log std Max                            -1.77651\n",
      "trainer/Policy log std Min                            -3.14715\n",
      "trainer/Alpha                                          0.0117426\n",
      "trainer/Alpha Loss                                     0.064244\n",
      "exploration/num steps total                        46000\n",
      "exploration/num paths total                           98\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.892688\n",
      "exploration/Rewards Std                                0.0649872\n",
      "exploration/Rewards Max                                1.29139\n",
      "exploration/Rewards Min                                0.247378\n",
      "exploration/Returns Mean                             892.688\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              892.688\n",
      "exploration/Returns Min                              892.688\n",
      "exploration/Actions Mean                               0.00303626\n",
      "exploration/Actions Std                                0.167212\n",
      "exploration/Actions Max                                0.525126\n",
      "exploration/Actions Min                               -0.719682\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          892.688\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0890545\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0890545\n",
      "exploration/env_infos/final/reward_forward Min        -0.0890545\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0147632\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0147632\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0147632\n",
      "exploration/env_infos/reward_forward Mean              0.0400944\n",
      "exploration/env_infos/reward_forward Std               0.253811\n",
      "exploration/env_infos/reward_forward Max               1.533\n",
      "exploration/env_infos/reward_forward Min              -1.03784\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.10742\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.10742\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.10742\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0195559\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0195559\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0195559\n",
      "exploration/env_infos/reward_ctrl Mean                -0.111876\n",
      "exploration/env_infos/reward_ctrl Std                  0.058304\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0143845\n",
      "exploration/env_infos/reward_ctrl Min                 -0.752622\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0483692\n",
      "exploration/env_infos/final/torso_velocity Std         0.085695\n",
      "exploration/env_infos/final/torso_velocity Max         0.0708368\n",
      "exploration/env_infos/final/torso_velocity Min        -0.12689\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.142527\n",
      "exploration/env_infos/initial/torso_velocity Std       0.312648\n",
      "exploration/env_infos/initial/torso_velocity Max       0.579039\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.136693\n",
      "exploration/env_infos/torso_velocity Mean              0.0208006\n",
      "exploration/env_infos/torso_velocity Std               0.204793\n",
      "exploration/env_infos/torso_velocity Max               1.533\n",
      "exploration/env_infos/torso_velocity Min              -1.21356\n",
      "evaluation/num steps total                             1.125e+06\n",
      "evaluation/num paths total                          1125\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.947507\n",
      "evaluation/Rewards Std                                 0.0355377\n",
      "evaluation/Rewards Max                                 2.26093\n",
      "evaluation/Rewards Min                                 0.521904\n",
      "evaluation/Returns Mean                              947.507\n",
      "evaluation/Returns Std                                15.2281\n",
      "evaluation/Returns Max                               977.925\n",
      "evaluation/Returns Min                               909.224\n",
      "evaluation/Actions Mean                               -0.042733\n",
      "evaluation/Actions Std                                 0.107772\n",
      "evaluation/Actions Max                                 0.562215\n",
      "evaluation/Actions Min                                -0.633334\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           947.507\n",
      "evaluation/env_infos/final/reward_forward Mean         3.69794e-07\n",
      "evaluation/env_infos/final/reward_forward Std          4.44111e-05\n",
      "evaluation/env_infos/final/reward_forward Max          0.000188866\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000101585\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0376074\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0968349\n",
      "evaluation/env_infos/initial/reward_forward Max        0.221838\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.160353\n",
      "evaluation/env_infos/reward_forward Mean               0.00357932\n",
      "evaluation/env_infos/reward_forward Std                0.0640709\n",
      "evaluation/env_infos/reward_forward Max                1.64993\n",
      "evaluation/env_infos/reward_forward Min               -0.755392\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0532711\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0151683\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0215469\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0908521\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0208505\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00517381\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0117804\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0321267\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.053764\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0206897\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00802815\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.561694\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.53257e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000119577\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000655042\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000642843\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.177892\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.225821\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.669931\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.162653\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000958374\n",
      "evaluation/env_infos/torso_velocity Std                0.0670204\n",
      "evaluation/env_infos/torso_velocity Max                1.64993\n",
      "evaluation/env_infos/torso_velocity Min               -1.98253\n",
      "time/data storing (s)                                  0.0166155\n",
      "time/evaluation sampling (s)                          49.3102\n",
      "time/exploration sampling (s)                          2.36674\n",
      "time/logging (s)                                       0.319742\n",
      "time/saving (s)                                        0.0336802\n",
      "time/training (s)                                      5.35562\n",
      "time/epoch (s)                                        57.4026\n",
      "time/total (s)                                      2589.09\n",
      "Epoch                                                 44\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:54:29.643382 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 45 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 47000\n",
      "trainer/QF1 Loss                                       0.128016\n",
      "trainer/QF2 Loss                                       0.122116\n",
      "trainer/Policy Loss                                   -9.61746\n",
      "trainer/Q1 Predictions Mean                           16.659\n",
      "trainer/Q1 Predictions Std                             1.13015\n",
      "trainer/Q1 Predictions Max                            19.3621\n",
      "trainer/Q1 Predictions Min                            11.8816\n",
      "trainer/Q2 Predictions Mean                           16.8442\n",
      "trainer/Q2 Predictions Std                             1.09086\n",
      "trainer/Q2 Predictions Max                            19.3744\n",
      "trainer/Q2 Predictions Min                            12.7865\n",
      "trainer/Q Targets Mean                                16.7677\n",
      "trainer/Q Targets Std                                  1.08956\n",
      "trainer/Q Targets Max                                 18.6195\n",
      "trainer/Q Targets Min                                 12.9354\n",
      "trainer/Log Pis Mean                                   7.29518\n",
      "trainer/Log Pis Std                                    2.14639\n",
      "trainer/Log Pis Max                                   13.7238\n",
      "trainer/Log Pis Min                                    0.0507977\n",
      "trainer/Policy mu Mean                                 0.0232571\n",
      "trainer/Policy mu Std                                  0.124186\n",
      "trainer/Policy mu Max                                  0.796871\n",
      "trainer/Policy mu Min                                 -0.92349\n",
      "trainer/Policy log std Mean                           -2.31037\n",
      "trainer/Policy log std Std                             0.151344\n",
      "trainer/Policy log std Max                            -1.87962\n",
      "trainer/Policy log std Min                            -3.15086\n",
      "trainer/Alpha                                          0.0116252\n",
      "trainer/Alpha Loss                                    -3.139\n",
      "exploration/num steps total                        47000\n",
      "exploration/num paths total                           99\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.95373\n",
      "exploration/Rewards Std                                0.168016\n",
      "exploration/Rewards Max                                2.11199\n",
      "exploration/Rewards Min                                0.723864\n",
      "exploration/Returns Mean                             953.73\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              953.73\n",
      "exploration/Returns Min                              953.73\n",
      "exploration/Actions Mean                               0.0408189\n",
      "exploration/Actions Std                                0.150692\n",
      "exploration/Actions Max                                0.525328\n",
      "exploration/Actions Min                               -0.5684\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          953.73\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0235385\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0235385\n",
      "exploration/env_infos/final/reward_forward Min        -0.0235385\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.106932\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.106932\n",
      "exploration/env_infos/initial/reward_forward Min      -0.106932\n",
      "exploration/env_infos/reward_forward Mean              0.0365853\n",
      "exploration/env_infos/reward_forward Std               0.271608\n",
      "exploration/env_infos/reward_forward Max               1.20709\n",
      "exploration/env_infos/reward_forward Min              -0.942669\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.166489\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.166489\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.166489\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0879054\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0879054\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0879054\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0974975\n",
      "exploration/env_infos/reward_ctrl Std                  0.0493425\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0111047\n",
      "exploration/env_infos/reward_ctrl Min                 -0.294654\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0102681\n",
      "exploration/env_infos/final/torso_velocity Std         0.138692\n",
      "exploration/env_infos/final/torso_velocity Max         0.194491\n",
      "exploration/env_infos/final/torso_velocity Min        -0.140148\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.132257\n",
      "exploration/env_infos/initial/torso_velocity Std       0.327627\n",
      "exploration/env_infos/initial/torso_velocity Max       0.595509\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.106932\n",
      "exploration/env_infos/torso_velocity Mean              0.0343789\n",
      "exploration/env_infos/torso_velocity Std               0.241407\n",
      "exploration/env_infos/torso_velocity Max               1.20709\n",
      "exploration/env_infos/torso_velocity Min              -1.08083\n",
      "evaluation/num steps total                             1.15e+06\n",
      "evaluation/num paths total                          1150\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.95054\n",
      "evaluation/Rewards Std                                 0.0439463\n",
      "evaluation/Rewards Max                                 2.49495\n",
      "evaluation/Rewards Min                                 0.66681\n",
      "evaluation/Returns Mean                              950.54\n",
      "evaluation/Returns Std                                26.8239\n",
      "evaluation/Returns Max                               985.938\n",
      "evaluation/Returns Min                               892.104\n",
      "evaluation/Actions Mean                               -0.00236088\n",
      "evaluation/Actions Std                                 0.112986\n",
      "evaluation/Actions Max                                 0.540344\n",
      "evaluation/Actions Min                                -0.378934\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           950.54\n",
      "evaluation/env_infos/final/reward_forward Mean        -2.50019e-06\n",
      "evaluation/env_infos/final/reward_forward Std          1.10919e-05\n",
      "evaluation/env_infos/final/reward_forward Max          5.40241e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.66107e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0171854\n",
      "evaluation/env_infos/initial/reward_forward Std        0.141819\n",
      "evaluation/env_infos/initial/reward_forward Max        0.29285\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.295959\n",
      "evaluation/env_infos/reward_forward Mean               0.00153733\n",
      "evaluation/env_infos/reward_forward Std                0.0569896\n",
      "evaluation/env_infos/reward_forward Max                1.47436\n",
      "evaluation/env_infos/reward_forward Min               -1.00257\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0503948\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0265664\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0166053\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.108518\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0223701\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00998162\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00944825\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0478644\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.051086\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0277931\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00479849\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.33319\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.06608e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          7.06544e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          2.88919e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -5.66107e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.131824\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.227798\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.61843\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.295959\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00105525\n",
      "evaluation/env_infos/torso_velocity Std                0.0679938\n",
      "evaluation/env_infos/torso_velocity Max                1.47436\n",
      "evaluation/env_infos/torso_velocity Min               -1.85656\n",
      "time/data storing (s)                                  0.0206525\n",
      "time/evaluation sampling (s)                          48.3164\n",
      "time/exploration sampling (s)                          2.90724\n",
      "time/logging (s)                                       0.328444\n",
      "time/saving (s)                                        0.0311592\n",
      "time/training (s)                                      6.6007\n",
      "time/epoch (s)                                        58.2046\n",
      "time/total (s)                                      2647.84\n",
      "Epoch                                                 45\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:55:29.087029 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 46 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 48000\n",
      "trainer/QF1 Loss                                       0.226775\n",
      "trainer/QF2 Loss                                       0.26121\n",
      "trainer/Policy Loss                                   -9.83348\n",
      "trainer/Q1 Predictions Mean                           17.0913\n",
      "trainer/Q1 Predictions Std                             1.23674\n",
      "trainer/Q1 Predictions Max                            19.8686\n",
      "trainer/Q1 Predictions Min                            11.6471\n",
      "trainer/Q2 Predictions Mean                           17.174\n",
      "trainer/Q2 Predictions Std                             1.24441\n",
      "trainer/Q2 Predictions Max                            21.0372\n",
      "trainer/Q2 Predictions Min                            12.1769\n",
      "trainer/Q Targets Mean                                17.0392\n",
      "trainer/Q Targets Std                                  1.1523\n",
      "trainer/Q Targets Max                                 21.8572\n",
      "trainer/Q Targets Min                                 12.0286\n",
      "trainer/Log Pis Mean                                   7.51324\n",
      "trainer/Log Pis Std                                    2.42038\n",
      "trainer/Log Pis Max                                   14.6076\n",
      "trainer/Log Pis Min                                   -0.420314\n",
      "trainer/Policy mu Mean                                 0.0141971\n",
      "trainer/Policy mu Std                                  0.150154\n",
      "trainer/Policy mu Max                                  0.975429\n",
      "trainer/Policy mu Min                                 -1.00323\n",
      "trainer/Policy log std Mean                           -2.31295\n",
      "trainer/Policy log std Std                             0.212183\n",
      "trainer/Policy log std Max                            -1.41257\n",
      "trainer/Policy log std Min                            -3.29304\n",
      "trainer/Alpha                                          0.011401\n",
      "trainer/Alpha Loss                                    -2.17734\n",
      "exploration/num steps total                        48000\n",
      "exploration/num paths total                          100\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.934275\n",
      "exploration/Rewards Std                                0.0681668\n",
      "exploration/Rewards Max                                1.57173\n",
      "exploration/Rewards Min                                0.536471\n",
      "exploration/Returns Mean                             934.275\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              934.275\n",
      "exploration/Returns Min                              934.275\n",
      "exploration/Actions Mean                              -0.012978\n",
      "exploration/Actions Std                                0.1471\n",
      "exploration/Actions Max                                0.583986\n",
      "exploration/Actions Min                               -0.458213\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          934.275\n",
      "exploration/env_infos/final/reward_forward Mean        0.0102127\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0102127\n",
      "exploration/env_infos/final/reward_forward Min         0.0102127\n",
      "exploration/env_infos/initial/reward_forward Mean      0.154787\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.154787\n",
      "exploration/env_infos/initial/reward_forward Min       0.154787\n",
      "exploration/env_infos/reward_forward Mean             -0.0154015\n",
      "exploration/env_infos/reward_forward Std               0.123384\n",
      "exploration/env_infos/reward_forward Max               0.529181\n",
      "exploration/env_infos/reward_forward Min              -0.679883\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0881423\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0881423\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0881423\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0342096\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0342096\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0342096\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0872269\n",
      "exploration/env_infos/reward_ctrl Std                  0.0365319\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0123369\n",
      "exploration/env_infos/reward_ctrl Min                 -0.463529\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0425173\n",
      "exploration/env_infos/final/torso_velocity Std         0.0552537\n",
      "exploration/env_infos/final/torso_velocity Max         0.120288\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00294839\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.165133\n",
      "exploration/env_infos/initial/torso_velocity Std       0.199301\n",
      "exploration/env_infos/initial/torso_velocity Max       0.414235\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0736227\n",
      "exploration/env_infos/torso_velocity Mean             -0.0056506\n",
      "exploration/env_infos/torso_velocity Std               0.103204\n",
      "exploration/env_infos/torso_velocity Max               0.529181\n",
      "exploration/env_infos/torso_velocity Min              -1.71661\n",
      "evaluation/num steps total                             1.175e+06\n",
      "evaluation/num paths total                          1175\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.897029\n",
      "evaluation/Rewards Std                                 0.0617555\n",
      "evaluation/Rewards Max                                 2.35464\n",
      "evaluation/Rewards Min                                 0.256403\n",
      "evaluation/Returns Mean                              897.029\n",
      "evaluation/Returns Std                                38.4538\n",
      "evaluation/Returns Max                               983.286\n",
      "evaluation/Returns Min                               799.79\n",
      "evaluation/Actions Mean                                7.38676e-05\n",
      "evaluation/Actions Std                                 0.162148\n",
      "evaluation/Actions Max                                 0.682902\n",
      "evaluation/Actions Min                                -0.797957\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           897.029\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.93346e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.57321e-07\n",
      "evaluation/env_infos/final/reward_forward Max          2.68008e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -7.31351e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00208373\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0751578\n",
      "evaluation/env_infos/initial/reward_forward Max        0.15946\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.116179\n",
      "evaluation/env_infos/reward_forward Mean               0.00273535\n",
      "evaluation/env_infos/reward_forward Std                0.0660755\n",
      "evaluation/env_infos/reward_forward Max                1.48935\n",
      "evaluation/env_infos/reward_forward Min               -0.79926\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.108278\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0424223\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0137344\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.205857\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0172442\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00387484\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0116388\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0291667\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.105167\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0466599\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0062993\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.743597\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -7.32951e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.5598e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          4.04002e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -7.41177e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.131653\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.23801\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.68354\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.278758\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000706558\n",
      "evaluation/env_infos/torso_velocity Std                0.0774403\n",
      "evaluation/env_infos/torso_velocity Max                1.48935\n",
      "evaluation/env_infos/torso_velocity Min               -2.1068\n",
      "time/data storing (s)                                  0.0188574\n",
      "time/evaluation sampling (s)                          51.6464\n",
      "time/exploration sampling (s)                          2.2566\n",
      "time/logging (s)                                       0.283097\n",
      "time/saving (s)                                        0.0277432\n",
      "time/training (s)                                      4.6218\n",
      "time/epoch (s)                                        58.8545\n",
      "time/total (s)                                      2707.24\n",
      "Epoch                                                 46\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:56:27.216937 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 47 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 49000\n",
      "trainer/QF1 Loss                                       0.144361\n",
      "trainer/QF2 Loss                                       0.242811\n",
      "trainer/Policy Loss                                   -9.58962\n",
      "trainer/Q1 Predictions Mean                           17.3541\n",
      "trainer/Q1 Predictions Std                             1.25849\n",
      "trainer/Q1 Predictions Max                            18.9787\n",
      "trainer/Q1 Predictions Min                            12.9875\n",
      "trainer/Q2 Predictions Mean                           17.5506\n",
      "trainer/Q2 Predictions Std                             1.24939\n",
      "trainer/Q2 Predictions Max                            18.8618\n",
      "trainer/Q2 Predictions Min                            13.1636\n",
      "trainer/Q Targets Mean                                17.3443\n",
      "trainer/Q Targets Std                                  1.2119\n",
      "trainer/Q Targets Max                                 18.7349\n",
      "trainer/Q Targets Min                                 12.7608\n",
      "trainer/Log Pis Mean                                   8.03819\n",
      "trainer/Log Pis Std                                    2.12826\n",
      "trainer/Log Pis Max                                   15.8176\n",
      "trainer/Log Pis Min                                    0.0846235\n",
      "trainer/Policy mu Mean                                -0.031935\n",
      "trainer/Policy mu Std                                  0.153422\n",
      "trainer/Policy mu Max                                  0.877044\n",
      "trainer/Policy mu Min                                 -0.858862\n",
      "trainer/Policy log std Mean                           -2.37721\n",
      "trainer/Policy log std Std                             0.174466\n",
      "trainer/Policy log std Max                            -1.66334\n",
      "trainer/Policy log std Min                            -3.30545\n",
      "trainer/Alpha                                          0.011159\n",
      "trainer/Alpha Loss                                     0.171686\n",
      "exploration/num steps total                        49000\n",
      "exploration/num paths total                          101\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.925737\n",
      "exploration/Rewards Std                                0.0518882\n",
      "exploration/Rewards Max                                1.25825\n",
      "exploration/Rewards Min                                0.643332\n",
      "exploration/Returns Mean                             925.737\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              925.737\n",
      "exploration/Returns Min                              925.737\n",
      "exploration/Actions Mean                              -0.0504405\n",
      "exploration/Actions Std                                0.134097\n",
      "exploration/Actions Max                                0.391215\n",
      "exploration/Actions Min                               -0.589582\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          925.737\n",
      "exploration/env_infos/final/reward_forward Mean       -0.119822\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.119822\n",
      "exploration/env_infos/final/reward_forward Min        -0.119822\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0988115\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0988115\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0988115\n",
      "exploration/env_infos/reward_forward Mean             -0.00738434\n",
      "exploration/env_infos/reward_forward Std               0.127424\n",
      "exploration/env_infos/reward_forward Max               0.985022\n",
      "exploration/env_infos/reward_forward Min              -0.504787\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0492226\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0492226\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0492226\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.032687\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.032687\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.032687\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0821052\n",
      "exploration/env_infos/reward_ctrl Std                  0.0369094\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00540746\n",
      "exploration/env_infos/reward_ctrl Min                 -0.356668\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0713901\n",
      "exploration/env_infos/final/torso_velocity Std         0.110343\n",
      "exploration/env_infos/final/torso_velocity Max         0.0812943\n",
      "exploration/env_infos/final/torso_velocity Min        -0.175643\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.00841061\n",
      "exploration/env_infos/initial/torso_velocity Std       0.284854\n",
      "exploration/env_infos/initial/torso_velocity Max       0.376765\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.303186\n",
      "exploration/env_infos/torso_velocity Mean             -0.00171095\n",
      "exploration/env_infos/torso_velocity Std               0.166553\n",
      "exploration/env_infos/torso_velocity Max               1.15562\n",
      "exploration/env_infos/torso_velocity Min              -1.42861\n",
      "evaluation/num steps total                             1.2e+06\n",
      "evaluation/num paths total                          1200\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.950371\n",
      "evaluation/Rewards Std                                 0.0327011\n",
      "evaluation/Rewards Max                                 2.08622\n",
      "evaluation/Rewards Min                                 0.389212\n",
      "evaluation/Returns Mean                              950.371\n",
      "evaluation/Returns Std                                16.7413\n",
      "evaluation/Returns Max                               965.086\n",
      "evaluation/Returns Min                               904.53\n",
      "evaluation/Actions Mean                               -0.0303983\n",
      "evaluation/Actions Std                                 0.108503\n",
      "evaluation/Actions Max                                 0.507286\n",
      "evaluation/Actions Min                                -0.773974\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           950.371\n",
      "evaluation/env_infos/final/reward_forward Mean         1.49056e-07\n",
      "evaluation/env_infos/final/reward_forward Std          4.69048e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.96792e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -8.33683e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0395258\n",
      "evaluation/env_infos/initial/reward_forward Std        0.159025\n",
      "evaluation/env_infos/initial/reward_forward Max        0.34303\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.345234\n",
      "evaluation/env_infos/reward_forward Mean              -0.000466453\n",
      "evaluation/env_infos/reward_forward Std                0.063791\n",
      "evaluation/env_infos/reward_forward Max                1.69206\n",
      "evaluation/env_infos/reward_forward Min               -1.24435\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0499074\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0180559\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0338869\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0967112\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0341711\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00891102\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.017415\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0494859\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0507879\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0230753\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.017415\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.610788\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.25044e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          4.17133e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.00007e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.33683e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.130622\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.274893\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.686824\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.345234\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00202563\n",
      "evaluation/env_infos/torso_velocity Std                0.0750194\n",
      "evaluation/env_infos/torso_velocity Max                1.69206\n",
      "evaluation/env_infos/torso_velocity Min               -2.08275\n",
      "time/data storing (s)                                  0.0195488\n",
      "time/evaluation sampling (s)                          49.454\n",
      "time/exploration sampling (s)                          2.31631\n",
      "time/logging (s)                                       0.297744\n",
      "time/saving (s)                                        0.0293045\n",
      "time/training (s)                                      5.5116\n",
      "time/epoch (s)                                        57.6285\n",
      "time/total (s)                                      2765.39\n",
      "Epoch                                                 47\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:57:24.425136 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 48 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 50000\n",
      "trainer/QF1 Loss                                       0.136968\n",
      "trainer/QF2 Loss                                       0.166354\n",
      "trainer/Policy Loss                                  -10.4524\n",
      "trainer/Q1 Predictions Mean                           17.6104\n",
      "trainer/Q1 Predictions Std                             1.00281\n",
      "trainer/Q1 Predictions Max                            19.2859\n",
      "trainer/Q1 Predictions Min                            13.1336\n",
      "trainer/Q2 Predictions Mean                           17.8246\n",
      "trainer/Q2 Predictions Std                             1.07569\n",
      "trainer/Q2 Predictions Max                            19.1873\n",
      "trainer/Q2 Predictions Min                            13.7249\n",
      "trainer/Q Targets Mean                                17.7203\n",
      "trainer/Q Targets Std                                  1.07458\n",
      "trainer/Q Targets Max                                 19.3897\n",
      "trainer/Q Targets Min                                 13.7299\n",
      "trainer/Log Pis Mean                                   7.32684\n",
      "trainer/Log Pis Std                                    2.27727\n",
      "trainer/Log Pis Max                                   13.6734\n",
      "trainer/Log Pis Min                                   -0.773978\n",
      "trainer/Policy mu Mean                                 0.0259908\n",
      "trainer/Policy mu Std                                  0.14499\n",
      "trainer/Policy mu Max                                  0.910317\n",
      "trainer/Policy mu Min                                 -0.687783\n",
      "trainer/Policy log std Mean                           -2.32429\n",
      "trainer/Policy log std Std                             0.181529\n",
      "trainer/Policy log std Max                            -1.44726\n",
      "trainer/Policy log std Min                            -3.0685\n",
      "trainer/Alpha                                          0.0109576\n",
      "trainer/Alpha Loss                                    -3.03851\n",
      "exploration/num steps total                        50000\n",
      "exploration/num paths total                          102\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.874639\n",
      "exploration/Rewards Std                                0.0546318\n",
      "exploration/Rewards Max                                1.54255\n",
      "exploration/Rewards Min                                0.673419\n",
      "exploration/Returns Mean                             874.639\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              874.639\n",
      "exploration/Returns Min                              874.639\n",
      "exploration/Actions Mean                               0.0795894\n",
      "exploration/Actions Std                                0.162092\n",
      "exploration/Actions Max                                0.490461\n",
      "exploration/Actions Min                               -0.502127\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          874.639\n",
      "exploration/env_infos/final/reward_forward Mean       -0.00133118\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.00133118\n",
      "exploration/env_infos/final/reward_forward Min        -0.00133118\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0902124\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0902124\n",
      "exploration/env_infos/initial/reward_forward Min       0.0902124\n",
      "exploration/env_infos/reward_forward Mean             -0.00294296\n",
      "exploration/env_infos/reward_forward Std               0.0873538\n",
      "exploration/env_infos/reward_forward Max               0.609689\n",
      "exploration/env_infos/reward_forward Min              -0.673796\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.180895\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.180895\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.180895\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0877299\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0877299\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0877299\n",
      "exploration/env_infos/reward_ctrl Mean                -0.130433\n",
      "exploration/env_infos/reward_ctrl Std                  0.0437283\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0339519\n",
      "exploration/env_infos/reward_ctrl Min                 -0.326581\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.000755819\n",
      "exploration/env_infos/final/torso_velocity Std         0.0012789\n",
      "exploration/env_infos/final/torso_velocity Max         0.00101682\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0019531\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.315622\n",
      "exploration/env_infos/initial/torso_velocity Std       0.229811\n",
      "exploration/env_infos/initial/torso_velocity Max       0.631089\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0902124\n",
      "exploration/env_infos/torso_velocity Mean              0.00147083\n",
      "exploration/env_infos/torso_velocity Std               0.1539\n",
      "exploration/env_infos/torso_velocity Max               1.02333\n",
      "exploration/env_infos/torso_velocity Min              -1.30926\n",
      "evaluation/num steps total                             1.225e+06\n",
      "evaluation/num paths total                          1225\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.933001\n",
      "evaluation/Rewards Std                                 0.0450922\n",
      "evaluation/Rewards Max                                 2.34876\n",
      "evaluation/Rewards Min                                 0.30238\n",
      "evaluation/Returns Mean                              933.001\n",
      "evaluation/Returns Std                                37.9797\n",
      "evaluation/Returns Max                               963.204\n",
      "evaluation/Returns Min                               837.553\n",
      "evaluation/Actions Mean                                0.0367389\n",
      "evaluation/Actions Std                                 0.124792\n",
      "evaluation/Actions Max                                 0.649762\n",
      "evaluation/Actions Min                                -0.579395\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           933.001\n",
      "evaluation/env_infos/final/reward_forward Mean        -3.24753e-08\n",
      "evaluation/env_infos/final/reward_forward Std          1.39223e-07\n",
      "evaluation/env_infos/final/reward_forward Max          2.58537e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -2.95803e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0288012\n",
      "evaluation/env_infos/initial/reward_forward Std        0.133811\n",
      "evaluation/env_infos/initial/reward_forward Max        0.303166\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.261815\n",
      "evaluation/env_infos/reward_forward Mean               0.000145954\n",
      "evaluation/env_infos/reward_forward Std                0.0568205\n",
      "evaluation/env_infos/reward_forward Max                1.59846\n",
      "evaluation/env_infos/reward_forward Min               -1.36691\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.067013\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0391344\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.034033\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.164113\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.043346\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0131557\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0266028\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0886933\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0676911\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0412345\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0206286\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.69762\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         3.56973e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.49631e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.05036e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -4.4193e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.150295\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.25921\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.671297\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.261815\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00200425\n",
      "evaluation/env_infos/torso_velocity Std                0.0719715\n",
      "evaluation/env_infos/torso_velocity Max                1.59846\n",
      "evaluation/env_infos/torso_velocity Min               -2.39429\n",
      "time/data storing (s)                                  0.0146027\n",
      "time/evaluation sampling (s)                          49.4455\n",
      "time/exploration sampling (s)                          2.14057\n",
      "time/logging (s)                                       0.282782\n",
      "time/saving (s)                                        0.0268132\n",
      "time/training (s)                                      4.75015\n",
      "time/epoch (s)                                        56.6604\n",
      "time/total (s)                                      2822.58\n",
      "Epoch                                                 48\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:58:17.341749 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 49 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 51000\n",
      "trainer/QF1 Loss                                       0.187504\n",
      "trainer/QF2 Loss                                       0.163847\n",
      "trainer/Policy Loss                                  -10.3612\n",
      "trainer/Q1 Predictions Mean                           17.9069\n",
      "trainer/Q1 Predictions Std                             1.18894\n",
      "trainer/Q1 Predictions Max                            19.3532\n",
      "trainer/Q1 Predictions Min                            12.8577\n",
      "trainer/Q2 Predictions Mean                           17.8922\n",
      "trainer/Q2 Predictions Std                             1.23816\n",
      "trainer/Q2 Predictions Max                            20.5545\n",
      "trainer/Q2 Predictions Min                            12.9221\n",
      "trainer/Q Targets Mean                                17.9716\n",
      "trainer/Q Targets Std                                  1.23551\n",
      "trainer/Q Targets Max                                 21.7328\n",
      "trainer/Q Targets Min                                 12.5867\n",
      "trainer/Log Pis Mean                                   7.78758\n",
      "trainer/Log Pis Std                                    2.37698\n",
      "trainer/Log Pis Max                                   15.151\n",
      "trainer/Log Pis Min                                    1.10301\n",
      "trainer/Policy mu Mean                                 0.021232\n",
      "trainer/Policy mu Std                                  0.145266\n",
      "trainer/Policy mu Max                                  1.01438\n",
      "trainer/Policy mu Min                                 -0.646623\n",
      "trainer/Policy log std Mean                           -2.39015\n",
      "trainer/Policy log std Std                             0.198828\n",
      "trainer/Policy log std Max                            -1.76608\n",
      "trainer/Policy log std Min                            -3.60545\n",
      "trainer/Alpha                                          0.010647\n",
      "trainer/Alpha Loss                                    -0.964708\n",
      "exploration/num steps total                        51000\n",
      "exploration/num paths total                          103\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.915702\n",
      "exploration/Rewards Std                                0.0600495\n",
      "exploration/Rewards Max                                1.55007\n",
      "exploration/Rewards Min                                0.668801\n",
      "exploration/Returns Mean                             915.702\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              915.702\n",
      "exploration/Returns Min                              915.702\n",
      "exploration/Actions Mean                               0.0311584\n",
      "exploration/Actions Std                                0.147235\n",
      "exploration/Actions Max                                0.512601\n",
      "exploration/Actions Min                               -0.493785\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          915.702\n",
      "exploration/env_infos/final/reward_forward Mean        0.0915314\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0915314\n",
      "exploration/env_infos/final/reward_forward Min         0.0915314\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0257832\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0257832\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0257832\n",
      "exploration/env_infos/reward_forward Mean              0.0175866\n",
      "exploration/env_infos/reward_forward Std               0.156949\n",
      "exploration/env_infos/reward_forward Max               0.718477\n",
      "exploration/env_infos/reward_forward Min              -0.535301\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0694058\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0694058\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0694058\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.017892\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.017892\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.017892\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0905963\n",
      "exploration/env_infos/reward_ctrl Std                  0.0417215\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00935945\n",
      "exploration/env_infos/reward_ctrl Min                 -0.331199\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.172955\n",
      "exploration/env_infos/final/torso_velocity Std         0.277874\n",
      "exploration/env_infos/final/torso_velocity Max         0.546606\n",
      "exploration/env_infos/final/torso_velocity Min        -0.119272\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0949414\n",
      "exploration/env_infos/initial/torso_velocity Std       0.280471\n",
      "exploration/env_infos/initial/torso_velocity Max       0.482512\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.171904\n",
      "exploration/env_infos/torso_velocity Mean             -0.000962982\n",
      "exploration/env_infos/torso_velocity Std               0.203899\n",
      "exploration/env_infos/torso_velocity Max               1.16067\n",
      "exploration/env_infos/torso_velocity Min              -1.15424\n",
      "evaluation/num steps total                             1.25e+06\n",
      "evaluation/num paths total                          1250\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.943664\n",
      "evaluation/Rewards Std                                 0.0469978\n",
      "evaluation/Rewards Max                                 2.48326\n",
      "evaluation/Rewards Min                                 0.492755\n",
      "evaluation/Returns Mean                              943.664\n",
      "evaluation/Returns Std                                25.8455\n",
      "evaluation/Returns Max                               973.613\n",
      "evaluation/Returns Min                               883.534\n",
      "evaluation/Actions Mean                                0.0372261\n",
      "evaluation/Actions Std                                 0.115177\n",
      "evaluation/Actions Max                                 0.505216\n",
      "evaluation/Actions Min                                -0.633675\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           943.664\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.892e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.90196e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.62036e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -4.16778e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0139513\n",
      "evaluation/env_infos/initial/reward_forward Std        0.154046\n",
      "evaluation/env_infos/initial/reward_forward Max        0.352822\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.258378\n",
      "evaluation/env_infos/reward_forward Mean               0.00138671\n",
      "evaluation/env_infos/reward_forward Std                0.0522526\n",
      "evaluation/env_infos/reward_forward Max                1.25109\n",
      "evaluation/env_infos/reward_forward Min               -0.993922\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0577751\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0270317\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0249262\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.117375\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0299194\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0112711\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.012266\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.054362\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0586058\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0312456\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.011291\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.507245\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         4.52727e-10\n",
      "evaluation/env_infos/final/torso_velocity Std          3.02181e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.77306e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -7.71355e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.15851\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.238616\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.634053\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.258378\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000869252\n",
      "evaluation/env_infos/torso_velocity Std                0.0725081\n",
      "evaluation/env_infos/torso_velocity Max                1.25109\n",
      "evaluation/env_infos/torso_velocity Min               -1.79956\n",
      "time/data storing (s)                                  0.0148046\n",
      "time/evaluation sampling (s)                          45.1458\n",
      "time/exploration sampling (s)                          1.97044\n",
      "time/logging (s)                                       0.296803\n",
      "time/saving (s)                                        0.0293846\n",
      "time/training (s)                                      4.96386\n",
      "time/epoch (s)                                        52.4211\n",
      "time/total (s)                                      2875.51\n",
      "Epoch                                                 49\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:59:17.258795 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 50 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 52000\n",
      "trainer/QF1 Loss                                       0.232535\n",
      "trainer/QF2 Loss                                       0.228773\n",
      "trainer/Policy Loss                                  -10.0701\n",
      "trainer/Q1 Predictions Mean                           18.3707\n",
      "trainer/Q1 Predictions Std                             1.29199\n",
      "trainer/Q1 Predictions Max                            20.4859\n",
      "trainer/Q1 Predictions Min                            13.8723\n",
      "trainer/Q2 Predictions Mean                           18.2988\n",
      "trainer/Q2 Predictions Std                             1.25264\n",
      "trainer/Q2 Predictions Max                            19.969\n",
      "trainer/Q2 Predictions Min                            12.7667\n",
      "trainer/Q Targets Mean                                18.2363\n",
      "trainer/Q Targets Std                                  1.34988\n",
      "trainer/Q Targets Max                                 21.0768\n",
      "trainer/Q Targets Min                                 10.7213\n",
      "trainer/Log Pis Mean                                   8.52626\n",
      "trainer/Log Pis Std                                    2.4626\n",
      "trainer/Log Pis Max                                   15.4104\n",
      "trainer/Log Pis Min                                   -2.0743\n",
      "trainer/Policy mu Mean                                 0.075051\n",
      "trainer/Policy mu Std                                  0.146125\n",
      "trainer/Policy mu Max                                  1.15352\n",
      "trainer/Policy mu Min                                 -0.685223\n",
      "trainer/Policy log std Mean                           -2.44632\n",
      "trainer/Policy log std Std                             0.187532\n",
      "trainer/Policy log std Max                            -1.92221\n",
      "trainer/Policy log std Min                            -3.3936\n",
      "trainer/Alpha                                          0.0104956\n",
      "trainer/Alpha Loss                                     2.39834\n",
      "exploration/num steps total                        52000\n",
      "exploration/num paths total                          104\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.794261\n",
      "exploration/Rewards Std                                0.0729716\n",
      "exploration/Rewards Max                                1.59851\n",
      "exploration/Rewards Min                                0.61478\n",
      "exploration/Returns Mean                             794.261\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              794.261\n",
      "exploration/Returns Min                              794.261\n",
      "exploration/Actions Mean                               0.148705\n",
      "exploration/Actions Std                                0.175955\n",
      "exploration/Actions Max                                0.618431\n",
      "exploration/Actions Min                               -0.456449\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          794.261\n",
      "exploration/env_infos/final/reward_forward Mean       -0.00126701\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.00126701\n",
      "exploration/env_infos/final/reward_forward Min        -0.00126701\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.190426\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.190426\n",
      "exploration/env_infos/initial/reward_forward Min      -0.190426\n",
      "exploration/env_infos/reward_forward Mean             -0.00504873\n",
      "exploration/env_infos/reward_forward Std               0.080263\n",
      "exploration/env_infos/reward_forward Max               1.0651\n",
      "exploration/env_infos/reward_forward Min              -0.500678\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.311866\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.311866\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.311866\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.114382\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.114382\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.114382\n",
      "exploration/env_infos/reward_ctrl Mean                -0.212293\n",
      "exploration/env_infos/reward_ctrl Std                  0.054754\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0144815\n",
      "exploration/env_infos/reward_ctrl Min                 -0.387555\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00063018\n",
      "exploration/env_infos/final/torso_velocity Std         0.00077507\n",
      "exploration/env_infos/final/torso_velocity Max         0.00046085\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00126701\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.0106277\n",
      "exploration/env_infos/initial/torso_velocity Std       0.176309\n",
      "exploration/env_infos/initial/torso_velocity Max       0.228877\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.190426\n",
      "exploration/env_infos/torso_velocity Mean             -0.00145567\n",
      "exploration/env_infos/torso_velocity Std               0.0776224\n",
      "exploration/env_infos/torso_velocity Max               1.0651\n",
      "exploration/env_infos/torso_velocity Min              -1.81287\n",
      "evaluation/num steps total                             1.275e+06\n",
      "evaluation/num paths total                          1275\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.881357\n",
      "evaluation/Rewards Std                                 0.104319\n",
      "evaluation/Rewards Max                                 2.17413\n",
      "evaluation/Rewards Min                                 0.526067\n",
      "evaluation/Returns Mean                              881.357\n",
      "evaluation/Returns Std                                94.9669\n",
      "evaluation/Returns Max                               965.601\n",
      "evaluation/Returns Min                               619.047\n",
      "evaluation/Actions Mean                                0.0881448\n",
      "evaluation/Actions Std                                 0.149437\n",
      "evaluation/Actions Max                                 0.593742\n",
      "evaluation/Actions Min                                -0.486493\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           881.357\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.78193e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.99515e-07\n",
      "evaluation/env_infos/final/reward_forward Max          4.25631e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -9.83929e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0270118\n",
      "evaluation/env_infos/initial/reward_forward Std        0.11926\n",
      "evaluation/env_infos/initial/reward_forward Max        0.227664\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.238504\n",
      "evaluation/env_infos/reward_forward Mean               0.000173155\n",
      "evaluation/env_infos/reward_forward Std                0.0580629\n",
      "evaluation/env_infos/reward_forward Max                1.62849\n",
      "evaluation/env_infos/reward_forward Min               -1.07509\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.122018\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0974989\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0348938\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.386508\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0403675\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0146679\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0146476\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0763469\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.120404\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0972146\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0146476\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.473933\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -5.45583e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.57126e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.07659e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.83929e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.153389\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.208972\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.536815\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.238504\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000452173\n",
      "evaluation/env_infos/torso_velocity Std                0.0657907\n",
      "evaluation/env_infos/torso_velocity Max                1.62849\n",
      "evaluation/env_infos/torso_velocity Min               -1.78002\n",
      "time/data storing (s)                                  0.0164962\n",
      "time/evaluation sampling (s)                          51.911\n",
      "time/exploration sampling (s)                          2.39646\n",
      "time/logging (s)                                       0.356451\n",
      "time/saving (s)                                        0.0597139\n",
      "time/training (s)                                      4.69159\n",
      "time/epoch (s)                                        59.4317\n",
      "time/total (s)                                      2935.48\n",
      "Epoch                                                 50\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:00:14.611727 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 51 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 53000\n",
      "trainer/QF1 Loss                                       0.172967\n",
      "trainer/QF2 Loss                                       0.292082\n",
      "trainer/Policy Loss                                  -10.6291\n",
      "trainer/Q1 Predictions Mean                           18.479\n",
      "trainer/Q1 Predictions Std                             1.4154\n",
      "trainer/Q1 Predictions Max                            20.4234\n",
      "trainer/Q1 Predictions Min                            13.2158\n",
      "trainer/Q2 Predictions Mean                           18.637\n",
      "trainer/Q2 Predictions Std                             1.43095\n",
      "trainer/Q2 Predictions Max                            20.2569\n",
      "trainer/Q2 Predictions Min                            12.9785\n",
      "trainer/Q Targets Mean                                18.4731\n",
      "trainer/Q Targets Std                                  1.37841\n",
      "trainer/Q Targets Max                                 20.3824\n",
      "trainer/Q Targets Min                                 13.843\n",
      "trainer/Log Pis Mean                                   8.1667\n",
      "trainer/Log Pis Std                                    2.40751\n",
      "trainer/Log Pis Max                                   19.373\n",
      "trainer/Log Pis Min                                    1.492\n",
      "trainer/Policy mu Mean                                 0.00235617\n",
      "trainer/Policy mu Std                                  0.168893\n",
      "trainer/Policy mu Max                                  1.11953\n",
      "trainer/Policy mu Min                                 -0.78004\n",
      "trainer/Policy log std Mean                           -2.39209\n",
      "trainer/Policy log std Std                             0.218604\n",
      "trainer/Policy log std Max                            -1.6783\n",
      "trainer/Policy log std Min                            -3.65929\n",
      "trainer/Alpha                                          0.0100101\n",
      "trainer/Alpha Loss                                     0.767437\n",
      "exploration/num steps total                        53000\n",
      "exploration/num paths total                          105\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.980825\n",
      "exploration/Rewards Std                                0.196827\n",
      "exploration/Rewards Max                                2.47388\n",
      "exploration/Rewards Min                                0.459584\n",
      "exploration/Returns Mean                             980.825\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              980.825\n",
      "exploration/Returns Min                              980.825\n",
      "exploration/Actions Mean                               0.000365753\n",
      "exploration/Actions Std                                0.134897\n",
      "exploration/Actions Max                                0.711795\n",
      "exploration/Actions Min                               -0.469814\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          980.825\n",
      "exploration/env_infos/final/reward_forward Mean        0.130533\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.130533\n",
      "exploration/env_infos/final/reward_forward Min         0.130533\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0289918\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0289918\n",
      "exploration/env_infos/initial/reward_forward Min       0.0289918\n",
      "exploration/env_infos/reward_forward Mean             -0.000782258\n",
      "exploration/env_infos/reward_forward Std               0.314915\n",
      "exploration/env_infos/reward_forward Max               1.01698\n",
      "exploration/env_infos/reward_forward Min              -1.3453\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0442638\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0442638\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0442638\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0778665\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0778665\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0778665\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0727895\n",
      "exploration/env_infos/reward_ctrl Std                  0.0568165\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00864011\n",
      "exploration/env_infos/reward_ctrl Min                 -0.540416\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.112188\n",
      "exploration/env_infos/final/torso_velocity Std         0.0360563\n",
      "exploration/env_infos/final/torso_velocity Max         0.144219\n",
      "exploration/env_infos/final/torso_velocity Min         0.0618127\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.262614\n",
      "exploration/env_infos/initial/torso_velocity Std       0.26418\n",
      "exploration/env_infos/initial/torso_velocity Max       0.631917\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0289918\n",
      "exploration/env_infos/torso_velocity Mean              0.0118123\n",
      "exploration/env_infos/torso_velocity Std               0.329271\n",
      "exploration/env_infos/torso_velocity Max               1.47805\n",
      "exploration/env_infos/torso_velocity Min              -1.49746\n",
      "evaluation/num steps total                             1.3e+06\n",
      "evaluation/num paths total                          1300\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.952223\n",
      "evaluation/Rewards Std                                 0.0394847\n",
      "evaluation/Rewards Max                                 2.31813\n",
      "evaluation/Rewards Min                                 0.34102\n",
      "evaluation/Returns Mean                              952.223\n",
      "evaluation/Returns Std                                21.6734\n",
      "evaluation/Returns Max                               981.299\n",
      "evaluation/Returns Min                               912.235\n",
      "evaluation/Actions Mean                               -0.00794176\n",
      "evaluation/Actions Std                                 0.110949\n",
      "evaluation/Actions Max                                 0.658426\n",
      "evaluation/Actions Min                                -0.622053\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           952.223\n",
      "evaluation/env_infos/final/reward_forward Mean        -4.54302e-08\n",
      "evaluation/env_infos/final/reward_forward Std          5.53134e-07\n",
      "evaluation/env_infos/final/reward_forward Max          1.08223e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -1.50259e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0139977\n",
      "evaluation/env_infos/initial/reward_forward Std        0.118154\n",
      "evaluation/env_infos/initial/reward_forward Max        0.265758\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.162314\n",
      "evaluation/env_infos/reward_forward Mean              -0.00128974\n",
      "evaluation/env_infos/reward_forward Std                0.0715775\n",
      "evaluation/env_infos/reward_forward Max                1.67476\n",
      "evaluation/env_infos/reward_forward Min               -1.32812\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0485086\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0222538\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0189237\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.088947\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0409214\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0122624\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0214459\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0724758\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0494907\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0265211\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00903686\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.65898\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         4.70591e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          4.28315e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.08223e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.50259e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.15751\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.272509\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.838649\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.237053\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00157566\n",
      "evaluation/env_infos/torso_velocity Std                0.0776002\n",
      "evaluation/env_infos/torso_velocity Max                1.67476\n",
      "evaluation/env_infos/torso_velocity Min               -1.82316\n",
      "time/data storing (s)                                  0.0168182\n",
      "time/evaluation sampling (s)                          49.6335\n",
      "time/exploration sampling (s)                          2.32502\n",
      "time/logging (s)                                       0.285588\n",
      "time/saving (s)                                        0.028842\n",
      "time/training (s)                                      4.38625\n",
      "time/epoch (s)                                        56.676\n",
      "time/total (s)                                      2992.76\n",
      "Epoch                                                 51\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:01:14.975174 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 52 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 54000\n",
      "trainer/QF1 Loss                                       0.24082\n",
      "trainer/QF2 Loss                                       0.419447\n",
      "trainer/Policy Loss                                  -11.4323\n",
      "trainer/Q1 Predictions Mean                           18.8489\n",
      "trainer/Q1 Predictions Std                             1.57999\n",
      "trainer/Q1 Predictions Max                            20.4032\n",
      "trainer/Q1 Predictions Min                             6.92115\n",
      "trainer/Q2 Predictions Mean                           19.1488\n",
      "trainer/Q2 Predictions Std                             1.51791\n",
      "trainer/Q2 Predictions Max                            20.7674\n",
      "trainer/Q2 Predictions Min                             8.73392\n",
      "trainer/Q Targets Mean                                18.8041\n",
      "trainer/Q Targets Std                                  1.61825\n",
      "trainer/Q Targets Max                                 21.4048\n",
      "trainer/Q Targets Min                                  6.05544\n",
      "trainer/Log Pis Mean                                   7.71289\n",
      "trainer/Log Pis Std                                    2.72685\n",
      "trainer/Log Pis Max                                   21.0909\n",
      "trainer/Log Pis Min                                   -1.29014\n",
      "trainer/Policy mu Mean                                -0.00789896\n",
      "trainer/Policy mu Std                                  0.160245\n",
      "trainer/Policy mu Max                                  0.992724\n",
      "trainer/Policy mu Min                                 -1.16367\n",
      "trainer/Policy log std Mean                           -2.37245\n",
      "trainer/Policy log std Std                             0.21422\n",
      "trainer/Policy log std Max                            -1.66613\n",
      "trainer/Policy log std Min                            -3.6431\n",
      "trainer/Alpha                                          0.00991454\n",
      "trainer/Alpha Loss                                    -1.32454\n",
      "exploration/num steps total                        54000\n",
      "exploration/num paths total                          106\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.953752\n",
      "exploration/Rewards Std                                0.109084\n",
      "exploration/Rewards Max                                1.68123\n",
      "exploration/Rewards Min                                0.680503\n",
      "exploration/Returns Mean                             953.752\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              953.752\n",
      "exploration/Returns Min                              953.752\n",
      "exploration/Actions Mean                               0.0249768\n",
      "exploration/Actions Std                                0.134023\n",
      "exploration/Actions Max                                0.452486\n",
      "exploration/Actions Min                               -0.49885\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          953.752\n",
      "exploration/env_infos/final/reward_forward Mean        0.0590484\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0590484\n",
      "exploration/env_infos/final/reward_forward Min         0.0590484\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0966889\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0966889\n",
      "exploration/env_infos/initial/reward_forward Min       0.0966889\n",
      "exploration/env_infos/reward_forward Mean             -0.0178167\n",
      "exploration/env_infos/reward_forward Std               0.143816\n",
      "exploration/env_infos/reward_forward Max               0.846798\n",
      "exploration/env_infos/reward_forward Min              -0.633273\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0751466\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0751466\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0751466\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.043525\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.043525\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.043525\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0743435\n",
      "exploration/env_infos/reward_ctrl Std                  0.0379649\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00721775\n",
      "exploration/env_infos/reward_ctrl Min                 -0.319497\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0139785\n",
      "exploration/env_infos/final/torso_velocity Std         0.0617104\n",
      "exploration/env_infos/final/torso_velocity Max         0.0590484\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0918752\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.170948\n",
      "exploration/env_infos/initial/torso_velocity Std       0.223519\n",
      "exploration/env_infos/initial/torso_velocity Max       0.47417\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0580148\n",
      "exploration/env_infos/torso_velocity Mean             -0.00113717\n",
      "exploration/env_infos/torso_velocity Std               0.181284\n",
      "exploration/env_infos/torso_velocity Max               0.950227\n",
      "exploration/env_infos/torso_velocity Min              -0.8726\n",
      "evaluation/num steps total                             1.325e+06\n",
      "evaluation/num paths total                          1325\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.963238\n",
      "evaluation/Rewards Std                                 0.0328487\n",
      "evaluation/Rewards Max                                 2.34999\n",
      "evaluation/Rewards Min                                 0.607778\n",
      "evaluation/Returns Mean                              963.238\n",
      "evaluation/Returns Std                                18.3465\n",
      "evaluation/Returns Max                               981.212\n",
      "evaluation/Returns Min                               910.453\n",
      "evaluation/Actions Mean                               -0.0108508\n",
      "evaluation/Actions Std                                 0.0969252\n",
      "evaluation/Actions Max                                 0.557162\n",
      "evaluation/Actions Min                                -0.643027\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           963.238\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.46154e-06\n",
      "evaluation/env_infos/final/reward_forward Std          7.42563e-06\n",
      "evaluation/env_infos/final/reward_forward Max          9.9324e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -3.76571e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0132891\n",
      "evaluation/env_infos/initial/reward_forward Std        0.124706\n",
      "evaluation/env_infos/initial/reward_forward Max        0.194819\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.287095\n",
      "evaluation/env_infos/reward_forward Mean               0.000188691\n",
      "evaluation/env_infos/reward_forward Std                0.0550864\n",
      "evaluation/env_infos/reward_forward Max                1.33909\n",
      "evaluation/env_infos/reward_forward Min               -1.32433\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0375622\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0178588\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0201675\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0902538\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0236039\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00752116\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0127266\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0416026\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.038049\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0201587\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00827672\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.392222\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -8.28292e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          5.35093e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          2.03253e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -3.76571e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.132448\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.244158\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.574993\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.287095\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00110342\n",
      "evaluation/env_infos/torso_velocity Std                0.0632793\n",
      "evaluation/env_infos/torso_velocity Max                1.33909\n",
      "evaluation/env_infos/torso_velocity Min               -1.8743\n",
      "time/data storing (s)                                  0.01501\n",
      "time/evaluation sampling (s)                          51.5477\n",
      "time/exploration sampling (s)                          1.96392\n",
      "time/logging (s)                                       0.303319\n",
      "time/saving (s)                                        0.0314599\n",
      "time/training (s)                                      5.97112\n",
      "time/epoch (s)                                        59.8326\n",
      "time/total (s)                                      3053.14\n",
      "Epoch                                                 52\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:02:08.471489 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 53 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 55000\n",
      "trainer/QF1 Loss                                       0.168491\n",
      "trainer/QF2 Loss                                       0.175748\n",
      "trainer/Policy Loss                                  -12.0837\n",
      "trainer/Q1 Predictions Mean                           19.3765\n",
      "trainer/Q1 Predictions Std                             1.11846\n",
      "trainer/Q1 Predictions Max                            20.9122\n",
      "trainer/Q1 Predictions Min                            14.5412\n",
      "trainer/Q2 Predictions Mean                           19.4421\n",
      "trainer/Q2 Predictions Std                             1.16598\n",
      "trainer/Q2 Predictions Max                            20.9338\n",
      "trainer/Q2 Predictions Min                            12.0521\n",
      "trainer/Q Targets Mean                                19.2811\n",
      "trainer/Q Targets Std                                  1.1953\n",
      "trainer/Q Targets Max                                 22.0294\n",
      "trainer/Q Targets Min                                 11.8352\n",
      "trainer/Log Pis Mean                                   7.45463\n",
      "trainer/Log Pis Std                                    2.23424\n",
      "trainer/Log Pis Max                                   14.4062\n",
      "trainer/Log Pis Min                                    1.55325\n",
      "trainer/Policy mu Mean                                 0.0269962\n",
      "trainer/Policy mu Std                                  0.122736\n",
      "trainer/Policy mu Max                                  1.30841\n",
      "trainer/Policy mu Min                                 -0.618146\n",
      "trainer/Policy log std Mean                           -2.3376\n",
      "trainer/Policy log std Std                             0.188858\n",
      "trainer/Policy log std Max                            -1.5789\n",
      "trainer/Policy log std Min                            -3.11741\n",
      "trainer/Alpha                                          0.0101032\n",
      "trainer/Alpha Loss                                    -2.50558\n",
      "exploration/num steps total                        55000\n",
      "exploration/num paths total                          107\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.947996\n",
      "exploration/Rewards Std                                0.0569445\n",
      "exploration/Rewards Max                                1.3671\n",
      "exploration/Rewards Min                                0.78174\n",
      "exploration/Returns Mean                             947.996\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              947.996\n",
      "exploration/Returns Min                              947.996\n",
      "exploration/Actions Mean                               0.0161401\n",
      "exploration/Actions Std                                0.124239\n",
      "exploration/Actions Max                                0.459342\n",
      "exploration/Actions Min                               -0.564172\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          947.996\n",
      "exploration/env_infos/final/reward_forward Mean        0.347333\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.347333\n",
      "exploration/env_infos/final/reward_forward Min         0.347333\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0555068\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0555068\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0555068\n",
      "exploration/env_infos/reward_forward Mean              0.0220281\n",
      "exploration/env_infos/reward_forward Std               0.14172\n",
      "exploration/env_infos/reward_forward Max               0.631463\n",
      "exploration/env_infos/reward_forward Min              -0.984707\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0934729\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0934729\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0934729\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0279932\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0279932\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0279932\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0627837\n",
      "exploration/env_infos/reward_ctrl Std                  0.0286298\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00650894\n",
      "exploration/env_infos/reward_ctrl Min                 -0.21826\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.239638\n",
      "exploration/env_infos/final/torso_velocity Std         0.0776974\n",
      "exploration/env_infos/final/torso_velocity Max         0.347333\n",
      "exploration/env_infos/final/torso_velocity Min         0.166906\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.134134\n",
      "exploration/env_infos/initial/torso_velocity Std       0.237096\n",
      "exploration/env_infos/initial/torso_velocity Max       0.468431\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0555068\n",
      "exploration/env_infos/torso_velocity Mean             -0.014012\n",
      "exploration/env_infos/torso_velocity Std               0.179115\n",
      "exploration/env_infos/torso_velocity Max               0.631463\n",
      "exploration/env_infos/torso_velocity Min              -1.6817\n",
      "evaluation/num steps total                             1.35e+06\n",
      "evaluation/num paths total                          1350\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.967333\n",
      "evaluation/Rewards Std                                 0.0308059\n",
      "evaluation/Rewards Max                                 2.53959\n",
      "evaluation/Rewards Min                                 0.526836\n",
      "evaluation/Returns Mean                              967.333\n",
      "evaluation/Returns Std                                10.6121\n",
      "evaluation/Returns Max                               979.997\n",
      "evaluation/Returns Min                               948.494\n",
      "evaluation/Actions Mean                                0.0169482\n",
      "evaluation/Actions Std                                 0.0903955\n",
      "evaluation/Actions Max                                 0.530308\n",
      "evaluation/Actions Min                                -0.568476\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           967.333\n",
      "evaluation/env_infos/final/reward_forward Mean         4.01473e-06\n",
      "evaluation/env_infos/final/reward_forward Std          1.68541e-05\n",
      "evaluation/env_infos/final/reward_forward Max          8.44685e-05\n",
      "evaluation/env_infos/final/reward_forward Min         -1.05582e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0363233\n",
      "evaluation/env_infos/initial/reward_forward Std        0.110859\n",
      "evaluation/env_infos/initial/reward_forward Max        0.279002\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.196715\n",
      "evaluation/env_infos/reward_forward Mean               0.00280852\n",
      "evaluation/env_infos/reward_forward Std                0.0597651\n",
      "evaluation/env_infos/reward_forward Max                1.71815\n",
      "evaluation/env_infos/reward_forward Min               -0.920299\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0329321\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0102691\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0199895\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.050879\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0310531\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00995479\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0177209\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0556868\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0338344\n",
      "evaluation/env_infos/reward_ctrl Std                   0.016588\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0140268\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.607336\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.60002e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          2.62345e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000126958\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000125853\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.152642\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.234344\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.713455\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.369112\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000345611\n",
      "evaluation/env_infos/torso_velocity Std                0.0635611\n",
      "evaluation/env_infos/torso_velocity Max                1.71815\n",
      "evaluation/env_infos/torso_velocity Min               -1.93053\n",
      "time/data storing (s)                                  0.0146662\n",
      "time/evaluation sampling (s)                          45.953\n",
      "time/exploration sampling (s)                          2.00575\n",
      "time/logging (s)                                       0.279576\n",
      "time/saving (s)                                        0.0288875\n",
      "time/training (s)                                      4.58374\n",
      "time/epoch (s)                                        52.8656\n",
      "time/total (s)                                      3106.62\n",
      "Epoch                                                 53\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:03:08.368590 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 54 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 56000\n",
      "trainer/QF1 Loss                                       0.166455\n",
      "trainer/QF2 Loss                                       0.222114\n",
      "trainer/Policy Loss                                  -11.667\n",
      "trainer/Q1 Predictions Mean                           19.3608\n",
      "trainer/Q1 Predictions Std                             1.33274\n",
      "trainer/Q1 Predictions Max                            20.9963\n",
      "trainer/Q1 Predictions Min                            14.0666\n",
      "trainer/Q2 Predictions Mean                           19.4443\n",
      "trainer/Q2 Predictions Std                             1.34328\n",
      "trainer/Q2 Predictions Max                            21.0908\n",
      "trainer/Q2 Predictions Min                            13.3736\n",
      "trainer/Q Targets Mean                                19.4531\n",
      "trainer/Q Targets Std                                  1.28748\n",
      "trainer/Q Targets Max                                 20.9617\n",
      "trainer/Q Targets Min                                 13.9213\n",
      "trainer/Log Pis Mean                                   7.89526\n",
      "trainer/Log Pis Std                                    2.55872\n",
      "trainer/Log Pis Max                                   17.422\n",
      "trainer/Log Pis Min                                    1.55971\n",
      "trainer/Policy mu Mean                                -0.0229646\n",
      "trainer/Policy mu Std                                  0.192771\n",
      "trainer/Policy mu Max                                  1.61364\n",
      "trainer/Policy mu Min                                 -1.03039\n",
      "trainer/Policy log std Mean                           -2.3506\n",
      "trainer/Policy log std Std                             0.202678\n",
      "trainer/Policy log std Max                            -1.43069\n",
      "trainer/Policy log std Min                            -3.53514\n",
      "trainer/Alpha                                          0.00958068\n",
      "trainer/Alpha Loss                                    -0.486823\n",
      "exploration/num steps total                        56000\n",
      "exploration/num paths total                          108\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.931786\n",
      "exploration/Rewards Std                                0.0762986\n",
      "exploration/Rewards Max                                1.54088\n",
      "exploration/Rewards Min                                0.710259\n",
      "exploration/Returns Mean                             931.786\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              931.786\n",
      "exploration/Returns Min                              931.786\n",
      "exploration/Actions Mean                              -0.0154054\n",
      "exploration/Actions Std                                0.151627\n",
      "exploration/Actions Max                                0.396215\n",
      "exploration/Actions Min                               -0.488133\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          931.786\n",
      "exploration/env_infos/final/reward_forward Mean        0.0148811\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0148811\n",
      "exploration/env_infos/final/reward_forward Min         0.0148811\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0800983\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0800983\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0800983\n",
      "exploration/env_infos/reward_forward Mean             -0.0114636\n",
      "exploration/env_infos/reward_forward Std               0.0949724\n",
      "exploration/env_infos/reward_forward Max               0.554546\n",
      "exploration/env_infos/reward_forward Min              -0.45459\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0971965\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0971965\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0971965\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0544999\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0544999\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0544999\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0929119\n",
      "exploration/env_infos/reward_ctrl Std                  0.0342589\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0148343\n",
      "exploration/env_infos/reward_ctrl Min                 -0.289741\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0243604\n",
      "exploration/env_infos/final/torso_velocity Std         0.00767004\n",
      "exploration/env_infos/final/torso_velocity Max         0.0336664\n",
      "exploration/env_infos/final/torso_velocity Min         0.0148811\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0429174\n",
      "exploration/env_infos/initial/torso_velocity Std       0.155943\n",
      "exploration/env_infos/initial/torso_velocity Max       0.262943\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0800983\n",
      "exploration/env_infos/torso_velocity Mean             -0.0145733\n",
      "exploration/env_infos/torso_velocity Std               0.125402\n",
      "exploration/env_infos/torso_velocity Max               0.589857\n",
      "exploration/env_infos/torso_velocity Min              -1.78034\n",
      "evaluation/num steps total                             1.375e+06\n",
      "evaluation/num paths total                          1375\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.928172\n",
      "evaluation/Rewards Std                                 0.0363865\n",
      "evaluation/Rewards Max                                 2.03917\n",
      "evaluation/Rewards Min                                -0.0523905\n",
      "evaluation/Returns Mean                              928.172\n",
      "evaluation/Returns Std                                24.382\n",
      "evaluation/Returns Max                               953.808\n",
      "evaluation/Returns Min                               830.308\n",
      "evaluation/Actions Mean                               -0.0215441\n",
      "evaluation/Actions Std                                 0.132724\n",
      "evaluation/Actions Max                                 0.697916\n",
      "evaluation/Actions Min                                -0.836806\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           928.172\n",
      "evaluation/env_infos/final/reward_forward Mean         6.63733e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.36053e-06\n",
      "evaluation/env_infos/final/reward_forward Max          9.05831e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -9.73663e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0139355\n",
      "evaluation/env_infos/initial/reward_forward Std        0.101929\n",
      "evaluation/env_infos/initial/reward_forward Max        0.202845\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.19283\n",
      "evaluation/env_infos/reward_forward Mean               0.000291908\n",
      "evaluation/env_infos/reward_forward Std                0.0462667\n",
      "evaluation/env_infos/reward_forward Max                1.38993\n",
      "evaluation/env_infos/reward_forward Min               -1.38314\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0712294\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0234294\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0458807\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.163205\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0568926\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0231591\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0330542\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.113315\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0723191\n",
      "evaluation/env_infos/reward_ctrl Std                   0.033456\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0192624\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.05239\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         5.28271e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          2.33965e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          1.55781e-05\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.04007e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.101014\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.230944\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.625512\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.354624\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00198667\n",
      "evaluation/env_infos/torso_velocity Std                0.053869\n",
      "evaluation/env_infos/torso_velocity Max                1.38993\n",
      "evaluation/env_infos/torso_velocity Min               -1.75594\n",
      "time/data storing (s)                                  0.016448\n",
      "time/evaluation sampling (s)                          51.8468\n",
      "time/exploration sampling (s)                          2.22276\n",
      "time/logging (s)                                       0.310734\n",
      "time/saving (s)                                        0.031582\n",
      "time/training (s)                                      4.97292\n",
      "time/epoch (s)                                        59.4013\n",
      "time/total (s)                                      3166.54\n",
      "Epoch                                                 54\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:04:04.996831 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 55 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 57000\n",
      "trainer/QF1 Loss                                       0.176069\n",
      "trainer/QF2 Loss                                       0.34989\n",
      "trainer/Policy Loss                                  -11.6074\n",
      "trainer/Q1 Predictions Mean                           19.8601\n",
      "trainer/Q1 Predictions Std                             1.3948\n",
      "trainer/Q1 Predictions Max                            22.8153\n",
      "trainer/Q1 Predictions Min                            14.8937\n",
      "trainer/Q2 Predictions Mean                           19.7664\n",
      "trainer/Q2 Predictions Std                             1.53932\n",
      "trainer/Q2 Predictions Max                            22.3223\n",
      "trainer/Q2 Predictions Min                            11.7833\n",
      "trainer/Q Targets Mean                                19.7903\n",
      "trainer/Q Targets Std                                  1.38594\n",
      "trainer/Q Targets Max                                 22.5419\n",
      "trainer/Q Targets Min                                 14.6841\n",
      "trainer/Log Pis Mean                                   8.3948\n",
      "trainer/Log Pis Std                                    2.73422\n",
      "trainer/Log Pis Max                                   23.4691\n",
      "trainer/Log Pis Min                                    0.0704582\n",
      "trainer/Policy mu Mean                                -0.011051\n",
      "trainer/Policy mu Std                                  0.148469\n",
      "trainer/Policy mu Max                                  1.23663\n",
      "trainer/Policy mu Min                                 -0.97136\n",
      "trainer/Policy log std Mean                           -2.42685\n",
      "trainer/Policy log std Std                             0.219558\n",
      "trainer/Policy log std Max                            -1.6934\n",
      "trainer/Policy log std Min                            -4.04879\n",
      "trainer/Alpha                                          0.00964972\n",
      "trainer/Alpha Loss                                     1.8324\n",
      "exploration/num steps total                        57000\n",
      "exploration/num paths total                          109\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.929555\n",
      "exploration/Rewards Std                                0.0321576\n",
      "exploration/Rewards Max                                1.06138\n",
      "exploration/Rewards Min                                0.736283\n",
      "exploration/Returns Mean                             929.555\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              929.555\n",
      "exploration/Returns Min                              929.555\n",
      "exploration/Actions Mean                              -0.0106047\n",
      "exploration/Actions Std                                0.134088\n",
      "exploration/Actions Max                                0.395865\n",
      "exploration/Actions Min                               -0.572088\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          929.555\n",
      "exploration/env_infos/final/reward_forward Mean       -0.460529\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.460529\n",
      "exploration/env_infos/final/reward_forward Min        -0.460529\n",
      "exploration/env_infos/initial/reward_forward Mean      0.060889\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.060889\n",
      "exploration/env_infos/initial/reward_forward Min       0.060889\n",
      "exploration/env_infos/reward_forward Mean             -0.0233868\n",
      "exploration/env_infos/reward_forward Std               0.145541\n",
      "exploration/env_infos/reward_forward Max               0.832362\n",
      "exploration/env_infos/reward_forward Min              -0.753871\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0437926\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0437926\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0437926\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0226295\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0226295\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0226295\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0723684\n",
      "exploration/env_infos/reward_ctrl Std                  0.0300962\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00961277\n",
      "exploration/env_infos/reward_ctrl Min                 -0.263717\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.319277\n",
      "exploration/env_infos/final/torso_velocity Std         0.107238\n",
      "exploration/env_infos/final/torso_velocity Max        -0.200842\n",
      "exploration/env_infos/final/torso_velocity Min        -0.460529\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.11012\n",
      "exploration/env_infos/initial/torso_velocity Std       0.278204\n",
      "exploration/env_infos/initial/torso_velocity Max       0.472786\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.203315\n",
      "exploration/env_infos/torso_velocity Mean             -0.00947187\n",
      "exploration/env_infos/torso_velocity Std               0.114072\n",
      "exploration/env_infos/torso_velocity Max               0.832362\n",
      "exploration/env_infos/torso_velocity Min              -1.68361\n",
      "evaluation/num steps total                             1.4e+06\n",
      "evaluation/num paths total                          1400\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.947178\n",
      "evaluation/Rewards Std                                 0.0209559\n",
      "evaluation/Rewards Max                                 1.87713\n",
      "evaluation/Rewards Min                                 0.348358\n",
      "evaluation/Returns Mean                              947.178\n",
      "evaluation/Returns Std                                12.3399\n",
      "evaluation/Returns Max                               974.927\n",
      "evaluation/Returns Min                               923.997\n",
      "evaluation/Actions Mean                               -0.0262764\n",
      "evaluation/Actions Std                                 0.112602\n",
      "evaluation/Actions Max                                 0.456465\n",
      "evaluation/Actions Min                                -0.855894\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           947.178\n",
      "evaluation/env_infos/final/reward_forward Mean        -4.17274e-07\n",
      "evaluation/env_infos/final/reward_forward Std          4.58032e-07\n",
      "evaluation/env_infos/final/reward_forward Max          9.78566e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -1.09536e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0575272\n",
      "evaluation/env_infos/initial/reward_forward Std        0.156144\n",
      "evaluation/env_infos/initial/reward_forward Max        0.310063\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.25307\n",
      "evaluation/env_infos/reward_forward Mean              -1.30923e-05\n",
      "evaluation/env_infos/reward_forward Std                0.051862\n",
      "evaluation/env_infos/reward_forward Max                1.21856\n",
      "evaluation/env_infos/reward_forward Min               -0.790121\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.053202\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0127489\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0247616\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.076488\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0403498\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0142957\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0241356\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0868479\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.053479\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0148523\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0145872\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.651642\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.08201e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          3.61916e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.78566e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.09536e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.161775\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.228857\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.752238\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.25307\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00173917\n",
      "evaluation/env_infos/torso_velocity Std                0.0549408\n",
      "evaluation/env_infos/torso_velocity Max                1.21856\n",
      "evaluation/env_infos/torso_velocity Min               -1.70046\n",
      "time/data storing (s)                                  0.015501\n",
      "time/evaluation sampling (s)                          48.8002\n",
      "time/exploration sampling (s)                          2.46122\n",
      "time/logging (s)                                       0.297061\n",
      "time/saving (s)                                        0.0278463\n",
      "time/training (s)                                      4.3599\n",
      "time/epoch (s)                                        55.9617\n",
      "time/total (s)                                      3223.16\n",
      "Epoch                                                 55\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:04:59.106039 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 56 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 58000\n",
      "trainer/QF1 Loss                                       0.461982\n",
      "trainer/QF2 Loss                                       0.697484\n",
      "trainer/Policy Loss                                  -12.1594\n",
      "trainer/Q1 Predictions Mean                           20.2286\n",
      "trainer/Q1 Predictions Std                             1.31264\n",
      "trainer/Q1 Predictions Max                            22.2177\n",
      "trainer/Q1 Predictions Min                            13.0759\n",
      "trainer/Q2 Predictions Mean                           20.0923\n",
      "trainer/Q2 Predictions Std                             1.28365\n",
      "trainer/Q2 Predictions Max                            21.6491\n",
      "trainer/Q2 Predictions Min                            14.4777\n",
      "trainer/Q Targets Mean                                20.0926\n",
      "trainer/Q Targets Std                                  1.62534\n",
      "trainer/Q Targets Max                                 22.0213\n",
      "trainer/Q Targets Min                                  4.63969\n",
      "trainer/Log Pis Mean                                   8.19998\n",
      "trainer/Log Pis Std                                    2.54967\n",
      "trainer/Log Pis Max                                   14.9256\n",
      "trainer/Log Pis Min                                   -1.56577\n",
      "trainer/Policy mu Mean                                -0.0458164\n",
      "trainer/Policy mu Std                                  0.175818\n",
      "trainer/Policy mu Max                                  0.731385\n",
      "trainer/Policy mu Min                                 -1.54892\n",
      "trainer/Policy log std Mean                           -2.43666\n",
      "trainer/Policy log std Std                             0.207569\n",
      "trainer/Policy log std Max                            -1.23184\n",
      "trainer/Policy log std Min                            -3.27302\n",
      "trainer/Alpha                                          0.00936557\n",
      "trainer/Alpha Loss                                     0.934108\n",
      "exploration/num steps total                        58000\n",
      "exploration/num paths total                          110\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.955669\n",
      "exploration/Rewards Std                                0.0719173\n",
      "exploration/Rewards Max                                1.45639\n",
      "exploration/Rewards Min                                0.381255\n",
      "exploration/Returns Mean                             955.669\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              955.669\n",
      "exploration/Returns Min                              955.669\n",
      "exploration/Actions Mean                              -0.0126497\n",
      "exploration/Actions Std                                0.122811\n",
      "exploration/Actions Max                                0.432585\n",
      "exploration/Actions Min                               -0.799633\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          955.669\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0497564\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0497564\n",
      "exploration/env_infos/final/reward_forward Min        -0.0497564\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0708588\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0708588\n",
      "exploration/env_infos/initial/reward_forward Min       0.0708588\n",
      "exploration/env_infos/reward_forward Mean             -0.000710077\n",
      "exploration/env_infos/reward_forward Std               0.126075\n",
      "exploration/env_infos/reward_forward Max               0.680488\n",
      "exploration/env_infos/reward_forward Min              -0.648025\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0506031\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0506031\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0506031\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0688965\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0688965\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0688965\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0609699\n",
      "exploration/env_infos/reward_ctrl Std                  0.0387993\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00558874\n",
      "exploration/env_infos/reward_ctrl Min                 -0.618745\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.00358533\n",
      "exploration/env_infos/final/torso_velocity Std         0.0382133\n",
      "exploration/env_infos/final/torso_velocity Max         0.0377648\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0497564\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.169423\n",
      "exploration/env_infos/initial/torso_velocity Std       0.144795\n",
      "exploration/env_infos/initial/torso_velocity Max       0.374148\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0632637\n",
      "exploration/env_infos/torso_velocity Mean              0.00207266\n",
      "exploration/env_infos/torso_velocity Std               0.13298\n",
      "exploration/env_infos/torso_velocity Max               1.01675\n",
      "exploration/env_infos/torso_velocity Min              -1.54177\n",
      "evaluation/num steps total                             1.425e+06\n",
      "evaluation/num paths total                          1425\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.903135\n",
      "evaluation/Rewards Std                                 0.0647322\n",
      "evaluation/Rewards Max                                 2.29888\n",
      "evaluation/Rewards Min                                 0.0552096\n",
      "evaluation/Returns Mean                              903.135\n",
      "evaluation/Returns Std                                54.0563\n",
      "evaluation/Returns Max                               972.009\n",
      "evaluation/Returns Min                               795.806\n",
      "evaluation/Actions Mean                               -0.0628057\n",
      "evaluation/Actions Std                                 0.14333\n",
      "evaluation/Actions Max                                 0.53632\n",
      "evaluation/Actions Min                                -0.864683\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           903.135\n",
      "evaluation/env_infos/final/reward_forward Mean         9.0275e-08\n",
      "evaluation/env_infos/final/reward_forward Std          4.12764e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.55788e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -8.44701e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0292989\n",
      "evaluation/env_infos/initial/reward_forward Std        0.126329\n",
      "evaluation/env_infos/initial/reward_forward Max        0.262029\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.221043\n",
      "evaluation/env_infos/reward_forward Mean               0.00256648\n",
      "evaluation/env_infos/reward_forward Std                0.0611273\n",
      "evaluation/env_infos/reward_forward Max                1.45327\n",
      "evaluation/env_infos/reward_forward Min               -0.801579\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0960161\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0540156\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0280629\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.201718\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0221973\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0114099\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0101356\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.052668\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0979525\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0596257\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0101356\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.94479\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.65524e-10\n",
      "evaluation/env_infos/final/torso_velocity Std          2.93767e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.55788e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.44701e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.158039\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.241572\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.645537\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.221043\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000800054\n",
      "evaluation/env_infos/torso_velocity Std                0.0705803\n",
      "evaluation/env_infos/torso_velocity Max                1.7205\n",
      "evaluation/env_infos/torso_velocity Min               -1.65182\n",
      "time/data storing (s)                                  0.015876\n",
      "time/evaluation sampling (s)                          46.8772\n",
      "time/exploration sampling (s)                          2.13717\n",
      "time/logging (s)                                       0.286428\n",
      "time/saving (s)                                        0.0271818\n",
      "time/training (s)                                      4.16163\n",
      "time/epoch (s)                                        53.5054\n",
      "time/total (s)                                      3277.25\n",
      "Epoch                                                 56\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:06:00.221923 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 57 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 59000\n",
      "trainer/QF1 Loss                                       0.518717\n",
      "trainer/QF2 Loss                                       0.930345\n",
      "trainer/Policy Loss                                  -12.6294\n",
      "trainer/Q1 Predictions Mean                           20.398\n",
      "trainer/Q1 Predictions Std                             1.41989\n",
      "trainer/Q1 Predictions Max                            21.9632\n",
      "trainer/Q1 Predictions Min                            10.3502\n",
      "trainer/Q2 Predictions Mean                           20.457\n",
      "trainer/Q2 Predictions Std                             1.36368\n",
      "trainer/Q2 Predictions Max                            21.9157\n",
      "trainer/Q2 Predictions Min                            14.369\n",
      "trainer/Q Targets Mean                                20.3575\n",
      "trainer/Q Targets Std                                  1.84162\n",
      "trainer/Q Targets Max                                 23.3893\n",
      "trainer/Q Targets Min                                  0.0917692\n",
      "trainer/Log Pis Mean                                   8.02283\n",
      "trainer/Log Pis Std                                    2.26568\n",
      "trainer/Log Pis Max                                   14.4229\n",
      "trainer/Log Pis Min                                   -1.57534\n",
      "trainer/Policy mu Mean                                -0.0343871\n",
      "trainer/Policy mu Std                                  0.177947\n",
      "trainer/Policy mu Max                                  1.1551\n",
      "trainer/Policy mu Min                                 -0.86216\n",
      "trainer/Policy log std Mean                           -2.37017\n",
      "trainer/Policy log std Std                             0.2279\n",
      "trainer/Policy log std Max                            -1.46887\n",
      "trainer/Policy log std Min                            -3.16207\n",
      "trainer/Alpha                                          0.00974246\n",
      "trainer/Alpha Loss                                     0.105709\n",
      "exploration/num steps total                        59000\n",
      "exploration/num paths total                          111\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.938878\n",
      "exploration/Rewards Std                                0.0838391\n",
      "exploration/Rewards Max                                1.92282\n",
      "exploration/Rewards Min                                0.71464\n",
      "exploration/Returns Mean                             938.878\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              938.878\n",
      "exploration/Returns Min                              938.878\n",
      "exploration/Actions Mean                              -0.0435955\n",
      "exploration/Actions Std                                0.134234\n",
      "exploration/Actions Max                                0.374588\n",
      "exploration/Actions Min                               -0.480391\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          938.878\n",
      "exploration/env_infos/final/reward_forward Mean        0.292867\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.292867\n",
      "exploration/env_infos/final/reward_forward Min         0.292867\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0736975\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0736975\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0736975\n",
      "exploration/env_infos/reward_forward Mean              0.0213785\n",
      "exploration/env_infos/reward_forward Std               0.171633\n",
      "exploration/env_infos/reward_forward Max               0.858912\n",
      "exploration/env_infos/reward_forward Min              -0.768834\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0406624\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0406624\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0406624\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0677854\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0677854\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0677854\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0796778\n",
      "exploration/env_infos/reward_ctrl Std                  0.036742\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0112237\n",
      "exploration/env_infos/reward_ctrl Min                 -0.28536\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0468405\n",
      "exploration/env_infos/final/torso_velocity Std         0.181187\n",
      "exploration/env_infos/final/torso_velocity Max         0.292867\n",
      "exploration/env_infos/final/torso_velocity Min        -0.13819\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.202356\n",
      "exploration/env_infos/initial/torso_velocity Std       0.304003\n",
      "exploration/env_infos/initial/torso_velocity Max       0.625817\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0736975\n",
      "exploration/env_infos/torso_velocity Mean              0.00372114\n",
      "exploration/env_infos/torso_velocity Std               0.180815\n",
      "exploration/env_infos/torso_velocity Max               0.858912\n",
      "exploration/env_infos/torso_velocity Min              -1.41518\n",
      "evaluation/num steps total                             1.45e+06\n",
      "evaluation/num paths total                          1450\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.893941\n",
      "evaluation/Rewards Std                                 0.0557633\n",
      "evaluation/Rewards Max                                 2.58411\n",
      "evaluation/Rewards Min                                 0.264813\n",
      "evaluation/Returns Mean                              893.941\n",
      "evaluation/Returns Std                                48.498\n",
      "evaluation/Returns Max                               943.001\n",
      "evaluation/Returns Min                               749.583\n",
      "evaluation/Actions Mean                               -0.0447177\n",
      "evaluation/Actions Std                                 0.157279\n",
      "evaluation/Actions Max                                 0.573509\n",
      "evaluation/Actions Min                                -0.777968\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           893.941\n",
      "evaluation/env_infos/final/reward_forward Mean        -3.57644e-08\n",
      "evaluation/env_infos/final/reward_forward Std          4.05563e-07\n",
      "evaluation/env_infos/final/reward_forward Max          6.33031e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -7.57515e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00815914\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0965674\n",
      "evaluation/env_infos/initial/reward_forward Max        0.225144\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.190607\n",
      "evaluation/env_infos/reward_forward Mean               0.00122131\n",
      "evaluation/env_infos/reward_forward Std                0.0472586\n",
      "evaluation/env_infos/reward_forward Max                1.67308\n",
      "evaluation/env_infos/reward_forward Min               -0.946807\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.106892\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0485183\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0556419\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.251815\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0303905\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0123404\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00955086\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0623298\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.106946\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0493525\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00955086\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.735187\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.75907e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.06981e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.21713e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -7.57515e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.140263\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.215416\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.629891\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.209389\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00177485\n",
      "evaluation/env_infos/torso_velocity Std                0.0615486\n",
      "evaluation/env_infos/torso_velocity Max                1.67308\n",
      "evaluation/env_infos/torso_velocity Min               -1.74321\n",
      "time/data storing (s)                                  0.017706\n",
      "time/evaluation sampling (s)                          51.1712\n",
      "time/exploration sampling (s)                          3.15648\n",
      "time/logging (s)                                       0.34716\n",
      "time/saving (s)                                        0.032991\n",
      "time/training (s)                                      5.86985\n",
      "time/epoch (s)                                        60.5954\n",
      "time/total (s)                                      3338.43\n",
      "Epoch                                                 57\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:07:00.303237 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 58 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 60000\n",
      "trainer/QF1 Loss                                       0.156489\n",
      "trainer/QF2 Loss                                       0.232635\n",
      "trainer/Policy Loss                                  -13.4564\n",
      "trainer/Q1 Predictions Mean                           20.8393\n",
      "trainer/Q1 Predictions Std                             1.1862\n",
      "trainer/Q1 Predictions Max                            22.5133\n",
      "trainer/Q1 Predictions Min                            14.2749\n",
      "trainer/Q2 Predictions Mean                           21.2479\n",
      "trainer/Q2 Predictions Std                             1.17617\n",
      "trainer/Q2 Predictions Max                            22.9653\n",
      "trainer/Q2 Predictions Min                            14.3091\n",
      "trainer/Q Targets Mean                                20.917\n",
      "trainer/Q Targets Std                                  1.21831\n",
      "trainer/Q Targets Max                                 23.3753\n",
      "trainer/Q Targets Min                                 14.9305\n",
      "trainer/Log Pis Mean                                   7.61718\n",
      "trainer/Log Pis Std                                    2.24639\n",
      "trainer/Log Pis Max                                   12.8951\n",
      "trainer/Log Pis Min                                    0.410469\n",
      "trainer/Policy mu Mean                                -0.0512043\n",
      "trainer/Policy mu Std                                  0.148511\n",
      "trainer/Policy mu Max                                  0.749408\n",
      "trainer/Policy mu Min                                 -0.856651\n",
      "trainer/Policy log std Mean                           -2.34122\n",
      "trainer/Policy log std Std                             0.204119\n",
      "trainer/Policy log std Max                            -1.31678\n",
      "trainer/Policy log std Min                            -2.95702\n",
      "trainer/Alpha                                          0.00956886\n",
      "trainer/Alpha Loss                                    -1.77963\n",
      "exploration/num steps total                        60000\n",
      "exploration/num paths total                          112\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.93942\n",
      "exploration/Rewards Std                                0.0701277\n",
      "exploration/Rewards Max                                1.60746\n",
      "exploration/Rewards Min                                0.69447\n",
      "exploration/Returns Mean                             939.42\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              939.42\n",
      "exploration/Returns Min                              939.42\n",
      "exploration/Actions Mean                              -0.0548633\n",
      "exploration/Actions Std                                0.122512\n",
      "exploration/Actions Max                                0.409622\n",
      "exploration/Actions Min                               -0.655199\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          939.42\n",
      "exploration/env_infos/final/reward_forward Mean       -0.00523623\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.00523623\n",
      "exploration/env_infos/final/reward_forward Min        -0.00523623\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.103303\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.103303\n",
      "exploration/env_infos/initial/reward_forward Min      -0.103303\n",
      "exploration/env_infos/reward_forward Mean              0.0172152\n",
      "exploration/env_infos/reward_forward Std               0.21613\n",
      "exploration/env_infos/reward_forward Max               0.749208\n",
      "exploration/env_infos/reward_forward Min              -1.65142\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.11231\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.11231\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.11231\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0682321\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0682321\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0682321\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0720765\n",
      "exploration/env_infos/reward_ctrl Std                  0.0351577\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00584775\n",
      "exploration/env_infos/reward_ctrl Min                 -0.30553\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0340553\n",
      "exploration/env_infos/final/torso_velocity Std         0.0259775\n",
      "exploration/env_infos/final/torso_velocity Max        -0.00523623\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0681966\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.059762\n",
      "exploration/env_infos/initial/torso_velocity Std       0.119129\n",
      "exploration/env_infos/initial/torso_velocity Max       0.177968\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.103303\n",
      "exploration/env_infos/torso_velocity Mean              0.00684914\n",
      "exploration/env_infos/torso_velocity Std               0.244549\n",
      "exploration/env_infos/torso_velocity Max               1.16981\n",
      "exploration/env_infos/torso_velocity Min              -1.65142\n",
      "evaluation/num steps total                             1.475e+06\n",
      "evaluation/num paths total                          1475\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.947603\n",
      "evaluation/Rewards Std                                 0.047037\n",
      "evaluation/Rewards Max                                 2.13919\n",
      "evaluation/Rewards Min                                 0.689758\n",
      "evaluation/Returns Mean                              947.603\n",
      "evaluation/Returns Std                                41.2805\n",
      "evaluation/Returns Max                               982.164\n",
      "evaluation/Returns Min                               833.665\n",
      "evaluation/Actions Mean                               -0.0416789\n",
      "evaluation/Actions Std                                 0.107491\n",
      "evaluation/Actions Max                                 0.531672\n",
      "evaluation/Actions Min                                -0.535476\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           947.603\n",
      "evaluation/env_infos/final/reward_forward Mean        -5.77369e-08\n",
      "evaluation/env_infos/final/reward_forward Std          3.7067e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.09808e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -1.115e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0211198\n",
      "evaluation/env_infos/initial/reward_forward Std        0.112364\n",
      "evaluation/env_infos/initial/reward_forward Max        0.241058\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.238646\n",
      "evaluation/env_infos/reward_forward Mean              -0.00126284\n",
      "evaluation/env_infos/reward_forward Std                0.0575383\n",
      "evaluation/env_infos/reward_forward Max                1.57826\n",
      "evaluation/env_infos/reward_forward Min               -1.68639\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0525541\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0412535\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0176434\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.166276\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0327306\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0241668\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0134051\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.100323\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0531662\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0418817\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0095971\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.310242\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -9.1119e-09\n",
      "evaluation/env_infos/final/torso_velocity Std          3.47987e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.11837e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.115e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.155418\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.227701\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.64752\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.238646\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00298861\n",
      "evaluation/env_infos/torso_velocity Std                0.0658508\n",
      "evaluation/env_infos/torso_velocity Max                1.57826\n",
      "evaluation/env_infos/torso_velocity Min               -1.84142\n",
      "time/data storing (s)                                  0.0169477\n",
      "time/evaluation sampling (s)                          51.0657\n",
      "time/exploration sampling (s)                          2.68062\n",
      "time/logging (s)                                       0.310043\n",
      "time/saving (s)                                        0.0313739\n",
      "time/training (s)                                      5.12285\n",
      "time/epoch (s)                                        59.2276\n",
      "time/total (s)                                      3398.47\n",
      "Epoch                                                 58\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:07:59.354021 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 59 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 61000\n",
      "trainer/QF1 Loss                                       0.141097\n",
      "trainer/QF2 Loss                                       0.190814\n",
      "trainer/Policy Loss                                  -13.4948\n",
      "trainer/Q1 Predictions Mean                           21.2374\n",
      "trainer/Q1 Predictions Std                             1.27626\n",
      "trainer/Q1 Predictions Max                            24.0583\n",
      "trainer/Q1 Predictions Min                            16.2027\n",
      "trainer/Q2 Predictions Mean                           21.1432\n",
      "trainer/Q2 Predictions Std                             1.31044\n",
      "trainer/Q2 Predictions Max                            22.6639\n",
      "trainer/Q2 Predictions Min                            15.9455\n",
      "trainer/Q Targets Mean                                21.1524\n",
      "trainer/Q Targets Std                                  1.28329\n",
      "trainer/Q Targets Max                                 25.6087\n",
      "trainer/Q Targets Min                                 15.0025\n",
      "trainer/Log Pis Mean                                   7.82084\n",
      "trainer/Log Pis Std                                    2.14602\n",
      "trainer/Log Pis Max                                   13.9127\n",
      "trainer/Log Pis Min                                   -0.369329\n",
      "trainer/Policy mu Mean                                -0.0546674\n",
      "trainer/Policy mu Std                                  0.143802\n",
      "trainer/Policy mu Max                                  0.873213\n",
      "trainer/Policy mu Min                                 -1.29987\n",
      "trainer/Policy log std Mean                           -2.36772\n",
      "trainer/Policy log std Std                             0.23093\n",
      "trainer/Policy log std Max                            -1.64855\n",
      "trainer/Policy log std Min                            -3.84335\n",
      "trainer/Alpha                                          0.00953951\n",
      "trainer/Alpha Loss                                    -0.83326\n",
      "exploration/num steps total                        61000\n",
      "exploration/num paths total                          113\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.932505\n",
      "exploration/Rewards Std                                0.0898745\n",
      "exploration/Rewards Max                                1.8448\n",
      "exploration/Rewards Min                                0.698611\n",
      "exploration/Returns Mean                             932.505\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              932.505\n",
      "exploration/Returns Min                              932.505\n",
      "exploration/Actions Mean                              -0.0410809\n",
      "exploration/Actions Std                                0.137573\n",
      "exploration/Actions Max                                0.440533\n",
      "exploration/Actions Min                               -0.564934\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          932.505\n",
      "exploration/env_infos/final/reward_forward Mean       -0.069653\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.069653\n",
      "exploration/env_infos/final/reward_forward Min        -0.069653\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0340726\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0340726\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0340726\n",
      "exploration/env_infos/reward_forward Mean             -0.019586\n",
      "exploration/env_infos/reward_forward Std               0.249564\n",
      "exploration/env_infos/reward_forward Max               0.862818\n",
      "exploration/env_infos/reward_forward Min              -1.17582\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0405196\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0405196\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0405196\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0082087\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0082087\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0082087\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0824559\n",
      "exploration/env_infos/reward_ctrl Std                  0.0457036\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0082087\n",
      "exploration/env_infos/reward_ctrl Min                 -0.301389\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.000125835\n",
      "exploration/env_infos/final/torso_velocity Std         0.0865636\n",
      "exploration/env_infos/final/torso_velocity Max         0.121898\n",
      "exploration/env_infos/final/torso_velocity Min        -0.069653\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.235391\n",
      "exploration/env_infos/initial/torso_velocity Std       0.251732\n",
      "exploration/env_infos/initial/torso_velocity Max       0.571606\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0340726\n",
      "exploration/env_infos/torso_velocity Mean              0.0168726\n",
      "exploration/env_infos/torso_velocity Std               0.282626\n",
      "exploration/env_infos/torso_velocity Max               1.58565\n",
      "exploration/env_infos/torso_velocity Min              -1.6918\n",
      "evaluation/num steps total                             1.5e+06\n",
      "evaluation/num paths total                          1500\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.947518\n",
      "evaluation/Rewards Std                                 0.0450839\n",
      "evaluation/Rewards Max                                 2.60872\n",
      "evaluation/Rewards Min                                 0.694633\n",
      "evaluation/Returns Mean                              947.518\n",
      "evaluation/Returns Std                                28.0154\n",
      "evaluation/Returns Max                               988.867\n",
      "evaluation/Returns Min                               888.23\n",
      "evaluation/Actions Mean                               -0.0579593\n",
      "evaluation/Actions Std                                 0.10148\n",
      "evaluation/Actions Max                                 0.578747\n",
      "evaluation/Actions Min                                -0.469772\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           947.518\n",
      "evaluation/env_infos/final/reward_forward Mean         1.97868e-05\n",
      "evaluation/env_infos/final/reward_forward Std          9.18545e-05\n",
      "evaluation/env_infos/final/reward_forward Max          0.000468906\n",
      "evaluation/env_infos/final/reward_forward Min         -1.0435e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0448283\n",
      "evaluation/env_infos/initial/reward_forward Std        0.139174\n",
      "evaluation/env_infos/initial/reward_forward Max        0.249525\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.278498\n",
      "evaluation/env_infos/reward_forward Mean              -0.00193634\n",
      "evaluation/env_infos/reward_forward Std                0.0973193\n",
      "evaluation/env_infos/reward_forward Max                1.67611\n",
      "evaluation/env_infos/reward_forward Min               -1.13229\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0536288\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0303266\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00891718\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.115078\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0144731\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00579023\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00622973\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0307149\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0546297\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0303268\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00621429\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.305367\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.55317e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000156938\n",
      "evaluation/env_infos/final/torso_velocity Max          0.0012908\n",
      "evaluation/env_infos/final/torso_velocity Min         -3.3987e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.125981\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.259104\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.678398\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.372101\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00216662\n",
      "evaluation/env_infos/torso_velocity Std                0.09084\n",
      "evaluation/env_infos/torso_velocity Max                1.67611\n",
      "evaluation/env_infos/torso_velocity Min               -1.86652\n",
      "time/data storing (s)                                  0.0157061\n",
      "time/evaluation sampling (s)                          51.0027\n",
      "time/exploration sampling (s)                          2.53137\n",
      "time/logging (s)                                       0.292289\n",
      "time/saving (s)                                        0.0294645\n",
      "time/training (s)                                      4.54422\n",
      "time/epoch (s)                                        58.4158\n",
      "time/total (s)                                      3457.5\n",
      "Epoch                                                 59\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:09:01.380619 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 60 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 62000\n",
      "trainer/QF1 Loss                                       0.151507\n",
      "trainer/QF2 Loss                                       0.134877\n",
      "trainer/Policy Loss                                  -13.7562\n",
      "trainer/Q1 Predictions Mean                           21.3896\n",
      "trainer/Q1 Predictions Std                             1.74616\n",
      "trainer/Q1 Predictions Max                            22.7389\n",
      "trainer/Q1 Predictions Min                            -0.271198\n",
      "trainer/Q2 Predictions Mean                           21.4568\n",
      "trainer/Q2 Predictions Std                             1.86595\n",
      "trainer/Q2 Predictions Max                            22.8628\n",
      "trainer/Q2 Predictions Min                            -0.689381\n",
      "trainer/Q Targets Mean                                21.3498\n",
      "trainer/Q Targets Std                                  1.84726\n",
      "trainer/Q Targets Max                                 22.7334\n",
      "trainer/Q Targets Min                                 -1.14843\n",
      "trainer/Log Pis Mean                                   7.86906\n",
      "trainer/Log Pis Std                                    2.49084\n",
      "trainer/Log Pis Max                                   25.6606\n",
      "trainer/Log Pis Min                                    1.00215\n",
      "trainer/Policy mu Mean                                -0.00451679\n",
      "trainer/Policy mu Std                                  0.188783\n",
      "trainer/Policy mu Max                                  2.46797\n",
      "trainer/Policy mu Min                                 -1.65341\n",
      "trainer/Policy log std Mean                           -2.36216\n",
      "trainer/Policy log std Std                             0.219535\n",
      "trainer/Policy log std Max                            -0.757331\n",
      "trainer/Policy log std Min                            -3.91344\n",
      "trainer/Alpha                                          0.00959626\n",
      "trainer/Alpha Loss                                    -0.608284\n",
      "exploration/num steps total                        62000\n",
      "exploration/num paths total                          114\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.916582\n",
      "exploration/Rewards Std                                0.0346505\n",
      "exploration/Rewards Max                                1.09723\n",
      "exploration/Rewards Min                                0.784434\n",
      "exploration/Returns Mean                             916.582\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              916.582\n",
      "exploration/Returns Min                              916.582\n",
      "exploration/Actions Mean                              -0.0262202\n",
      "exploration/Actions Std                                0.143047\n",
      "exploration/Actions Max                                0.437983\n",
      "exploration/Actions Min                               -0.437008\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          916.582\n",
      "exploration/env_infos/final/reward_forward Mean        0.0824325\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0824325\n",
      "exploration/env_infos/final/reward_forward Min         0.0824325\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0124218\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0124218\n",
      "exploration/env_infos/initial/reward_forward Min       0.0124218\n",
      "exploration/env_infos/reward_forward Mean             -0.0188403\n",
      "exploration/env_infos/reward_forward Std               0.106498\n",
      "exploration/env_infos/reward_forward Max               0.535652\n",
      "exploration/env_infos/reward_forward Min              -0.500222\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0364633\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0364633\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0364633\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0810462\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0810462\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0810462\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0845999\n",
      "exploration/env_infos/reward_ctrl Std                  0.0335586\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0132765\n",
      "exploration/env_infos/reward_ctrl Min                 -0.215566\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.103638\n",
      "exploration/env_infos/final/torso_velocity Std         0.161081\n",
      "exploration/env_infos/final/torso_velocity Max         0.0824325\n",
      "exploration/env_infos/final/torso_velocity Min        -0.310489\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.122388\n",
      "exploration/env_infos/initial/torso_velocity Std       0.114303\n",
      "exploration/env_infos/initial/torso_velocity Max       0.279978\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0124218\n",
      "exploration/env_infos/torso_velocity Mean             -0.00912612\n",
      "exploration/env_infos/torso_velocity Std               0.0901851\n",
      "exploration/env_infos/torso_velocity Max               0.535652\n",
      "exploration/env_infos/torso_velocity Min              -1.33395\n",
      "evaluation/num steps total                             1.525e+06\n",
      "evaluation/num paths total                          1525\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.937894\n",
      "evaluation/Rewards Std                                 0.023168\n",
      "evaluation/Rewards Max                                 2.05712\n",
      "evaluation/Rewards Min                                 0.805273\n",
      "evaluation/Returns Mean                              937.894\n",
      "evaluation/Returns Std                                15.8114\n",
      "evaluation/Returns Max                               967.104\n",
      "evaluation/Returns Min                               901.464\n",
      "evaluation/Actions Mean                               -0.00746386\n",
      "evaluation/Actions Std                                 0.124881\n",
      "evaluation/Actions Max                                 0.38057\n",
      "evaluation/Actions Min                                -0.363556\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           937.894\n",
      "evaluation/env_infos/final/reward_forward Mean         7.80947e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.91921e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.51873e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.01477e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00991359\n",
      "evaluation/env_infos/initial/reward_forward Std        0.113546\n",
      "evaluation/env_infos/initial/reward_forward Max        0.16569\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.303813\n",
      "evaluation/env_infos/reward_forward Mean              -0.000813704\n",
      "evaluation/env_infos/reward_forward Std                0.0422087\n",
      "evaluation/env_infos/reward_forward Max                1.46958\n",
      "evaluation/env_infos/reward_forward Min               -1.26869\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0627006\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0160328\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0336035\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0995336\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0430934\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0214006\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0188296\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0988979\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0626035\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0163729\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00558762\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.194727\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -4.19217e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.60174e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.68878e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.77209e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.142496\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.254689\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.723802\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.303813\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00158184\n",
      "evaluation/env_infos/torso_velocity Std                0.0476662\n",
      "evaluation/env_infos/torso_velocity Max                1.46958\n",
      "evaluation/env_infos/torso_velocity Min               -1.71987\n",
      "time/data storing (s)                                  0.01626\n",
      "time/evaluation sampling (s)                          52.4444\n",
      "time/exploration sampling (s)                          2.14404\n",
      "time/logging (s)                                       0.448965\n",
      "time/saving (s)                                        0.0309944\n",
      "time/training (s)                                      6.49567\n",
      "time/epoch (s)                                        61.5803\n",
      "time/total (s)                                      3519.69\n",
      "Epoch                                                 60\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:10:00.819273 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 61 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 63000\n",
      "trainer/QF1 Loss                                       0.257008\n",
      "trainer/QF2 Loss                                       0.176564\n",
      "trainer/Policy Loss                                  -13.2805\n",
      "trainer/Q1 Predictions Mean                           21.7561\n",
      "trainer/Q1 Predictions Std                             1.29535\n",
      "trainer/Q1 Predictions Max                            23.4289\n",
      "trainer/Q1 Predictions Min                            16.1653\n",
      "trainer/Q2 Predictions Mean                           21.8877\n",
      "trainer/Q2 Predictions Std                             1.28646\n",
      "trainer/Q2 Predictions Max                            23.6589\n",
      "trainer/Q2 Predictions Min                            16.0822\n",
      "trainer/Q Targets Mean                                21.7381\n",
      "trainer/Q Targets Std                                  1.19913\n",
      "trainer/Q Targets Max                                 23.3532\n",
      "trainer/Q Targets Min                                 16.9337\n",
      "trainer/Log Pis Mean                                   8.70417\n",
      "trainer/Log Pis Std                                    2.55188\n",
      "trainer/Log Pis Max                                   16.6781\n",
      "trainer/Log Pis Min                                    1.57189\n",
      "trainer/Policy mu Mean                                -0.0777808\n",
      "trainer/Policy mu Std                                  0.166157\n",
      "trainer/Policy mu Max                                  1.35912\n",
      "trainer/Policy mu Min                                 -0.949231\n",
      "trainer/Policy log std Mean                           -2.47634\n",
      "trainer/Policy log std Std                             0.195765\n",
      "trainer/Policy log std Max                            -1.86849\n",
      "trainer/Policy log std Min                            -3.42893\n",
      "trainer/Alpha                                          0.00926269\n",
      "trainer/Alpha Loss                                     3.29791\n",
      "exploration/num steps total                        63000\n",
      "exploration/num paths total                          115\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.891575\n",
      "exploration/Rewards Std                                0.0697752\n",
      "exploration/Rewards Max                                1.46182\n",
      "exploration/Rewards Min                                0.708664\n",
      "exploration/Returns Mean                             891.575\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              891.575\n",
      "exploration/Returns Min                              891.575\n",
      "exploration/Actions Mean                              -0.0881928\n",
      "exploration/Actions Std                                0.149588\n",
      "exploration/Actions Max                                0.384565\n",
      "exploration/Actions Min                               -0.559374\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          891.575\n",
      "exploration/env_infos/final/reward_forward Mean        0.0219551\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0219551\n",
      "exploration/env_infos/final/reward_forward Min         0.0219551\n",
      "exploration/env_infos/initial/reward_forward Mean      0.00738102\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.00738102\n",
      "exploration/env_infos/initial/reward_forward Min       0.00738102\n",
      "exploration/env_infos/reward_forward Mean             -0.00729054\n",
      "exploration/env_infos/reward_forward Std               0.108112\n",
      "exploration/env_infos/reward_forward Max               0.381048\n",
      "exploration/env_infos/reward_forward Min              -1.00093\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.125858\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.125858\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.125858\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0757745\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0757745\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0757745\n",
      "exploration/env_infos/reward_ctrl Mean                -0.120618\n",
      "exploration/env_infos/reward_ctrl Std                  0.0464434\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0199581\n",
      "exploration/env_infos/reward_ctrl Min                 -0.309342\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0549799\n",
      "exploration/env_infos/final/torso_velocity Std         0.154921\n",
      "exploration/env_infos/final/torso_velocity Max         0.259063\n",
      "exploration/env_infos/final/torso_velocity Min        -0.116078\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.12792\n",
      "exploration/env_infos/initial/torso_velocity Std       0.195479\n",
      "exploration/env_infos/initial/torso_velocity Max       0.403643\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0272658\n",
      "exploration/env_infos/torso_velocity Mean              0.00343957\n",
      "exploration/env_infos/torso_velocity Std               0.132352\n",
      "exploration/env_infos/torso_velocity Max               0.778221\n",
      "exploration/env_infos/torso_velocity Min              -1.36079\n",
      "evaluation/num steps total                             1.55e+06\n",
      "evaluation/num paths total                          1550\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.854687\n",
      "evaluation/Rewards Std                                 0.0453799\n",
      "evaluation/Rewards Max                                 2.63088\n",
      "evaluation/Rewards Min                                 0.652017\n",
      "evaluation/Returns Mean                              854.687\n",
      "evaluation/Returns Std                                23.8073\n",
      "evaluation/Returns Max                               908.005\n",
      "evaluation/Returns Min                               817.632\n",
      "evaluation/Actions Mean                               -0.0775875\n",
      "evaluation/Actions Std                                 0.175293\n",
      "evaluation/Actions Max                                 0.386173\n",
      "evaluation/Actions Min                                -0.57064\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           854.687\n",
      "evaluation/env_infos/final/reward_forward Mean         2.76977e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.71119e-07\n",
      "evaluation/env_infos/final/reward_forward Max          9.4883e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -3.05471e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0457944\n",
      "evaluation/env_infos/initial/reward_forward Std        0.142456\n",
      "evaluation/env_infos/initial/reward_forward Max        0.350349\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.23886\n",
      "evaluation/env_infos/reward_forward Mean              -0.00202601\n",
      "evaluation/env_infos/reward_forward Std                0.0576291\n",
      "evaluation/env_infos/reward_forward Max                1.31434\n",
      "evaluation/env_infos/reward_forward Min               -1.43989\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.148887\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0240126\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0920856\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.183496\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0888763\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.030878\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0485842\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.171121\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.146991\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0263992\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.02299\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.347983\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -7.02047e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          4.85782e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.4883e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.17844e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.177762\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.228954\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.597243\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.23886\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000781059\n",
      "evaluation/env_infos/torso_velocity Std                0.0623966\n",
      "evaluation/env_infos/torso_velocity Max                1.47048\n",
      "evaluation/env_infos/torso_velocity Min               -2.09996\n",
      "time/data storing (s)                                  0.0153791\n",
      "time/evaluation sampling (s)                          51.1874\n",
      "time/exploration sampling (s)                          2.05878\n",
      "time/logging (s)                                       0.328355\n",
      "time/saving (s)                                        0.0319323\n",
      "time/training (s)                                      4.83952\n",
      "time/epoch (s)                                        58.4613\n",
      "time/total (s)                                      3579\n",
      "Epoch                                                 61\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:11:02.891236 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 62 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 64000\n",
      "trainer/QF1 Loss                                       0.119396\n",
      "trainer/QF2 Loss                                       0.209717\n",
      "trainer/Policy Loss                                  -14.3754\n",
      "trainer/Q1 Predictions Mean                           22.1685\n",
      "trainer/Q1 Predictions Std                             1.33976\n",
      "trainer/Q1 Predictions Max                            24.1643\n",
      "trainer/Q1 Predictions Min                            14.9414\n",
      "trainer/Q2 Predictions Mean                           22.3743\n",
      "trainer/Q2 Predictions Std                             1.33821\n",
      "trainer/Q2 Predictions Max                            24.1985\n",
      "trainer/Q2 Predictions Min                            15.6664\n",
      "trainer/Q Targets Mean                                22.1009\n",
      "trainer/Q Targets Std                                  1.26572\n",
      "trainer/Q Targets Max                                 24.5214\n",
      "trainer/Q Targets Min                                 15.7557\n",
      "trainer/Log Pis Mean                                   7.97404\n",
      "trainer/Log Pis Std                                    2.68661\n",
      "trainer/Log Pis Max                                   25.3738\n",
      "trainer/Log Pis Min                                   -1.62069\n",
      "trainer/Policy mu Mean                                -0.023648\n",
      "trainer/Policy mu Std                                  0.174775\n",
      "trainer/Policy mu Max                                  1.16313\n",
      "trainer/Policy mu Min                                 -1.22804\n",
      "trainer/Policy log std Mean                           -2.36539\n",
      "trainer/Policy log std Std                             0.241325\n",
      "trainer/Policy log std Max                            -1.68047\n",
      "trainer/Policy log std Min                            -4.52378\n",
      "trainer/Alpha                                          0.00904689\n",
      "trainer/Alpha Loss                                    -0.12218\n",
      "exploration/num steps total                        64000\n",
      "exploration/num paths total                          116\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.875989\n",
      "exploration/Rewards Std                                0.0717595\n",
      "exploration/Rewards Max                                1.30323\n",
      "exploration/Rewards Min                                0.538454\n",
      "exploration/Returns Mean                             875.989\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              875.989\n",
      "exploration/Returns Min                              875.989\n",
      "exploration/Actions Mean                              -0.0506854\n",
      "exploration/Actions Std                                0.17965\n",
      "exploration/Actions Max                                0.592784\n",
      "exploration/Actions Min                               -0.624418\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          875.989\n",
      "exploration/env_infos/final/reward_forward Mean        0.0100087\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0100087\n",
      "exploration/env_infos/final/reward_forward Min         0.0100087\n",
      "exploration/env_infos/initial/reward_forward Mean      0.045395\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.045395\n",
      "exploration/env_infos/initial/reward_forward Min       0.045395\n",
      "exploration/env_infos/reward_forward Mean             -0.0171906\n",
      "exploration/env_infos/reward_forward Std               0.110151\n",
      "exploration/env_infos/reward_forward Max               0.433489\n",
      "exploration/env_infos/reward_forward Min              -0.842375\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.162717\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.162717\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.162717\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.155119\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.155119\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.155119\n",
      "exploration/env_infos/reward_ctrl Mean                -0.139372\n",
      "exploration/env_infos/reward_ctrl Std                  0.050294\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0265308\n",
      "exploration/env_infos/reward_ctrl Min                 -0.461546\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.00149558\n",
      "exploration/env_infos/final/torso_velocity Std         0.0201402\n",
      "exploration/env_infos/final/torso_velocity Max         0.020778\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0263\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.202669\n",
      "exploration/env_infos/initial/torso_velocity Std       0.235665\n",
      "exploration/env_infos/initial/torso_velocity Max       0.535777\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0268343\n",
      "exploration/env_infos/torso_velocity Mean             -0.0088735\n",
      "exploration/env_infos/torso_velocity Std               0.137253\n",
      "exploration/env_infos/torso_velocity Max               0.717192\n",
      "exploration/env_infos/torso_velocity Min              -1.97384\n",
      "evaluation/num steps total                             1.575e+06\n",
      "evaluation/num paths total                          1575\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.892139\n",
      "evaluation/Rewards Std                                 0.0451503\n",
      "evaluation/Rewards Max                                 1.57625\n",
      "evaluation/Rewards Min                                 0.490523\n",
      "evaluation/Returns Mean                              892.139\n",
      "evaluation/Returns Std                                40.1332\n",
      "evaluation/Returns Max                               935.018\n",
      "evaluation/Returns Min                               788.669\n",
      "evaluation/Actions Mean                               -0.0281344\n",
      "evaluation/Actions Std                                 0.162286\n",
      "evaluation/Actions Max                                 0.576161\n",
      "evaluation/Actions Min                                -0.750732\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           892.139\n",
      "evaluation/env_infos/final/reward_forward Mean        -2.00939e-05\n",
      "evaluation/env_infos/final/reward_forward Std          7.58143e-05\n",
      "evaluation/env_infos/final/reward_forward Max          9.70363e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000369939\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0348278\n",
      "evaluation/env_infos/initial/reward_forward Std        0.136812\n",
      "evaluation/env_infos/initial/reward_forward Max        0.176091\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.307353\n",
      "evaluation/env_infos/reward_forward Mean              -0.000855466\n",
      "evaluation/env_infos/reward_forward Std                0.0517083\n",
      "evaluation/env_infos/reward_forward Max                1.40651\n",
      "evaluation/env_infos/reward_forward Min               -1.48379\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.106008\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0403599\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0633707\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.21143\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0791336\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0220704\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0520135\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.154112\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.108514\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0430069\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0333783\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.51457\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         5.54697e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000362148\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00226629\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000369939\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.139722\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.275358\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.711258\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.443018\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00160029\n",
      "evaluation/env_infos/torso_velocity Std                0.0633029\n",
      "evaluation/env_infos/torso_velocity Max                1.40651\n",
      "evaluation/env_infos/torso_velocity Min               -2.03532\n",
      "time/data storing (s)                                  0.0167087\n",
      "time/evaluation sampling (s)                          54.0299\n",
      "time/exploration sampling (s)                          2.41363\n",
      "time/logging (s)                                       0.28892\n",
      "time/saving (s)                                        0.0292382\n",
      "time/training (s)                                      4.51439\n",
      "time/epoch (s)                                        61.2928\n",
      "time/total (s)                                      3641.03\n",
      "Epoch                                                 62\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:12:00.853571 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 63 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 65000\n",
      "trainer/QF1 Loss                                       0.327826\n",
      "trainer/QF2 Loss                                       0.16786\n",
      "trainer/Policy Loss                                  -13.5686\n",
      "trainer/Q1 Predictions Mean                           22.098\n",
      "trainer/Q1 Predictions Std                             1.44021\n",
      "trainer/Q1 Predictions Max                            23.9643\n",
      "trainer/Q1 Predictions Min                            14.8805\n",
      "trainer/Q2 Predictions Mean                           22.3246\n",
      "trainer/Q2 Predictions Std                             1.33961\n",
      "trainer/Q2 Predictions Max                            23.7726\n",
      "trainer/Q2 Predictions Min                            14.3483\n",
      "trainer/Q Targets Mean                                22.3406\n",
      "trainer/Q Targets Std                                  1.42097\n",
      "trainer/Q Targets Max                                 23.6778\n",
      "trainer/Q Targets Min                                 11.426\n",
      "trainer/Log Pis Mean                                   8.79073\n",
      "trainer/Log Pis Std                                    2.48642\n",
      "trainer/Log Pis Max                                   17.7853\n",
      "trainer/Log Pis Min                                   -0.624646\n",
      "trainer/Policy mu Mean                                -0.00967072\n",
      "trainer/Policy mu Std                                  0.1379\n",
      "trainer/Policy mu Max                                  0.850592\n",
      "trainer/Policy mu Min                                 -0.728366\n",
      "trainer/Policy log std Mean                           -2.4792\n",
      "trainer/Policy log std Std                             0.246163\n",
      "trainer/Policy log std Max                            -1.86332\n",
      "trainer/Policy log std Min                            -4.00347\n",
      "trainer/Alpha                                          0.00857163\n",
      "trainer/Alpha Loss                                     3.76329\n",
      "exploration/num steps total                        65000\n",
      "exploration/num paths total                          117\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.927897\n",
      "exploration/Rewards Std                                0.0461739\n",
      "exploration/Rewards Max                                1.34251\n",
      "exploration/Rewards Min                                0.786978\n",
      "exploration/Returns Mean                             927.897\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              927.897\n",
      "exploration/Returns Min                              927.897\n",
      "exploration/Actions Mean                               0.00293859\n",
      "exploration/Actions Std                                0.141392\n",
      "exploration/Actions Max                                0.440334\n",
      "exploration/Actions Min                               -0.523898\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          927.897\n",
      "exploration/env_infos/final/reward_forward Mean       -0.00791326\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.00791326\n",
      "exploration/env_infos/final/reward_forward Min        -0.00791326\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.053191\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.053191\n",
      "exploration/env_infos/initial/reward_forward Min      -0.053191\n",
      "exploration/env_infos/reward_forward Mean              0.00365284\n",
      "exploration/env_infos/reward_forward Std               0.0663927\n",
      "exploration/env_infos/reward_forward Max               0.301494\n",
      "exploration/env_infos/reward_forward Min              -0.524983\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0472393\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0472393\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0472393\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0731582\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0731582\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0731582\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0800012\n",
      "exploration/env_infos/reward_ctrl Std                  0.0287849\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0149705\n",
      "exploration/env_infos/reward_ctrl Min                 -0.213022\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0363317\n",
      "exploration/env_infos/final/torso_velocity Std         0.0612388\n",
      "exploration/env_infos/final/torso_velocity Max         0.12293\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00791326\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.084648\n",
      "exploration/env_infos/initial/torso_velocity Std       0.11083\n",
      "exploration/env_infos/initial/torso_velocity Max       0.218185\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.053191\n",
      "exploration/env_infos/torso_velocity Mean             -0.00439831\n",
      "exploration/env_infos/torso_velocity Std               0.12299\n",
      "exploration/env_infos/torso_velocity Max               0.48977\n",
      "exploration/env_infos/torso_velocity Min              -1.51412\n",
      "evaluation/num steps total                             1.6e+06\n",
      "evaluation/num paths total                          1600\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.942556\n",
      "evaluation/Rewards Std                                 0.0469253\n",
      "evaluation/Rewards Max                                 2.23068\n",
      "evaluation/Rewards Min                                 0.663234\n",
      "evaluation/Returns Mean                              942.556\n",
      "evaluation/Returns Std                                32.0946\n",
      "evaluation/Returns Max                               974.116\n",
      "evaluation/Returns Min                               859.646\n",
      "evaluation/Actions Mean                               -0.00837945\n",
      "evaluation/Actions Std                                 0.12138\n",
      "evaluation/Actions Max                                 0.461157\n",
      "evaluation/Actions Min                                -0.596713\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           942.556\n",
      "evaluation/env_infos/final/reward_forward Mean        -5.80809e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.90895e-07\n",
      "evaluation/env_infos/final/reward_forward Max          3.72703e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -7.09603e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0280331\n",
      "evaluation/env_infos/initial/reward_forward Std        0.144416\n",
      "evaluation/env_infos/initial/reward_forward Max        0.232458\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.393037\n",
      "evaluation/env_infos/reward_forward Mean               0.0028623\n",
      "evaluation/env_infos/reward_forward Std                0.0568331\n",
      "evaluation/env_infos/reward_forward Max                1.42447\n",
      "evaluation/env_infos/reward_forward Min               -1.39412\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0599104\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0326272\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0249763\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.148207\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0334423\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0128225\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0156648\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0647273\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0592137\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0338844\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00612697\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.336766\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         3.18843e-09\n",
      "evaluation/env_infos/final/torso_velocity Std          2.80951e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          8.47956e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -7.11472e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.15354\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.245859\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.664814\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.397902\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00153614\n",
      "evaluation/env_infos/torso_velocity Std                0.0668867\n",
      "evaluation/env_infos/torso_velocity Max                1.42447\n",
      "evaluation/env_infos/torso_velocity Min               -1.83995\n",
      "time/data storing (s)                                  0.0154744\n",
      "time/evaluation sampling (s)                          50.365\n",
      "time/exploration sampling (s)                          2.13484\n",
      "time/logging (s)                                       0.314929\n",
      "time/saving (s)                                        0.0287453\n",
      "time/training (s)                                      4.49879\n",
      "time/epoch (s)                                        57.3578\n",
      "time/total (s)                                      3699.02\n",
      "Epoch                                                 63\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:12:59.170607 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 64 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 66000\n",
      "trainer/QF1 Loss                                       0.275315\n",
      "trainer/QF2 Loss                                       0.29052\n",
      "trainer/Policy Loss                                  -14.312\n",
      "trainer/Q1 Predictions Mean                           22.7849\n",
      "trainer/Q1 Predictions Std                             1.63935\n",
      "trainer/Q1 Predictions Max                            24.686\n",
      "trainer/Q1 Predictions Min                            12.0739\n",
      "trainer/Q2 Predictions Mean                           22.6149\n",
      "trainer/Q2 Predictions Std                             1.42174\n",
      "trainer/Q2 Predictions Max                            24.202\n",
      "trainer/Q2 Predictions Min                            17.361\n",
      "trainer/Q Targets Mean                                22.5404\n",
      "trainer/Q Targets Std                                  1.57746\n",
      "trainer/Q Targets Max                                 24.7292\n",
      "trainer/Q Targets Min                                 13.8588\n",
      "trainer/Log Pis Mean                                   8.49924\n",
      "trainer/Log Pis Std                                    2.62055\n",
      "trainer/Log Pis Max                                   19.831\n",
      "trainer/Log Pis Min                                    0.16505\n",
      "trainer/Policy mu Mean                                 0.0399398\n",
      "trainer/Policy mu Std                                  0.19556\n",
      "trainer/Policy mu Max                                  2.05778\n",
      "trainer/Policy mu Min                                 -1.90332\n",
      "trainer/Policy log std Mean                           -2.44101\n",
      "trainer/Policy log std Std                             0.230301\n",
      "trainer/Policy log std Max                            -1.07041\n",
      "trainer/Policy log std Min                            -4.03732\n",
      "trainer/Alpha                                          0.00871916\n",
      "trainer/Alpha Loss                                     2.36774\n",
      "exploration/num steps total                        66000\n",
      "exploration/num paths total                          118\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.918617\n",
      "exploration/Rewards Std                                0.124413\n",
      "exploration/Rewards Max                                1.93874\n",
      "exploration/Rewards Min                                0.598132\n",
      "exploration/Returns Mean                             918.617\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              918.617\n",
      "exploration/Returns Min                              918.617\n",
      "exploration/Actions Mean                               0.0407053\n",
      "exploration/Actions Std                                0.159404\n",
      "exploration/Actions Max                                0.610676\n",
      "exploration/Actions Min                               -0.454068\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          918.617\n",
      "exploration/env_infos/final/reward_forward Mean        0.0133991\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0133991\n",
      "exploration/env_infos/final/reward_forward Min         0.0133991\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.00853801\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.00853801\n",
      "exploration/env_infos/initial/reward_forward Min      -0.00853801\n",
      "exploration/env_infos/reward_forward Mean              0.00143751\n",
      "exploration/env_infos/reward_forward Std               0.218506\n",
      "exploration/env_infos/reward_forward Max               1.23109\n",
      "exploration/env_infos/reward_forward Min              -0.856985\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.153845\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.153845\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.153845\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0486797\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0486797\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0486797\n",
      "exploration/env_infos/reward_ctrl Mean                -0.108267\n",
      "exploration/env_infos/reward_ctrl Std                  0.0480091\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00414096\n",
      "exploration/env_infos/reward_ctrl Min                 -0.401868\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.129327\n",
      "exploration/env_infos/final/torso_velocity Std         0.188382\n",
      "exploration/env_infos/final/torso_velocity Max         0.395022\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0204413\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.145416\n",
      "exploration/env_infos/initial/torso_velocity Std       0.283833\n",
      "exploration/env_infos/initial/torso_velocity Max       0.543432\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0986443\n",
      "exploration/env_infos/torso_velocity Mean              0.00324683\n",
      "exploration/env_infos/torso_velocity Std               0.258184\n",
      "exploration/env_infos/torso_velocity Max               1.24487\n",
      "exploration/env_infos/torso_velocity Min              -1.04077\n",
      "evaluation/num steps total                             1.625e+06\n",
      "evaluation/num paths total                          1625\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.924028\n",
      "evaluation/Rewards Std                                 0.0463663\n",
      "evaluation/Rewards Max                                 2.19032\n",
      "evaluation/Rewards Min                                 0.669415\n",
      "evaluation/Returns Mean                              924.028\n",
      "evaluation/Returns Std                                36.6598\n",
      "evaluation/Returns Max                               973.1\n",
      "evaluation/Returns Min                               857.148\n",
      "evaluation/Actions Mean                                0.037302\n",
      "evaluation/Actions Std                                 0.133775\n",
      "evaluation/Actions Max                                 0.610769\n",
      "evaluation/Actions Min                                -0.523584\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           924.028\n",
      "evaluation/env_infos/final/reward_forward Mean        -7.07892e-08\n",
      "evaluation/env_infos/final/reward_forward Std          4.51201e-07\n",
      "evaluation/env_infos/final/reward_forward Max          4.79862e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -1.51901e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00571986\n",
      "evaluation/env_infos/initial/reward_forward Std        0.142052\n",
      "evaluation/env_infos/initial/reward_forward Max        0.311387\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.323152\n",
      "evaluation/env_infos/reward_forward Mean               0.00204553\n",
      "evaluation/env_infos/reward_forward Std                0.0577125\n",
      "evaluation/env_infos/reward_forward Max                1.5041\n",
      "evaluation/env_infos/reward_forward Min               -1.05281\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0766631\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0366473\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0259906\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.143242\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0727532\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0358977\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0295368\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.143332\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0771493\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0372787\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.021366\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.330585\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.46613e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          5.14238e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          2.30524e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.51901e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.123228\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.25599\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.613156\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.323152\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00105225\n",
      "evaluation/env_infos/torso_velocity Std                0.0704109\n",
      "evaluation/env_infos/torso_velocity Max                1.5041\n",
      "evaluation/env_infos/torso_velocity Min               -2.02617\n",
      "time/data storing (s)                                  0.0187798\n",
      "time/evaluation sampling (s)                          50.7392\n",
      "time/exploration sampling (s)                          2.33257\n",
      "time/logging (s)                                       0.285516\n",
      "time/saving (s)                                        0.0282894\n",
      "time/training (s)                                      4.21554\n",
      "time/epoch (s)                                        57.6199\n",
      "time/total (s)                                      3757.31\n",
      "Epoch                                                 64\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:13:59.084652 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 65 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 67000\n",
      "trainer/QF1 Loss                                       0.221257\n",
      "trainer/QF2 Loss                                       0.268145\n",
      "trainer/Policy Loss                                  -15.1903\n",
      "trainer/Q1 Predictions Mean                           22.8927\n",
      "trainer/Q1 Predictions Std                             1.50483\n",
      "trainer/Q1 Predictions Max                            24.7916\n",
      "trainer/Q1 Predictions Min                            17.1344\n",
      "trainer/Q2 Predictions Mean                           22.8147\n",
      "trainer/Q2 Predictions Std                             1.60519\n",
      "trainer/Q2 Predictions Max                            25.0178\n",
      "trainer/Q2 Predictions Min                            15.3554\n",
      "trainer/Q Targets Mean                                22.833\n",
      "trainer/Q Targets Std                                  1.54338\n",
      "trainer/Q Targets Max                                 24.6197\n",
      "trainer/Q Targets Min                                 15.4395\n",
      "trainer/Log Pis Mean                                   7.86882\n",
      "trainer/Log Pis Std                                    2.28365\n",
      "trainer/Log Pis Max                                   13.8587\n",
      "trainer/Log Pis Min                                   -4.90607\n",
      "trainer/Policy mu Mean                                 0.0152552\n",
      "trainer/Policy mu Std                                  0.144254\n",
      "trainer/Policy mu Max                                  1.25036\n",
      "trainer/Policy mu Min                                 -0.753862\n",
      "trainer/Policy log std Mean                           -2.39172\n",
      "trainer/Policy log std Std                             0.20462\n",
      "trainer/Policy log std Max                            -1.59682\n",
      "trainer/Policy log std Min                            -3.2058\n",
      "trainer/Alpha                                          0.00885287\n",
      "trainer/Alpha Loss                                    -0.620154\n",
      "exploration/num steps total                        67000\n",
      "exploration/num paths total                          119\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.937796\n",
      "exploration/Rewards Std                                0.0486596\n",
      "exploration/Rewards Max                                1.37276\n",
      "exploration/Rewards Min                                0.796492\n",
      "exploration/Returns Mean                             937.796\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              937.796\n",
      "exploration/Returns Min                              937.796\n",
      "exploration/Actions Mean                               0.0022095\n",
      "exploration/Actions Std                                0.130428\n",
      "exploration/Actions Max                                0.46255\n",
      "exploration/Actions Min                               -0.449403\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          937.796\n",
      "exploration/env_infos/final/reward_forward Mean        0.0454293\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0454293\n",
      "exploration/env_infos/final/reward_forward Min         0.0454293\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.00254486\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.00254486\n",
      "exploration/env_infos/initial/reward_forward Min      -0.00254486\n",
      "exploration/env_infos/reward_forward Mean             -0.0181859\n",
      "exploration/env_infos/reward_forward Std               0.160613\n",
      "exploration/env_infos/reward_forward Max               0.721483\n",
      "exploration/env_infos/reward_forward Min              -0.702021\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.050254\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.050254\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.050254\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0648469\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0648469\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0648469\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0680651\n",
      "exploration/env_infos/reward_ctrl Std                  0.0337452\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00962279\n",
      "exploration/env_infos/reward_ctrl Min                 -0.203508\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0987693\n",
      "exploration/env_infos/final/torso_velocity Std         0.102215\n",
      "exploration/env_infos/final/torso_velocity Max         0.0454293\n",
      "exploration/env_infos/final/torso_velocity Min        -0.179635\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.1662\n",
      "exploration/env_infos/initial/torso_velocity Std       0.18553\n",
      "exploration/env_infos/initial/torso_velocity Max       0.424572\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.00254486\n",
      "exploration/env_infos/torso_velocity Mean             -0.0107402\n",
      "exploration/env_infos/torso_velocity Std               0.189662\n",
      "exploration/env_infos/torso_velocity Max               0.840264\n",
      "exploration/env_infos/torso_velocity Min              -1.09987\n",
      "evaluation/num steps total                             1.65e+06\n",
      "evaluation/num paths total                          1650\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.95192\n",
      "evaluation/Rewards Std                                 0.049593\n",
      "evaluation/Rewards Max                                 2.71313\n",
      "evaluation/Rewards Min                                 0.749896\n",
      "evaluation/Returns Mean                              951.92\n",
      "evaluation/Returns Std                                34.9874\n",
      "evaluation/Returns Max                               985.47\n",
      "evaluation/Returns Min                               852.516\n",
      "evaluation/Actions Mean                               -0.00354304\n",
      "evaluation/Actions Std                                 0.111483\n",
      "evaluation/Actions Max                                 0.561777\n",
      "evaluation/Actions Min                                -0.477432\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           951.92\n",
      "evaluation/env_infos/final/reward_forward Mean        -6.15282e-06\n",
      "evaluation/env_infos/final/reward_forward Std          3.01158e-05\n",
      "evaluation/env_infos/final/reward_forward Max          6.47028e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000153679\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0169343\n",
      "evaluation/env_infos/initial/reward_forward Std        0.155578\n",
      "evaluation/env_infos/initial/reward_forward Max        0.369055\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.371389\n",
      "evaluation/env_infos/reward_forward Mean              -0.00048265\n",
      "evaluation/env_infos/reward_forward Std                0.0604516\n",
      "evaluation/env_infos/reward_forward Max                1.62103\n",
      "evaluation/env_infos/reward_forward Min               -1.05714\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0497824\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0345957\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.015353\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.149117\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.027965\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00799811\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0123693\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0400598\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0497642\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0349095\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00872951\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.250104\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -5.17431e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          2.5806e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          1.08549e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000153679\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.160331\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.256868\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.629235\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.371389\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00301392\n",
      "evaluation/env_infos/torso_velocity Std                0.0658845\n",
      "evaluation/env_infos/torso_velocity Max                1.62103\n",
      "evaluation/env_infos/torso_velocity Min               -1.99125\n",
      "time/data storing (s)                                  0.0163236\n",
      "time/evaluation sampling (s)                          51.979\n",
      "time/exploration sampling (s)                          2.36317\n",
      "time/logging (s)                                       0.295001\n",
      "time/saving (s)                                        0.0304924\n",
      "time/training (s)                                      4.54056\n",
      "time/epoch (s)                                        59.2246\n",
      "time/total (s)                                      3817.23\n",
      "Epoch                                                 65\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:14:58.566130 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 66 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 68000\n",
      "trainer/QF1 Loss                                       0.821071\n",
      "trainer/QF2 Loss                                       0.323066\n",
      "trainer/Policy Loss                                  -14.7728\n",
      "trainer/Q1 Predictions Mean                           23.2482\n",
      "trainer/Q1 Predictions Std                             2.75937\n",
      "trainer/Q1 Predictions Max                            25.4467\n",
      "trainer/Q1 Predictions Min                           -13.6939\n",
      "trainer/Q2 Predictions Mean                           23.4288\n",
      "trainer/Q2 Predictions Std                             2.38312\n",
      "trainer/Q2 Predictions Max                            25.0582\n",
      "trainer/Q2 Predictions Min                            -7.21194\n",
      "trainer/Q Targets Mean                                23.2111\n",
      "trainer/Q Targets Std                                  2.03972\n",
      "trainer/Q Targets Max                                 24.9088\n",
      "trainer/Q Targets Min                                 -0.786591\n",
      "trainer/Log Pis Mean                                   8.65661\n",
      "trainer/Log Pis Std                                    4.61531\n",
      "trainer/Log Pis Max                                   67.3749\n",
      "trainer/Log Pis Min                                   -0.00533998\n",
      "trainer/Policy mu Mean                                 0.00725212\n",
      "trainer/Policy mu Std                                  0.29377\n",
      "trainer/Policy mu Max                                  7.7835\n",
      "trainer/Policy mu Min                                 -3.93313\n",
      "trainer/Policy log std Mean                           -2.46216\n",
      "trainer/Policy log std Std                             0.274799\n",
      "trainer/Policy log std Max                            -1.35535\n",
      "trainer/Policy log std Min                            -6.45685\n",
      "trainer/Alpha                                          0.00888078\n",
      "trainer/Alpha Loss                                     3.10133\n",
      "exploration/num steps total                        68000\n",
      "exploration/num paths total                          120\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.86264\n",
      "exploration/Rewards Std                                0.0457582\n",
      "exploration/Rewards Max                                1.62868\n",
      "exploration/Rewards Min                                0.680878\n",
      "exploration/Returns Mean                             862.64\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              862.64\n",
      "exploration/Returns Min                              862.64\n",
      "exploration/Actions Mean                              -0.00801739\n",
      "exploration/Actions Std                                0.185744\n",
      "exploration/Actions Max                                0.48804\n",
      "exploration/Actions Min                               -0.496527\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          862.64\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0992577\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0992577\n",
      "exploration/env_infos/final/reward_forward Min        -0.0992577\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.103901\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.103901\n",
      "exploration/env_infos/initial/reward_forward Min      -0.103901\n",
      "exploration/env_infos/reward_forward Mean             -0.00577426\n",
      "exploration/env_infos/reward_forward Std               0.0688157\n",
      "exploration/env_infos/reward_forward Max               0.501473\n",
      "exploration/env_infos/reward_forward Min              -1.2625\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0747811\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0747811\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0747811\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0758396\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0758396\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0758396\n",
      "exploration/env_infos/reward_ctrl Mean                -0.13826\n",
      "exploration/env_infos/reward_ctrl Std                  0.0388336\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0294086\n",
      "exploration/env_infos/reward_ctrl Min                 -0.319122\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0876259\n",
      "exploration/env_infos/final/torso_velocity Std         0.0257205\n",
      "exploration/env_infos/final/torso_velocity Max        -0.051963\n",
      "exploration/env_infos/final/torso_velocity Min        -0.111657\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.140813\n",
      "exploration/env_infos/initial/torso_velocity Std       0.25207\n",
      "exploration/env_infos/initial/torso_velocity Max       0.48766\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.103901\n",
      "exploration/env_infos/torso_velocity Mean             -0.00413362\n",
      "exploration/env_infos/torso_velocity Std               0.0728026\n",
      "exploration/env_infos/torso_velocity Max               0.520145\n",
      "exploration/env_infos/torso_velocity Min              -1.2625\n",
      "evaluation/num steps total                             1.675e+06\n",
      "evaluation/num paths total                          1675\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.898753\n",
      "evaluation/Rewards Std                                 0.0481631\n",
      "evaluation/Rewards Max                                 2.05939\n",
      "evaluation/Rewards Min                                 0.610846\n",
      "evaluation/Returns Mean                              898.753\n",
      "evaluation/Returns Std                                44.2262\n",
      "evaluation/Returns Max                               968.461\n",
      "evaluation/Returns Min                               839.812\n",
      "evaluation/Actions Mean                               -0.0172809\n",
      "evaluation/Actions Std                                 0.158612\n",
      "evaluation/Actions Max                                 0.605351\n",
      "evaluation/Actions Min                                -0.450472\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           898.753\n",
      "evaluation/env_infos/final/reward_forward Mean         1.27389e-05\n",
      "evaluation/env_infos/final/reward_forward Std          6.26405e-05\n",
      "evaluation/env_infos/final/reward_forward Max          0.000319607\n",
      "evaluation/env_infos/final/reward_forward Min         -8.13315e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0164144\n",
      "evaluation/env_infos/initial/reward_forward Std        0.119483\n",
      "evaluation/env_infos/initial/reward_forward Max        0.328396\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.19473\n",
      "evaluation/env_infos/reward_forward Mean              -0.00128574\n",
      "evaluation/env_infos/reward_forward Std                0.0625796\n",
      "evaluation/env_infos/reward_forward Max                1.83397\n",
      "evaluation/env_infos/reward_forward Min               -1.66382\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.102015\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0444742\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0309544\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.161048\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0623229\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0129686\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0402676\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0877584\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.101825\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0448994\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0233326\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.389154\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         6.95544e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          5.19383e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000319607\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000106134\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.159006\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.230871\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.5966\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.312158\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00284874\n",
      "evaluation/env_infos/torso_velocity Std                0.0631668\n",
      "evaluation/env_infos/torso_velocity Max                1.83397\n",
      "evaluation/env_infos/torso_velocity Min               -1.88928\n",
      "time/data storing (s)                                  0.0161061\n",
      "time/evaluation sampling (s)                          51.4498\n",
      "time/exploration sampling (s)                          2.58856\n",
      "time/logging (s)                                       0.282695\n",
      "time/saving (s)                                        0.0270269\n",
      "time/training (s)                                      4.46749\n",
      "time/epoch (s)                                        58.8316\n",
      "time/total (s)                                      3876.7\n",
      "Epoch                                                 66\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:15:55.934877 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 67 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 69000\n",
      "trainer/QF1 Loss                                       0.163306\n",
      "trainer/QF2 Loss                                       0.22872\n",
      "trainer/Policy Loss                                  -17.2405\n",
      "trainer/Q1 Predictions Mean                           23.717\n",
      "trainer/Q1 Predictions Std                             1.30204\n",
      "trainer/Q1 Predictions Max                            24.9259\n",
      "trainer/Q1 Predictions Min                            17.9062\n",
      "trainer/Q2 Predictions Mean                           23.773\n",
      "trainer/Q2 Predictions Std                             1.19323\n",
      "trainer/Q2 Predictions Max                            25.072\n",
      "trainer/Q2 Predictions Min                            18.3966\n",
      "trainer/Q Targets Mean                                23.6375\n",
      "trainer/Q Targets Std                                  1.27692\n",
      "trainer/Q Targets Max                                 25.1288\n",
      "trainer/Q Targets Min                                 15.6674\n",
      "trainer/Log Pis Mean                                   6.60219\n",
      "trainer/Log Pis Std                                    2.27476\n",
      "trainer/Log Pis Max                                   11.8969\n",
      "trainer/Log Pis Min                                   -2.53609\n",
      "trainer/Policy mu Mean                                -0.00173428\n",
      "trainer/Policy mu Std                                  0.135064\n",
      "trainer/Policy mu Max                                  0.947963\n",
      "trainer/Policy mu Min                                 -0.665139\n",
      "trainer/Policy log std Mean                           -2.21692\n",
      "trainer/Policy log std Std                             0.218017\n",
      "trainer/Policy log std Max                            -1.46861\n",
      "trainer/Policy log std Min                            -2.81412\n",
      "trainer/Alpha                                          0.00877858\n",
      "trainer/Alpha Loss                                    -6.61445\n",
      "exploration/num steps total                        69000\n",
      "exploration/num paths total                          121\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.88082\n",
      "exploration/Rewards Std                                0.0467709\n",
      "exploration/Rewards Max                                1.04798\n",
      "exploration/Rewards Min                                0.67991\n",
      "exploration/Returns Mean                             880.82\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              880.82\n",
      "exploration/Returns Min                              880.82\n",
      "exploration/Actions Mean                              -0.0227659\n",
      "exploration/Actions Std                                0.171408\n",
      "exploration/Actions Max                                0.505826\n",
      "exploration/Actions Min                               -0.587856\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          880.82\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0568903\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0568903\n",
      "exploration/env_infos/final/reward_forward Min        -0.0568903\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.062461\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.062461\n",
      "exploration/env_infos/initial/reward_forward Min      -0.062461\n",
      "exploration/env_infos/reward_forward Mean             -0.0219261\n",
      "exploration/env_infos/reward_forward Std               0.146762\n",
      "exploration/env_infos/reward_forward Max               1.46364\n",
      "exploration/env_infos/reward_forward Min              -0.594581\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.156376\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.156376\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.156376\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0533212\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0533212\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0533212\n",
      "exploration/env_infos/reward_ctrl Mean                -0.119596\n",
      "exploration/env_infos/reward_ctrl Std                  0.0463375\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0190337\n",
      "exploration/env_infos/reward_ctrl Min                 -0.32009\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.012111\n",
      "exploration/env_infos/final/torso_velocity Std         0.052785\n",
      "exploration/env_infos/final/torso_velocity Max         0.0620038\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0568903\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.115402\n",
      "exploration/env_infos/initial/torso_velocity Std       0.252668\n",
      "exploration/env_infos/initial/torso_velocity Max       0.472727\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0640613\n",
      "exploration/env_infos/torso_velocity Mean             -0.00197493\n",
      "exploration/env_infos/torso_velocity Std               0.148147\n",
      "exploration/env_infos/torso_velocity Max               1.46364\n",
      "exploration/env_infos/torso_velocity Min              -1.25262\n",
      "evaluation/num steps total                             1.7e+06\n",
      "evaluation/num paths total                          1700\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.930517\n",
      "evaluation/Rewards Std                                 0.0418683\n",
      "evaluation/Rewards Max                                 2.49516\n",
      "evaluation/Rewards Min                                 0.447736\n",
      "evaluation/Returns Mean                              930.517\n",
      "evaluation/Returns Std                                28.0714\n",
      "evaluation/Returns Max                               980.001\n",
      "evaluation/Returns Min                               896.494\n",
      "evaluation/Actions Mean                               -0.0170461\n",
      "evaluation/Actions Std                                 0.131889\n",
      "evaluation/Actions Max                                 0.667354\n",
      "evaluation/Actions Min                                -0.687948\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           930.517\n",
      "evaluation/env_infos/final/reward_forward Mean         2.2505e-09\n",
      "evaluation/env_infos/final/reward_forward Std          4.19394e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.73412e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -6.81762e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0256904\n",
      "evaluation/env_infos/initial/reward_forward Std        0.139556\n",
      "evaluation/env_infos/initial/reward_forward Max        0.233303\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.299699\n",
      "evaluation/env_infos/reward_forward Mean              -0.00146622\n",
      "evaluation/env_infos/reward_forward Std                0.0586807\n",
      "evaluation/env_infos/reward_forward Max                1.72618\n",
      "evaluation/env_infos/reward_forward Min               -1.14621\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0706996\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0286423\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0191636\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.105585\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0362226\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0114748\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0112089\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0619909\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.070741\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0303298\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0100107\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.552264\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.48922e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.37367e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          8.69591e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.27698e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.12797\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.253724\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.577841\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.299699\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00326117\n",
      "evaluation/env_infos/torso_velocity Std                0.0687363\n",
      "evaluation/env_infos/torso_velocity Max                1.72618\n",
      "evaluation/env_infos/torso_velocity Min               -1.97166\n",
      "time/data storing (s)                                  0.0166339\n",
      "time/evaluation sampling (s)                          48.6426\n",
      "time/exploration sampling (s)                          2.28508\n",
      "time/logging (s)                                       0.28504\n",
      "time/saving (s)                                        0.0277728\n",
      "time/training (s)                                      5.50264\n",
      "time/epoch (s)                                        56.7597\n",
      "time/total (s)                                      3934.07\n",
      "Epoch                                                 67\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:16:51.034724 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 68 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 70000\n",
      "trainer/QF1 Loss                                       0.943996\n",
      "trainer/QF2 Loss                                       0.212501\n",
      "trainer/Policy Loss                                  -16.2768\n",
      "trainer/Q1 Predictions Mean                           23.9538\n",
      "trainer/Q1 Predictions Std                             2.57137\n",
      "trainer/Q1 Predictions Max                            26.0039\n",
      "trainer/Q1 Predictions Min                           -10.1278\n",
      "trainer/Q2 Predictions Mean                           23.7406\n",
      "trainer/Q2 Predictions Std                             2.21968\n",
      "trainer/Q2 Predictions Max                            25.9407\n",
      "trainer/Q2 Predictions Min                             2.77778\n",
      "trainer/Q Targets Mean                                23.7731\n",
      "trainer/Q Targets Std                                  2.37328\n",
      "trainer/Q Targets Max                                 26.5346\n",
      "trainer/Q Targets Min                                 -0.41638\n",
      "trainer/Log Pis Mean                                   7.67321\n",
      "trainer/Log Pis Std                                    2.30938\n",
      "trainer/Log Pis Max                                   18.1344\n",
      "trainer/Log Pis Min                                    0.379451\n",
      "trainer/Policy mu Mean                                -0.00270677\n",
      "trainer/Policy mu Std                                  0.171896\n",
      "trainer/Policy mu Max                                  3.35589\n",
      "trainer/Policy mu Min                                 -0.854816\n",
      "trainer/Policy log std Mean                           -2.33777\n",
      "trainer/Policy log std Std                             0.240746\n",
      "trainer/Policy log std Max                            -1.40358\n",
      "trainer/Policy log std Min                            -3.6659\n",
      "trainer/Alpha                                          0.00874031\n",
      "trainer/Alpha Loss                                    -1.54856\n",
      "exploration/num steps total                        70000\n",
      "exploration/num paths total                          122\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.912118\n",
      "exploration/Rewards Std                                0.0648246\n",
      "exploration/Rewards Max                                1.43541\n",
      "exploration/Rewards Min                                0.710621\n",
      "exploration/Returns Mean                             912.118\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              912.118\n",
      "exploration/Returns Min                              912.118\n",
      "exploration/Actions Mean                              -0.00289271\n",
      "exploration/Actions Std                                0.154477\n",
      "exploration/Actions Max                                0.534693\n",
      "exploration/Actions Min                               -0.51004\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          912.118\n",
      "exploration/env_infos/final/reward_forward Mean        0.0374614\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0374614\n",
      "exploration/env_infos/final/reward_forward Min         0.0374614\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0635379\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0635379\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0635379\n",
      "exploration/env_infos/reward_forward Mean             -0.000950167\n",
      "exploration/env_infos/reward_forward Std               0.14168\n",
      "exploration/env_infos/reward_forward Max               0.561563\n",
      "exploration/env_infos/reward_forward Min              -0.79672\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.105125\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.105125\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.105125\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.125299\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.125299\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.125299\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0954865\n",
      "exploration/env_infos/reward_ctrl Std                  0.0442486\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00767623\n",
      "exploration/env_infos/reward_ctrl Min                 -0.289379\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0506989\n",
      "exploration/env_infos/final/torso_velocity Std         0.0132951\n",
      "exploration/env_infos/final/torso_velocity Max         0.0688813\n",
      "exploration/env_infos/final/torso_velocity Min         0.0374614\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0845118\n",
      "exploration/env_infos/initial/torso_velocity Std       0.272756\n",
      "exploration/env_infos/initial/torso_velocity Max       0.467008\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.149935\n",
      "exploration/env_infos/torso_velocity Mean             -0.00143416\n",
      "exploration/env_infos/torso_velocity Std               0.16982\n",
      "exploration/env_infos/torso_velocity Max               1.01763\n",
      "exploration/env_infos/torso_velocity Min              -1.54564\n",
      "evaluation/num steps total                             1.725e+06\n",
      "evaluation/num paths total                          1725\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.955774\n",
      "evaluation/Rewards Std                                 0.0466389\n",
      "evaluation/Rewards Max                                 2.59667\n",
      "evaluation/Rewards Min                                 0.664504\n",
      "evaluation/Returns Mean                              955.774\n",
      "evaluation/Returns Std                                27.0292\n",
      "evaluation/Returns Max                               989.455\n",
      "evaluation/Returns Min                               846.999\n",
      "evaluation/Actions Mean                               -0.0244036\n",
      "evaluation/Actions Std                                 0.103959\n",
      "evaluation/Actions Max                                 0.52282\n",
      "evaluation/Actions Min                                -0.517551\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           955.774\n",
      "evaluation/env_infos/final/reward_forward Mean         9.37094e-08\n",
      "evaluation/env_infos/final/reward_forward Std          3.18253e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.24105e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -2.85342e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0129294\n",
      "evaluation/env_infos/initial/reward_forward Std        0.109683\n",
      "evaluation/env_infos/initial/reward_forward Max        0.292144\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.183399\n",
      "evaluation/env_infos/reward_forward Mean              -0.00108785\n",
      "evaluation/env_infos/reward_forward Std                0.0767376\n",
      "evaluation/env_infos/reward_forward Max                1.57061\n",
      "evaluation/env_infos/reward_forward Min               -1.91318\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.045238\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0276952\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0150562\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.156861\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0398709\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0188\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0132686\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0875747\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0456122\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0291522\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00912734\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.341021\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.11799e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          2.69393e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.01097e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -3.76064e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.123773\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.244696\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.682475\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.27155\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00313939\n",
      "evaluation/env_infos/torso_velocity Std                0.0732587\n",
      "evaluation/env_infos/torso_velocity Max                1.57061\n",
      "evaluation/env_infos/torso_velocity Min               -1.91318\n",
      "time/data storing (s)                                  0.0166279\n",
      "time/evaluation sampling (s)                          47.3291\n",
      "time/exploration sampling (s)                          2.09441\n",
      "time/logging (s)                                       0.292919\n",
      "time/saving (s)                                        0.0280907\n",
      "time/training (s)                                      4.68337\n",
      "time/epoch (s)                                        54.4446\n",
      "time/total (s)                                      3989.18\n",
      "Epoch                                                 68\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:17:54.378373 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 69 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 71000\n",
      "trainer/QF1 Loss                                       0.193418\n",
      "trainer/QF2 Loss                                       0.153991\n",
      "trainer/Policy Loss                                  -16.6771\n",
      "trainer/Q1 Predictions Mean                           24.363\n",
      "trainer/Q1 Predictions Std                             1.25733\n",
      "trainer/Q1 Predictions Max                            26.6598\n",
      "trainer/Q1 Predictions Min                            18.0502\n",
      "trainer/Q2 Predictions Mean                           24.0913\n",
      "trainer/Q2 Predictions Std                             1.28875\n",
      "trainer/Q2 Predictions Max                            25.7192\n",
      "trainer/Q2 Predictions Min                            15.903\n",
      "trainer/Q Targets Mean                                24.1714\n",
      "trainer/Q Targets Std                                  1.27182\n",
      "trainer/Q Targets Max                                 26.0921\n",
      "trainer/Q Targets Min                                 16.4494\n",
      "trainer/Log Pis Mean                                   7.61048\n",
      "trainer/Log Pis Std                                    2.50906\n",
      "trainer/Log Pis Max                                   15.6102\n",
      "trainer/Log Pis Min                                   -2.32701\n",
      "trainer/Policy mu Mean                                -0.00597785\n",
      "trainer/Policy mu Std                                  0.14009\n",
      "trainer/Policy mu Max                                  0.736766\n",
      "trainer/Policy mu Min                                 -0.725741\n",
      "trainer/Policy log std Mean                           -2.33579\n",
      "trainer/Policy log std Std                             0.195848\n",
      "trainer/Policy log std Max                            -1.58697\n",
      "trainer/Policy log std Min                            -3.38992\n",
      "trainer/Alpha                                          0.00884678\n",
      "trainer/Alpha Loss                                    -1.84106\n",
      "exploration/num steps total                        71000\n",
      "exploration/num paths total                          123\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.961274\n",
      "exploration/Rewards Std                                0.101193\n",
      "exploration/Rewards Max                                1.98255\n",
      "exploration/Rewards Min                                0.75139\n",
      "exploration/Returns Mean                             961.274\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              961.274\n",
      "exploration/Returns Min                              961.274\n",
      "exploration/Actions Mean                              -0.00821673\n",
      "exploration/Actions Std                                0.126989\n",
      "exploration/Actions Max                                0.539178\n",
      "exploration/Actions Min                               -0.473391\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          961.274\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0350289\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0350289\n",
      "exploration/env_infos/final/reward_forward Min        -0.0350289\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0291618\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0291618\n",
      "exploration/env_infos/initial/reward_forward Min       0.0291618\n",
      "exploration/env_infos/reward_forward Mean             -0.00553748\n",
      "exploration/env_infos/reward_forward Std               0.241213\n",
      "exploration/env_infos/reward_forward Max               1.05566\n",
      "exploration/env_infos/reward_forward Min              -0.958816\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0512917\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0512917\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0512917\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0378737\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0378737\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0378737\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0647749\n",
      "exploration/env_infos/reward_ctrl Std                  0.0344467\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00521357\n",
      "exploration/env_infos/reward_ctrl Min                 -0.24861\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.102007\n",
      "exploration/env_infos/final/torso_velocity Std         0.0486099\n",
      "exploration/env_infos/final/torso_velocity Max        -0.0350289\n",
      "exploration/env_infos/final/torso_velocity Min        -0.148906\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.167798\n",
      "exploration/env_infos/initial/torso_velocity Std       0.144213\n",
      "exploration/env_infos/initial/torso_velocity Max       0.366659\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0291618\n",
      "exploration/env_infos/torso_velocity Mean             -0.00253072\n",
      "exploration/env_infos/torso_velocity Std               0.22018\n",
      "exploration/env_infos/torso_velocity Max               1.05566\n",
      "exploration/env_infos/torso_velocity Min              -1.80486\n",
      "evaluation/num steps total                             1.75e+06\n",
      "evaluation/num paths total                          1750\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.928147\n",
      "evaluation/Rewards Std                                 0.0672335\n",
      "evaluation/Rewards Max                                 2.71484\n",
      "evaluation/Rewards Min                                 0.704192\n",
      "evaluation/Returns Mean                              928.147\n",
      "evaluation/Returns Std                                54.7156\n",
      "evaluation/Returns Max                               984.446\n",
      "evaluation/Returns Min                               821.418\n",
      "evaluation/Actions Mean                               -0.0122884\n",
      "evaluation/Actions Std                                 0.134854\n",
      "evaluation/Actions Max                                 0.471908\n",
      "evaluation/Actions Min                                -0.633332\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           928.147\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.15093e-07\n",
      "evaluation/env_infos/final/reward_forward Std          4.46688e-07\n",
      "evaluation/env_infos/final/reward_forward Max          9.06264e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -9.35494e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00727127\n",
      "evaluation/env_infos/initial/reward_forward Std        0.149855\n",
      "evaluation/env_infos/initial/reward_forward Max        0.426623\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.246192\n",
      "evaluation/env_infos/reward_forward Mean              -0.00267204\n",
      "evaluation/env_infos/reward_forward Std                0.0667867\n",
      "evaluation/env_infos/reward_forward Max                1.7367\n",
      "evaluation/env_infos/reward_forward Min               -1.79195\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.074149\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0567234\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0164341\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.185554\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0462633\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0310651\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0094057\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.150131\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.073346\n",
      "evaluation/env_infos/reward_ctrl Std                   0.056033\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00503968\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.295808\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.6274e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          4.61147e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.06264e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.28721e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.170051\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.245688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/torso_velocity Max        0.647338\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.271456\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00025063\n",
      "evaluation/env_infos/torso_velocity Std                0.0687412\n",
      "evaluation/env_infos/torso_velocity Max                1.7367\n",
      "evaluation/env_infos/torso_velocity Min               -2.2043\n",
      "time/data storing (s)                                  0.0153625\n",
      "time/evaluation sampling (s)                          53.5379\n",
      "time/exploration sampling (s)                          2.39403\n",
      "time/logging (s)                                       0.30185\n",
      "time/saving (s)                                        0.0305447\n",
      "time/training (s)                                      6.36696\n",
      "time/epoch (s)                                        62.6466\n",
      "time/total (s)                                      4052.53\n",
      "Epoch                                                 69\n",
      "-------------------------------------------------  ---------------\n",
      "2021-05-25 20:18:54.431352 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 70 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 72000\n",
      "trainer/QF1 Loss                                       0.153015\n",
      "trainer/QF2 Loss                                       0.122321\n",
      "trainer/Policy Loss                                  -17.4067\n",
      "trainer/Q1 Predictions Mean                           24.4899\n",
      "trainer/Q1 Predictions Std                             1.52331\n",
      "trainer/Q1 Predictions Max                            26.6884\n",
      "trainer/Q1 Predictions Min                            17.4264\n",
      "trainer/Q2 Predictions Mean                           24.5992\n",
      "trainer/Q2 Predictions Std                             1.45074\n",
      "trainer/Q2 Predictions Max                            26.5251\n",
      "trainer/Q2 Predictions Min                            17.4907\n",
      "trainer/Q Targets Mean                                24.4145\n",
      "trainer/Q Targets Std                                  1.43793\n",
      "trainer/Q Targets Max                                 26.2112\n",
      "trainer/Q Targets Min                                 18.2327\n",
      "trainer/Log Pis Mean                                   7.23858\n",
      "trainer/Log Pis Std                                    2.33225\n",
      "trainer/Log Pis Max                                   18.8263\n",
      "trainer/Log Pis Min                                    0.0888846\n",
      "trainer/Policy mu Mean                                -0.0347515\n",
      "trainer/Policy mu Std                                  0.160356\n",
      "trainer/Policy mu Max                                  1.32127\n",
      "trainer/Policy mu Min                                 -1.05461\n",
      "trainer/Policy log std Mean                           -2.27879\n",
      "trainer/Policy log std Std                             0.221523\n",
      "trainer/Policy log std Max                            -1.2888\n",
      "trainer/Policy log std Min                            -3.52327\n",
      "trainer/Alpha                                          0.00853131\n",
      "trainer/Alpha Loss                                    -3.62552\n",
      "exploration/num steps total                        72000\n",
      "exploration/num paths total                          124\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.901086\n",
      "exploration/Rewards Std                                0.0470588\n",
      "exploration/Rewards Max                                1.42574\n",
      "exploration/Rewards Min                                0.735774\n",
      "exploration/Returns Mean                             901.086\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              901.086\n",
      "exploration/Returns Min                              901.086\n",
      "exploration/Actions Mean                              -0.0536912\n",
      "exploration/Actions Std                                0.150118\n",
      "exploration/Actions Max                                0.428756\n",
      "exploration/Actions Min                               -0.491148\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          901.086\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0543545\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0543545\n",
      "exploration/env_infos/final/reward_forward Min        -0.0543545\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0157979\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0157979\n",
      "exploration/env_infos/initial/reward_forward Min       0.0157979\n",
      "exploration/env_infos/reward_forward Mean             -0.00613926\n",
      "exploration/env_infos/reward_forward Std               0.0720824\n",
      "exploration/env_infos/reward_forward Max               0.504567\n",
      "exploration/env_infos/reward_forward Min              -0.3322\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0812289\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0812289\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0812289\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0345827\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0345827\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0345827\n",
      "exploration/env_infos/reward_ctrl Mean                -0.101673\n",
      "exploration/env_infos/reward_ctrl Std                  0.0394804\n",
      "exploration/env_infos/reward_ctrl Max                 -0.024503\n",
      "exploration/env_infos/reward_ctrl Min                 -0.264226\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0118304\n",
      "exploration/env_infos/final/torso_velocity Std         0.0355549\n",
      "exploration/env_infos/final/torso_velocity Max         0.0326696\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0543545\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0240474\n",
      "exploration/env_infos/initial/torso_velocity Std       0.251436\n",
      "exploration/env_infos/initial/torso_velocity Max       0.336035\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.27969\n",
      "exploration/env_infos/torso_velocity Mean             -0.00651402\n",
      "exploration/env_infos/torso_velocity Std               0.083413\n",
      "exploration/env_infos/torso_velocity Max               0.635198\n",
      "exploration/env_infos/torso_velocity Min              -1.07121\n",
      "evaluation/num steps total                             1.775e+06\n",
      "evaluation/num paths total                          1775\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.936654\n",
      "evaluation/Rewards Std                                 0.0449383\n",
      "evaluation/Rewards Max                                 2.59662\n",
      "evaluation/Rewards Min                                 0.451823\n",
      "evaluation/Returns Mean                              936.654\n",
      "evaluation/Returns Std                                27.7818\n",
      "evaluation/Returns Max                               971.413\n",
      "evaluation/Returns Min                               881.397\n",
      "evaluation/Actions Mean                               -0.0501564\n",
      "evaluation/Actions Std                                 0.116915\n",
      "evaluation/Actions Max                                 0.475443\n",
      "evaluation/Actions Min                                -0.56503\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           936.654\n",
      "evaluation/env_infos/final/reward_forward Mean        -2.52138e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.85075e-07\n",
      "evaluation/env_infos/final/reward_forward Max          6.31637e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -6.28011e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00461983\n",
      "evaluation/env_infos/initial/reward_forward Std        0.147796\n",
      "evaluation/env_infos/initial/reward_forward Max        0.215885\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.335065\n",
      "evaluation/env_infos/reward_forward Mean               0.00169812\n",
      "evaluation/env_infos/reward_forward Std                0.0653327\n",
      "evaluation/env_infos/reward_forward Max                1.67742\n",
      "evaluation/env_infos/reward_forward Min               -1.40308\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0643452\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0288585\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0281695\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.121547\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0297892\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00453892\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0223507\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0406736\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0647392\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0301451\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00862084\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.548177\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         8.21683e-09\n",
      "evaluation/env_infos/final/torso_velocity Std          2.63456e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.48867e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -6.28011e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.140086\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.225558\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.629733\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.335065\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00254188\n",
      "evaluation/env_infos/torso_velocity Std                0.0735234\n",
      "evaluation/env_infos/torso_velocity Max                1.67742\n",
      "evaluation/env_infos/torso_velocity Min               -1.81775\n",
      "time/data storing (s)                                  0.0154838\n",
      "time/evaluation sampling (s)                          51.4571\n",
      "time/exploration sampling (s)                          2.24908\n",
      "time/logging (s)                                       0.295557\n",
      "time/saving (s)                                        0.0293889\n",
      "time/training (s)                                      5.29569\n",
      "time/epoch (s)                                        59.3423\n",
      "time/total (s)                                      4112.57\n",
      "Epoch                                                 70\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:19:55.697529 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 71 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 73000\n",
      "trainer/QF1 Loss                                       0.164651\n",
      "trainer/QF2 Loss                                       0.197117\n",
      "trainer/Policy Loss                                  -17.1372\n",
      "trainer/Q1 Predictions Mean                           24.7399\n",
      "trainer/Q1 Predictions Std                             1.36328\n",
      "trainer/Q1 Predictions Max                            28.0096\n",
      "trainer/Q1 Predictions Min                            17.704\n",
      "trainer/Q2 Predictions Mean                           24.8101\n",
      "trainer/Q2 Predictions Std                             1.31806\n",
      "trainer/Q2 Predictions Max                            26.8896\n",
      "trainer/Q2 Predictions Min                            18.859\n",
      "trainer/Q Targets Mean                                24.8639\n",
      "trainer/Q Targets Std                                  1.34929\n",
      "trainer/Q Targets Max                                 29.0298\n",
      "trainer/Q Targets Min                                 18.4655\n",
      "trainer/Log Pis Mean                                   7.74287\n",
      "trainer/Log Pis Std                                    2.09876\n",
      "trainer/Log Pis Max                                   12.7045\n",
      "trainer/Log Pis Min                                   -2.58248\n",
      "trainer/Policy mu Mean                                 0.0103161\n",
      "trainer/Policy mu Std                                  0.114203\n",
      "trainer/Policy mu Max                                  0.752292\n",
      "trainer/Policy mu Min                                 -0.661057\n",
      "trainer/Policy log std Mean                           -2.32198\n",
      "trainer/Policy log std Std                             0.200271\n",
      "trainer/Policy log std Max                            -1.47358\n",
      "trainer/Policy log std Min                            -3.12632\n",
      "trainer/Alpha                                          0.00846565\n",
      "trainer/Alpha Loss                                    -1.22638\n",
      "exploration/num steps total                        73000\n",
      "exploration/num paths total                          125\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.924051\n",
      "exploration/Rewards Std                                0.0431835\n",
      "exploration/Rewards Max                                1.59098\n",
      "exploration/Rewards Min                                0.776053\n",
      "exploration/Returns Mean                             924.051\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              924.051\n",
      "exploration/Returns Min                              924.051\n",
      "exploration/Actions Mean                               0.0281662\n",
      "exploration/Actions Std                                0.137758\n",
      "exploration/Actions Max                                0.455949\n",
      "exploration/Actions Min                               -0.461298\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          924.051\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0292079\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0292079\n",
      "exploration/env_infos/final/reward_forward Min        -0.0292079\n",
      "exploration/env_infos/initial/reward_forward Mean      0.148983\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.148983\n",
      "exploration/env_infos/initial/reward_forward Min       0.148983\n",
      "exploration/env_infos/reward_forward Mean              0.0343446\n",
      "exploration/env_infos/reward_forward Std               0.126537\n",
      "exploration/env_infos/reward_forward Max               0.722459\n",
      "exploration/env_infos/reward_forward Min              -0.847745\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.049282\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.049282\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.049282\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0146377\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0146377\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0146377\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0790823\n",
      "exploration/env_infos/reward_ctrl Std                  0.0327668\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00959823\n",
      "exploration/env_infos/reward_ctrl Min                 -0.223947\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00119717\n",
      "exploration/env_infos/final/torso_velocity Std         0.0605463\n",
      "exploration/env_infos/final/torso_velocity Max         0.082882\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0572656\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.127423\n",
      "exploration/env_infos/initial/torso_velocity Std       0.144911\n",
      "exploration/env_infos/initial/torso_velocity Max       0.293137\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0598509\n",
      "exploration/env_infos/torso_velocity Mean              0.00175781\n",
      "exploration/env_infos/torso_velocity Std               0.151589\n",
      "exploration/env_infos/torso_velocity Max               0.722459\n",
      "exploration/env_infos/torso_velocity Min              -0.944681\n",
      "evaluation/num steps total                             1.8e+06\n",
      "evaluation/num paths total                          1800\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.967501\n",
      "evaluation/Rewards Std                                 0.0267716\n",
      "evaluation/Rewards Max                                 2.29071\n",
      "evaluation/Rewards Min                                 0.667652\n",
      "evaluation/Returns Mean                              967.501\n",
      "evaluation/Returns Std                                11.0522\n",
      "evaluation/Returns Max                               984.422\n",
      "evaluation/Returns Min                               947.471\n",
      "evaluation/Actions Mean                               -0.00679049\n",
      "evaluation/Actions Std                                 0.0911772\n",
      "evaluation/Actions Max                                 0.515649\n",
      "evaluation/Actions Min                                -0.520184\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           967.501\n",
      "evaluation/env_infos/final/reward_forward Mean         5.81534e-08\n",
      "evaluation/env_infos/final/reward_forward Std          5.23756e-07\n",
      "evaluation/env_infos/final/reward_forward Max          1.04286e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -1.02434e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00167391\n",
      "evaluation/env_infos/initial/reward_forward Std        0.100782\n",
      "evaluation/env_infos/initial/reward_forward Max        0.22781\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.206736\n",
      "evaluation/env_infos/reward_forward Mean               0.0031867\n",
      "evaluation/env_infos/reward_forward Std                0.0670495\n",
      "evaluation/env_infos/reward_forward Max                1.86239\n",
      "evaluation/env_infos/reward_forward Min               -1.4098\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0333838\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.011689\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0148373\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.055419\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0529279\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0188268\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0221625\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0817015\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0334375\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0146391\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00450537\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.353798\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.20529e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          1.29041e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          3.20835e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -7.54561e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.129289\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.234589\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.640665\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.2328\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00164776\n",
      "evaluation/env_infos/torso_velocity Std                0.0685208\n",
      "evaluation/env_infos/torso_velocity Max                1.86239\n",
      "evaluation/env_infos/torso_velocity Min               -1.86009\n",
      "time/data storing (s)                                  0.0168348\n",
      "time/evaluation sampling (s)                          51.5995\n",
      "time/exploration sampling (s)                          2.20963\n",
      "time/logging (s)                                       0.302733\n",
      "time/saving (s)                                        0.0292447\n",
      "time/training (s)                                      6.37021\n",
      "time/epoch (s)                                        60.5281\n",
      "time/total (s)                                      4173.84\n",
      "Epoch                                                 71\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:20:55.059203 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 72 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 74000\n",
      "trainer/QF1 Loss                                       0.683902\n",
      "trainer/QF2 Loss                                       0.657893\n",
      "trainer/Policy Loss                                  -15.5922\n",
      "trainer/Q1 Predictions Mean                           24.6792\n",
      "trainer/Q1 Predictions Std                             1.76162\n",
      "trainer/Q1 Predictions Max                            26.9752\n",
      "trainer/Q1 Predictions Min                             9.64616\n",
      "trainer/Q2 Predictions Mean                           24.5921\n",
      "trainer/Q2 Predictions Std                             1.8454\n",
      "trainer/Q2 Predictions Max                            29.0693\n",
      "trainer/Q2 Predictions Min                             8.44554\n",
      "trainer/Q Targets Mean                                24.7816\n",
      "trainer/Q Targets Std                                  2.20316\n",
      "trainer/Q Targets Max                                 27.4952\n",
      "trainer/Q Targets Min                                 -1.2366\n",
      "trainer/Log Pis Mean                                   9.1887\n",
      "trainer/Log Pis Std                                    2.23832\n",
      "trainer/Log Pis Max                                   19.1805\n",
      "trainer/Log Pis Min                                    2.13627\n",
      "trainer/Policy mu Mean                                 0.0497317\n",
      "trainer/Policy mu Std                                  0.182112\n",
      "trainer/Policy mu Max                                  2.69377\n",
      "trainer/Policy mu Min                                 -0.730458\n",
      "trainer/Policy log std Mean                           -2.50481\n",
      "trainer/Policy log std Std                             0.220395\n",
      "trainer/Policy log std Max                            -1.89729\n",
      "trainer/Policy log std Min                            -3.56463\n",
      "trainer/Alpha                                          0.00832975\n",
      "trainer/Alpha Loss                                     5.69387\n",
      "exploration/num steps total                        74000\n",
      "exploration/num paths total                          126\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.888998\n",
      "exploration/Rewards Std                                0.063467\n",
      "exploration/Rewards Max                                1.71983\n",
      "exploration/Rewards Min                                0.679873\n",
      "exploration/Returns Mean                             888.998\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              888.998\n",
      "exploration/Returns Min                              888.998\n",
      "exploration/Actions Mean                               0.040004\n",
      "exploration/Actions Std                                0.166295\n",
      "exploration/Actions Max                                0.494771\n",
      "exploration/Actions Min                               -0.466294\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          888.998\n",
      "exploration/env_infos/final/reward_forward Mean        0.0424951\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0424951\n",
      "exploration/env_infos/final/reward_forward Min         0.0424951\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0145481\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0145481\n",
      "exploration/env_infos/initial/reward_forward Min       0.0145481\n",
      "exploration/env_infos/reward_forward Mean              0.0070126\n",
      "exploration/env_infos/reward_forward Std               0.0928502\n",
      "exploration/env_infos/reward_forward Max               0.38817\n",
      "exploration/env_infos/reward_forward Min              -0.815549\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.163866\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.163866\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.163866\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.167654\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.167654\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.167654\n",
      "exploration/env_infos/reward_ctrl Mean                -0.117017\n",
      "exploration/env_infos/reward_ctrl Std                  0.0473746\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0126241\n",
      "exploration/env_infos/reward_ctrl Min                 -0.320127\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0109298\n",
      "exploration/env_infos/final/torso_velocity Std         0.0252281\n",
      "exploration/env_infos/final/torso_velocity Max         0.0424951\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0192546\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.104088\n",
      "exploration/env_infos/initial/torso_velocity Std       0.244158\n",
      "exploration/env_infos/initial/torso_velocity Max       0.437659\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.139945\n",
      "exploration/env_infos/torso_velocity Mean              0.00146242\n",
      "exploration/env_infos/torso_velocity Std               0.128472\n",
      "exploration/env_infos/torso_velocity Max               0.742148\n",
      "exploration/env_infos/torso_velocity Min              -2.16107\n",
      "evaluation/num steps total                             1.825e+06\n",
      "evaluation/num paths total                          1825\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.847999\n",
      "evaluation/Rewards Std                                 0.0283736\n",
      "evaluation/Rewards Max                                 2.09487\n",
      "evaluation/Rewards Min                                 0.571211\n",
      "evaluation/Returns Mean                              847.999\n",
      "evaluation/Returns Std                                23.7333\n",
      "evaluation/Returns Max                               881.597\n",
      "evaluation/Returns Min                               790.497\n",
      "evaluation/Actions Mean                                0.0467437\n",
      "evaluation/Actions Std                                 0.189439\n",
      "evaluation/Actions Max                                 0.577063\n",
      "evaluation/Actions Min                                -0.422875\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           847.999\n",
      "evaluation/env_infos/final/reward_forward Mean         2.02846e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.38909e-07\n",
      "evaluation/env_infos/final/reward_forward Max          5.40403e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -4.61493e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0264112\n",
      "evaluation/env_infos/initial/reward_forward Std        0.126206\n",
      "evaluation/env_infos/initial/reward_forward Max        0.338227\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.236342\n",
      "evaluation/env_infos/reward_forward Mean              -0.000743039\n",
      "evaluation/env_infos/reward_forward Std                0.0453376\n",
      "evaluation/env_infos/reward_forward Max                1.0657\n",
      "evaluation/env_infos/reward_forward Min               -1.40205\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.152844\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0239072\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.118665\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.210179\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0608397\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0332969\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0287646\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.149283\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.152289\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0254453\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0229492\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.428789\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         3.66233e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.93992e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          8.76056e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -6.62169e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.14833\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.246633\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.758106\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.249463\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00269052\n",
      "evaluation/env_infos/torso_velocity Std                0.0539443\n",
      "evaluation/env_infos/torso_velocity Max                1.3554\n",
      "evaluation/env_infos/torso_velocity Min               -1.82526\n",
      "time/data storing (s)                                  0.0168696\n",
      "time/evaluation sampling (s)                          49.5418\n",
      "time/exploration sampling (s)                          2.41481\n",
      "time/logging (s)                                       0.307732\n",
      "time/saving (s)                                        0.0305748\n",
      "time/training (s)                                      6.36059\n",
      "time/epoch (s)                                        58.6724\n",
      "time/total (s)                                      4233.21\n",
      "Epoch                                                 72\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:22:00.203843 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 73 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 75000\n",
      "trainer/QF1 Loss                                       0.191478\n",
      "trainer/QF2 Loss                                       0.212652\n",
      "trainer/Policy Loss                                  -17.1053\n",
      "trainer/Q1 Predictions Mean                           25.1284\n",
      "trainer/Q1 Predictions Std                             1.23437\n",
      "trainer/Q1 Predictions Max                            26.9926\n",
      "trainer/Q1 Predictions Min                            15.7491\n",
      "trainer/Q2 Predictions Mean                           25.3907\n",
      "trainer/Q2 Predictions Std                             1.1726\n",
      "trainer/Q2 Predictions Max                            26.6962\n",
      "trainer/Q2 Predictions Min                            14.7599\n",
      "trainer/Q Targets Mean                                25.3578\n",
      "trainer/Q Targets Std                                  1.20647\n",
      "trainer/Q Targets Max                                 27.51\n",
      "trainer/Q Targets Min                                 16.2222\n",
      "trainer/Log Pis Mean                                   8.25014\n",
      "trainer/Log Pis Std                                    2.16473\n",
      "trainer/Log Pis Max                                   13.4805\n",
      "trainer/Log Pis Min                                    1.12798\n",
      "trainer/Policy mu Mean                                -0.0453618\n",
      "trainer/Policy mu Std                                  0.167167\n",
      "trainer/Policy mu Max                                  0.639897\n",
      "trainer/Policy mu Min                                 -1.22477\n",
      "trainer/Policy log std Mean                           -2.39997\n",
      "trainer/Policy log std Std                             0.211955\n",
      "trainer/Policy log std Max                            -1.78821\n",
      "trainer/Policy log std Min                            -3.37548\n",
      "trainer/Alpha                                          0.00839879\n",
      "trainer/Alpha Loss                                     1.1956\n",
      "exploration/num steps total                        75000\n",
      "exploration/num paths total                          127\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.798201\n",
      "exploration/Rewards Std                                0.0693854\n",
      "exploration/Rewards Max                                1.16143\n",
      "exploration/Rewards Min                                0.544109\n",
      "exploration/Returns Mean                             798.201\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              798.201\n",
      "exploration/Returns Min                              798.201\n",
      "exploration/Actions Mean                              -0.0475036\n",
      "exploration/Actions Std                                0.221678\n",
      "exploration/Actions Max                                0.553863\n",
      "exploration/Actions Min                               -0.624411\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          798.201\n",
      "exploration/env_infos/final/reward_forward Mean        0.0189802\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0189802\n",
      "exploration/env_infos/final/reward_forward Min         0.0189802\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.14418\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.14418\n",
      "exploration/env_infos/initial/reward_forward Min      -0.14418\n",
      "exploration/env_infos/reward_forward Mean             -0.0234967\n",
      "exploration/env_infos/reward_forward Std               0.127327\n",
      "exploration/env_infos/reward_forward Max               0.607018\n",
      "exploration/env_infos/reward_forward Min              -0.727886\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.210601\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.210601\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.210601\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0269348\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0269348\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0269348\n",
      "exploration/env_infos/reward_ctrl Mean                -0.205591\n",
      "exploration/env_infos/reward_ctrl Std                  0.0661829\n",
      "exploration/env_infos/reward_ctrl Max                 -0.016456\n",
      "exploration/env_infos/reward_ctrl Min                 -0.455891\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.00324226\n",
      "exploration/env_infos/final/torso_velocity Std         0.0148958\n",
      "exploration/env_infos/final/torso_velocity Max         0.0189802\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0167537\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.0183355\n",
      "exploration/env_infos/initial/torso_velocity Std       0.122057\n",
      "exploration/env_infos/initial/torso_velocity Max       0.146905\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.14418\n",
      "exploration/env_infos/torso_velocity Mean             -0.00694836\n",
      "exploration/env_infos/torso_velocity Std               0.113759\n",
      "exploration/env_infos/torso_velocity Max               0.607018\n",
      "exploration/env_infos/torso_velocity Min              -1.46062\n",
      "evaluation/num steps total                             1.85e+06\n",
      "evaluation/num paths total                          1850\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.883977\n",
      "evaluation/Rewards Std                                 0.086632\n",
      "evaluation/Rewards Max                                 2.46623\n",
      "evaluation/Rewards Min                                 0.573737\n",
      "evaluation/Returns Mean                              883.977\n",
      "evaluation/Returns Std                                77.3412\n",
      "evaluation/Returns Max                               981.909\n",
      "evaluation/Returns Min                               738.342\n",
      "evaluation/Actions Mean                               -0.0608474\n",
      "evaluation/Actions Std                                 0.160175\n",
      "evaluation/Actions Max                                 0.520044\n",
      "evaluation/Actions Min                                -0.691138\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           883.977\n",
      "evaluation/env_infos/final/reward_forward Mean        -7.75188e-09\n",
      "evaluation/env_infos/final/reward_forward Std          5.04624e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.69069e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -1.06166e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0167555\n",
      "evaluation/env_infos/initial/reward_forward Std        0.127478\n",
      "evaluation/env_infos/initial/reward_forward Max        0.164242\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.274318\n",
      "evaluation/env_infos/reward_forward Mean               0.00173909\n",
      "evaluation/env_infos/reward_forward Std                0.0615328\n",
      "evaluation/env_infos/reward_forward Max                1.33702\n",
      "evaluation/env_infos/reward_forward Min               -0.885404\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.120048\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0813418\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.017366\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.27065\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0299811\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0112318\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0102862\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0551965\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.117434\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0809296\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00810284\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.426263\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.63835e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.88465e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.69069e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/torso_velocity Min         -1.34514e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.127082\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.255542\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.672518\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.386558\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00197842\n",
      "evaluation/env_infos/torso_velocity Std                0.0711692\n",
      "evaluation/env_infos/torso_velocity Max                1.33702\n",
      "evaluation/env_infos/torso_velocity Min               -1.89445\n",
      "time/data storing (s)                                  0.0184078\n",
      "time/evaluation sampling (s)                          52.6843\n",
      "time/exploration sampling (s)                          3.45816\n",
      "time/logging (s)                                       0.356082\n",
      "time/saving (s)                                        0.0352899\n",
      "time/training (s)                                      7.88622\n",
      "time/epoch (s)                                        64.4384\n",
      "time/total (s)                                      4298.4\n",
      "Epoch                                                 73\n",
      "-------------------------------------------------  ---------------\n",
      "2021-05-25 20:22:57.635054 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 74 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 76000\n",
      "trainer/QF1 Loss                                       0.26837\n",
      "trainer/QF2 Loss                                       0.249991\n",
      "trainer/Policy Loss                                  -17.9925\n",
      "trainer/Q1 Predictions Mean                           25.8409\n",
      "trainer/Q1 Predictions Std                             1.43982\n",
      "trainer/Q1 Predictions Max                            27.4879\n",
      "trainer/Q1 Predictions Min                            18.217\n",
      "trainer/Q2 Predictions Mean                           25.7308\n",
      "trainer/Q2 Predictions Std                             1.46919\n",
      "trainer/Q2 Predictions Max                            27.6623\n",
      "trainer/Q2 Predictions Min                            16.2524\n",
      "trainer/Q Targets Mean                                25.5482\n",
      "trainer/Q Targets Std                                  1.47202\n",
      "trainer/Q Targets Max                                 27.3752\n",
      "trainer/Q Targets Min                                 19.644\n",
      "trainer/Log Pis Mean                                   7.93658\n",
      "trainer/Log Pis Std                                    2.62051\n",
      "trainer/Log Pis Max                                   18.007\n",
      "trainer/Log Pis Min                                   -1.23459\n",
      "trainer/Policy mu Mean                                -0.0357341\n",
      "trainer/Policy mu Std                                  0.152219\n",
      "trainer/Policy mu Max                                  1.31658\n",
      "trainer/Policy mu Min                                 -0.799078\n",
      "trainer/Policy log std Mean                           -2.38671\n",
      "trainer/Policy log std Std                             0.213708\n",
      "trainer/Policy log std Max                            -1.72031\n",
      "trainer/Policy log std Min                            -3.60812\n",
      "trainer/Alpha                                          0.0084543\n",
      "trainer/Alpha Loss                                    -0.302699\n",
      "exploration/num steps total                        76000\n",
      "exploration/num paths total                          128\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.906093\n",
      "exploration/Rewards Std                                0.029789\n",
      "exploration/Rewards Max                                1.02883\n",
      "exploration/Rewards Min                                0.718036\n",
      "exploration/Returns Mean                             906.093\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              906.093\n",
      "exploration/Returns Min                              906.093\n",
      "exploration/Actions Mean                              -0.0092593\n",
      "exploration/Actions Std                                0.153156\n",
      "exploration/Actions Max                                0.578232\n",
      "exploration/Actions Min                               -0.572609\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          906.093\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0065354\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0065354\n",
      "exploration/env_infos/final/reward_forward Min        -0.0065354\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0607864\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0607864\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0607864\n",
      "exploration/env_infos/reward_forward Mean              0.00409056\n",
      "exploration/env_infos/reward_forward Std               0.0724972\n",
      "exploration/env_infos/reward_forward Max               1.14292\n",
      "exploration/env_infos/reward_forward Min              -0.399333\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.109728\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.109728\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.109728\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0233861\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0233861\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0233861\n",
      "exploration/env_infos/reward_ctrl Mean                -0.09417\n",
      "exploration/env_infos/reward_ctrl Std                  0.0295438\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0134989\n",
      "exploration/env_infos/reward_ctrl Min                 -0.281964\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00722525\n",
      "exploration/env_infos/final/torso_velocity Std         0.00100981\n",
      "exploration/env_infos/final/torso_velocity Max        -0.00648727\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00865307\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.138604\n",
      "exploration/env_infos/initial/torso_velocity Std       0.307214\n",
      "exploration/env_infos/initial/torso_velocity Max       0.572595\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0959954\n",
      "exploration/env_infos/torso_velocity Mean              0.00265583\n",
      "exploration/env_infos/torso_velocity Std               0.0980179\n",
      "exploration/env_infos/torso_velocity Max               1.14292\n",
      "exploration/env_infos/torso_velocity Min              -1.67298\n",
      "evaluation/num steps total                             1.875e+06\n",
      "evaluation/num paths total                          1875\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.93533\n",
      "evaluation/Rewards Std                                 0.05968\n",
      "evaluation/Rewards Max                                 2.46453\n",
      "evaluation/Rewards Min                                 0.609705\n",
      "evaluation/Returns Mean                              935.33\n",
      "evaluation/Returns Std                                49.8975\n",
      "evaluation/Returns Max                               986.997\n",
      "evaluation/Returns Min                               812.181\n",
      "evaluation/Actions Mean                               -0.0430344\n",
      "evaluation/Actions Std                                 0.1209\n",
      "evaluation/Actions Max                                 0.464617\n",
      "evaluation/Actions Min                                -0.622601\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           935.33\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00020662\n",
      "evaluation/env_infos/final/reward_forward Std          0.00101316\n",
      "evaluation/env_infos/final/reward_forward Max          0.00517007\n",
      "evaluation/env_infos/final/reward_forward Min         -8.97268e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.031406\n",
      "evaluation/env_infos/initial/reward_forward Std        0.155161\n",
      "evaluation/env_infos/initial/reward_forward Max        0.296347\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.377271\n",
      "evaluation/env_infos/reward_forward Mean              -0.000484759\n",
      "evaluation/env_infos/reward_forward Std                0.0676254\n",
      "evaluation/env_infos/reward_forward Max                1.49765\n",
      "evaluation/env_infos/reward_forward Min               -1.64476\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0660347\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0503691\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0190303\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.189586\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0515358\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0315939\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0130177\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.132406\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0658752\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0510151\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0073703\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.390295\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         4.50588e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.00101451\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00517007\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00584292\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.131859\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.259415\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.66444\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.377271\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00138117\n",
      "evaluation/env_infos/torso_velocity Std                0.0647222\n",
      "evaluation/env_infos/torso_velocity Max                1.49765\n",
      "evaluation/env_infos/torso_velocity Min               -2.06071\n",
      "time/data storing (s)                                  0.0152053\n",
      "time/evaluation sampling (s)                          49.31\n",
      "time/exploration sampling (s)                          2.28235\n",
      "time/logging (s)                                       0.288818\n",
      "time/saving (s)                                        0.0275535\n",
      "time/training (s)                                      4.67858\n",
      "time/epoch (s)                                        56.6025\n",
      "time/total (s)                                      4355.76\n",
      "Epoch                                                 74\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:23:53.093871 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 75 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 77000\n",
      "trainer/QF1 Loss                                       0.094051\n",
      "trainer/QF2 Loss                                       0.0769876\n",
      "trainer/Policy Loss                                  -18.3011\n",
      "trainer/Q1 Predictions Mean                           25.9736\n",
      "trainer/Q1 Predictions Std                             1.18565\n",
      "trainer/Q1 Predictions Max                            27.3371\n",
      "trainer/Q1 Predictions Min                            19.5076\n",
      "trainer/Q2 Predictions Mean                           25.9415\n",
      "trainer/Q2 Predictions Std                             1.15723\n",
      "trainer/Q2 Predictions Max                            27.2305\n",
      "trainer/Q2 Predictions Min                            18.8815\n",
      "trainer/Q Targets Mean                                25.9989\n",
      "trainer/Q Targets Std                                  1.15282\n",
      "trainer/Q Targets Max                                 27.8518\n",
      "trainer/Q Targets Min                                 19.5048\n",
      "trainer/Log Pis Mean                                   7.77105\n",
      "trainer/Log Pis Std                                    2.18585\n",
      "trainer/Log Pis Max                                   17.6442\n",
      "trainer/Log Pis Min                                    1.25911\n",
      "trainer/Policy mu Mean                                -0.0229575\n",
      "trainer/Policy mu Std                                  0.168259\n",
      "trainer/Policy mu Max                                  0.813024\n",
      "trainer/Policy mu Min                                 -1.02478\n",
      "trainer/Policy log std Mean                           -2.35987\n",
      "trainer/Policy log std Std                             0.218101\n",
      "trainer/Policy log std Max                            -1.52978\n",
      "trainer/Policy log std Min                            -3.6625\n",
      "trainer/Alpha                                          0.00817684\n",
      "trainer/Alpha Loss                                    -1.10077\n",
      "exploration/num steps total                        77000\n",
      "exploration/num paths total                          129\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.913061\n",
      "exploration/Rewards Std                                0.132491\n",
      "exploration/Rewards Max                                1.77077\n",
      "exploration/Rewards Min                                0.490127\n",
      "exploration/Returns Mean                             913.061\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              913.061\n",
      "exploration/Returns Min                              913.061\n",
      "exploration/Actions Mean                              -0.00771911\n",
      "exploration/Actions Std                                0.174613\n",
      "exploration/Actions Max                                0.606547\n",
      "exploration/Actions Min                               -0.673316\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          913.061\n",
      "exploration/env_infos/final/reward_forward Mean       -0.149315\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.149315\n",
      "exploration/env_infos/final/reward_forward Min        -0.149315\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0416444\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0416444\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0416444\n",
      "exploration/env_infos/reward_forward Mean             -0.0138999\n",
      "exploration/env_infos/reward_forward Std               0.26001\n",
      "exploration/env_infos/reward_forward Max               1.63376\n",
      "exploration/env_infos/reward_forward Min              -1.02171\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.100644\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.100644\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.100644\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.148168\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.148168\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.148168\n",
      "exploration/env_infos/reward_ctrl Mean                -0.122198\n",
      "exploration/env_infos/reward_ctrl Std                  0.0589675\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0248695\n",
      "exploration/env_infos/reward_ctrl Min                 -0.509873\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0851769\n",
      "exploration/env_infos/final/torso_velocity Std         0.17877\n",
      "exploration/env_infos/final/torso_velocity Max         0.158677\n",
      "exploration/env_infos/final/torso_velocity Min        -0.264893\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.170937\n",
      "exploration/env_infos/initial/torso_velocity Std       0.389483\n",
      "exploration/env_infos/initial/torso_velocity Max       0.717287\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.162832\n",
      "exploration/env_infos/torso_velocity Mean              0.0129657\n",
      "exploration/env_infos/torso_velocity Std               0.277655\n",
      "exploration/env_infos/torso_velocity Max               1.63376\n",
      "exploration/env_infos/torso_velocity Min              -1.62749\n",
      "evaluation/num steps total                             1.9e+06\n",
      "evaluation/num paths total                          1900\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.91387\n",
      "evaluation/Rewards Std                                 0.0462374\n",
      "evaluation/Rewards Max                                 1.71302\n",
      "evaluation/Rewards Min                                 0.545344\n",
      "evaluation/Returns Mean                              913.87\n",
      "evaluation/Returns Std                                43.343\n",
      "evaluation/Returns Max                               950.934\n",
      "evaluation/Returns Min                               793.485\n",
      "evaluation/Actions Mean                               -0.01935\n",
      "evaluation/Actions Std                                 0.145808\n",
      "evaluation/Actions Max                                 0.544613\n",
      "evaluation/Actions Min                                -0.654468\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           913.87\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.50558e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.63196e-06\n",
      "evaluation/env_infos/final/reward_forward Max          7.402e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -1.0769e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0212277\n",
      "evaluation/env_infos/initial/reward_forward Std        0.138524\n",
      "evaluation/env_infos/initial/reward_forward Max        0.32241\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.272841\n",
      "evaluation/env_infos/reward_forward Mean              -0.00107746\n",
      "evaluation/env_infos/reward_forward Std                0.054211\n",
      "evaluation/env_infos/reward_forward Max                1.28824\n",
      "evaluation/env_infos/reward_forward Min               -1.61831\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0858232\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0438437\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0476047\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.207119\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.111525\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0409734\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0650072\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.198751\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.086538\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0449555\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0255518\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.454656\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -4.13147e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          3.69077e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          8.51032e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -2.08183e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.137485\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.2273\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.608771\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.272841\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00262672\n",
      "evaluation/env_infos/torso_velocity Std                0.0632434\n",
      "evaluation/env_infos/torso_velocity Max                1.28824\n",
      "evaluation/env_infos/torso_velocity Min               -2.08528\n",
      "time/data storing (s)                                  0.015416\n",
      "time/evaluation sampling (s)                          47.7966\n",
      "time/exploration sampling (s)                          1.94915\n",
      "time/logging (s)                                       0.295271\n",
      "time/saving (s)                                        0.0310359\n",
      "time/training (s)                                      4.68428\n",
      "time/epoch (s)                                        54.7717\n",
      "time/total (s)                                      4411.23\n",
      "Epoch                                                 75\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:24:50.365252 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 76 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 78000\n",
      "trainer/QF1 Loss                                       0.246017\n",
      "trainer/QF2 Loss                                       0.317986\n",
      "trainer/Policy Loss                                  -17.1502\n",
      "trainer/Q1 Predictions Mean                           25.8207\n",
      "trainer/Q1 Predictions Std                             1.54015\n",
      "trainer/Q1 Predictions Max                            28.0759\n",
      "trainer/Q1 Predictions Min                            19.467\n",
      "trainer/Q2 Predictions Mean                           25.8534\n",
      "trainer/Q2 Predictions Std                             1.73245\n",
      "trainer/Q2 Predictions Max                            27.8923\n",
      "trainer/Q2 Predictions Min                            16.2569\n",
      "trainer/Q Targets Mean                                25.9603\n",
      "trainer/Q Targets Std                                  1.58555\n",
      "trainer/Q Targets Max                                 27.5858\n",
      "trainer/Q Targets Min                                 18.9098\n",
      "trainer/Log Pis Mean                                   8.80909\n",
      "trainer/Log Pis Std                                    2.81891\n",
      "trainer/Log Pis Max                                   20.152\n",
      "trainer/Log Pis Min                                   -1.70698\n",
      "trainer/Policy mu Mean                                -0.0237085\n",
      "trainer/Policy mu Std                                  0.221335\n",
      "trainer/Policy mu Max                                  1.63854\n",
      "trainer/Policy mu Min                                 -1.00909\n",
      "trainer/Policy log std Mean                           -2.47598\n",
      "trainer/Policy log std Std                             0.232138\n",
      "trainer/Policy log std Max                            -1.94823\n",
      "trainer/Policy log std Min                            -3.85592\n",
      "trainer/Alpha                                          0.00879114\n",
      "trainer/Alpha Loss                                     3.83056\n",
      "exploration/num steps total                        78000\n",
      "exploration/num paths total                          130\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.838917\n",
      "exploration/Rewards Std                                0.0428646\n",
      "exploration/Rewards Max                                0.972831\n",
      "exploration/Rewards Min                                0.662095\n",
      "exploration/Returns Mean                             838.917\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              838.917\n",
      "exploration/Returns Min                              838.917\n",
      "exploration/Actions Mean                               0.00386453\n",
      "exploration/Actions Std                                0.200683\n",
      "exploration/Actions Max                                0.566899\n",
      "exploration/Actions Min                               -0.653321\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          838.917\n",
      "exploration/env_infos/final/reward_forward Mean       -0.000855647\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.000855647\n",
      "exploration/env_infos/final/reward_forward Min        -0.000855647\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.307972\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.307972\n",
      "exploration/env_infos/initial/reward_forward Min      -0.307972\n",
      "exploration/env_infos/reward_forward Mean             -0.000782652\n",
      "exploration/env_infos/reward_forward Std               0.0480945\n",
      "exploration/env_infos/reward_forward Max               0.869412\n",
      "exploration/env_infos/reward_forward Min              -0.307972\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.101224\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.101224\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.101224\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.121664\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.121664\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.121664\n",
      "exploration/env_infos/reward_ctrl Mean                -0.161155\n",
      "exploration/env_infos/reward_ctrl Std                  0.0428291\n",
      "exploration/env_infos/reward_ctrl Max                 -0.027169\n",
      "exploration/env_infos/reward_ctrl Min                 -0.337905\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.000449887\n",
      "exploration/env_infos/final/torso_velocity Std         0.000534823\n",
      "exploration/env_infos/final/torso_velocity Max         0.000305779\n",
      "exploration/env_infos/final/torso_velocity Min        -0.000855647\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.0394381\n",
      "exploration/env_infos/initial/torso_velocity Std       0.243936\n",
      "exploration/env_infos/initial/torso_velocity Max       0.282377\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.307972\n",
      "exploration/env_infos/torso_velocity Mean             -0.000890188\n",
      "exploration/env_infos/torso_velocity Std               0.0622305\n",
      "exploration/env_infos/torso_velocity Max               0.869412\n",
      "exploration/env_infos/torso_velocity Min              -1.75196\n",
      "evaluation/num steps total                             1.925e+06\n",
      "evaluation/num paths total                          1925\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.892521\n",
      "evaluation/Rewards Std                                 0.0456946\n",
      "evaluation/Rewards Max                                 2.14866\n",
      "evaluation/Rewards Min                                 0.642143\n",
      "evaluation/Returns Mean                              892.521\n",
      "evaluation/Returns Std                                40.6267\n",
      "evaluation/Returns Max                               947.624\n",
      "evaluation/Returns Min                               798.899\n",
      "evaluation/Actions Mean                               -0.0182534\n",
      "evaluation/Actions Std                                 0.163269\n",
      "evaluation/Actions Max                                 0.557181\n",
      "evaluation/Actions Min                                -0.692809\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           892.521\n",
      "evaluation/env_infos/final/reward_forward Mean         1.87837e-07\n",
      "evaluation/env_infos/final/reward_forward Std          5.79802e-06\n",
      "evaluation/env_infos/final/reward_forward Max          2.4084e-05\n",
      "evaluation/env_infos/final/reward_forward Min         -1.53033e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00552732\n",
      "evaluation/env_infos/initial/reward_forward Std        0.165579\n",
      "evaluation/env_infos/initial/reward_forward Max        0.318014\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.284316\n",
      "evaluation/env_infos/reward_forward Mean              -0.000223612\n",
      "evaluation/env_infos/reward_forward Std                0.0464056\n",
      "evaluation/env_infos/reward_forward Max                1.12383\n",
      "evaluation/env_infos/reward_forward Min               -1.2861\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.108032\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0425502\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0517419\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.220238\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.128138\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0308895\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0733378\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.199119\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.10796\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0415435\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0203564\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.405967\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -5.6633e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          5.66416e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          2.4084e-05\n",
      "evaluation/env_infos/final/torso_velocity Min         -3.83825e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.14177\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.250932\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.63756\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.286649\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00104417\n",
      "evaluation/env_infos/torso_velocity Std                0.0514088\n",
      "evaluation/env_infos/torso_velocity Max                1.12478\n",
      "evaluation/env_infos/torso_velocity Min               -1.74155\n",
      "time/data storing (s)                                  0.0151007\n",
      "time/evaluation sampling (s)                          49.6838\n",
      "time/exploration sampling (s)                          2.07355\n",
      "time/logging (s)                                       0.351185\n",
      "time/saving (s)                                        0.0441438\n",
      "time/training (s)                                      4.42271\n",
      "time/epoch (s)                                        56.5905\n",
      "time/total (s)                                      4468.55\n",
      "Epoch                                                 76\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:25:44.795335 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 77 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 79000\n",
      "trainer/QF1 Loss                                       0.19655\n",
      "trainer/QF2 Loss                                       0.26809\n",
      "trainer/Policy Loss                                  -18.0535\n",
      "trainer/Q1 Predictions Mean                           26.4071\n",
      "trainer/Q1 Predictions Std                             1.33782\n",
      "trainer/Q1 Predictions Max                            28.4424\n",
      "trainer/Q1 Predictions Min                            18.7783\n",
      "trainer/Q2 Predictions Mean                           26.2184\n",
      "trainer/Q2 Predictions Std                             1.43318\n",
      "trainer/Q2 Predictions Max                            27.6644\n",
      "trainer/Q2 Predictions Min                            17.9174\n",
      "trainer/Q Targets Mean                                26.5303\n",
      "trainer/Q Targets Std                                  1.31247\n",
      "trainer/Q Targets Max                                 28.4501\n",
      "trainer/Q Targets Min                                 18.3908\n",
      "trainer/Log Pis Mean                                   8.27626\n",
      "trainer/Log Pis Std                                    2.50415\n",
      "trainer/Log Pis Max                                   20.2072\n",
      "trainer/Log Pis Min                                   -0.000950873\n",
      "trainer/Policy mu Mean                                -0.0161657\n",
      "trainer/Policy mu Std                                  0.119103\n",
      "trainer/Policy mu Max                                  0.707293\n",
      "trainer/Policy mu Min                                 -1.2026\n",
      "trainer/Policy log std Mean                           -2.4366\n",
      "trainer/Policy log std Std                             0.208513\n",
      "trainer/Policy log std Max                            -1.70807\n",
      "trainer/Policy log std Min                            -3.80691\n",
      "trainer/Alpha                                          0.00772376\n",
      "trainer/Alpha Loss                                     1.34367\n",
      "exploration/num steps total                        79000\n",
      "exploration/num paths total                          131\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.934592\n",
      "exploration/Rewards Std                                0.0741592\n",
      "exploration/Rewards Max                                2.1646\n",
      "exploration/Rewards Min                                0.795474\n",
      "exploration/Returns Mean                             934.592\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              934.592\n",
      "exploration/Returns Min                              934.592\n",
      "exploration/Actions Mean                               0.000107444\n",
      "exploration/Actions Std                                0.136391\n",
      "exploration/Actions Max                                0.483392\n",
      "exploration/Actions Min                               -0.538056\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          934.592\n",
      "exploration/env_infos/final/reward_forward Mean        0.0144332\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0144332\n",
      "exploration/env_infos/final/reward_forward Min         0.0144332\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0623933\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0623933\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0623933\n",
      "exploration/env_infos/reward_forward Mean              0.0186796\n",
      "exploration/env_infos/reward_forward Std               0.245155\n",
      "exploration/env_infos/reward_forward Max               1.03164\n",
      "exploration/env_infos/reward_forward Min              -1.06767\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.071048\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.071048\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.071048\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0596719\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0596719\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0596719\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0744096\n",
      "exploration/env_infos/reward_ctrl Std                  0.038373\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00413565\n",
      "exploration/env_infos/reward_ctrl Min                 -0.277847\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0661941\n",
      "exploration/env_infos/final/torso_velocity Std         0.0394348\n",
      "exploration/env_infos/final/torso_velocity Max         0.110054\n",
      "exploration/env_infos/final/torso_velocity Min         0.0144332\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0240061\n",
      "exploration/env_infos/initial/torso_velocity Std       0.199092\n",
      "exploration/env_infos/initial/torso_velocity Max       0.299278\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.164867\n",
      "exploration/env_infos/torso_velocity Mean              0.00121007\n",
      "exploration/env_infos/torso_velocity Std               0.239563\n",
      "exploration/env_infos/torso_velocity Max               1.39392\n",
      "exploration/env_infos/torso_velocity Min              -1.32924\n",
      "evaluation/num steps total                             1.95e+06\n",
      "evaluation/num paths total                          1950\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.970503\n",
      "evaluation/Rewards Std                                 0.028861\n",
      "evaluation/Rewards Max                                 2.2969\n",
      "evaluation/Rewards Min                                 0.788652\n",
      "evaluation/Returns Mean                              970.503\n",
      "evaluation/Returns Std                                11.9916\n",
      "evaluation/Returns Max                               989.727\n",
      "evaluation/Returns Min                               945.922\n",
      "evaluation/Actions Mean                               -0.0106251\n",
      "evaluation/Actions Std                                 0.0869727\n",
      "evaluation/Actions Max                                 0.396074\n",
      "evaluation/Actions Min                                -0.519582\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           970.503\n",
      "evaluation/env_infos/final/reward_forward Mean         2.53845e-06\n",
      "evaluation/env_infos/final/reward_forward Std          1.17585e-05\n",
      "evaluation/env_infos/final/reward_forward Max          6.01333e-05\n",
      "evaluation/env_infos/final/reward_forward Min         -4.03801e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0224819\n",
      "evaluation/env_infos/initial/reward_forward Std        0.114697\n",
      "evaluation/env_infos/initial/reward_forward Max        0.194751\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.192311\n",
      "evaluation/env_infos/reward_forward Mean               0.000861667\n",
      "evaluation/env_infos/reward_forward Std                0.0625094\n",
      "evaluation/env_infos/reward_forward Max                1.68756\n",
      "evaluation/env_infos/reward_forward Min               -1.29086\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0305193\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0124752\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0102701\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0552414\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0362783\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.013357\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0142997\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0647435\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0307086\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0135977\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00710996\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.211348\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -6.8474e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          1.25815e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          6.01333e-05\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.24862e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.110726\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.242271\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.607491\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.455654\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00154221\n",
      "evaluation/env_infos/torso_velocity Std                0.0686765\n",
      "evaluation/env_infos/torso_velocity Max                1.68756\n",
      "evaluation/env_infos/torso_velocity Min               -1.91229\n",
      "time/data storing (s)                                  0.0163405\n",
      "time/evaluation sampling (s)                          46.5778\n",
      "time/exploration sampling (s)                          2.17166\n",
      "time/logging (s)                                       0.292845\n",
      "time/saving (s)                                        0.0270886\n",
      "time/training (s)                                      4.45718\n",
      "time/epoch (s)                                        53.5429\n",
      "time/total (s)                                      4522.92\n",
      "Epoch                                                 77\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:26:41.973669 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 78 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 80000\n",
      "trainer/QF1 Loss                                       0.442007\n",
      "trainer/QF2 Loss                                       0.25778\n",
      "trainer/Policy Loss                                  -18.402\n",
      "trainer/Q1 Predictions Mean                           26.8562\n",
      "trainer/Q1 Predictions Std                             1.64011\n",
      "trainer/Q1 Predictions Max                            28.6187\n",
      "trainer/Q1 Predictions Min                            16.4307\n",
      "trainer/Q2 Predictions Mean                           26.5664\n",
      "trainer/Q2 Predictions Std                             1.61441\n",
      "trainer/Q2 Predictions Max                            28.3208\n",
      "trainer/Q2 Predictions Min                            19.3815\n",
      "trainer/Q Targets Mean                                26.665\n",
      "trainer/Q Targets Std                                  1.48368\n",
      "trainer/Q Targets Max                                 29.0703\n",
      "trainer/Q Targets Min                                 19.5685\n",
      "trainer/Log Pis Mean                                   8.28317\n",
      "trainer/Log Pis Std                                    2.58887\n",
      "trainer/Log Pis Max                                   18.2359\n",
      "trainer/Log Pis Min                                   -0.41164\n",
      "trainer/Policy mu Mean                                 0.0205905\n",
      "trainer/Policy mu Std                                  0.151036\n",
      "trainer/Policy mu Max                                  1.14404\n",
      "trainer/Policy mu Min                                 -1.55579\n",
      "trainer/Policy log std Mean                           -2.39641\n",
      "trainer/Policy log std Std                             0.220552\n",
      "trainer/Policy log std Max                            -1.78114\n",
      "trainer/Policy log std Min                            -3.68095\n",
      "trainer/Alpha                                          0.00793394\n",
      "trainer/Alpha Loss                                     1.36986\n",
      "exploration/num steps total                        80000\n",
      "exploration/num paths total                          132\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.921694\n",
      "exploration/Rewards Std                                0.0747147\n",
      "exploration/Rewards Max                                1.70131\n",
      "exploration/Rewards Min                                0.720446\n",
      "exploration/Returns Mean                             921.694\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              921.694\n",
      "exploration/Returns Min                              921.694\n",
      "exploration/Actions Mean                               0.0358097\n",
      "exploration/Actions Std                                0.145979\n",
      "exploration/Actions Max                                0.528665\n",
      "exploration/Actions Min                               -0.473176\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          921.694\n",
      "exploration/env_infos/final/reward_forward Mean        0.0984641\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0984641\n",
      "exploration/env_infos/final/reward_forward Min         0.0984641\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.106168\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.106168\n",
      "exploration/env_infos/initial/reward_forward Min      -0.106168\n",
      "exploration/env_infos/reward_forward Mean              0.0270614\n",
      "exploration/env_infos/reward_forward Std               0.234911\n",
      "exploration/env_infos/reward_forward Max               0.979952\n",
      "exploration/env_infos/reward_forward Min              -1.14378\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0601506\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0601506\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0601506\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.170263\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.170263\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.170263\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0903694\n",
      "exploration/env_infos/reward_ctrl Std                  0.0457275\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00997206\n",
      "exploration/env_infos/reward_ctrl Min                 -0.408172\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0719955\n",
      "exploration/env_infos/final/torso_velocity Std         0.199553\n",
      "exploration/env_infos/final/torso_velocity Max         0.0984641\n",
      "exploration/env_infos/final/torso_velocity Min        -0.352007\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0496608\n",
      "exploration/env_infos/initial/torso_velocity Std       0.312145\n",
      "exploration/env_infos/initial/torso_velocity Max       0.485262\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.230112\n",
      "exploration/env_infos/torso_velocity Mean             -0.00116703\n",
      "exploration/env_infos/torso_velocity Std               0.22595\n",
      "exploration/env_infos/torso_velocity Max               0.979952\n",
      "exploration/env_infos/torso_velocity Min              -1.14663\n",
      "evaluation/num steps total                             1.975e+06\n",
      "evaluation/num paths total                          1975\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.921089\n",
      "evaluation/Rewards Std                                 0.049955\n",
      "evaluation/Rewards Max                                 2.74965\n",
      "evaluation/Rewards Min                                 0.477396\n",
      "evaluation/Returns Mean                              921.089\n",
      "evaluation/Returns Std                                36.7851\n",
      "evaluation/Returns Max                               979.949\n",
      "evaluation/Returns Min                               844.02\n",
      "evaluation/Actions Mean                                0.0193968\n",
      "evaluation/Actions Std                                 0.140492\n",
      "evaluation/Actions Max                                 0.514288\n",
      "evaluation/Actions Min                                -0.794782\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           921.089\n",
      "evaluation/env_infos/final/reward_forward Mean        -3.76473e-05\n",
      "evaluation/env_infos/final/reward_forward Std          0.000131884\n",
      "evaluation/env_infos/final/reward_forward Max          6.4252e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000591936\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.025937\n",
      "evaluation/env_infos/initial/reward_forward Std        0.160512\n",
      "evaluation/env_infos/initial/reward_forward Max        0.457927\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.224157\n",
      "evaluation/env_infos/reward_forward Mean               0.000686593\n",
      "evaluation/env_infos/reward_forward Std                0.0533748\n",
      "evaluation/env_infos/reward_forward Max                1.591\n",
      "evaluation/env_infos/reward_forward Min               -1.43037\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0812112\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0373333\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0202828\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.161768\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.104983\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0410747\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0252558\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.191667\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0804572\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0369551\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0111263\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.522604\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -5.14768e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000191055\n",
      "evaluation/env_infos/final/torso_velocity Max          8.93289e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00109464\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.105015\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.288164\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.621883\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.349231\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00145318\n",
      "evaluation/env_infos/torso_velocity Std                0.0610205\n",
      "evaluation/env_infos/torso_velocity Max                1.591\n",
      "evaluation/env_infos/torso_velocity Min               -2.07108\n",
      "time/data storing (s)                                  0.0156612\n",
      "time/evaluation sampling (s)                          48.5856\n",
      "time/exploration sampling (s)                          1.99573\n",
      "time/logging (s)                                       0.312447\n",
      "time/saving (s)                                        0.0269632\n",
      "time/training (s)                                      5.51942\n",
      "time/epoch (s)                                        56.4558\n",
      "time/total (s)                                      4580.12\n",
      "Epoch                                                 78\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:27:44.307560 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 79 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 81000\n",
      "trainer/QF1 Loss                                       0.118587\n",
      "trainer/QF2 Loss                                       0.170369\n",
      "trainer/Policy Loss                                  -20.3648\n",
      "trainer/Q1 Predictions Mean                           27.1051\n",
      "trainer/Q1 Predictions Std                             1.10668\n",
      "trainer/Q1 Predictions Max                            28.6151\n",
      "trainer/Q1 Predictions Min                            20.7684\n",
      "trainer/Q2 Predictions Mean                           27.2074\n",
      "trainer/Q2 Predictions Std                             1.1532\n",
      "trainer/Q2 Predictions Max                            28.589\n",
      "trainer/Q2 Predictions Min                            20.4013\n",
      "trainer/Q Targets Mean                                27.124\n",
      "trainer/Q Targets Std                                  1.14321\n",
      "trainer/Q Targets Max                                 28.5188\n",
      "trainer/Q Targets Min                                 20.4343\n",
      "trainer/Log Pis Mean                                   6.79989\n",
      "trainer/Log Pis Std                                    2.21868\n",
      "trainer/Log Pis Max                                   12.5803\n",
      "trainer/Log Pis Min                                   -1.94369\n",
      "trainer/Policy mu Mean                                -0.0200465\n",
      "trainer/Policy mu Std                                  0.116912\n",
      "trainer/Policy mu Max                                  1.05789\n",
      "trainer/Policy mu Min                                 -1.03066\n",
      "trainer/Policy log std Mean                           -2.22047\n",
      "trainer/Policy log std Std                             0.20879\n",
      "trainer/Policy log std Max                            -1.46788\n",
      "trainer/Policy log std Min                            -3.02703\n",
      "trainer/Alpha                                          0.00815562\n",
      "trainer/Alpha Loss                                    -5.77231\n",
      "exploration/num steps total                        81000\n",
      "exploration/num paths total                          133\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.960438\n",
      "exploration/Rewards Std                                0.102968\n",
      "exploration/Rewards Max                                1.79232\n",
      "exploration/Rewards Min                                0.761489\n",
      "exploration/Returns Mean                             960.438\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              960.438\n",
      "exploration/Returns Min                              960.438\n",
      "exploration/Actions Mean                              -0.034911\n",
      "exploration/Actions Std                                0.11921\n",
      "exploration/Actions Max                                0.503596\n",
      "exploration/Actions Min                               -0.561575\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          960.438\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0021412\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0021412\n",
      "exploration/env_infos/final/reward_forward Min        -0.0021412\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.00502456\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.00502456\n",
      "exploration/env_infos/initial/reward_forward Min      -0.00502456\n",
      "exploration/env_infos/reward_forward Mean             -0.00897878\n",
      "exploration/env_infos/reward_forward Std               0.192924\n",
      "exploration/env_infos/reward_forward Max               0.627658\n",
      "exploration/env_infos/reward_forward Min              -0.873558\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0522989\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0522989\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0522989\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0704373\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0704373\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0704373\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0617189\n",
      "exploration/env_infos/reward_ctrl Std                  0.03395\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00732109\n",
      "exploration/env_infos/reward_ctrl Min                 -0.238511\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0252036\n",
      "exploration/env_infos/final/torso_velocity Std         0.0221072\n",
      "exploration/env_infos/final/torso_velocity Max        -0.0021412\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0550156\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.171149\n",
      "exploration/env_infos/initial/torso_velocity Std       0.297071\n",
      "exploration/env_infos/initial/torso_velocity Max       0.589537\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0710644\n",
      "exploration/env_infos/torso_velocity Mean              0.00130052\n",
      "exploration/env_infos/torso_velocity Std               0.178248\n",
      "exploration/env_infos/torso_velocity Max               0.87845\n",
      "exploration/env_infos/torso_velocity Min              -0.889792\n",
      "evaluation/num steps total                             2e+06\n",
      "evaluation/num paths total                          2000\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.960668\n",
      "evaluation/Rewards Std                                 0.0293026\n",
      "evaluation/Rewards Max                                 2.49058\n",
      "evaluation/Rewards Min                                 0.754273\n",
      "evaluation/Returns Mean                              960.668\n",
      "evaluation/Returns Std                                11.1284\n",
      "evaluation/Returns Max                               974.39\n",
      "evaluation/Returns Min                               934.83\n",
      "evaluation/Actions Mean                               -0.0240389\n",
      "evaluation/Actions Std                                 0.0974652\n",
      "evaluation/Actions Max                                 0.383515\n",
      "evaluation/Actions Min                                -0.495262\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           960.668\n",
      "evaluation/env_infos/final/reward_forward Mean        -2.3108e-06\n",
      "evaluation/env_infos/final/reward_forward Std          1.09912e-05\n",
      "evaluation/env_infos/final/reward_forward Max          3.31099e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.61495e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0256009\n",
      "evaluation/env_infos/initial/reward_forward Std        0.155374\n",
      "evaluation/env_infos/initial/reward_forward Max        0.364032\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.261083\n",
      "evaluation/env_infos/reward_forward Mean               0.00329248\n",
      "evaluation/env_infos/reward_forward Std                0.0564354\n",
      "evaluation/env_infos/reward_forward Max                1.4927\n",
      "evaluation/env_infos/reward_forward Min               -1.10189\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0404368\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0119453\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0254182\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0659774\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0477553\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0107227\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.031621\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0721455\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0403093\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0126178\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0148176\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.245727\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -3.23663e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          2.26944e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          4.38858e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000190357\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.137983\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.248282\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.602582\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.261083\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00243903\n",
      "evaluation/env_infos/torso_velocity Std                0.0664404\n",
      "evaluation/env_infos/torso_velocity Max                1.4927\n",
      "evaluation/env_infos/torso_velocity Min               -2.00902\n",
      "time/data storing (s)                                  0.0182313\n",
      "time/evaluation sampling (s)                          52.6024\n",
      "time/exploration sampling (s)                          2.33747\n",
      "time/logging (s)                                       0.298911\n",
      "time/saving (s)                                        0.0283321\n",
      "time/training (s)                                      6.26268\n",
      "time/epoch (s)                                        61.548\n",
      "time/total (s)                                      4642.44\n",
      "Epoch                                                 79\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:28:48.450005 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 80 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 82000\n",
      "trainer/QF1 Loss                                       0.210445\n",
      "trainer/QF2 Loss                                       0.216721\n",
      "trainer/Policy Loss                                  -18.5561\n",
      "trainer/Q1 Predictions Mean                           27.0815\n",
      "trainer/Q1 Predictions Std                             1.34539\n",
      "trainer/Q1 Predictions Max                            28.6964\n",
      "trainer/Q1 Predictions Min                            19.772\n",
      "trainer/Q2 Predictions Mean                           27.0056\n",
      "trainer/Q2 Predictions Std                             1.18678\n",
      "trainer/Q2 Predictions Max                            28.4749\n",
      "trainer/Q2 Predictions Min                            20.8326\n",
      "trainer/Q Targets Mean                                27.3275\n",
      "trainer/Q Targets Std                                  1.15215\n",
      "trainer/Q Targets Max                                 28.9657\n",
      "trainer/Q Targets Min                                 20.9731\n",
      "trainer/Log Pis Mean                                   8.57161\n",
      "trainer/Log Pis Std                                    2.38618\n",
      "trainer/Log Pis Max                                   14.8139\n",
      "trainer/Log Pis Min                                    3.05612\n",
      "trainer/Policy mu Mean                                -0.0139399\n",
      "trainer/Policy mu Std                                  0.172106\n",
      "trainer/Policy mu Max                                  1.97379\n",
      "trainer/Policy mu Min                                 -1.01582\n",
      "trainer/Policy log std Mean                           -2.44766\n",
      "trainer/Policy log std Std                             0.252387\n",
      "trainer/Policy log std Max                            -1.73574\n",
      "trainer/Policy log std Min                            -3.548\n",
      "trainer/Alpha                                          0.00759616\n",
      "trainer/Alpha Loss                                     2.79105\n",
      "exploration/num steps total                        82000\n",
      "exploration/num paths total                          134\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.930348\n",
      "exploration/Rewards Std                                0.0437017\n",
      "exploration/Rewards Max                                1.58655\n",
      "exploration/Rewards Min                                0.777698\n",
      "exploration/Returns Mean                             930.348\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              930.348\n",
      "exploration/Returns Min                              930.348\n",
      "exploration/Actions Mean                               0.00930644\n",
      "exploration/Actions Std                                0.133938\n",
      "exploration/Actions Max                                0.406564\n",
      "exploration/Actions Min                               -0.350266\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          930.348\n",
      "exploration/env_infos/final/reward_forward Mean        0.00098861\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.00098861\n",
      "exploration/env_infos/final/reward_forward Min         0.00098861\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.154678\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.154678\n",
      "exploration/env_infos/initial/reward_forward Min      -0.154678\n",
      "exploration/env_infos/reward_forward Mean              0.00728277\n",
      "exploration/env_infos/reward_forward Std               0.0738596\n",
      "exploration/env_infos/reward_forward Max               1.01578\n",
      "exploration/env_infos/reward_forward Min              -0.551052\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.093906\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.093906\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.093906\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0423147\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0423147\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0423147\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0721037\n",
      "exploration/env_infos/reward_ctrl Std                  0.0237098\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0131846\n",
      "exploration/env_infos/reward_ctrl Min                 -0.222302\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.000950453\n",
      "exploration/env_infos/final/torso_velocity Std         0.000385705\n",
      "exploration/env_infos/final/torso_velocity Max         0.00140261\n",
      "exploration/env_infos/final/torso_velocity Min         0.000460141\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.136296\n",
      "exploration/env_infos/initial/torso_velocity Std       0.268358\n",
      "exploration/env_infos/initial/torso_velocity Max       0.492791\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.154678\n",
      "exploration/env_infos/torso_velocity Mean              0.00186175\n",
      "exploration/env_infos/torso_velocity Std               0.075799\n",
      "exploration/env_infos/torso_velocity Max               1.01578\n",
      "exploration/env_infos/torso_velocity Min              -1.56009\n",
      "evaluation/num steps total                             2.025e+06\n",
      "evaluation/num paths total                          2025\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.908865\n",
      "evaluation/Rewards Std                                 0.0505688\n",
      "evaluation/Rewards Max                                 2.38776\n",
      "evaluation/Rewards Min                                 0.724521\n",
      "evaluation/Returns Mean                              908.865\n",
      "evaluation/Returns Std                                38.6079\n",
      "evaluation/Returns Max                               965.281\n",
      "evaluation/Returns Min                               856.165\n",
      "evaluation/Actions Mean                               -0.0258735\n",
      "evaluation/Actions Std                                 0.149905\n",
      "evaluation/Actions Max                                 0.433334\n",
      "evaluation/Actions Min                                -0.46974\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           908.865\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.89394e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.96497e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.54034e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -8.01494e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0424199\n",
      "evaluation/env_infos/initial/reward_forward Std        0.137451\n",
      "evaluation/env_infos/initial/reward_forward Max        0.149019\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.334417\n",
      "evaluation/env_infos/reward_forward Mean               0.000447575\n",
      "evaluation/env_infos/reward_forward Std                0.055472\n",
      "evaluation/env_infos/reward_forward Max                1.67178\n",
      "evaluation/env_infos/reward_forward Min               -1.21898\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0933544\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0396839\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0342764\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.148251\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0333442\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0232575\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00401293\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0982936\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0925636\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0401546\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00337629\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.275479\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -7.72128e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.49948e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.64462e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.82338e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.118905\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.267178\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.644066\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.334417\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00171757\n",
      "evaluation/env_infos/torso_velocity Std                0.0669182\n",
      "evaluation/env_infos/torso_velocity Max                1.67178\n",
      "evaluation/env_infos/torso_velocity Min               -1.885\n",
      "time/data storing (s)                                  0.0215842\n",
      "time/evaluation sampling (s)                          55.5169\n",
      "time/exploration sampling (s)                          2.51154\n",
      "time/logging (s)                                       0.288597\n",
      "time/saving (s)                                        0.0278871\n",
      "time/training (s)                                      5.02619\n",
      "time/epoch (s)                                        63.3927\n",
      "time/total (s)                                      4706.57\n",
      "Epoch                                                 80\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:29:47.732146 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 81 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 83000\n",
      "trainer/QF1 Loss                                       0.359354\n",
      "trainer/QF2 Loss                                       0.186942\n",
      "trainer/Policy Loss                                  -19.0825\n",
      "trainer/Q1 Predictions Mean                           27.6722\n",
      "trainer/Q1 Predictions Std                             1.58699\n",
      "trainer/Q1 Predictions Max                            29.2497\n",
      "trainer/Q1 Predictions Min                            20.8708\n",
      "trainer/Q2 Predictions Mean                           27.3548\n",
      "trainer/Q2 Predictions Std                             1.59841\n",
      "trainer/Q2 Predictions Max                            29.1927\n",
      "trainer/Q2 Predictions Min                            21.0374\n",
      "trainer/Q Targets Mean                                27.41\n",
      "trainer/Q Targets Std                                  1.54859\n",
      "trainer/Q Targets Max                                 29.5381\n",
      "trainer/Q Targets Min                                 20.422\n",
      "trainer/Log Pis Mean                                   8.4771\n",
      "trainer/Log Pis Std                                    2.46241\n",
      "trainer/Log Pis Max                                   17.9801\n",
      "trainer/Log Pis Min                                    0.976226\n",
      "trainer/Policy mu Mean                                -0.0348847\n",
      "trainer/Policy mu Std                                  0.153105\n",
      "trainer/Policy mu Max                                  0.888485\n",
      "trainer/Policy mu Min                                 -1.49319\n",
      "trainer/Policy log std Mean                           -2.44551\n",
      "trainer/Policy log std Std                             0.250114\n",
      "trainer/Policy log std Max                            -1.25388\n",
      "trainer/Policy log std Min                            -3.45493\n",
      "trainer/Alpha                                          0.00777254\n",
      "trainer/Alpha Loss                                     2.31828\n",
      "exploration/num steps total                        83000\n",
      "exploration/num paths total                          135\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.943297\n",
      "exploration/Rewards Std                                0.0366468\n",
      "exploration/Rewards Max                                1.16853\n",
      "exploration/Rewards Min                                0.800817\n",
      "exploration/Returns Mean                             943.297\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              943.297\n",
      "exploration/Returns Min                              943.297\n",
      "exploration/Actions Mean                              -0.045245\n",
      "exploration/Actions Std                                0.115162\n",
      "exploration/Actions Max                                0.359257\n",
      "exploration/Actions Min                               -0.469553\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          943.297\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0439331\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0439331\n",
      "exploration/env_infos/final/reward_forward Min        -0.0439331\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0852302\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0852302\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0852302\n",
      "exploration/env_infos/reward_forward Mean             -0.00155193\n",
      "exploration/env_infos/reward_forward Std               0.109606\n",
      "exploration/env_infos/reward_forward Max               0.593082\n",
      "exploration/env_infos/reward_forward Min              -0.47158\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.121195\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.121195\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.121195\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0328278\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0328278\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0328278\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0612377\n",
      "exploration/env_infos/reward_ctrl Std                  0.0287385\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0103092\n",
      "exploration/env_infos/reward_ctrl Min                 -0.199183\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0283921\n",
      "exploration/env_infos/final/torso_velocity Std         0.0171698\n",
      "exploration/env_infos/final/torso_velocity Max        -0.00446433\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0439331\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.120823\n",
      "exploration/env_infos/initial/torso_velocity Std       0.180564\n",
      "exploration/env_infos/initial/torso_velocity Max       0.35447\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0852302\n",
      "exploration/env_infos/torso_velocity Mean             -0.0040555\n",
      "exploration/env_infos/torso_velocity Std               0.122747\n",
      "exploration/env_infos/torso_velocity Max               0.593082\n",
      "exploration/env_infos/torso_velocity Min              -0.769539\n",
      "evaluation/num steps total                             2.05e+06\n",
      "evaluation/num paths total                          2050\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.968374\n",
      "evaluation/Rewards Std                                 0.0276552\n",
      "evaluation/Rewards Max                                 2.5655\n",
      "evaluation/Rewards Min                                 0.690531\n",
      "evaluation/Returns Mean                              968.374\n",
      "evaluation/Returns Std                                17.3427\n",
      "evaluation/Returns Max                               990.88\n",
      "evaluation/Returns Min                               923.339\n",
      "evaluation/Actions Mean                               -0.0292299\n",
      "evaluation/Actions Std                                 0.0848408\n",
      "evaluation/Actions Max                                 0.37967\n",
      "evaluation/Actions Min                                -0.729311\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           968.374\n",
      "evaluation/env_infos/final/reward_forward Mean         4.77948e-08\n",
      "evaluation/env_infos/final/reward_forward Std          3.13299e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.53332e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -4.94009e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0151257\n",
      "evaluation/env_infos/initial/reward_forward Std        0.123685\n",
      "evaluation/env_infos/initial/reward_forward Max        0.261275\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.199032\n",
      "evaluation/env_infos/reward_forward Mean               0.002019\n",
      "evaluation/env_infos/reward_forward Std                0.0635149\n",
      "evaluation/env_infos/reward_forward Max                1.93399\n",
      "evaluation/env_infos/reward_forward Min               -1.20546\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0321378\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0172544\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0121404\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0775159\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0466913\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0239579\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0116877\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0856602\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0322094\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0175229\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00452652\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.309469\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.04008e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.73925e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.53332e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -6.29678e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.132088\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.245503\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.622141\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.263111\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00215917\n",
      "evaluation/env_infos/torso_velocity Std                0.0654619\n",
      "evaluation/env_infos/torso_velocity Max                1.93399\n",
      "evaluation/env_infos/torso_velocity Min               -1.92458\n",
      "time/data storing (s)                                  0.0153458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/evaluation sampling (s)                          50.5552\n",
      "time/exploration sampling (s)                          2.08379\n",
      "time/logging (s)                                       0.407798\n",
      "time/saving (s)                                        0.0383725\n",
      "time/training (s)                                      5.56035\n",
      "time/epoch (s)                                        58.6609\n",
      "time/total (s)                                      4765.97\n",
      "Epoch                                                 81\n",
      "-------------------------------------------------  ---------------\n",
      "2021-05-25 20:30:56.841727 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 82 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 84000\n",
      "trainer/QF1 Loss                                       0.143809\n",
      "trainer/QF2 Loss                                       0.135673\n",
      "trainer/Policy Loss                                  -20.1071\n",
      "trainer/Q1 Predictions Mean                           27.803\n",
      "trainer/Q1 Predictions Std                             1.47657\n",
      "trainer/Q1 Predictions Max                            29.46\n",
      "trainer/Q1 Predictions Min                            20.0716\n",
      "trainer/Q2 Predictions Mean                           27.6792\n",
      "trainer/Q2 Predictions Std                             1.4428\n",
      "trainer/Q2 Predictions Max                            29.3614\n",
      "trainer/Q2 Predictions Min                            20.3055\n",
      "trainer/Q Targets Mean                                27.8912\n",
      "trainer/Q Targets Std                                  1.40751\n",
      "trainer/Q Targets Max                                 29.3657\n",
      "trainer/Q Targets Min                                 19.0162\n",
      "trainer/Log Pis Mean                                   7.69535\n",
      "trainer/Log Pis Std                                    2.38562\n",
      "trainer/Log Pis Max                                   19.7469\n",
      "trainer/Log Pis Min                                   -1.48701\n",
      "trainer/Policy mu Mean                                -0.016423\n",
      "trainer/Policy mu Std                                  0.179442\n",
      "trainer/Policy mu Max                                  1.98148\n",
      "trainer/Policy mu Min                                 -1.52809\n",
      "trainer/Policy log std Mean                           -2.32857\n",
      "trainer/Policy log std Std                             0.232921\n",
      "trainer/Policy log std Max                            -1.59018\n",
      "trainer/Policy log std Min                            -3.70811\n",
      "trainer/Alpha                                          0.00800489\n",
      "trainer/Alpha Loss                                    -1.47043\n",
      "exploration/num steps total                        84000\n",
      "exploration/num paths total                          136\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.926403\n",
      "exploration/Rewards Std                                0.0629848\n",
      "exploration/Rewards Max                                1.37748\n",
      "exploration/Rewards Min                                0.59212\n",
      "exploration/Returns Mean                             926.403\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              926.403\n",
      "exploration/Returns Min                              926.403\n",
      "exploration/Actions Mean                              -0.0199107\n",
      "exploration/Actions Std                                0.142696\n",
      "exploration/Actions Max                                0.632242\n",
      "exploration/Actions Min                               -0.552229\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          926.403\n",
      "exploration/env_infos/final/reward_forward Mean       -0.114554\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.114554\n",
      "exploration/env_infos/final/reward_forward Min        -0.114554\n",
      "exploration/env_infos/initial/reward_forward Mean      0.134944\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.134944\n",
      "exploration/env_infos/initial/reward_forward Min       0.134944\n",
      "exploration/env_infos/reward_forward Mean             -0.000932171\n",
      "exploration/env_infos/reward_forward Std               0.15608\n",
      "exploration/env_infos/reward_forward Max               1.35001\n",
      "exploration/env_infos/reward_forward Min              -0.582282\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0562573\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0562573\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0562573\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0894035\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0894035\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0894035\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0830347\n",
      "exploration/env_infos/reward_ctrl Std                  0.0437252\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0118641\n",
      "exploration/env_infos/reward_ctrl Min                 -0.40788\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0273245\n",
      "exploration/env_infos/final/torso_velocity Std         0.105497\n",
      "exploration/env_infos/final/torso_velocity Max         0.13823\n",
      "exploration/env_infos/final/torso_velocity Min        -0.114554\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.134943\n",
      "exploration/env_infos/initial/torso_velocity Std       0.278981\n",
      "exploration/env_infos/initial/torso_velocity Max       0.476623\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.206738\n",
      "exploration/env_infos/torso_velocity Mean             -0.00839998\n",
      "exploration/env_infos/torso_velocity Std               0.121156\n",
      "exploration/env_infos/torso_velocity Max               1.35001\n",
      "exploration/env_infos/torso_velocity Min              -1.09876\n",
      "evaluation/num steps total                             2.075e+06\n",
      "evaluation/num paths total                          2075\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.922696\n",
      "evaluation/Rewards Std                                 0.0744533\n",
      "evaluation/Rewards Max                                 2.33455\n",
      "evaluation/Rewards Min                                 0.33186\n",
      "evaluation/Returns Mean                              922.696\n",
      "evaluation/Returns Std                                38.9189\n",
      "evaluation/Returns Max                               978.542\n",
      "evaluation/Returns Min                               860.997\n",
      "evaluation/Actions Mean                               -0.0223993\n",
      "evaluation/Actions Std                                 0.142342\n",
      "evaluation/Actions Max                                 0.749746\n",
      "evaluation/Actions Min                                -0.781518\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           922.696\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00661065\n",
      "evaluation/env_infos/final/reward_forward Std          0.0545941\n",
      "evaluation/env_infos/final/reward_forward Max          0.252611\n",
      "evaluation/env_infos/final/reward_forward Min         -0.106833\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0440388\n",
      "evaluation/env_infos/initial/reward_forward Std        0.113218\n",
      "evaluation/env_infos/initial/reward_forward Max        0.217408\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.286939\n",
      "evaluation/env_infos/reward_forward Mean               0.00242851\n",
      "evaluation/env_infos/reward_forward Std                0.183206\n",
      "evaluation/env_infos/reward_forward Max                1.57135\n",
      "evaluation/env_infos/reward_forward Min               -1.40082\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0886118\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0452013\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0242809\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.176448\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0886051\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0195474\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0614916\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.13878\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0830521\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0457725\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0031022\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.66814\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.00167354\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0435452\n",
      "evaluation/env_infos/final/torso_velocity Max          0.252611\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.182851\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.154021\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.231887\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.716524\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.286939\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00243114\n",
      "evaluation/env_infos/torso_velocity Std                0.127997\n",
      "evaluation/env_infos/torso_velocity Max                1.57135\n",
      "evaluation/env_infos/torso_velocity Min               -2.11484\n",
      "time/data storing (s)                                  0.0208671\n",
      "time/evaluation sampling (s)                          58.3691\n",
      "time/exploration sampling (s)                          2.72851\n",
      "time/logging (s)                                       0.332744\n",
      "time/saving (s)                                        0.0317144\n",
      "time/training (s)                                      6.51869\n",
      "time/epoch (s)                                        68.0017\n",
      "time/total (s)                                      4835\n",
      "Epoch                                                 82\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:31:59.106722 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 83 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 85000\n",
      "trainer/QF1 Loss                                       0.230747\n",
      "trainer/QF2 Loss                                       0.209722\n",
      "trainer/Policy Loss                                  -19.0457\n",
      "trainer/Q1 Predictions Mean                           28.048\n",
      "trainer/Q1 Predictions Std                             1.55447\n",
      "trainer/Q1 Predictions Max                            30.1369\n",
      "trainer/Q1 Predictions Min                            21.3438\n",
      "trainer/Q2 Predictions Mean                           28.0834\n",
      "trainer/Q2 Predictions Std                             1.67804\n",
      "trainer/Q2 Predictions Max                            30.1559\n",
      "trainer/Q2 Predictions Min                            20.1712\n",
      "trainer/Q Targets Mean                                27.9345\n",
      "trainer/Q Targets Std                                  1.7883\n",
      "trainer/Q Targets Max                                 30.0714\n",
      "trainer/Q Targets Min                                 20.3087\n",
      "trainer/Log Pis Mean                                   9.11733\n",
      "trainer/Log Pis Std                                    2.71991\n",
      "trainer/Log Pis Max                                   17.9959\n",
      "trainer/Log Pis Min                                   -1.42246\n",
      "trainer/Policy mu Mean                                -0.0121027\n",
      "trainer/Policy mu Std                                  0.155534\n",
      "trainer/Policy mu Max                                  0.997652\n",
      "trainer/Policy mu Min                                 -1.29916\n",
      "trainer/Policy log std Mean                           -2.54419\n",
      "trainer/Policy log std Std                             0.255968\n",
      "trainer/Policy log std Max                            -1.79183\n",
      "trainer/Policy log std Min                            -3.75795\n",
      "trainer/Alpha                                          0.00785056\n",
      "trainer/Alpha Loss                                     5.41777\n",
      "exploration/num steps total                        85000\n",
      "exploration/num paths total                          137\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.96581\n",
      "exploration/Rewards Std                                0.103827\n",
      "exploration/Rewards Max                                1.67556\n",
      "exploration/Rewards Min                                0.575241\n",
      "exploration/Returns Mean                             965.81\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              965.81\n",
      "exploration/Returns Min                              965.81\n",
      "exploration/Actions Mean                              -0.0147939\n",
      "exploration/Actions Std                                0.127001\n",
      "exploration/Actions Max                                0.456529\n",
      "exploration/Actions Min                               -0.601123\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          965.81\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0267508\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0267508\n",
      "exploration/env_infos/final/reward_forward Min        -0.0267508\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.00690145\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.00690145\n",
      "exploration/env_infos/initial/reward_forward Min      -0.00690145\n",
      "exploration/env_infos/reward_forward Mean              0.000102531\n",
      "exploration/env_infos/reward_forward Std               0.127555\n",
      "exploration/env_infos/reward_forward Max               1.29166\n",
      "exploration/env_infos/reward_forward Min              -0.342051\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0882947\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0882947\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0882947\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.108871\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.108871\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.108871\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0653925\n",
      "exploration/env_infos/reward_ctrl Std                  0.0361946\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00739765\n",
      "exploration/env_infos/reward_ctrl Min                 -0.424759\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.346376\n",
      "exploration/env_infos/final/torso_velocity Std         0.245364\n",
      "exploration/env_infos/final/torso_velocity Max        -0.0267508\n",
      "exploration/env_infos/final/torso_velocity Min        -0.623172\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.201016\n",
      "exploration/env_infos/initial/torso_velocity Std       0.320143\n",
      "exploration/env_infos/initial/torso_velocity Max       0.653278\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.043327\n",
      "exploration/env_infos/torso_velocity Mean             -0.00777261\n",
      "exploration/env_infos/torso_velocity Std               0.154272\n",
      "exploration/env_infos/torso_velocity Max               1.29166\n",
      "exploration/env_infos/torso_velocity Min              -1.36978\n",
      "evaluation/num steps total                             2.1e+06\n",
      "evaluation/num paths total                          2100\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.957037\n",
      "evaluation/Rewards Std                                 0.0345354\n",
      "evaluation/Rewards Max                                 2.28207\n",
      "evaluation/Rewards Min                                 0.243244\n",
      "evaluation/Returns Mean                              957.037\n",
      "evaluation/Returns Std                                17.706\n",
      "evaluation/Returns Max                               978.446\n",
      "evaluation/Returns Min                               917.536\n",
      "evaluation/Actions Mean                               -0.0073206\n",
      "evaluation/Actions Std                                 0.10482\n",
      "evaluation/Actions Max                                 0.545875\n",
      "evaluation/Actions Min                                -0.816159\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           957.037\n",
      "evaluation/env_infos/final/reward_forward Mean        -6.43487e-06\n",
      "evaluation/env_infos/final/reward_forward Std          3.18606e-05\n",
      "evaluation/env_infos/final/reward_forward Max          2.52926e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000162478\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0276647\n",
      "evaluation/env_infos/initial/reward_forward Std        0.138917\n",
      "evaluation/env_infos/initial/reward_forward Max        0.296859\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.263266\n",
      "evaluation/env_infos/reward_forward Mean               0.00308543\n",
      "evaluation/env_infos/reward_forward Std                0.0661272\n",
      "evaluation/env_infos/reward_forward Max                1.61617\n",
      "evaluation/env_infos/reward_forward Min               -0.972155\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0434663\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0181485\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0207847\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0860365\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0600701\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0129657\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0408886\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0966039\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0441637\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0229863\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00363367\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.756756\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -7.75711e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.58643e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00015413\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000162478\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.157326\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.237775\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.586328\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.263266\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00168551\n",
      "evaluation/env_infos/torso_velocity Std                0.0740173\n",
      "evaluation/env_infos/torso_velocity Max                1.61617\n",
      "evaluation/env_infos/torso_velocity Min               -2.18501\n",
      "time/data storing (s)                                  0.0182998\n",
      "time/evaluation sampling (s)                          51.7726\n",
      "time/exploration sampling (s)                          2.9638\n",
      "time/logging (s)                                       0.340447\n",
      "time/saving (s)                                        0.0480234\n",
      "time/training (s)                                      6.13715\n",
      "time/epoch (s)                                        61.2804\n",
      "time/total (s)                                      4897.27\n",
      "Epoch                                                 83\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:33:13.800772 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 84 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 86000\n",
      "trainer/QF1 Loss                                       0.111295\n",
      "trainer/QF2 Loss                                       0.135481\n",
      "trainer/Policy Loss                                  -20.8057\n",
      "trainer/Q1 Predictions Mean                           28.3186\n",
      "trainer/Q1 Predictions Std                             1.42053\n",
      "trainer/Q1 Predictions Max                            30.4554\n",
      "trainer/Q1 Predictions Min                            20.8028\n",
      "trainer/Q2 Predictions Mean                           28.2792\n",
      "trainer/Q2 Predictions Std                             1.47418\n",
      "trainer/Q2 Predictions Max                            29.9067\n",
      "trainer/Q2 Predictions Min                            20.1717\n",
      "trainer/Q Targets Mean                                28.3462\n",
      "trainer/Q Targets Std                                  1.44795\n",
      "trainer/Q Targets Max                                 30.4301\n",
      "trainer/Q Targets Min                                 21.1475\n",
      "trainer/Log Pis Mean                                   7.54432\n",
      "trainer/Log Pis Std                                    2.29369\n",
      "trainer/Log Pis Max                                   13.8121\n",
      "trainer/Log Pis Min                                   -3.4085\n",
      "trainer/Policy mu Mean                                 0.0233585\n",
      "trainer/Policy mu Std                                  0.144951\n",
      "trainer/Policy mu Max                                  2.28848\n",
      "trainer/Policy mu Min                                 -0.889409\n",
      "trainer/Policy log std Mean                           -2.31843\n",
      "trainer/Policy log std Std                             0.208579\n",
      "trainer/Policy log std Max                            -1.65287\n",
      "trainer/Policy log std Min                            -3.65072\n",
      "trainer/Alpha                                          0.00805207\n",
      "trainer/Alpha Loss                                    -2.1957\n",
      "exploration/num steps total                        86000\n",
      "exploration/num paths total                          138\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.93511\n",
      "exploration/Rewards Std                                0.0611059\n",
      "exploration/Rewards Max                                1.41917\n",
      "exploration/Rewards Min                                0.656849\n",
      "exploration/Returns Mean                             935.11\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              935.11\n",
      "exploration/Returns Min                              935.11\n",
      "exploration/Actions Mean                               0.0131728\n",
      "exploration/Actions Std                                0.134569\n",
      "exploration/Actions Max                                0.572263\n",
      "exploration/Actions Min                               -0.513082\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          935.11\n",
      "exploration/env_infos/final/reward_forward Mean        0.0982411\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0982411\n",
      "exploration/env_infos/final/reward_forward Min         0.0982411\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0715383\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0715383\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0715383\n",
      "exploration/env_infos/reward_forward Mean              0.0147511\n",
      "exploration/env_infos/reward_forward Std               0.146371\n",
      "exploration/env_infos/reward_forward Max               0.806138\n",
      "exploration/env_infos/reward_forward Min              -1.02372\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.155055\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.155055\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.155055\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0852322\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0852322\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0852322\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0731291\n",
      "exploration/env_infos/reward_ctrl Std                  0.0417128\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00597892\n",
      "exploration/env_infos/reward_ctrl Min                 -0.343151\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0984774\n",
      "exploration/env_infos/final/torso_velocity Std         0.0666132\n",
      "exploration/env_infos/final/torso_velocity Max         0.18018\n",
      "exploration/env_infos/final/torso_velocity Min         0.0170117\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0134278\n",
      "exploration/env_infos/initial/torso_velocity Std       0.178539\n",
      "exploration/env_infos/initial/torso_velocity Max       0.261823\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.150001\n",
      "exploration/env_infos/torso_velocity Mean              0.00991299\n",
      "exploration/env_infos/torso_velocity Std               0.200218\n",
      "exploration/env_infos/torso_velocity Max               1.11059\n",
      "exploration/env_infos/torso_velocity Min              -1.10782\n",
      "evaluation/num steps total                             2.125e+06\n",
      "evaluation/num paths total                          2125\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.963877\n",
      "evaluation/Rewards Std                                 0.0255196\n",
      "evaluation/Rewards Max                                 2.36886\n",
      "evaluation/Rewards Min                                 0.309416\n",
      "evaluation/Returns Mean                              963.877\n",
      "evaluation/Returns Std                                 8.57686\n",
      "evaluation/Returns Max                               985.43\n",
      "evaluation/Returns Min                               952.175\n",
      "evaluation/Actions Mean                                0.0123731\n",
      "evaluation/Actions Std                                 0.0950343\n",
      "evaluation/Actions Max                                 0.557351\n",
      "evaluation/Actions Min                                -0.739321\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           963.877\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.21523e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.43378e-07\n",
      "evaluation/env_infos/final/reward_forward Max          3.33822e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -1.19788e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00356054\n",
      "evaluation/env_infos/initial/reward_forward Std        0.12563\n",
      "evaluation/env_infos/initial/reward_forward Max        0.297246\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.270876\n",
      "evaluation/env_infos/reward_forward Mean               0.00223132\n",
      "evaluation/env_infos/reward_forward Std                0.0670356\n",
      "evaluation/env_infos/reward_forward Max                1.91223\n",
      "evaluation/env_infos/reward_forward Min               -1.44363\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0354984\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.008671\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0135461\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0474533\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.068688\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0188905\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0377625\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.130173\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0367385\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0175905\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00274045\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.690584\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.58332e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          4.34954e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          6.05462e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -2.30799e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.122185\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.239474\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.625097\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.270876\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000421502\n",
      "evaluation/env_infos/torso_velocity Std                0.0672057\n",
      "evaluation/env_infos/torso_velocity Max                1.91223\n",
      "evaluation/env_infos/torso_velocity Min               -1.82697\n",
      "time/data storing (s)                                  0.0164828\n",
      "time/evaluation sampling (s)                          66.1779\n",
      "time/exploration sampling (s)                          2.50596\n",
      "time/logging (s)                                       0.303531\n",
      "time/saving (s)                                        0.02825\n",
      "time/training (s)                                      4.81556\n",
      "time/epoch (s)                                        73.8477\n",
      "time/total (s)                                      4971.92\n",
      "Epoch                                                 84\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:34:18.257625 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 85 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 87000\n",
      "trainer/QF1 Loss                                       0.11997\n",
      "trainer/QF2 Loss                                       0.209653\n",
      "trainer/Policy Loss                                  -19.63\n",
      "trainer/Q1 Predictions Mean                           28.5425\n",
      "trainer/Q1 Predictions Std                             1.56294\n",
      "trainer/Q1 Predictions Max                            30.1519\n",
      "trainer/Q1 Predictions Min                            19.9316\n",
      "trainer/Q2 Predictions Mean                           28.418\n",
      "trainer/Q2 Predictions Std                             1.60048\n",
      "trainer/Q2 Predictions Max                            30.0925\n",
      "trainer/Q2 Predictions Min                            20.5732\n",
      "trainer/Q Targets Mean                                28.5621\n",
      "trainer/Q Targets Std                                  1.58127\n",
      "trainer/Q Targets Max                                 30.5031\n",
      "trainer/Q Targets Min                                 18.681\n",
      "trainer/Log Pis Mean                                   9.00503\n",
      "trainer/Log Pis Std                                    2.71676\n",
      "trainer/Log Pis Max                                   18.3406\n",
      "trainer/Log Pis Min                                    2.07911\n",
      "trainer/Policy mu Mean                                -0.00912311\n",
      "trainer/Policy mu Std                                  0.175931\n",
      "trainer/Policy mu Max                                  1.36879\n",
      "trainer/Policy mu Min                                 -1.60848\n",
      "trainer/Policy log std Mean                           -2.49766\n",
      "trainer/Policy log std Std                             0.283514\n",
      "trainer/Policy log std Max                            -1.40175\n",
      "trainer/Policy log std Min                            -3.85874\n",
      "trainer/Alpha                                          0.0076072\n",
      "trainer/Alpha Loss                                     4.90408\n",
      "exploration/num steps total                        87000\n",
      "exploration/num paths total                          139\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.933457\n",
      "exploration/Rewards Std                                0.0406815\n",
      "exploration/Rewards Max                                1.25661\n",
      "exploration/Rewards Min                                0.592679\n",
      "exploration/Returns Mean                             933.457\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              933.457\n",
      "exploration/Returns Min                              933.457\n",
      "exploration/Actions Mean                              -0.00565791\n",
      "exploration/Actions Std                                0.134299\n",
      "exploration/Actions Max                                0.404322\n",
      "exploration/Actions Min                               -0.638966\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          933.457\n",
      "exploration/env_infos/final/reward_forward Mean       -0.036886\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.036886\n",
      "exploration/env_infos/final/reward_forward Min        -0.036886\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0051922\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0051922\n",
      "exploration/env_infos/initial/reward_forward Min       0.0051922\n",
      "exploration/env_infos/reward_forward Mean             -0.000306315\n",
      "exploration/env_infos/reward_forward Std               0.071158\n",
      "exploration/env_infos/reward_forward Max               0.821268\n",
      "exploration/env_infos/reward_forward Min              -0.256779\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.081445\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.081445\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.081445\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.102721\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.102721\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.102721\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0722729\n",
      "exploration/env_infos/reward_ctrl Std                  0.0324842\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0136732\n",
      "exploration/env_infos/reward_ctrl Min                 -0.407321\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0115059\n",
      "exploration/env_infos/final/torso_velocity Std         0.0379411\n",
      "exploration/env_infos/final/torso_velocity Max         0.0557757\n",
      "exploration/env_infos/final/torso_velocity Min        -0.036886\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.182102\n",
      "exploration/env_infos/initial/torso_velocity Std       0.21484\n",
      "exploration/env_infos/initial/torso_velocity Max       0.484476\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0051922\n",
      "exploration/env_infos/torso_velocity Mean              0.000833202\n",
      "exploration/env_infos/torso_velocity Std               0.0873178\n",
      "exploration/env_infos/torso_velocity Max               0.821268\n",
      "exploration/env_infos/torso_velocity Min              -2.26303\n",
      "evaluation/num steps total                             2.15e+06\n",
      "evaluation/num paths total                          2150\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.926002\n",
      "evaluation/Rewards Std                                 0.0376962\n",
      "evaluation/Rewards Max                                 2.29389\n",
      "evaluation/Rewards Min                                -0.214169\n",
      "evaluation/Returns Mean                              926.002\n",
      "evaluation/Returns Std                                26.0015\n",
      "evaluation/Returns Max                               967.972\n",
      "evaluation/Returns Min                               877.237\n",
      "evaluation/Actions Mean                               -0.00729621\n",
      "evaluation/Actions Std                                 0.13651\n",
      "evaluation/Actions Max                                 0.558789\n",
      "evaluation/Actions Min                                -0.937316\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           926.002\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.00239e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.94604e-07\n",
      "evaluation/env_infos/final/reward_forward Max          5.43366e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -6.50098e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0515918\n",
      "evaluation/env_infos/initial/reward_forward Std        0.114332\n",
      "evaluation/env_infos/initial/reward_forward Max        0.257047\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.174799\n",
      "evaluation/env_infos/reward_forward Mean               0.00421481\n",
      "evaluation/env_infos/reward_forward Std                0.0547539\n",
      "evaluation/env_infos/reward_forward Max                1.34197\n",
      "evaluation/env_infos/reward_forward Min               -0.701425\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0745299\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0254568\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0347447\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.123423\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0708862\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0159235\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0458204\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0995943\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0747527\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0303289\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0067221\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.21417\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -7.38392e-09\n",
      "evaluation/env_infos/final/torso_velocity Std          2.8276e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          6.92372e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.15971e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.152999\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.23761\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.63102\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.280125\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00166291\n",
      "evaluation/env_infos/torso_velocity Std                0.0649425\n",
      "evaluation/env_infos/torso_velocity Max                1.34197\n",
      "evaluation/env_infos/torso_velocity Min               -1.85189\n",
      "time/data storing (s)                                  0.0191133\n",
      "time/evaluation sampling (s)                          54.7912\n",
      "time/exploration sampling (s)                          3.18576\n",
      "time/logging (s)                                       0.293157\n",
      "time/saving (s)                                        0.0272917\n",
      "time/training (s)                                      5.28617\n",
      "time/epoch (s)                                        63.6027\n",
      "time/total (s)                                      5036.37\n",
      "Epoch                                                 85\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:35:26.647072 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 86 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 88000\n",
      "trainer/QF1 Loss                                       0.166861\n",
      "trainer/QF2 Loss                                       0.260543\n",
      "trainer/Policy Loss                                  -21.1055\n",
      "trainer/Q1 Predictions Mean                           28.734\n",
      "trainer/Q1 Predictions Std                             1.74777\n",
      "trainer/Q1 Predictions Max                            30.3451\n",
      "trainer/Q1 Predictions Min                            19.6141\n",
      "trainer/Q2 Predictions Mean                           28.5922\n",
      "trainer/Q2 Predictions Std                             1.83703\n",
      "trainer/Q2 Predictions Max                            30.2571\n",
      "trainer/Q2 Predictions Min                            18.7106\n",
      "trainer/Q Targets Mean                                28.8853\n",
      "trainer/Q Targets Std                                  1.78174\n",
      "trainer/Q Targets Max                                 30.5482\n",
      "trainer/Q Targets Min                                 19.3268\n",
      "trainer/Log Pis Mean                                   7.60168\n",
      "trainer/Log Pis Std                                    2.24306\n",
      "trainer/Log Pis Max                                   14.4846\n",
      "trainer/Log Pis Min                                   -0.0261674\n",
      "trainer/Policy mu Mean                                -0.0496767\n",
      "trainer/Policy mu Std                                  0.136549\n",
      "trainer/Policy mu Max                                  0.51038\n",
      "trainer/Policy mu Min                                 -1.14977\n",
      "trainer/Policy log std Mean                           -2.32282\n",
      "trainer/Policy log std Std                             0.197987\n",
      "trainer/Policy log std Max                            -1.70162\n",
      "trainer/Policy log std Min                            -3.159\n",
      "trainer/Alpha                                          0.00805851\n",
      "trainer/Alpha Loss                                    -1.91974\n",
      "exploration/num steps total                        88000\n",
      "exploration/num paths total                          140\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.8309\n",
      "exploration/Rewards Std                                0.0513606\n",
      "exploration/Rewards Max                                0.983265\n",
      "exploration/Rewards Min                                0.555535\n",
      "exploration/Returns Mean                             830.9\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              830.9\n",
      "exploration/Returns Min                              830.9\n",
      "exploration/Actions Mean                              -0.0975228\n",
      "exploration/Actions Std                                0.181506\n",
      "exploration/Actions Max                                0.567605\n",
      "exploration/Actions Min                               -0.630171\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          830.9\n",
      "exploration/env_infos/final/reward_forward Mean        0.0452904\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0452904\n",
      "exploration/env_infos/final/reward_forward Min         0.0452904\n",
      "exploration/env_infos/initial/reward_forward Mean      0.153411\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.153411\n",
      "exploration/env_infos/initial/reward_forward Min       0.153411\n",
      "exploration/env_infos/reward_forward Mean              0.00441275\n",
      "exploration/env_infos/reward_forward Std               0.0843557\n",
      "exploration/env_infos/reward_forward Max               0.432294\n",
      "exploration/env_infos/reward_forward Min              -1.12705\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.106964\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.106964\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.106964\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.122109\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.122109\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.122109\n",
      "exploration/env_infos/reward_ctrl Mean                -0.169821\n",
      "exploration/env_infos/reward_ctrl Std                  0.0513442\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0167349\n",
      "exploration/env_infos/reward_ctrl Min                 -0.444465\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.067206\n",
      "exploration/env_infos/final/torso_velocity Std         0.0795542\n",
      "exploration/env_infos/final/torso_velocity Max         0.0452904\n",
      "exploration/env_infos/final/torso_velocity Min        -0.124767\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.191629\n",
      "exploration/env_infos/initial/torso_velocity Std       0.118611\n",
      "exploration/env_infos/initial/torso_velocity Max       0.352185\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0692905\n",
      "exploration/env_infos/torso_velocity Mean              0.00051524\n",
      "exploration/env_infos/torso_velocity Std               0.0883809\n",
      "exploration/env_infos/torso_velocity Max               0.648806\n",
      "exploration/env_infos/torso_velocity Min              -1.12705\n",
      "evaluation/num steps total                             2.175e+06\n",
      "evaluation/num paths total                          2175\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.935056\n",
      "evaluation/Rewards Std                                 0.0437685\n",
      "evaluation/Rewards Max                                 2.16674\n",
      "evaluation/Rewards Min                                 0.594314\n",
      "evaluation/Returns Mean                              935.056\n",
      "evaluation/Returns Std                                34.681\n",
      "evaluation/Returns Max                               983.239\n",
      "evaluation/Returns Min                               847.651\n",
      "evaluation/Actions Mean                               -0.0618903\n",
      "evaluation/Actions Std                                 0.112276\n",
      "evaluation/Actions Max                                 0.584726\n",
      "evaluation/Actions Min                                -0.638133\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           935.056\n",
      "evaluation/env_infos/final/reward_forward Mean        -4.71966e-08\n",
      "evaluation/env_infos/final/reward_forward Std          3.10261e-07\n",
      "evaluation/env_infos/final/reward_forward Max          5.79499e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -6.85729e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0158147\n",
      "evaluation/env_infos/initial/reward_forward Std        0.148782\n",
      "evaluation/env_infos/initial/reward_forward Max        0.271942\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.345317\n",
      "evaluation/env_infos/reward_forward Mean               0.00245134\n",
      "evaluation/env_infos/reward_forward Std                0.066292\n",
      "evaluation/env_infos/reward_forward Max                1.62862\n",
      "evaluation/env_infos/reward_forward Min               -1.031\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0652874\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0353313\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0164264\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.153902\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0982195\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0231266\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0645782\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.145452\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.065745\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0361439\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00936912\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.602786\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         5.31783e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          4.49846e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          3.03511e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -6.85729e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.162282\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.237989\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.614228\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.345317\n",
      "evaluation/env_infos/torso_velocity Mean               0.000332074\n",
      "evaluation/env_infos/torso_velocity Std                0.0713869\n",
      "evaluation/env_infos/torso_velocity Max                1.62862\n",
      "evaluation/env_infos/torso_velocity Min               -1.83647\n",
      "time/data storing (s)                                  0.0177515\n",
      "time/evaluation sampling (s)                          57.9692\n",
      "time/exploration sampling (s)                          2.49834\n",
      "time/logging (s)                                       0.316731\n",
      "time/saving (s)                                        0.0331745\n",
      "time/training (s)                                      6.71854\n",
      "time/epoch (s)                                        67.5537\n",
      "time/total (s)                                      5104.78\n",
      "Epoch                                                 86\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:36:25.950492 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 87 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 89000\n",
      "trainer/QF1 Loss                                       0.0682209\n",
      "trainer/QF2 Loss                                       0.121884\n",
      "trainer/Policy Loss                                  -20.7616\n",
      "trainer/Q1 Predictions Mean                           29.2388\n",
      "trainer/Q1 Predictions Std                             1.40379\n",
      "trainer/Q1 Predictions Max                            36.4321\n",
      "trainer/Q1 Predictions Min                            22.0817\n",
      "trainer/Q2 Predictions Mean                           29.4045\n",
      "trainer/Q2 Predictions Std                             1.32327\n",
      "trainer/Q2 Predictions Max                            34.826\n",
      "trainer/Q2 Predictions Min                            22.2328\n",
      "trainer/Q Targets Mean                                29.3269\n",
      "trainer/Q Targets Std                                  1.39326\n",
      "trainer/Q Targets Max                                 36.4943\n",
      "trainer/Q Targets Min                                 22.3379\n",
      "trainer/Log Pis Mean                                   8.64089\n",
      "trainer/Log Pis Std                                    2.41738\n",
      "trainer/Log Pis Max                                   18.0982\n",
      "trainer/Log Pis Min                                    1.20821\n",
      "trainer/Policy mu Mean                                 0.0312712\n",
      "trainer/Policy mu Std                                  0.213628\n",
      "trainer/Policy mu Max                                  1.68032\n",
      "trainer/Policy mu Min                                 -1.55732\n",
      "trainer/Policy log std Mean                           -2.45043\n",
      "trainer/Policy log std Std                             0.217401\n",
      "trainer/Policy log std Max                            -0.902527\n",
      "trainer/Policy log std Min                            -3.9735\n",
      "trainer/Alpha                                          0.00879\n",
      "trainer/Alpha Loss                                     3.03646\n",
      "exploration/num steps total                        89000\n",
      "exploration/num paths total                          141\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.928255\n",
      "exploration/Rewards Std                                0.187228\n",
      "exploration/Rewards Max                                2.08239\n",
      "exploration/Rewards Min                                0.699496\n",
      "exploration/Returns Mean                             928.255\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              928.255\n",
      "exploration/Returns Min                              928.255\n",
      "exploration/Actions Mean                               0.0244009\n",
      "exploration/Actions Std                                0.17548\n",
      "exploration/Actions Max                                0.494291\n",
      "exploration/Actions Min                               -0.61783\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          928.255\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0822222\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0822222\n",
      "exploration/env_infos/final/reward_forward Min        -0.0822222\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0548231\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0548231\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0548231\n",
      "exploration/env_infos/reward_forward Mean              0.0525984\n",
      "exploration/env_infos/reward_forward Std               0.204482\n",
      "exploration/env_infos/reward_forward Max               1.04387\n",
      "exploration/env_infos/reward_forward Min              -0.516785\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0899264\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0899264\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0899264\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0895797\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0895797\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0895797\n",
      "exploration/env_infos/reward_ctrl Mean                -0.125555\n",
      "exploration/env_infos/reward_ctrl Std                  0.0509232\n",
      "exploration/env_infos/reward_ctrl Max                 -0.018507\n",
      "exploration/env_infos/reward_ctrl Min                 -0.300504\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0776995\n",
      "exploration/env_infos/final/torso_velocity Std         0.0249999\n",
      "exploration/env_infos/final/torso_velocity Max        -0.0450711\n",
      "exploration/env_infos/final/torso_velocity Min        -0.105805\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0859644\n",
      "exploration/env_infos/initial/torso_velocity Std       0.258514\n",
      "exploration/env_infos/initial/torso_velocity Max       0.448554\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.135837\n",
      "exploration/env_infos/torso_velocity Mean              0.0176262\n",
      "exploration/env_infos/torso_velocity Std               0.24367\n",
      "exploration/env_infos/torso_velocity Max               1.15162\n",
      "exploration/env_infos/torso_velocity Min              -1.71265\n",
      "evaluation/num steps total                             2.2e+06\n",
      "evaluation/num paths total                          2200\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.911667\n",
      "evaluation/Rewards Std                                 0.0331274\n",
      "evaluation/Rewards Max                                 2.07155\n",
      "evaluation/Rewards Min                                 0.300907\n",
      "evaluation/Returns Mean                              911.667\n",
      "evaluation/Returns Std                                24.9994\n",
      "evaluation/Returns Max                               951.098\n",
      "evaluation/Returns Min                               875.632\n",
      "evaluation/Actions Mean                                0.0253881\n",
      "evaluation/Actions Std                                 0.146897\n",
      "evaluation/Actions Max                                 0.598767\n",
      "evaluation/Actions Min                                -0.743064\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           911.667\n",
      "evaluation/env_infos/final/reward_forward Mean        -3.63849e-05\n",
      "evaluation/env_infos/final/reward_forward Std          0.0001783\n",
      "evaluation/env_infos/final/reward_forward Max          9.19567e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000909872\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0256271\n",
      "evaluation/env_infos/initial/reward_forward Std        0.114413\n",
      "evaluation/env_infos/initial/reward_forward Max        0.2062\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.155433\n",
      "evaluation/env_infos/reward_forward Mean              -0.00185024\n",
      "evaluation/env_infos/reward_forward Std                0.0457494\n",
      "evaluation/env_infos/reward_forward Max                1.16119\n",
      "evaluation/env_infos/reward_forward Min               -1.39954\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.088101\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0253745\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0476758\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.125174\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0633591\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0291755\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0301517\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.139553\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0888926\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0278606\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0154924\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.699093\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.84991e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000112082\n",
      "evaluation/env_infos/final/torso_velocity Max          9.19567e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000909872\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.148363\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.237266\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.661937\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.379135\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00217763\n",
      "evaluation/env_infos/torso_velocity Std                0.0558321\n",
      "evaluation/env_infos/torso_velocity Max                1.27579\n",
      "evaluation/env_infos/torso_velocity Min               -2.03676\n",
      "time/data storing (s)                                  0.0169585\n",
      "time/evaluation sampling (s)                          51.1825\n",
      "time/exploration sampling (s)                          2.26289\n",
      "time/logging (s)                                       0.297208\n",
      "time/saving (s)                                        0.0267848\n",
      "time/training (s)                                      4.60521\n",
      "time/epoch (s)                                        58.3915\n",
      "time/total (s)                                      5164.06\n",
      "Epoch                                                 87\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:37:27.320754 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 88 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 90000\n",
      "trainer/QF1 Loss                                       0.176306\n",
      "trainer/QF2 Loss                                       0.210535\n",
      "trainer/Policy Loss                                  -20.4803\n",
      "trainer/Q1 Predictions Mean                           29.2592\n",
      "trainer/Q1 Predictions Std                             1.83889\n",
      "trainer/Q1 Predictions Max                            31.1082\n",
      "trainer/Q1 Predictions Min                            17.6044\n",
      "trainer/Q2 Predictions Mean                           29.2595\n",
      "trainer/Q2 Predictions Std                             1.85339\n",
      "trainer/Q2 Predictions Max                            31.1295\n",
      "trainer/Q2 Predictions Min                            18.9198\n",
      "trainer/Q Targets Mean                                29.4279\n",
      "trainer/Q Targets Std                                  1.77908\n",
      "trainer/Q Targets Max                                 31.4129\n",
      "trainer/Q Targets Min                                 19.9293\n",
      "trainer/Log Pis Mean                                   8.90442\n",
      "trainer/Log Pis Std                                    2.79392\n",
      "trainer/Log Pis Max                                   19.9768\n",
      "trainer/Log Pis Min                                   -4.24869\n",
      "trainer/Policy mu Mean                                -0.0294571\n",
      "trainer/Policy mu Std                                  0.175501\n",
      "trainer/Policy mu Max                                  0.698061\n",
      "trainer/Policy mu Min                                 -1.38158\n",
      "trainer/Policy log std Mean                           -2.48293\n",
      "trainer/Policy log std Std                             0.238258\n",
      "trainer/Policy log std Max                            -1.5149\n",
      "trainer/Policy log std Min                            -3.8865\n",
      "trainer/Alpha                                          0.00828111\n",
      "trainer/Alpha Loss                                     4.33685\n",
      "exploration/num steps total                        90000\n",
      "exploration/num paths total                          142\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.904096\n",
      "exploration/Rewards Std                                0.0394648\n",
      "exploration/Rewards Max                                1.05538\n",
      "exploration/Rewards Min                                0.669082\n",
      "exploration/Returns Mean                             904.096\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              904.096\n",
      "exploration/Returns Min                              904.096\n",
      "exploration/Actions Mean                              -0.0666529\n",
      "exploration/Actions Std                                0.141057\n",
      "exploration/Actions Max                                0.386282\n",
      "exploration/Actions Min                               -0.526809\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          904.096\n",
      "exploration/env_infos/final/reward_forward Mean        0.0165896\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0165896\n",
      "exploration/env_infos/final/reward_forward Min         0.0165896\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0183344\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0183344\n",
      "exploration/env_infos/initial/reward_forward Min       0.0183344\n",
      "exploration/env_infos/reward_forward Mean              0.000846298\n",
      "exploration/env_infos/reward_forward Std               0.104958\n",
      "exploration/env_infos/reward_forward Max               0.549776\n",
      "exploration/env_infos/reward_forward Min              -0.882239\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.118012\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.118012\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.118012\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.126822\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.126822\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.126822\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0973589\n",
      "exploration/env_infos/reward_ctrl Std                  0.0384618\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0195235\n",
      "exploration/env_infos/reward_ctrl Min                 -0.330918\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.023202\n",
      "exploration/env_infos/final/torso_velocity Std         0.0106704\n",
      "exploration/env_infos/final/torso_velocity Max         0.0382553\n",
      "exploration/env_infos/final/torso_velocity Min         0.014761\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.145313\n",
      "exploration/env_infos/initial/torso_velocity Std       0.157078\n",
      "exploration/env_infos/initial/torso_velocity Max       0.366655\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0183344\n",
      "exploration/env_infos/torso_velocity Mean             -0.00413393\n",
      "exploration/env_infos/torso_velocity Std               0.119905\n",
      "exploration/env_infos/torso_velocity Max               0.780701\n",
      "exploration/env_infos/torso_velocity Min              -0.952915\n",
      "evaluation/num steps total                             2.225e+06\n",
      "evaluation/num paths total                          2225\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.899329\n",
      "evaluation/Rewards Std                                 0.0502703\n",
      "evaluation/Rewards Max                                 2.23909\n",
      "evaluation/Rewards Min                                 0.595554\n",
      "evaluation/Returns Mean                              899.329\n",
      "evaluation/Returns Std                                42.5572\n",
      "evaluation/Returns Max                               958.997\n",
      "evaluation/Returns Min                               771.667\n",
      "evaluation/Actions Mean                               -0.0274591\n",
      "evaluation/Actions Std                                 0.156963\n",
      "evaluation/Actions Max                                 0.457213\n",
      "evaluation/Actions Min                                -0.690287\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           899.329\n",
      "evaluation/env_infos/final/reward_forward Mean        -3.0941e-09\n",
      "evaluation/env_infos/final/reward_forward Std          2.34133e-07\n",
      "evaluation/env_infos/final/reward_forward Max          4.41199e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.29896e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0101136\n",
      "evaluation/env_infos/initial/reward_forward Std        0.126834\n",
      "evaluation/env_infos/initial/reward_forward Max        0.331669\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.216255\n",
      "evaluation/env_infos/reward_forward Mean               0.000660232\n",
      "evaluation/env_infos/reward_forward Std                0.0513163\n",
      "evaluation/env_infos/reward_forward Max                1.10688\n",
      "evaluation/env_infos/reward_forward Min               -1.48162\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.101665\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0428708\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0385174\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.23077\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0987235\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0278128\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.042967\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.149871\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.101565\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0434248\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0207887\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.404446\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         4.07344e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.17812e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.62763e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.31857e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.119198\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.239306\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.550188\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.389492\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00213036\n",
      "evaluation/env_infos/torso_velocity Std                0.0642921\n",
      "evaluation/env_infos/torso_velocity Max                1.55129\n",
      "evaluation/env_infos/torso_velocity Min               -1.7553\n",
      "time/data storing (s)                                  0.0148705\n",
      "time/evaluation sampling (s)                          53.9097\n",
      "time/exploration sampling (s)                          1.98404\n",
      "time/logging (s)                                       0.285257\n",
      "time/saving (s)                                        0.026637\n",
      "time/training (s)                                      4.30613\n",
      "time/epoch (s)                                        60.5266\n",
      "time/total (s)                                      5225.42\n",
      "Epoch                                                 88\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:38:20.932479 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 89 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 91000\n",
      "trainer/QF1 Loss                                       0.265008\n",
      "trainer/QF2 Loss                                       0.169823\n",
      "trainer/Policy Loss                                  -20.9592\n",
      "trainer/Q1 Predictions Mean                           29.2186\n",
      "trainer/Q1 Predictions Std                             1.81535\n",
      "trainer/Q1 Predictions Max                            30.9109\n",
      "trainer/Q1 Predictions Min                            21.6962\n",
      "trainer/Q2 Predictions Mean                           29.5315\n",
      "trainer/Q2 Predictions Std                             1.8183\n",
      "trainer/Q2 Predictions Max                            31.5463\n",
      "trainer/Q2 Predictions Min                            22.2061\n",
      "trainer/Q Targets Mean                                29.5133\n",
      "trainer/Q Targets Std                                  1.81711\n",
      "trainer/Q Targets Max                                 31.617\n",
      "trainer/Q Targets Min                                 22.3688\n",
      "trainer/Log Pis Mean                                   8.47469\n",
      "trainer/Log Pis Std                                    2.58027\n",
      "trainer/Log Pis Max                                   16.3396\n",
      "trainer/Log Pis Min                                   -0.196242\n",
      "trainer/Policy mu Mean                                 0.0231029\n",
      "trainer/Policy mu Std                                  0.168212\n",
      "trainer/Policy mu Max                                  0.887116\n",
      "trainer/Policy mu Min                                 -1.60833\n",
      "trainer/Policy log std Mean                           -2.43713\n",
      "trainer/Policy log std Std                             0.243336\n",
      "trainer/Policy log std Max                            -1.50306\n",
      "trainer/Policy log std Min                            -3.41638\n",
      "trainer/Alpha                                          0.00789976\n",
      "trainer/Alpha Loss                                     2.29903\n",
      "exploration/num steps total                        91000\n",
      "exploration/num paths total                          143\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.930092\n",
      "exploration/Rewards Std                                0.0813999\n",
      "exploration/Rewards Max                                1.89349\n",
      "exploration/Rewards Min                                0.40028\n",
      "exploration/Returns Mean                             930.092\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              930.092\n",
      "exploration/Returns Min                              930.092\n",
      "exploration/Actions Mean                              -0.00632551\n",
      "exploration/Actions Std                                0.149991\n",
      "exploration/Actions Max                                0.602811\n",
      "exploration/Actions Min                               -0.542719\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          930.092\n",
      "exploration/env_infos/final/reward_forward Mean        0.0831738\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0831738\n",
      "exploration/env_infos/final/reward_forward Min         0.0831738\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0402087\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0402087\n",
      "exploration/env_infos/initial/reward_forward Min       0.0402087\n",
      "exploration/env_infos/reward_forward Mean              0.00481512\n",
      "exploration/env_infos/reward_forward Std               0.124331\n",
      "exploration/env_infos/reward_forward Max               0.810273\n",
      "exploration/env_infos/reward_forward Min              -1.05278\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0255714\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0255714\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0255714\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.259918\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.259918\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.259918\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0901488\n",
      "exploration/env_infos/reward_ctrl Std                  0.0404869\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00664154\n",
      "exploration/env_infos/reward_ctrl Min                 -0.59972\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00105128\n",
      "exploration/env_infos/final/torso_velocity Std         0.0799536\n",
      "exploration/env_infos/final/torso_velocity Max         0.0831738\n",
      "exploration/env_infos/final/torso_velocity Min        -0.108497\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.107062\n",
      "exploration/env_infos/initial/torso_velocity Std       0.352386\n",
      "exploration/env_infos/initial/torso_velocity Max       0.56817\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.287194\n",
      "exploration/env_infos/torso_velocity Mean             -8.58603e-05\n",
      "exploration/env_infos/torso_velocity Std               0.118007\n",
      "exploration/env_infos/torso_velocity Max               0.821439\n",
      "exploration/env_infos/torso_velocity Min              -1.26869\n",
      "evaluation/num steps total                             2.25e+06\n",
      "evaluation/num paths total                          2250\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.93008\n",
      "evaluation/Rewards Std                                 0.0501041\n",
      "evaluation/Rewards Max                                 2.73075\n",
      "evaluation/Rewards Min                                 0.253108\n",
      "evaluation/Returns Mean                              930.08\n",
      "evaluation/Returns Std                                33.7573\n",
      "evaluation/Returns Max                               969.993\n",
      "evaluation/Returns Min                               849.636\n",
      "evaluation/Actions Mean                                0.000138791\n",
      "evaluation/Actions Std                                 0.133274\n",
      "evaluation/Actions Max                                 0.639657\n",
      "evaluation/Actions Min                                -0.617348\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           930.08\n",
      "evaluation/env_infos/final/reward_forward Mean         1.57447e-08\n",
      "evaluation/env_infos/final/reward_forward Std          3.71719e-07\n",
      "evaluation/env_infos/final/reward_forward Max          8.03526e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.37324e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0114993\n",
      "evaluation/env_infos/initial/reward_forward Std        0.137187\n",
      "evaluation/env_infos/initial/reward_forward Max        0.231374\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.368817\n",
      "evaluation/env_infos/reward_forward Mean              -0.00211197\n",
      "evaluation/env_infos/reward_forward Std                0.0572965\n",
      "evaluation/env_infos/reward_forward Max                1.65947\n",
      "evaluation/env_infos/reward_forward Min               -1.54501\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0704492\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0346637\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0349728\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.150917\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.157221\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0555511\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0336657\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.246675\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0710477\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0398162\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0135547\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.746892\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -6.31082e-09\n",
      "evaluation/env_infos/final/torso_velocity Std          3.84385e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.13969e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.2018e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.119792\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.24922\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.63117\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.368817\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00288333\n",
      "evaluation/env_infos/torso_velocity Std                0.0654234\n",
      "evaluation/env_infos/torso_velocity Max                1.65947\n",
      "evaluation/env_infos/torso_velocity Min               -2.05516\n",
      "time/data storing (s)                                  0.0155444\n",
      "time/evaluation sampling (s)                          46.2996\n",
      "time/exploration sampling (s)                          1.94444\n",
      "time/logging (s)                                       0.281327\n",
      "time/saving (s)                                        0.0267212\n",
      "time/training (s)                                      4.27826\n",
      "time/epoch (s)                                        52.8459\n",
      "time/total (s)                                      5279.02\n",
      "Epoch                                                 89\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:39:16.346435 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 90 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 92000\n",
      "trainer/QF1 Loss                                       0.139917\n",
      "trainer/QF2 Loss                                       0.222331\n",
      "trainer/Policy Loss                                  -21.8975\n",
      "trainer/Q1 Predictions Mean                           29.9991\n",
      "trainer/Q1 Predictions Std                             1.33585\n",
      "trainer/Q1 Predictions Max                            31.2494\n",
      "trainer/Q1 Predictions Min                            21.2601\n",
      "trainer/Q2 Predictions Mean                           29.8773\n",
      "trainer/Q2 Predictions Std                             1.39935\n",
      "trainer/Q2 Predictions Max                            31.3185\n",
      "trainer/Q2 Predictions Min                            20.7802\n",
      "trainer/Q Targets Mean                                30.2059\n",
      "trainer/Q Targets Std                                  1.36662\n",
      "trainer/Q Targets Max                                 31.7283\n",
      "trainer/Q Targets Min                                 21.5674\n",
      "trainer/Log Pis Mean                                   8.04362\n",
      "trainer/Log Pis Std                                    2.44059\n",
      "trainer/Log Pis Max                                   15.0263\n",
      "trainer/Log Pis Min                                   -0.549\n",
      "trainer/Policy mu Mean                                 0.0218766\n",
      "trainer/Policy mu Std                                  0.189593\n",
      "trainer/Policy mu Max                                  0.917806\n",
      "trainer/Policy mu Min                                 -1.83422\n",
      "trainer/Policy log std Mean                           -2.4265\n",
      "trainer/Policy log std Std                             0.208814\n",
      "trainer/Policy log std Max                            -1.34279\n",
      "trainer/Policy log std Min                            -3.50983\n",
      "trainer/Alpha                                          0.00754955\n",
      "trainer/Alpha Loss                                     0.213181\n",
      "exploration/num steps total                        92000\n",
      "exploration/num paths total                          144\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.791133\n",
      "exploration/Rewards Std                                0.080958\n",
      "exploration/Rewards Max                                2.18442\n",
      "exploration/Rewards Min                                0.631121\n",
      "exploration/Returns Mean                             791.133\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              791.133\n",
      "exploration/Returns Min                              791.133\n",
      "exploration/Actions Mean                               0.00737799\n",
      "exploration/Actions Std                                0.231146\n",
      "exploration/Actions Max                                0.463009\n",
      "exploration/Actions Min                               -0.742656\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          791.133\n",
      "exploration/env_infos/final/reward_forward Mean        0.00144752\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.00144752\n",
      "exploration/env_infos/final/reward_forward Min         0.00144752\n",
      "exploration/env_infos/initial/reward_forward Mean      0.254284\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.254284\n",
      "exploration/env_infos/initial/reward_forward Min       0.254284\n",
      "exploration/env_infos/reward_forward Mean             -0.00117514\n",
      "exploration/env_infos/reward_forward Std               0.0316016\n",
      "exploration/env_infos/reward_forward Max               0.279986\n",
      "exploration/env_infos/reward_forward Min              -0.454153\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.138453\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.138453\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.138453\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.222615\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.222615\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.222615\n",
      "exploration/env_infos/reward_ctrl Mean                -0.213932\n",
      "exploration/env_infos/reward_ctrl Std                  0.0429379\n",
      "exploration/env_infos/reward_ctrl Max                 -0.090957\n",
      "exploration/env_infos/reward_ctrl Min                 -0.368879\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0197536\n",
      "exploration/env_infos/final/torso_velocity Std         0.0178398\n",
      "exploration/env_infos/final/torso_velocity Max         0.0439418\n",
      "exploration/env_infos/final/torso_velocity Min         0.00144752\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.252367\n",
      "exploration/env_infos/initial/torso_velocity Std       0.243662\n",
      "exploration/env_infos/initial/torso_velocity Max       0.549827\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0470111\n",
      "exploration/env_infos/torso_velocity Mean             -0.00555784\n",
      "exploration/env_infos/torso_velocity Std               0.0736971\n",
      "exploration/env_infos/torso_velocity Max               0.549827\n",
      "exploration/env_infos/torso_velocity Min              -1.63679\n",
      "evaluation/num steps total                             2.275e+06\n",
      "evaluation/num paths total                          2275\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.796076\n",
      "evaluation/Rewards Std                                 0.161887\n",
      "evaluation/Rewards Max                                 2.58213\n",
      "evaluation/Rewards Min                                -0.430215\n",
      "evaluation/Returns Mean                              796.076\n",
      "evaluation/Returns Std                               157.165\n",
      "evaluation/Returns Max                               934.948\n",
      "evaluation/Returns Min                               425.605\n",
      "evaluation/Actions Mean                                0.0151685\n",
      "evaluation/Actions Std                                 0.226054\n",
      "evaluation/Actions Max                                 0.739176\n",
      "evaluation/Actions Min                                -0.932953\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           796.076\n",
      "evaluation/env_infos/final/reward_forward Mean         1.79347e-08\n",
      "evaluation/env_infos/final/reward_forward Std          1.99028e-07\n",
      "evaluation/env_infos/final/reward_forward Max          3.54677e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -4.74683e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0262259\n",
      "evaluation/env_infos/initial/reward_forward Std        0.108905\n",
      "evaluation/env_infos/initial/reward_forward Max        0.216201\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.142593\n",
      "evaluation/env_infos/reward_forward Mean               0.00327078\n",
      "evaluation/env_infos/reward_forward Std                0.0656535\n",
      "evaluation/env_infos/reward_forward Max                1.85064\n",
      "evaluation/env_infos/reward_forward Min               -1.10293\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.20527\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.157667\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0646715\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.579417\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.133119\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0436041\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0752577\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.241133\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.205322\n",
      "evaluation/env_infos/reward_ctrl Std                   0.158241\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0315272\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.43021\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.19947e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.54266e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          6.76788e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.17215e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.121024\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.247472\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.631244\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.328239\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000668273\n",
      "evaluation/env_infos/torso_velocity Std                0.0731141\n",
      "evaluation/env_infos/torso_velocity Max                1.85064\n",
      "evaluation/env_infos/torso_velocity Min               -2.07335\n",
      "time/data storing (s)                                  0.0157311\n",
      "time/evaluation sampling (s)                          47.8551\n",
      "time/exploration sampling (s)                          2.11288\n",
      "time/logging (s)                                       0.299103\n",
      "time/saving (s)                                        0.0272256\n",
      "time/training (s)                                      4.3551\n",
      "time/epoch (s)                                        54.6652\n",
      "time/total (s)                                      5334.45\n",
      "Epoch                                                 90\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:40:11.938811 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 91 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 93000\n",
      "trainer/QF1 Loss                                       0.192186\n",
      "trainer/QF2 Loss                                       0.081971\n",
      "trainer/Policy Loss                                  -22.7707\n",
      "trainer/Q1 Predictions Mean                           30.1754\n",
      "trainer/Q1 Predictions Std                             1.12115\n",
      "trainer/Q1 Predictions Max                            31.4331\n",
      "trainer/Q1 Predictions Min                            20.8661\n",
      "trainer/Q2 Predictions Mean                           30.3622\n",
      "trainer/Q2 Predictions Std                             1.13949\n",
      "trainer/Q2 Predictions Max                            31.8327\n",
      "trainer/Q2 Predictions Min                            20.8787\n",
      "trainer/Q Targets Mean                                30.4973\n",
      "trainer/Q Targets Std                                  1.10151\n",
      "trainer/Q Targets Max                                 32.1681\n",
      "trainer/Q Targets Min                                 22.1143\n",
      "trainer/Log Pis Mean                                   7.47209\n",
      "trainer/Log Pis Std                                    2.23702\n",
      "trainer/Log Pis Max                                   16.9837\n",
      "trainer/Log Pis Min                                   -0.820833\n",
      "trainer/Policy mu Mean                                -0.0437875\n",
      "trainer/Policy mu Std                                  0.136561\n",
      "trainer/Policy mu Max                                  0.986171\n",
      "trainer/Policy mu Min                                 -1.15666\n",
      "trainer/Policy log std Mean                           -2.31401\n",
      "trainer/Policy log std Std                             0.20048\n",
      "trainer/Policy log std Max                            -1.18596\n",
      "trainer/Policy log std Min                            -3.97733\n",
      "trainer/Alpha                                          0.00754143\n",
      "trainer/Alpha Loss                                    -2.57819\n",
      "exploration/num steps total                        93000\n",
      "exploration/num paths total                          145\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.939417\n",
      "exploration/Rewards Std                                0.120053\n",
      "exploration/Rewards Max                                1.90022\n",
      "exploration/Rewards Min                               -0.0382317\n",
      "exploration/Returns Mean                             939.417\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              939.417\n",
      "exploration/Returns Min                              939.417\n",
      "exploration/Actions Mean                              -0.0491333\n",
      "exploration/Actions Std                                0.144961\n",
      "exploration/Actions Max                                0.553756\n",
      "exploration/Actions Min                               -0.946918\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          939.417\n",
      "exploration/env_infos/final/reward_forward Mean        0.116592\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.116592\n",
      "exploration/env_infos/final/reward_forward Min         0.116592\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0423303\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0423303\n",
      "exploration/env_infos/initial/reward_forward Min       0.0423303\n",
      "exploration/env_infos/reward_forward Mean              0.0136934\n",
      "exploration/env_infos/reward_forward Std               0.209376\n",
      "exploration/env_infos/reward_forward Max               0.79453\n",
      "exploration/env_infos/reward_forward Min              -0.911526\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0533532\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0533532\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0533532\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.13431\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.13431\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.13431\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0937108\n",
      "exploration/env_infos/reward_ctrl Std                  0.0574259\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00785397\n",
      "exploration/env_infos/reward_ctrl Min                 -1.03823\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0368843\n",
      "exploration/env_infos/final/torso_velocity Std         0.114122\n",
      "exploration/env_infos/final/torso_velocity Max         0.116592\n",
      "exploration/env_infos/final/torso_velocity Min        -0.156861\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.207483\n",
      "exploration/env_infos/initial/torso_velocity Std       0.201566\n",
      "exploration/env_infos/initial/torso_velocity Max       0.491273\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0423303\n",
      "exploration/env_infos/torso_velocity Mean              0.0116505\n",
      "exploration/env_infos/torso_velocity Std               0.203552\n",
      "exploration/env_infos/torso_velocity Max               1.5595\n",
      "exploration/env_infos/torso_velocity Min              -1.73723\n",
      "evaluation/num steps total                             2.3e+06\n",
      "evaluation/num paths total                          2300\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.938994\n",
      "evaluation/Rewards Std                                 0.0456235\n",
      "evaluation/Rewards Max                                 1.97\n",
      "evaluation/Rewards Min                                -0.0453291\n",
      "evaluation/Returns Mean                              938.994\n",
      "evaluation/Returns Std                                31.1817\n",
      "evaluation/Returns Max                               986.432\n",
      "evaluation/Returns Min                               886.863\n",
      "evaluation/Actions Mean                               -0.0504137\n",
      "evaluation/Actions Std                                 0.113305\n",
      "evaluation/Actions Max                                 0.566585\n",
      "evaluation/Actions Min                                -0.935548\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           938.994\n",
      "evaluation/env_infos/final/reward_forward Mean        -5.79583e-08\n",
      "evaluation/env_infos/final/reward_forward Std          7.63132e-07\n",
      "evaluation/env_infos/final/reward_forward Max          1.70406e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -1.76225e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0168349\n",
      "evaluation/env_infos/initial/reward_forward Std        0.093767\n",
      "evaluation/env_infos/initial/reward_forward Max        0.248319\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.148735\n",
      "evaluation/env_infos/reward_forward Mean               0.00205933\n",
      "evaluation/env_infos/reward_forward Std                0.0492377\n",
      "evaluation/env_infos/reward_forward Max                1.4266\n",
      "evaluation/env_infos/reward_forward Min               -1.15291\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.059588\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.031799\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0118385\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.111589\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0774871\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.028399\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0284039\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.122628\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0615187\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0432452\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00346384\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.04533\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.33761e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          7.08448e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.70406e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -3.37803e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.147938\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.22792\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.630833\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.271338\n",
      "evaluation/env_infos/torso_velocity Mean               0.000726868\n",
      "evaluation/env_infos/torso_velocity Std                0.0604473\n",
      "evaluation/env_infos/torso_velocity Max                1.59948\n",
      "evaluation/env_infos/torso_velocity Min               -2.22013\n",
      "time/data storing (s)                                  0.017751\n",
      "time/evaluation sampling (s)                          47.7274\n",
      "time/exploration sampling (s)                          2.09739\n",
      "time/logging (s)                                       0.291408\n",
      "time/saving (s)                                        0.027398\n",
      "time/training (s)                                      4.52057\n",
      "time/epoch (s)                                        54.6819\n",
      "time/total (s)                                      5390.04\n",
      "Epoch                                                 91\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:41:08.415214 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 92 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 94000\n",
      "trainer/QF1 Loss                                       0.134662\n",
      "trainer/QF2 Loss                                       0.119501\n",
      "trainer/Policy Loss                                  -22.7096\n",
      "trainer/Q1 Predictions Mean                           30.5575\n",
      "trainer/Q1 Predictions Std                             1.33913\n",
      "trainer/Q1 Predictions Max                            32.0538\n",
      "trainer/Q1 Predictions Min                            24.3718\n",
      "trainer/Q2 Predictions Mean                           30.5073\n",
      "trainer/Q2 Predictions Std                             1.27828\n",
      "trainer/Q2 Predictions Max                            32.0245\n",
      "trainer/Q2 Predictions Min                            23.7156\n",
      "trainer/Q Targets Mean                                30.5954\n",
      "trainer/Q Targets Std                                  1.36116\n",
      "trainer/Q Targets Max                                 32.1166\n",
      "trainer/Q Targets Min                                 23.746\n",
      "trainer/Log Pis Mean                                   7.88161\n",
      "trainer/Log Pis Std                                    2.1893\n",
      "trainer/Log Pis Max                                   13.3799\n",
      "trainer/Log Pis Min                                    0.195245\n",
      "trainer/Policy mu Mean                                -0.0167448\n",
      "trainer/Policy mu Std                                  0.137121\n",
      "trainer/Policy mu Max                                  0.924954\n",
      "trainer/Policy mu Min                                 -0.867103\n",
      "trainer/Policy log std Mean                           -2.38191\n",
      "trainer/Policy log std Std                             0.200593\n",
      "trainer/Policy log std Max                            -1.60695\n",
      "trainer/Policy log std Min                            -3.03644\n",
      "trainer/Alpha                                          0.00731539\n",
      "trainer/Alpha Loss                                    -0.582396\n",
      "exploration/num steps total                        94000\n",
      "exploration/num paths total                          146\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.734114\n",
      "exploration/Rewards Std                                0.096447\n",
      "exploration/Rewards Max                                1.45312\n",
      "exploration/Rewards Min                                0.359219\n",
      "exploration/Returns Mean                             734.114\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              734.114\n",
      "exploration/Returns Min                              734.114\n",
      "exploration/Actions Mean                              -0.00916979\n",
      "exploration/Actions Std                                0.258892\n",
      "exploration/Actions Max                                0.727699\n",
      "exploration/Actions Min                               -0.666652\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          734.114\n",
      "exploration/env_infos/final/reward_forward Mean        0.240946\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.240946\n",
      "exploration/env_infos/final/reward_forward Min         0.240946\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0662302\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0662302\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0662302\n",
      "exploration/env_infos/reward_forward Mean              0.00160493\n",
      "exploration/env_infos/reward_forward Std               0.0749543\n",
      "exploration/env_infos/reward_forward Max               1.15723\n",
      "exploration/env_infos/reward_forward Min              -0.502277\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.228096\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.228096\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.228096\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0168767\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0168767\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0168767\n",
      "exploration/env_infos/reward_ctrl Mean                -0.268436\n",
      "exploration/env_infos/reward_ctrl Std                  0.0907141\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0102443\n",
      "exploration/env_infos/reward_ctrl Min                 -0.640781\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.00594564\n",
      "exploration/env_infos/final/torso_velocity Std         0.170011\n",
      "exploration/env_infos/final/torso_velocity Max         0.240946\n",
      "exploration/env_infos/final/torso_velocity Min        -0.155562\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.171921\n",
      "exploration/env_infos/initial/torso_velocity Std       0.205255\n",
      "exploration/env_infos/initial/torso_velocity Max       0.434723\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0662302\n",
      "exploration/env_infos/torso_velocity Mean             -0.00262432\n",
      "exploration/env_infos/torso_velocity Std               0.0705503\n",
      "exploration/env_infos/torso_velocity Max               1.15723\n",
      "exploration/env_infos/torso_velocity Min              -1.15935\n",
      "evaluation/num steps total                             2.325e+06\n",
      "evaluation/num paths total                          2325\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.937751\n",
      "evaluation/Rewards Std                                 0.0666582\n",
      "evaluation/Rewards Max                                 2.41025\n",
      "evaluation/Rewards Min                                 0.690955\n",
      "evaluation/Returns Mean                              937.751\n",
      "evaluation/Returns Std                                54.8774\n",
      "evaluation/Returns Max                               986.102\n",
      "evaluation/Returns Min                               821.657\n",
      "evaluation/Actions Mean                               -0.00579636\n",
      "evaluation/Actions Std                                 0.126485\n",
      "evaluation/Actions Max                                 0.494235\n",
      "evaluation/Actions Min                                -0.479518\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           937.751\n",
      "evaluation/env_infos/final/reward_forward Mean         1.32137e-06\n",
      "evaluation/env_infos/final/reward_forward Std          5.69262e-06\n",
      "evaluation/env_infos/final/reward_forward Max          2.91264e-05\n",
      "evaluation/env_infos/final/reward_forward Min         -8.52259e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0151139\n",
      "evaluation/env_infos/initial/reward_forward Std        0.109725\n",
      "evaluation/env_infos/initial/reward_forward Max        0.201113\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.161708\n",
      "evaluation/env_infos/reward_forward Mean               0.00116163\n",
      "evaluation/env_infos/reward_forward Std                0.0771694\n",
      "evaluation/env_infos/reward_forward Max                1.67511\n",
      "evaluation/env_infos/reward_forward Min               -1.00827\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0647979\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0582559\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0195823\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.194769\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0934396\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.058401\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0179847\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.189948\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0641284\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0573987\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00501005\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.361698\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.5408e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          1.33915e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          8.28094e-05\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.64675e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.145073\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.248332\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.715037\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.350596\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000648578\n",
      "evaluation/env_infos/torso_velocity Std                0.0733218\n",
      "evaluation/env_infos/torso_velocity Max                1.67511\n",
      "evaluation/env_infos/torso_velocity Min               -2.06816\n",
      "time/data storing (s)                                  0.0160334\n",
      "time/evaluation sampling (s)                          48.9334\n",
      "time/exploration sampling (s)                          2.13229\n",
      "time/logging (s)                                       0.282451\n",
      "time/saving (s)                                        0.0261533\n",
      "time/training (s)                                      4.27496\n",
      "time/epoch (s)                                        55.6653\n",
      "time/total (s)                                      5446.5\n",
      "Epoch                                                 92\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:42:15.244148 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 93 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 95000\n",
      "trainer/QF1 Loss                                       0.210487\n",
      "trainer/QF2 Loss                                       0.126283\n",
      "trainer/Policy Loss                                  -23.3106\n",
      "trainer/Q1 Predictions Mean                           30.5482\n",
      "trainer/Q1 Predictions Std                             1.75194\n",
      "trainer/Q1 Predictions Max                            32.8452\n",
      "trainer/Q1 Predictions Min                            19.3411\n",
      "trainer/Q2 Predictions Mean                           30.8018\n",
      "trainer/Q2 Predictions Std                             1.63093\n",
      "trainer/Q2 Predictions Max                            32.722\n",
      "trainer/Q2 Predictions Min                            20.7647\n",
      "trainer/Q Targets Mean                                30.7575\n",
      "trainer/Q Targets Std                                  1.70173\n",
      "trainer/Q Targets Max                                 32.508\n",
      "trainer/Q Targets Min                                 19.0889\n",
      "trainer/Log Pis Mean                                   7.41377\n",
      "trainer/Log Pis Std                                    2.37774\n",
      "trainer/Log Pis Max                                   16.2753\n",
      "trainer/Log Pis Min                                   -0.545066\n",
      "trainer/Policy mu Mean                                 0.00935724\n",
      "trainer/Policy mu Std                                  0.177229\n",
      "trainer/Policy mu Max                                  0.952426\n",
      "trainer/Policy mu Min                                 -2.02167\n",
      "trainer/Policy log std Mean                           -2.29792\n",
      "trainer/Policy log std Std                             0.215686\n",
      "trainer/Policy log std Max                            -0.85952\n",
      "trainer/Policy log std Min                            -3.61554\n",
      "trainer/Alpha                                          0.00796567\n",
      "trainer/Alpha Loss                                    -2.83218\n",
      "exploration/num steps total                        95000\n",
      "exploration/num paths total                          147\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.915315\n",
      "exploration/Rewards Std                                0.0832723\n",
      "exploration/Rewards Max                                2.00174\n",
      "exploration/Rewards Min                                0.666476\n",
      "exploration/Returns Mean                             915.315\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              915.315\n",
      "exploration/Returns Min                              915.315\n",
      "exploration/Actions Mean                               0.0269893\n",
      "exploration/Actions Std                                0.153034\n",
      "exploration/Actions Max                                0.597665\n",
      "exploration/Actions Min                               -0.492343\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          915.315\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0561299\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0561299\n",
      "exploration/env_infos/final/reward_forward Min        -0.0561299\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0423083\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0423083\n",
      "exploration/env_infos/initial/reward_forward Min       0.0423083\n",
      "exploration/env_infos/reward_forward Mean              0.00269809\n",
      "exploration/env_infos/reward_forward Std               0.168041\n",
      "exploration/env_infos/reward_forward Max               1.03079\n",
      "exploration/env_infos/reward_forward Min              -0.569161\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.17185\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.17185\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.17185\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.178467\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.178467\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.178467\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0965917\n",
      "exploration/env_infos/reward_ctrl Std                  0.0452331\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0106929\n",
      "exploration/env_infos/reward_ctrl Min                 -0.333524\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0097885\n",
      "exploration/env_infos/final/torso_velocity Std         0.0685282\n",
      "exploration/env_infos/final/torso_velocity Max         0.104272\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0561299\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.137537\n",
      "exploration/env_infos/initial/torso_velocity Std       0.265603\n",
      "exploration/env_infos/initial/torso_velocity Max       0.49982\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.129516\n",
      "exploration/env_infos/torso_velocity Mean              0.00442363\n",
      "exploration/env_infos/torso_velocity Std               0.172765\n",
      "exploration/env_infos/torso_velocity Max               1.03079\n",
      "exploration/env_infos/torso_velocity Min              -1.29856\n",
      "evaluation/num steps total                             2.35e+06\n",
      "evaluation/num paths total                          2350\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.951943\n",
      "evaluation/Rewards Std                                 0.0405551\n",
      "evaluation/Rewards Max                                 2.18484\n",
      "evaluation/Rewards Min                                 0.322934\n",
      "evaluation/Returns Mean                              951.943\n",
      "evaluation/Returns Std                                24.441\n",
      "evaluation/Returns Max                               984.297\n",
      "evaluation/Returns Min                               886.365\n",
      "evaluation/Actions Mean                                0.00862616\n",
      "evaluation/Actions Std                                 0.111043\n",
      "evaluation/Actions Max                                 0.750358\n",
      "evaluation/Actions Min                                -0.773148\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           951.943\n",
      "evaluation/env_infos/final/reward_forward Mean         2.70999e-06\n",
      "evaluation/env_infos/final/reward_forward Std          1.26421e-05\n",
      "evaluation/env_infos/final/reward_forward Max          6.46043e-05\n",
      "evaluation/env_infos/final/reward_forward Min         -1.04932e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0436409\n",
      "evaluation/env_infos/initial/reward_forward Std        0.121581\n",
      "evaluation/env_infos/initial/reward_forward Max        0.269247\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.204129\n",
      "evaluation/env_infos/reward_forward Mean               0.00082345\n",
      "evaluation/env_infos/reward_forward Std                0.0557683\n",
      "evaluation/env_infos/reward_forward Max                1.51956\n",
      "evaluation/env_infos/reward_forward Min               -1.03159\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0489803\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.025122\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0152501\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.115341\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.137843\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0320933\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0831811\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.198729\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0496202\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0297073\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0104597\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.677066\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.000174486\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0011111\n",
      "evaluation/env_infos/final/torso_velocity Max          6.46043e-05\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00861915\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.125082\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.2436\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.662606\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.337458\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00191782\n",
      "evaluation/env_infos/torso_velocity Std                0.0682957\n",
      "evaluation/env_infos/torso_velocity Max                1.51956\n",
      "evaluation/env_infos/torso_velocity Min               -1.89928\n",
      "time/data storing (s)                                  0.0171689\n",
      "time/evaluation sampling (s)                          56.9394\n",
      "time/exploration sampling (s)                          2.69504\n",
      "time/logging (s)                                       0.342289\n",
      "time/saving (s)                                        0.0347478\n",
      "time/training (s)                                      6.05205\n",
      "time/epoch (s)                                        66.0807\n",
      "time/total (s)                                      5513.39\n",
      "Epoch                                                 93\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:43:16.816734 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 94 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 96000\n",
      "trainer/QF1 Loss                                       0.3417\n",
      "trainer/QF2 Loss                                       0.734147\n",
      "trainer/Policy Loss                                  -22.2355\n",
      "trainer/Q1 Predictions Mean                           30.9185\n",
      "trainer/Q1 Predictions Std                             1.79332\n",
      "trainer/Q1 Predictions Max                            32.7325\n",
      "trainer/Q1 Predictions Min                            17.4995\n",
      "trainer/Q2 Predictions Mean                           30.854\n",
      "trainer/Q2 Predictions Std                             1.62836\n",
      "trainer/Q2 Predictions Max                            32.8346\n",
      "trainer/Q2 Predictions Min                            22.3655\n",
      "trainer/Q Targets Mean                                30.8916\n",
      "trainer/Q Targets Std                                  2.0994\n",
      "trainer/Q Targets Max                                 32.7765\n",
      "trainer/Q Targets Min                                 12.2447\n",
      "trainer/Log Pis Mean                                   8.76707\n",
      "trainer/Log Pis Std                                    2.16137\n",
      "trainer/Log Pis Max                                   15.9412\n",
      "trainer/Log Pis Min                                    0.0081476\n",
      "trainer/Policy mu Mean                                -0.0828225\n",
      "trainer/Policy mu Std                                  0.217223\n",
      "trainer/Policy mu Max                                  1.15636\n",
      "trainer/Policy mu Min                                 -1.16303\n",
      "trainer/Policy log std Mean                           -2.44634\n",
      "trainer/Policy log std Std                             0.186753\n",
      "trainer/Policy log std Max                            -1.18801\n",
      "trainer/Policy log std Min                            -3.19122\n",
      "trainer/Alpha                                          0.0074977\n",
      "trainer/Alpha Loss                                     3.75347\n",
      "exploration/num steps total                        96000\n",
      "exploration/num paths total                          148\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.794018\n",
      "exploration/Rewards Std                                0.0594559\n",
      "exploration/Rewards Max                                1.13386\n",
      "exploration/Rewards Min                                0.553414\n",
      "exploration/Returns Mean                             794.018\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              794.018\n",
      "exploration/Returns Min                              794.018\n",
      "exploration/Actions Mean                              -0.121605\n",
      "exploration/Actions Std                                0.192708\n",
      "exploration/Actions Max                                0.452066\n",
      "exploration/Actions Min                               -0.615147\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          794.018\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0058267\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0058267\n",
      "exploration/env_infos/final/reward_forward Min        -0.0058267\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0914573\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0914573\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0914573\n",
      "exploration/env_infos/reward_forward Mean              0.000159094\n",
      "exploration/env_infos/reward_forward Std               0.076215\n",
      "exploration/env_infos/reward_forward Max               0.969268\n",
      "exploration/env_infos/reward_forward Min              -0.529168\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.11397\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.11397\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.11397\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.141141\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.141141\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.141141\n",
      "exploration/env_infos/reward_ctrl Mean                -0.207697\n",
      "exploration/env_infos/reward_ctrl Std                  0.0572672\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0402628\n",
      "exploration/env_infos/reward_ctrl Min                 -0.446586\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.012769\n",
      "exploration/env_infos/final/torso_velocity Std         0.0182679\n",
      "exploration/env_infos/final/torso_velocity Max         0.0375983\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0058267\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.196552\n",
      "exploration/env_infos/initial/torso_velocity Std       0.246962\n",
      "exploration/env_infos/initial/torso_velocity Max       0.51165\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0914573\n",
      "exploration/env_infos/torso_velocity Mean             -0.00302122\n",
      "exploration/env_infos/torso_velocity Std               0.0814757\n",
      "exploration/env_infos/torso_velocity Max               0.969268\n",
      "exploration/env_infos/torso_velocity Min              -0.977982\n",
      "evaluation/num steps total                             2.375e+06\n",
      "evaluation/num paths total                          2375\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.836629\n",
      "evaluation/Rewards Std                                 0.0656378\n",
      "evaluation/Rewards Max                                 2.28358\n",
      "evaluation/Rewards Min                                 0.575113\n",
      "evaluation/Returns Mean                              836.629\n",
      "evaluation/Returns Std                                59.7864\n",
      "evaluation/Returns Max                               940.835\n",
      "evaluation/Returns Min                               760.152\n",
      "evaluation/Actions Mean                               -0.102714\n",
      "evaluation/Actions Std                                 0.174612\n",
      "evaluation/Actions Max                                 0.618166\n",
      "evaluation/Actions Min                                -0.555315\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           836.629\n",
      "evaluation/env_infos/final/reward_forward Mean         6.99836e-08\n",
      "evaluation/env_infos/final/reward_forward Std          4.67745e-07\n",
      "evaluation/env_infos/final/reward_forward Max          8.09673e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -8.28323e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00166608\n",
      "evaluation/env_infos/initial/reward_forward Std        0.107003\n",
      "evaluation/env_infos/initial/reward_forward Max        0.184252\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.238186\n",
      "evaluation/env_infos/reward_forward Mean               0.00118581\n",
      "evaluation/env_infos/reward_forward Std                0.0489511\n",
      "evaluation/env_infos/reward_forward Max                1.40554\n",
      "evaluation/env_infos/reward_forward Min               -1.1746\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.164419\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0601701\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0585634\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.240714\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.105637\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0288566\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.053237\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.171268\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.164159\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0603915\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0200394\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.424887\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         9.18541e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.70812e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.30752e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.28323e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.139439\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.235642\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.750813\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.277744\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00162013\n",
      "evaluation/env_infos/torso_velocity Std                0.0548626\n",
      "evaluation/env_infos/torso_velocity Max                1.55931\n",
      "evaluation/env_infos/torso_velocity Min               -1.76191\n",
      "time/data storing (s)                                  0.0160308\n",
      "time/evaluation sampling (s)                          53.3487\n",
      "time/exploration sampling (s)                          2.24439\n",
      "time/logging (s)                                       0.307204\n",
      "time/saving (s)                                        0.028589\n",
      "time/training (s)                                      4.39903\n",
      "time/epoch (s)                                        60.3439\n",
      "time/total (s)                                      5574.92\n",
      "Epoch                                                 94\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:44:13.512490 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 95 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 97000\n",
      "trainer/QF1 Loss                                       0.185116\n",
      "trainer/QF2 Loss                                       0.129007\n",
      "trainer/Policy Loss                                  -23.5144\n",
      "trainer/Q1 Predictions Mean                           31.576\n",
      "trainer/Q1 Predictions Std                             1.6086\n",
      "trainer/Q1 Predictions Max                            33.5117\n",
      "trainer/Q1 Predictions Min                            22.8711\n",
      "trainer/Q2 Predictions Mean                           31.3083\n",
      "trainer/Q2 Predictions Std                             1.68221\n",
      "trainer/Q2 Predictions Max                            33.804\n",
      "trainer/Q2 Predictions Min                            21.7851\n",
      "trainer/Q Targets Mean                                31.3501\n",
      "trainer/Q Targets Std                                  1.60847\n",
      "trainer/Q Targets Max                                 33.5283\n",
      "trainer/Q Targets Min                                 22.5422\n",
      "trainer/Log Pis Mean                                   7.96672\n",
      "trainer/Log Pis Std                                    2.69294\n",
      "trainer/Log Pis Max                                   26.3425\n",
      "trainer/Log Pis Min                                   -2.41243\n",
      "trainer/Policy mu Mean                                -0.0122669\n",
      "trainer/Policy mu Std                                  0.153753\n",
      "trainer/Policy mu Max                                  2.41707\n",
      "trainer/Policy mu Min                                 -2.83221\n",
      "trainer/Policy log std Mean                           -2.38478\n",
      "trainer/Policy log std Std                             0.260198\n",
      "trainer/Policy log std Max                            -1.75443\n",
      "trainer/Policy log std Min                            -4.9193\n",
      "trainer/Alpha                                          0.00656145\n",
      "trainer/Alpha Loss                                    -0.167263\n",
      "exploration/num steps total                        97000\n",
      "exploration/num paths total                          149\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.953931\n",
      "exploration/Rewards Std                                0.113658\n",
      "exploration/Rewards Max                                1.98563\n",
      "exploration/Rewards Min                                0.653728\n",
      "exploration/Returns Mean                             953.931\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              953.931\n",
      "exploration/Returns Min                              953.931\n",
      "exploration/Actions Mean                               0.000782741\n",
      "exploration/Actions Std                                0.131066\n",
      "exploration/Actions Max                                0.524787\n",
      "exploration/Actions Min                               -0.556994\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          953.931\n",
      "exploration/env_infos/final/reward_forward Mean       -0.220911\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.220911\n",
      "exploration/env_infos/final/reward_forward Min        -0.220911\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0955826\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0955826\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0955826\n",
      "exploration/env_infos/reward_forward Mean              0.0402234\n",
      "exploration/env_infos/reward_forward Std               0.322788\n",
      "exploration/env_infos/reward_forward Max               1.17173\n",
      "exploration/env_infos/reward_forward Min              -0.773968\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0272286\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0272286\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0272286\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.271984\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.271984\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.271984\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0687154\n",
      "exploration/env_infos/reward_ctrl Std                  0.0379105\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00665484\n",
      "exploration/env_infos/reward_ctrl Min                 -0.346272\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.135974\n",
      "exploration/env_infos/final/torso_velocity Std         0.13557\n",
      "exploration/env_infos/final/torso_velocity Max         0.0553505\n",
      "exploration/env_infos/final/torso_velocity Min        -0.24236\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.00376994\n",
      "exploration/env_infos/initial/torso_velocity Std       0.321808\n",
      "exploration/env_infos/initial/torso_velocity Max       0.428166\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.343893\n",
      "exploration/env_infos/torso_velocity Mean              0.00448883\n",
      "exploration/env_infos/torso_velocity Std               0.293284\n",
      "exploration/env_infos/torso_velocity Max               1.17173\n",
      "exploration/env_infos/torso_velocity Min              -1.78094\n",
      "evaluation/num steps total                             2.4e+06\n",
      "evaluation/num paths total                          2400\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.97725\n",
      "evaluation/Rewards Std                                 0.0383866\n",
      "evaluation/Rewards Max                                 2.40348\n",
      "evaluation/Rewards Min                                 0.210377\n",
      "evaluation/Returns Mean                              977.25\n",
      "evaluation/Returns Std                                 7.20469\n",
      "evaluation/Returns Max                               990.825\n",
      "evaluation/Returns Min                               963.46\n",
      "evaluation/Actions Mean                               -0.012064\n",
      "evaluation/Actions Std                                 0.0775806\n",
      "evaluation/Actions Max                                 0.596137\n",
      "evaluation/Actions Min                                -0.695843\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           977.25\n",
      "evaluation/env_infos/final/reward_forward Mean        -6.72525e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.53287e-06\n",
      "evaluation/env_infos/final/reward_forward Max          1.10048e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -1.14436e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0549655\n",
      "evaluation/env_infos/initial/reward_forward Std        0.126978\n",
      "evaluation/env_infos/initial/reward_forward Max        0.194072\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.2562\n",
      "evaluation/env_infos/reward_forward Mean               0.00432465\n",
      "evaluation/env_infos/reward_forward Std                0.0709886\n",
      "evaluation/env_infos/reward_forward Max                1.54063\n",
      "evaluation/env_infos/reward_forward Min               -1.40776\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0235256\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00715483\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00914296\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0359115\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.159064\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0426131\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0886191\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.285322\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0246571\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0189341\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00193708\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.789623\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.6278e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          2.56739e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          1.09986e-05\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.14436e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0919432\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.25636\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.60264\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.26286\n",
      "evaluation/env_infos/torso_velocity Mean               0.000448937\n",
      "evaluation/env_infos/torso_velocity Std                0.0705599\n",
      "evaluation/env_infos/torso_velocity Max                1.59423\n",
      "evaluation/env_infos/torso_velocity Min               -1.75725\n",
      "time/data storing (s)                                  0.0163739\n",
      "time/evaluation sampling (s)                          48.6342\n",
      "time/exploration sampling (s)                          2.1914\n",
      "time/logging (s)                                       0.285657\n",
      "time/saving (s)                                        0.0274831\n",
      "time/training (s)                                      4.65531\n",
      "time/epoch (s)                                        55.8104\n",
      "time/total (s)                                      5631.59\n",
      "Epoch                                                 95\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:45:12.286688 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 96 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 98000\n",
      "trainer/QF1 Loss                                       0.225123\n",
      "trainer/QF2 Loss                                       0.266725\n",
      "trainer/Policy Loss                                  -24.2289\n",
      "trainer/Q1 Predictions Mean                           31.3717\n",
      "trainer/Q1 Predictions Std                             1.61154\n",
      "trainer/Q1 Predictions Max                            33.0787\n",
      "trainer/Q1 Predictions Min                            23.9578\n",
      "trainer/Q2 Predictions Mean                           31.4676\n",
      "trainer/Q2 Predictions Std                             1.59829\n",
      "trainer/Q2 Predictions Max                            33.4194\n",
      "trainer/Q2 Predictions Min                            23.5652\n",
      "trainer/Q Targets Mean                                31.5822\n",
      "trainer/Q Targets Std                                  1.68219\n",
      "trainer/Q Targets Max                                 34.1168\n",
      "trainer/Q Targets Min                                 23.2429\n",
      "trainer/Log Pis Mean                                   7.22149\n",
      "trainer/Log Pis Std                                    2.10269\n",
      "trainer/Log Pis Max                                   13.847\n",
      "trainer/Log Pis Min                                    1.04286\n",
      "trainer/Policy mu Mean                                -0.0109466\n",
      "trainer/Policy mu Std                                  0.18453\n",
      "trainer/Policy mu Max                                  1.24774\n",
      "trainer/Policy mu Min                                 -1.41753\n",
      "trainer/Policy log std Mean                           -2.27537\n",
      "trainer/Policy log std Std                             0.154511\n",
      "trainer/Policy log std Max                            -1.11868\n",
      "trainer/Policy log std Min                            -2.97037\n",
      "trainer/Alpha                                          0.00713691\n",
      "trainer/Alpha Loss                                    -3.84812\n",
      "exploration/num steps total                        98000\n",
      "exploration/num paths total                          150\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.888086\n",
      "exploration/Rewards Std                                0.0711717\n",
      "exploration/Rewards Max                                1.3062\n",
      "exploration/Rewards Min                                0.584871\n",
      "exploration/Returns Mean                             888.086\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              888.086\n",
      "exploration/Returns Min                              888.086\n",
      "exploration/Actions Mean                              -0.0183493\n",
      "exploration/Actions Std                                0.172667\n",
      "exploration/Actions Max                                0.55016\n",
      "exploration/Actions Min                               -0.729589\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          888.086\n",
      "exploration/env_infos/final/reward_forward Mean       -0.136161\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.136161\n",
      "exploration/env_infos/final/reward_forward Min        -0.136161\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.149187\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.149187\n",
      "exploration/env_infos/initial/reward_forward Min      -0.149187\n",
      "exploration/env_infos/reward_forward Mean             -0.0223339\n",
      "exploration/env_infos/reward_forward Std               0.147236\n",
      "exploration/env_infos/reward_forward Max               0.433436\n",
      "exploration/env_infos/reward_forward Min              -0.620894\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0792664\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0792664\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0792664\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.199939\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.199939\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.199939\n",
      "exploration/env_infos/reward_ctrl Mean                -0.120602\n",
      "exploration/env_infos/reward_ctrl Std                  0.0598206\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0172866\n",
      "exploration/env_infos/reward_ctrl Min                 -0.415129\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.099979\n",
      "exploration/env_infos/final/torso_velocity Std         0.0337801\n",
      "exploration/env_infos/final/torso_velocity Max        -0.0548733\n",
      "exploration/env_infos/final/torso_velocity Min        -0.136161\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.0547357\n",
      "exploration/env_infos/initial/torso_velocity Std       0.284298\n",
      "exploration/env_infos/initial/torso_velocity Max       0.330938\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.345959\n",
      "exploration/env_infos/torso_velocity Mean             -0.0155002\n",
      "exploration/env_infos/torso_velocity Std               0.168888\n",
      "exploration/env_infos/torso_velocity Max               1.0099\n",
      "exploration/env_infos/torso_velocity Min              -1.55254\n",
      "evaluation/num steps total                             2.425e+06\n",
      "evaluation/num paths total                          2425\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.936779\n",
      "evaluation/Rewards Std                                 0.033103\n",
      "evaluation/Rewards Max                                 2.24242\n",
      "evaluation/Rewards Min                                 0.315838\n",
      "evaluation/Returns Mean                              936.779\n",
      "evaluation/Returns Std                                22.9003\n",
      "evaluation/Returns Max                               967.863\n",
      "evaluation/Returns Min                               888.601\n",
      "evaluation/Actions Mean                               -0.00566388\n",
      "evaluation/Actions Std                                 0.126098\n",
      "evaluation/Actions Max                                 0.610645\n",
      "evaluation/Actions Min                                -0.706436\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           936.779\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.67142e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.58765e-07\n",
      "evaluation/env_infos/final/reward_forward Max          4.59284e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.25863e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0606953\n",
      "evaluation/env_infos/initial/reward_forward Std        0.135632\n",
      "evaluation/env_infos/initial/reward_forward Max        0.291377\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.287248\n",
      "evaluation/env_infos/reward_forward Mean              -0.000211619\n",
      "evaluation/env_infos/reward_forward Std                0.0442048\n",
      "evaluation/env_infos/reward_forward Max                1.16358\n",
      "evaluation/env_infos/reward_forward Min               -0.992444\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0628187\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0229249\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0313571\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.110488\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.240587\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0460404\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.141227\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.371077\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0637309\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0273881\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00838415\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.684162\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.40065e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          3.70221e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.3238e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.80948e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.153879\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.259511\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.618137\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.31161\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00194452\n",
      "evaluation/env_infos/torso_velocity Std                0.0518576\n",
      "evaluation/env_infos/torso_velocity Max                1.40183\n",
      "evaluation/env_infos/torso_velocity Min               -1.79624\n",
      "time/data storing (s)                                  0.020572\n",
      "time/evaluation sampling (s)                          49.8228\n",
      "time/exploration sampling (s)                          2.45995\n",
      "time/logging (s)                                       0.361375\n",
      "time/saving (s)                                        0.0284113\n",
      "time/training (s)                                      5.28495\n",
      "time/epoch (s)                                        57.9781\n",
      "time/total (s)                                      5690.44\n",
      "Epoch                                                 96\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:46:12.686428 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 97 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 99000\n",
      "trainer/QF1 Loss                                       0.236928\n",
      "trainer/QF2 Loss                                       0.316924\n",
      "trainer/Policy Loss                                  -23.6531\n",
      "trainer/Q1 Predictions Mean                           31.7898\n",
      "trainer/Q1 Predictions Std                             1.61288\n",
      "trainer/Q1 Predictions Max                            34.2255\n",
      "trainer/Q1 Predictions Min                            24.1975\n",
      "trainer/Q2 Predictions Mean                           31.9707\n",
      "trainer/Q2 Predictions Std                             1.67163\n",
      "trainer/Q2 Predictions Max                            34.162\n",
      "trainer/Q2 Predictions Min                            24.1039\n",
      "trainer/Q Targets Mean                                31.7584\n",
      "trainer/Q Targets Std                                  1.69439\n",
      "trainer/Q Targets Max                                 34.5496\n",
      "trainer/Q Targets Min                                 24.0517\n",
      "trainer/Log Pis Mean                                   8.25587\n",
      "trainer/Log Pis Std                                    2.47827\n",
      "trainer/Log Pis Max                                   16.0109\n",
      "trainer/Log Pis Min                                    0.733566\n",
      "trainer/Policy mu Mean                                -0.0522894\n",
      "trainer/Policy mu Std                                  0.142287\n",
      "trainer/Policy mu Max                                  0.661866\n",
      "trainer/Policy mu Min                                 -1.38346\n",
      "trainer/Policy log std Mean                           -2.42501\n",
      "trainer/Policy log std Std                             0.193194\n",
      "trainer/Policy log std Max                            -1.95678\n",
      "trainer/Policy log std Min                            -3.43403\n",
      "trainer/Alpha                                          0.00800594\n",
      "trainer/Alpha Loss                                     1.23589\n",
      "exploration/num steps total                        99000\n",
      "exploration/num paths total                          151\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.938911\n",
      "exploration/Rewards Std                                0.0857267\n",
      "exploration/Rewards Max                                1.59516\n",
      "exploration/Rewards Min                                0.53913\n",
      "exploration/Returns Mean                             938.911\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              938.911\n",
      "exploration/Returns Min                              938.911\n",
      "exploration/Actions Mean                              -0.0420542\n",
      "exploration/Actions Std                                0.138861\n",
      "exploration/Actions Max                                0.503855\n",
      "exploration/Actions Min                               -0.672867\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          938.911\n",
      "exploration/env_infos/final/reward_forward Mean        0.0953079\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0953079\n",
      "exploration/env_infos/final/reward_forward Min         0.0953079\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0356843\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0356843\n",
      "exploration/env_infos/initial/reward_forward Min       0.0356843\n",
      "exploration/env_infos/reward_forward Mean              0.00490911\n",
      "exploration/env_infos/reward_forward Std               0.159314\n",
      "exploration/env_infos/reward_forward Max               0.619989\n",
      "exploration/env_infos/reward_forward Min              -0.687704\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0919052\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0919052\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0919052\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.139721\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.139721\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.139721\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0842036\n",
      "exploration/env_infos/reward_ctrl Std                  0.0419381\n",
      "exploration/env_infos/reward_ctrl Max                 -0.012941\n",
      "exploration/env_infos/reward_ctrl Min                 -0.46087\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.108878\n",
      "exploration/env_infos/final/torso_velocity Std         0.102771\n",
      "exploration/env_infos/final/torso_velocity Max         0.240981\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00965439\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0669403\n",
      "exploration/env_infos/initial/torso_velocity Std       0.241301\n",
      "exploration/env_infos/initial/torso_velocity Max       0.376859\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.211722\n",
      "exploration/env_infos/torso_velocity Mean             -0.00267422\n",
      "exploration/env_infos/torso_velocity Std               0.192326\n",
      "exploration/env_infos/torso_velocity Max               0.845521\n",
      "exploration/env_infos/torso_velocity Min              -1.079\n",
      "evaluation/num steps total                             2.45e+06\n",
      "evaluation/num paths total                          2450\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.962993\n",
      "evaluation/Rewards Std                                 0.0263379\n",
      "evaluation/Rewards Max                                 2.24428\n",
      "evaluation/Rewards Min                                 0.437794\n",
      "evaluation/Returns Mean                              962.993\n",
      "evaluation/Returns Std                                 5.72722\n",
      "evaluation/Returns Max                               978.649\n",
      "evaluation/Returns Min                               950.522\n",
      "evaluation/Actions Mean                               -0.0288937\n",
      "evaluation/Actions Std                                 0.0927787\n",
      "evaluation/Actions Max                                 0.583\n",
      "evaluation/Actions Min                                -0.73028\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           962.993\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.03933e-07\n",
      "evaluation/env_infos/final/reward_forward Std          4.14911e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.13378e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -9.32107e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00372744\n",
      "evaluation/env_infos/initial/reward_forward Std        0.105242\n",
      "evaluation/env_infos/initial/reward_forward Max        0.225323\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.241534\n",
      "evaluation/env_infos/reward_forward Mean              -0.000872858\n",
      "evaluation/env_infos/reward_forward Std                0.0561859\n",
      "evaluation/env_infos/reward_forward Max                1.49069\n",
      "evaluation/env_infos/reward_forward Min               -1.70777\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0368451\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00642634\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0202096\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.051556\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.119955\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0328517\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0341021\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.170126\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0377709\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0166531\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0124929\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.641519\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -3.21689e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.52257e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.84585e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.32107e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.145763\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.247588\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.658102\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.298963\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000730322\n",
      "evaluation/env_infos/torso_velocity Std                0.0684619\n",
      "evaluation/env_infos/torso_velocity Max                1.52139\n",
      "evaluation/env_infos/torso_velocity Min               -1.86425\n",
      "time/data storing (s)                                  0.0180053\n",
      "time/evaluation sampling (s)                          52.1897\n",
      "time/exploration sampling (s)                          2.24186\n",
      "time/logging (s)                                       0.288229\n",
      "time/saving (s)                                        0.0266539\n",
      "time/training (s)                                      4.64813\n",
      "time/epoch (s)                                        59.4126\n",
      "time/total (s)                                      5750.77\n",
      "Epoch                                                 97\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:47:12.734192 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 98 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 100000\n",
      "trainer/QF1 Loss                                        0.129832\n",
      "trainer/QF2 Loss                                        0.121886\n",
      "trainer/Policy Loss                                   -24.7823\n",
      "trainer/Q1 Predictions Mean                            32.0711\n",
      "trainer/Q1 Predictions Std                              1.39041\n",
      "trainer/Q1 Predictions Max                             33.8638\n",
      "trainer/Q1 Predictions Min                             22.8815\n",
      "trainer/Q2 Predictions Mean                            32.254\n",
      "trainer/Q2 Predictions Std                              1.35671\n",
      "trainer/Q2 Predictions Max                             33.8716\n",
      "trainer/Q2 Predictions Min                             24.3672\n",
      "trainer/Q Targets Mean                                 32.1883\n",
      "trainer/Q Targets Std                                   1.29697\n",
      "trainer/Q Targets Max                                  33.9306\n",
      "trainer/Q Targets Min                                  24.5159\n",
      "trainer/Log Pis Mean                                    7.39235\n",
      "trainer/Log Pis Std                                     2.3037\n",
      "trainer/Log Pis Max                                    13.685\n",
      "trainer/Log Pis Min                                    -0.327079\n",
      "trainer/Policy mu Mean                                 -0.0520291\n",
      "trainer/Policy mu Std                                   0.143066\n",
      "trainer/Policy mu Max                                   1.23142\n",
      "trainer/Policy mu Min                                  -1.53826\n",
      "trainer/Policy log std Mean                            -2.3327\n",
      "trainer/Policy log std Std                              0.20872\n",
      "trainer/Policy log std Max                             -1.85637\n",
      "trainer/Policy log std Min                             -3.30628\n",
      "trainer/Alpha                                           0.00723364\n",
      "trainer/Alpha Loss                                     -2.99426\n",
      "exploration/num steps total                        100000\n",
      "exploration/num paths total                           152\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.949042\n",
      "exploration/Rewards Std                                 0.0984072\n",
      "exploration/Rewards Max                                 1.8066\n",
      "exploration/Rewards Min                                 0.781685\n",
      "exploration/Returns Mean                              949.042\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               949.042\n",
      "exploration/Returns Min                               949.042\n",
      "exploration/Actions Mean                               -0.0476545\n",
      "exploration/Actions Std                                 0.126842\n",
      "exploration/Actions Max                                 0.409817\n",
      "exploration/Actions Min                                -0.556948\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           949.042\n",
      "exploration/env_infos/final/reward_forward Mean        -0.238511\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.238511\n",
      "exploration/env_infos/final/reward_forward Min         -0.238511\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0803261\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0803261\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0803261\n",
      "exploration/env_infos/reward_forward Mean              -0.0397215\n",
      "exploration/env_infos/reward_forward Std                0.243795\n",
      "exploration/env_infos/reward_forward Max                1.13023\n",
      "exploration/env_infos/reward_forward Min               -1.16746\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.113844\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.113844\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.113844\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.122643\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.122643\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.122643\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0734394\n",
      "exploration/env_infos/reward_ctrl Std                   0.0348521\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00906644\n",
      "exploration/env_infos/reward_ctrl Min                  -0.218315\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.189989\n",
      "exploration/env_infos/final/torso_velocity Std          0.16487\n",
      "exploration/env_infos/final/torso_velocity Max          0.0317741\n",
      "exploration/env_infos/final/torso_velocity Min         -0.363232\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.214019\n",
      "exploration/env_infos/initial/torso_velocity Std        0.250251\n",
      "exploration/env_infos/initial/torso_velocity Max        0.531364\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0803261\n",
      "exploration/env_infos/torso_velocity Mean              -0.0147647\n",
      "exploration/env_infos/torso_velocity Std                0.244253\n",
      "exploration/env_infos/torso_velocity Max                1.13023\n",
      "exploration/env_infos/torso_velocity Min               -1.22785\n",
      "evaluation/num steps total                              2.475e+06\n",
      "evaluation/num paths total                           2475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.924384\n",
      "evaluation/Rewards Std                                  0.043673\n",
      "evaluation/Rewards Max                                  2.63909\n",
      "evaluation/Rewards Min                                  0.740136\n",
      "evaluation/Returns Mean                               924.384\n",
      "evaluation/Returns Std                                 29.1743\n",
      "evaluation/Returns Max                                968.943\n",
      "evaluation/Returns Min                                868.909\n",
      "evaluation/Actions Mean                                -0.0427632\n",
      "evaluation/Actions Std                                  0.131982\n",
      "evaluation/Actions Max                                  0.443961\n",
      "evaluation/Actions Min                                 -0.572023\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            924.384\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.58878e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.4919e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.12139e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -4.44298e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.015431\n",
      "evaluation/env_infos/initial/reward_forward Std         0.129312\n",
      "evaluation/env_infos/initial/reward_forward Max         0.309919\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.169041\n",
      "evaluation/env_infos/reward_forward Mean                0.00085759\n",
      "evaluation/env_infos/reward_forward Std                 0.0638984\n",
      "evaluation/env_infos/reward_forward Max                 1.71393\n",
      "evaluation/env_infos/reward_forward Min                -1.32245\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0771995\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0300381\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0327863\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.136496\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.130995\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0593637\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0255487\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.259864\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0769917\n",
      "evaluation/env_infos/reward_ctrl Std                    0.030554\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0105744\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.259864\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.07361e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.45404e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.00228e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.89487e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.134884\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.252893\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.689704\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.253518\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00144909\n",
      "evaluation/env_infos/torso_velocity Std                 0.0659552\n",
      "evaluation/env_infos/torso_velocity Max                 1.71393\n",
      "evaluation/env_infos/torso_velocity Min                -1.72808\n",
      "time/data storing (s)                                   0.0166567\n",
      "time/evaluation sampling (s)                           51.2609\n",
      "time/exploration sampling (s)                           2.57802\n",
      "time/logging (s)                                        0.333993\n",
      "time/saving (s)                                         0.0267106\n",
      "time/training (s)                                       5.00933\n",
      "time/epoch (s)                                         59.2257\n",
      "time/total (s)                                       5810.86\n",
      "Epoch                                                  98\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:48:07.906862 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 99 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 101000\n",
      "trainer/QF1 Loss                                        0.432295\n",
      "trainer/QF2 Loss                                        0.66994\n",
      "trainer/Policy Loss                                   -24.0222\n",
      "trainer/Q1 Predictions Mean                            32.1745\n",
      "trainer/Q1 Predictions Std                              2.31876\n",
      "trainer/Q1 Predictions Max                             35.2417\n",
      "trainer/Q1 Predictions Min                              6.76622\n",
      "trainer/Q2 Predictions Mean                            31.9923\n",
      "trainer/Q2 Predictions Std                              2.25684\n",
      "trainer/Q2 Predictions Max                             34.5867\n",
      "trainer/Q2 Predictions Min                              9.18045\n",
      "trainer/Q Targets Mean                                 32.141\n",
      "trainer/Q Targets Std                                   2.72893\n",
      "trainer/Q Targets Max                                  34.4166\n",
      "trainer/Q Targets Min                                  -1.14843\n",
      "trainer/Log Pis Mean                                    8.0931\n",
      "trainer/Log Pis Std                                     2.63758\n",
      "trainer/Log Pis Max                                    28.1248\n",
      "trainer/Log Pis Min                                     0.99269\n",
      "trainer/Policy mu Mean                                 -0.0102987\n",
      "trainer/Policy mu Std                                   0.219883\n",
      "trainer/Policy mu Max                                   3.46545\n",
      "trainer/Policy mu Min                                  -1.87766\n",
      "trainer/Policy log std Mean                            -2.36717\n",
      "trainer/Policy log std Std                              0.228239\n",
      "trainer/Policy log std Max                             -0.691867\n",
      "trainer/Policy log std Min                             -3.91381\n",
      "trainer/Alpha                                           0.00686677\n",
      "trainer/Alpha Loss                                      0.463821\n",
      "exploration/num steps total                        101000\n",
      "exploration/num paths total                           153\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.920653\n",
      "exploration/Rewards Std                                 0.0545821\n",
      "exploration/Rewards Max                                 1.47047\n",
      "exploration/Rewards Min                                 0.732233\n",
      "exploration/Returns Mean                              920.653\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               920.653\n",
      "exploration/Returns Min                               920.653\n",
      "exploration/Actions Mean                               -0.0276211\n",
      "exploration/Actions Std                                 0.143989\n",
      "exploration/Actions Max                                 0.440497\n",
      "exploration/Actions Min                                -0.571828\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           920.653\n",
      "exploration/env_infos/final/reward_forward Mean         0.274049\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.274049\n",
      "exploration/env_infos/final/reward_forward Min          0.274049\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.00165718\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.00165718\n",
      "exploration/env_infos/initial/reward_forward Min       -0.00165718\n",
      "exploration/env_infos/reward_forward Mean               0.0151571\n",
      "exploration/env_infos/reward_forward Std                0.158049\n",
      "exploration/env_infos/reward_forward Max                1.46175\n",
      "exploration/env_infos/reward_forward Min               -0.936801\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.159923\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.159923\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.159923\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.136335\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.136335\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.136335\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0859829\n",
      "exploration/env_infos/reward_ctrl Std                   0.0385726\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0112648\n",
      "exploration/env_infos/reward_ctrl Min                  -0.267767\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0161428\n",
      "exploration/env_infos/final/torso_velocity Std          0.233386\n",
      "exploration/env_infos/final/torso_velocity Max          0.274049\n",
      "exploration/env_infos/final/torso_velocity Min         -0.291184\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.116239\n",
      "exploration/env_infos/initial/torso_velocity Std        0.162523\n",
      "exploration/env_infos/initial/torso_velocity Max        0.346055\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.00165718\n",
      "exploration/env_infos/torso_velocity Mean              -0.00017792\n",
      "exploration/env_infos/torso_velocity Std                0.131477\n",
      "exploration/env_infos/torso_velocity Max                1.46175\n",
      "exploration/env_infos/torso_velocity Min               -1.00198\n",
      "evaluation/num steps total                              2.5e+06\n",
      "evaluation/num paths total                           2500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.921636\n",
      "evaluation/Rewards Std                                  0.0350307\n",
      "evaluation/Rewards Max                                  2.6025\n",
      "evaluation/Rewards Min                                  0.597499\n",
      "evaluation/Returns Mean                               921.636\n",
      "evaluation/Returns Std                                 20.8529\n",
      "evaluation/Returns Max                                946.18\n",
      "evaluation/Returns Min                                867.078\n",
      "evaluation/Actions Mean                                -0.0370611\n",
      "evaluation/Actions Std                                  0.135983\n",
      "evaluation/Actions Max                                  0.382997\n",
      "evaluation/Actions Min                                 -0.638177\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            921.636\n",
      "evaluation/env_infos/final/reward_forward Mean          1.13927e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.38197e-07\n",
      "evaluation/env_infos/final/reward_forward Max           6.21761e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.1379e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0270377\n",
      "evaluation/env_infos/initial/reward_forward Std         0.131026\n",
      "evaluation/env_infos/initial/reward_forward Max         0.329042\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.248397\n",
      "evaluation/env_infos/reward_forward Mean                0.00110378\n",
      "evaluation/env_infos/reward_forward Std                 0.050618\n",
      "evaluation/env_infos/reward_forward Max                 1.34682\n",
      "evaluation/env_infos/reward_forward Min                -1.19955\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.080103\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.022256\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0538873\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.135417\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.149181\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0535326\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0347619\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.246152\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0794598\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0229617\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00536382\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.402501\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.38858e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.86432e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           6.21761e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.1379e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.148308\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.264082\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.639229\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.310775\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00168931\n",
      "evaluation/env_infos/torso_velocity Std                 0.0595468\n",
      "evaluation/env_infos/torso_velocity Max                 1.34682\n",
      "evaluation/env_infos/torso_velocity Min                -1.86831\n",
      "time/data storing (s)                                   0.0163122\n",
      "time/evaluation sampling (s)                           47.302\n",
      "time/exploration sampling (s)                           2.11107\n",
      "time/logging (s)                                        0.286542\n",
      "time/saving (s)                                         0.0277688\n",
      "time/training (s)                                       4.29152\n",
      "time/epoch (s)                                         54.0352\n",
      "time/total (s)                                       5865.98\n",
      "Epoch                                                  99\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:49:03.891527 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 100 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 102000\n",
      "trainer/QF1 Loss                                        2.38967\n",
      "trainer/QF2 Loss                                        0.304386\n",
      "trainer/Policy Loss                                   -22.8928\n",
      "trainer/Q1 Predictions Mean                            32.5806\n",
      "trainer/Q1 Predictions Std                              3.95542\n",
      "trainer/Q1 Predictions Max                             34.2618\n",
      "trainer/Q1 Predictions Min                            -23.7035\n",
      "trainer/Q2 Predictions Mean                            32.4784\n",
      "trainer/Q2 Predictions Std                              2.58084\n",
      "trainer/Q2 Predictions Max                             34.3895\n",
      "trainer/Q2 Predictions Min                              4.09007\n",
      "trainer/Q Targets Mean                                 32.3555\n",
      "trainer/Q Targets Std                                   2.74946\n",
      "trainer/Q Targets Max                                  34.4045\n",
      "trainer/Q Targets Min                                  -0.786591\n",
      "trainer/Log Pis Mean                                    9.73514\n",
      "trainer/Log Pis Std                                     4.74217\n",
      "trainer/Log Pis Max                                    71.8669\n",
      "trainer/Log Pis Min                                    -0.0214349\n",
      "trainer/Policy mu Mean                                 -0.0739334\n",
      "trainer/Policy mu Std                                   0.32716\n",
      "trainer/Policy mu Max                                   6.06928\n",
      "trainer/Policy mu Min                                  -4.1848\n",
      "trainer/Policy log std Mean                            -2.55924\n",
      "trainer/Policy log std Std                              0.333356\n",
      "trainer/Policy log std Max                             -1.07611\n",
      "trainer/Policy log std Min                             -5.92618\n",
      "trainer/Alpha                                           0.0072683\n",
      "trainer/Alpha Loss                                      8.54878\n",
      "exploration/num steps total                        102000\n",
      "exploration/num paths total                           154\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.956975\n",
      "exploration/Rewards Std                                 0.0234459\n",
      "exploration/Rewards Max                                 1.26547\n",
      "exploration/Rewards Min                                 0.685165\n",
      "exploration/Returns Mean                              956.975\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               956.975\n",
      "exploration/Returns Min                               956.975\n",
      "exploration/Actions Mean                               -0.0562648\n",
      "exploration/Actions Std                                 0.089591\n",
      "exploration/Actions Max                                 0.308544\n",
      "exploration/Actions Min                                -0.429329\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           956.975\n",
      "exploration/env_infos/final/reward_forward Mean        -0.160599\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.160599\n",
      "exploration/env_infos/final/reward_forward Min         -0.160599\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0586983\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0586983\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0586983\n",
      "exploration/env_infos/reward_forward Mean              -0.00948175\n",
      "exploration/env_infos/reward_forward Std                0.0954934\n",
      "exploration/env_infos/reward_forward Max                0.30552\n",
      "exploration/env_infos/reward_forward Min               -1.2988\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0621765\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0621765\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0621765\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.314835\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.314835\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.314835\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0447691\n",
      "exploration/env_infos/reward_ctrl Std                   0.0179048\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0102224\n",
      "exploration/env_infos/reward_ctrl Min                  -0.314835\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.12979\n",
      "exploration/env_infos/final/torso_velocity Std          0.0716877\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0307385\n",
      "exploration/env_infos/final/torso_velocity Min         -0.198032\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.163791\n",
      "exploration/env_infos/initial/torso_velocity Std        0.209888\n",
      "exploration/env_infos/initial/torso_velocity Max        0.445192\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0586983\n",
      "exploration/env_infos/torso_velocity Mean              -0.00301362\n",
      "exploration/env_infos/torso_velocity Std                0.0915666\n",
      "exploration/env_infos/torso_velocity Max                0.800536\n",
      "exploration/env_infos/torso_velocity Min               -1.2988\n",
      "evaluation/num steps total                              2.525e+06\n",
      "evaluation/num paths total                           2525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.895677\n",
      "evaluation/Rewards Std                                  0.137939\n",
      "evaluation/Rewards Max                                  2.17187\n",
      "evaluation/Rewards Min                                  0.39434\n",
      "evaluation/Returns Mean                               895.677\n",
      "evaluation/Returns Std                                135.633\n",
      "evaluation/Returns Max                                967.983\n",
      "evaluation/Returns Min                                488.731\n",
      "evaluation/Actions Mean                                -0.0804704\n",
      "evaluation/Actions Std                                  0.140459\n",
      "evaluation/Actions Max                                  0.58749\n",
      "evaluation/Actions Min                                 -0.683482\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            895.677\n",
      "evaluation/env_infos/final/reward_forward Mean          2.19948e-08\n",
      "evaluation/env_infos/final/reward_forward Std           5.03602e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.53669e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.38945e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0246329\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124985\n",
      "evaluation/env_infos/initial/reward_forward Max         0.239372\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.215979\n",
      "evaluation/env_infos/reward_forward Mean               -0.00173775\n",
      "evaluation/env_infos/reward_forward Std                 0.0503604\n",
      "evaluation/env_infos/reward_forward Max                 1.16103\n",
      "evaluation/env_infos/reward_forward Min                -1.6258\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.104741\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.137359\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.030636\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.518455\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.310635\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.102502\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.112816\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.491526\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.104816\n",
      "evaluation/env_infos/reward_ctrl Std                    0.136994\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00524386\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.60566\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          6.50305e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           4.23294e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.73224e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.38945e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.128117\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.26233\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.577385\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.326249\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00200373\n",
      "evaluation/env_infos/torso_velocity Std                 0.0500692\n",
      "evaluation/env_infos/torso_velocity Max                 1.16103\n",
      "evaluation/env_infos/torso_velocity Min                -2.07866\n",
      "time/data storing (s)                                   0.0153495\n",
      "time/evaluation sampling (s)                           48.2218\n",
      "time/exploration sampling (s)                           2.21465\n",
      "time/logging (s)                                        0.290846\n",
      "time/saving (s)                                         0.0535871\n",
      "time/training (s)                                       4.33688\n",
      "time/epoch (s)                                         55.1331\n",
      "time/total (s)                                       5921.97\n",
      "Epoch                                                 100\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:50:03.031876 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 101 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 103000\n",
      "trainer/QF1 Loss                                        0.289933\n",
      "trainer/QF2 Loss                                        0.225152\n",
      "trainer/Policy Loss                                   -24.9827\n",
      "trainer/Q1 Predictions Mean                            32.9381\n",
      "trainer/Q1 Predictions Std                              1.85466\n",
      "trainer/Q1 Predictions Max                             34.661\n",
      "trainer/Q1 Predictions Min                             23.0217\n",
      "trainer/Q2 Predictions Mean                            32.8233\n",
      "trainer/Q2 Predictions Std                              1.91385\n",
      "trainer/Q2 Predictions Max                             34.5453\n",
      "trainer/Q2 Predictions Min                             23.1203\n",
      "trainer/Q Targets Mean                                 32.7658\n",
      "trainer/Q Targets Std                                   1.98545\n",
      "trainer/Q Targets Max                                  36.3044\n",
      "trainer/Q Targets Min                                  19.9589\n",
      "trainer/Log Pis Mean                                    7.98823\n",
      "trainer/Log Pis Std                                     2.20225\n",
      "trainer/Log Pis Max                                    13.4851\n",
      "trainer/Log Pis Min                                     0.582313\n",
      "trainer/Policy mu Mean                                 -0.00370207\n",
      "trainer/Policy mu Std                                   0.166151\n",
      "trainer/Policy mu Max                                   1.52507\n",
      "trainer/Policy mu Min                                  -1.58971\n",
      "trainer/Policy log std Mean                            -2.38373\n",
      "trainer/Policy log std Std                              0.218922\n",
      "trainer/Policy log std Max                             -1.05091\n",
      "trainer/Policy log std Min                             -3.18724\n",
      "trainer/Alpha                                           0.00689773\n",
      "trainer/Alpha Loss                                     -0.058553\n",
      "exploration/num steps total                        103000\n",
      "exploration/num paths total                           155\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.956981\n",
      "exploration/Rewards Std                                 0.0418404\n",
      "exploration/Rewards Max                                 1.6502\n",
      "exploration/Rewards Min                                 0.860317\n",
      "exploration/Returns Mean                              956.981\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               956.981\n",
      "exploration/Returns Min                               956.981\n",
      "exploration/Actions Mean                                0.00246704\n",
      "exploration/Actions Std                                 0.11147\n",
      "exploration/Actions Max                                 0.394439\n",
      "exploration/Actions Min                                -0.370675\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           956.981\n",
      "exploration/env_infos/final/reward_forward Mean         0.073714\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.073714\n",
      "exploration/env_infos/final/reward_forward Min          0.073714\n",
      "exploration/env_infos/initial/reward_forward Mean       0.057055\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.057055\n",
      "exploration/env_infos/initial/reward_forward Min        0.057055\n",
      "exploration/env_infos/reward_forward Mean              -0.0112913\n",
      "exploration/env_infos/reward_forward Std                0.0781857\n",
      "exploration/env_infos/reward_forward Max                0.770027\n",
      "exploration/env_infos/reward_forward Min               -0.371638\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0411073\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0411073\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0411073\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.12441\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.12441\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.12441\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0497262\n",
      "exploration/env_infos/reward_ctrl Std                   0.0211397\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00498752\n",
      "exploration/env_infos/reward_ctrl Min                  -0.139683\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0849005\n",
      "exploration/env_infos/final/torso_velocity Std          0.0131856\n",
      "exploration/env_infos/final/torso_velocity Max          0.103414\n",
      "exploration/env_infos/final/torso_velocity Min          0.073714\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.146115\n",
      "exploration/env_infos/initial/torso_velocity Std        0.199601\n",
      "exploration/env_infos/initial/torso_velocity Max        0.42262\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0413285\n",
      "exploration/env_infos/torso_velocity Mean              -0.00828089\n",
      "exploration/env_infos/torso_velocity Std                0.0968061\n",
      "exploration/env_infos/torso_velocity Max                0.770027\n",
      "exploration/env_infos/torso_velocity Min               -1.2371\n",
      "evaluation/num steps total                              2.55e+06\n",
      "evaluation/num paths total                           2550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.951681\n",
      "evaluation/Rewards Std                                  0.0276936\n",
      "evaluation/Rewards Max                                  2.0035\n",
      "evaluation/Rewards Min                                  0.747214\n",
      "evaluation/Returns Mean                               951.681\n",
      "evaluation/Returns Std                                 18.632\n",
      "evaluation/Returns Max                                991.127\n",
      "evaluation/Returns Min                                918.137\n",
      "evaluation/Actions Mean                                 0.00538269\n",
      "evaluation/Actions Std                                  0.110474\n",
      "evaluation/Actions Max                                  0.536583\n",
      "evaluation/Actions Min                                 -0.425601\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            951.681\n",
      "evaluation/env_infos/final/reward_forward Mean          1.63845e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.02674e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.19806e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -3.53792e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0171188\n",
      "evaluation/env_infos/initial/reward_forward Std         0.141057\n",
      "evaluation/env_infos/initial/reward_forward Max         0.308918\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.284834\n",
      "evaluation/env_infos/reward_forward Mean               -0.00268858\n",
      "evaluation/env_infos/reward_forward Std                 0.0596812\n",
      "evaluation/env_infos/reward_forward Max                 1.44105\n",
      "evaluation/env_infos/reward_forward Min                -1.64192\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0488498\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0185194\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00777535\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0822264\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.145935\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0412373\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0561712\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.252786\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0489341\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0194667\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00242638\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.255287\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.47046e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.09747e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.35451e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.77361e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.106414\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244672\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.56896\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.333111\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00423635\n",
      "evaluation/env_infos/torso_velocity Std                 0.0643868\n",
      "evaluation/env_infos/torso_velocity Max                 1.44105\n",
      "evaluation/env_infos/torso_velocity Min                -1.74608\n",
      "time/data storing (s)                                   0.0152956\n",
      "time/evaluation sampling (s)                           50.1092\n",
      "time/exploration sampling (s)                           2.58312\n",
      "time/logging (s)                                        0.301346\n",
      "time/saving (s)                                         0.0300041\n",
      "time/training (s)                                       5.12984\n",
      "time/epoch (s)                                         58.1688\n",
      "time/total (s)                                       5981.12\n",
      "Epoch                                                 101\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:51:03.141729 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 102 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 104000\n",
      "trainer/QF1 Loss                                        0.270763\n",
      "trainer/QF2 Loss                                        0.249924\n",
      "trainer/Policy Loss                                   -24.9335\n",
      "trainer/Q1 Predictions Mean                            32.9419\n",
      "trainer/Q1 Predictions Std                              2.81796\n",
      "trainer/Q1 Predictions Max                             36.434\n",
      "trainer/Q1 Predictions Min                             -3.45578\n",
      "trainer/Q2 Predictions Mean                            33.0803\n",
      "trainer/Q2 Predictions Std                              2.36055\n",
      "trainer/Q2 Predictions Max                             35.1213\n",
      "trainer/Q2 Predictions Min                              4.75082\n",
      "trainer/Q Targets Mean                                 32.9573\n",
      "trainer/Q Targets Std                                   2.63372\n",
      "trainer/Q Targets Max                                  34.8803\n",
      "trainer/Q Targets Min                                  -0.673269\n",
      "trainer/Log Pis Mean                                    8.29652\n",
      "trainer/Log Pis Std                                     2.43866\n",
      "trainer/Log Pis Max                                    20.1069\n",
      "trainer/Log Pis Min                                     0.552897\n",
      "trainer/Policy mu Mean                                  0.00279684\n",
      "trainer/Policy mu Std                                   0.177759\n",
      "trainer/Policy mu Max                                   2.76224\n",
      "trainer/Policy mu Min                                  -2.31601\n",
      "trainer/Policy log std Mean                            -2.39569\n",
      "trainer/Policy log std Std                              0.209161\n",
      "trainer/Policy log std Max                             -1.40073\n",
      "trainer/Policy log std Min                             -3.6201\n",
      "trainer/Alpha                                           0.00682262\n",
      "trainer/Alpha Loss                                      1.4795\n",
      "exploration/num steps total                        104000\n",
      "exploration/num paths total                           156\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.918865\n",
      "exploration/Rewards Std                                 0.110157\n",
      "exploration/Rewards Max                                 1.84407\n",
      "exploration/Rewards Min                                 0.64348\n",
      "exploration/Returns Mean                              918.865\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               918.865\n",
      "exploration/Returns Min                               918.865\n",
      "exploration/Actions Mean                               -0.0268921\n",
      "exploration/Actions Std                                 0.159262\n",
      "exploration/Actions Max                                 0.496751\n",
      "exploration/Actions Min                                -0.690896\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           918.865\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0406022\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0406022\n",
      "exploration/env_infos/final/reward_forward Min         -0.0406022\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.221414\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.221414\n",
      "exploration/env_infos/initial/reward_forward Min       -0.221414\n",
      "exploration/env_infos/reward_forward Mean               0.0206556\n",
      "exploration/env_infos/reward_forward Std                0.185891\n",
      "exploration/env_infos/reward_forward Max                1.05788\n",
      "exploration/env_infos/reward_forward Min               -0.728232\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.116806\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.116806\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.116806\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.167603\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.167603\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.167603\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.10435\n",
      "exploration/env_infos/reward_ctrl Std                   0.0565077\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00596703\n",
      "exploration/env_infos/reward_ctrl Min                  -0.35652\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0345026\n",
      "exploration/env_infos/final/torso_velocity Std          0.0778056\n",
      "exploration/env_infos/final/torso_velocity Max          0.141697\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0406022\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.104174\n",
      "exploration/env_infos/initial/torso_velocity Std        0.315842\n",
      "exploration/env_infos/initial/torso_velocity Max        0.531785\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.221414\n",
      "exploration/env_infos/torso_velocity Mean               0.00277374\n",
      "exploration/env_infos/torso_velocity Std                0.198626\n",
      "exploration/env_infos/torso_velocity Max                1.05788\n",
      "exploration/env_infos/torso_velocity Min               -1.28526\n",
      "evaluation/num steps total                              2.575e+06\n",
      "evaluation/num paths total                           2575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.953118\n",
      "evaluation/Rewards Std                                  0.0495985\n",
      "evaluation/Rewards Max                                  2.28993\n",
      "evaluation/Rewards Min                                  0.268145\n",
      "evaluation/Returns Mean                               953.118\n",
      "evaluation/Returns Std                                 40.7182\n",
      "evaluation/Returns Max                                993.262\n",
      "evaluation/Returns Min                                831.871\n",
      "evaluation/Actions Mean                                -0.0113885\n",
      "evaluation/Actions Std                                  0.109016\n",
      "evaluation/Actions Max                                  0.642701\n",
      "evaluation/Actions Min                                 -0.803386\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            953.118\n",
      "evaluation/env_infos/final/reward_forward Mean          1.26228e-07\n",
      "evaluation/env_infos/final/reward_forward Std           9.57466e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.51649e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -6.58027e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0527202\n",
      "evaluation/env_infos/initial/reward_forward Std         0.112805\n",
      "evaluation/env_infos/initial/reward_forward Max         0.276667\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.164473\n",
      "evaluation/env_infos/reward_forward Mean                0.00260191\n",
      "evaluation/env_infos/reward_forward Std                 0.0611975\n",
      "evaluation/env_infos/reward_forward Max                 1.66189\n",
      "evaluation/env_infos/reward_forward Min                -1.28502\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0482049\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0416595\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00603548\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.168833\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.116991\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0395958\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0205244\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.19291\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0480571\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0424294\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00215988\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.784611\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.39067e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           2.11946e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           1.74383e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -3.44996e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.145163\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244869\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.798199\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.372395\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00203971\n",
      "evaluation/env_infos/torso_velocity Std                 0.072981\n",
      "evaluation/env_infos/torso_velocity Max                 1.66189\n",
      "evaluation/env_infos/torso_velocity Min                -1.8455\n",
      "time/data storing (s)                                   0.0152757\n",
      "time/evaluation sampling (s)                           52.2916\n",
      "time/exploration sampling (s)                           2.12311\n",
      "time/logging (s)                                        0.275649\n",
      "time/saving (s)                                         0.025046\n",
      "time/training (s)                                       4.41635\n",
      "time/epoch (s)                                         59.147\n",
      "time/total (s)                                       6041.2\n",
      "Epoch                                                 102\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:51:59.745221 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 103 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 105000\n",
      "trainer/QF1 Loss                                        0.164396\n",
      "trainer/QF2 Loss                                        0.472952\n",
      "trainer/Policy Loss                                   -25.7464\n",
      "trainer/Q1 Predictions Mean                            33.3686\n",
      "trainer/Q1 Predictions Std                              2.11363\n",
      "trainer/Q1 Predictions Max                             35.3362\n",
      "trainer/Q1 Predictions Min                             14.1542\n",
      "trainer/Q2 Predictions Mean                            33.329\n",
      "trainer/Q2 Predictions Std                              1.89966\n",
      "trainer/Q2 Predictions Max                             35.2648\n",
      "trainer/Q2 Predictions Min                             21.7021\n",
      "trainer/Q Targets Mean                                 33.1817\n",
      "trainer/Q Targets Std                                   2.16082\n",
      "trainer/Q Targets Max                                  35.2735\n",
      "trainer/Q Targets Min                                  14.7729\n",
      "trainer/Log Pis Mean                                    7.72832\n",
      "trainer/Log Pis Std                                     2.47003\n",
      "trainer/Log Pis Max                                    16.3054\n",
      "trainer/Log Pis Min                                     0.878978\n",
      "trainer/Policy mu Mean                                  0.0111324\n",
      "trainer/Policy mu Std                                   0.158353\n",
      "trainer/Policy mu Max                                   2.33814\n",
      "trainer/Policy mu Min                                  -2.40684\n",
      "trainer/Policy log std Mean                            -2.36504\n",
      "trainer/Policy log std Std                              0.231816\n",
      "trainer/Policy log std Max                             -1.19469\n",
      "trainer/Policy log std Min                             -3.48106\n",
      "trainer/Alpha                                           0.00754453\n",
      "trainer/Alpha Loss                                     -1.32768\n",
      "exploration/num steps total                        105000\n",
      "exploration/num paths total                           157\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.96136\n",
      "exploration/Rewards Std                                 0.0703997\n",
      "exploration/Rewards Max                                 1.65161\n",
      "exploration/Rewards Min                                 0.713959\n",
      "exploration/Returns Mean                              961.36\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               961.36\n",
      "exploration/Returns Min                               961.36\n",
      "exploration/Actions Mean                                0.00834161\n",
      "exploration/Actions Std                                 0.114447\n",
      "exploration/Actions Max                                 0.378803\n",
      "exploration/Actions Min                                -0.493967\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           961.36\n",
      "exploration/env_infos/final/reward_forward Mean         0.112872\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.112872\n",
      "exploration/env_infos/final/reward_forward Min          0.112872\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0409682\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0409682\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0409682\n",
      "exploration/env_infos/reward_forward Mean               0.036491\n",
      "exploration/env_infos/reward_forward Std                0.164303\n",
      "exploration/env_infos/reward_forward Max                1.31779\n",
      "exploration/env_infos/reward_forward Min               -0.4498\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0360087\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0360087\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0360087\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.286041\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.286041\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.286041\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.052671\n",
      "exploration/env_infos/reward_ctrl Std                   0.0286224\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00683546\n",
      "exploration/env_infos/reward_ctrl Min                  -0.286041\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0480485\n",
      "exploration/env_infos/final/torso_velocity Std          0.0464012\n",
      "exploration/env_infos/final/torso_velocity Max          0.112872\n",
      "exploration/env_infos/final/torso_velocity Min          0.00680419\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.127725\n",
      "exploration/env_infos/initial/torso_velocity Std        0.161987\n",
      "exploration/env_infos/initial/torso_velocity Max        0.346298\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0409682\n",
      "exploration/env_infos/torso_velocity Mean               0.0067715\n",
      "exploration/env_infos/torso_velocity Std                0.180235\n",
      "exploration/env_infos/torso_velocity Max                1.31779\n",
      "exploration/env_infos/torso_velocity Min               -1.06968\n",
      "evaluation/num steps total                              2.6e+06\n",
      "evaluation/num paths total                           2600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.979071\n",
      "evaluation/Rewards Std                                  0.0243446\n",
      "evaluation/Rewards Max                                  2.38203\n",
      "evaluation/Rewards Min                                  0.676905\n",
      "evaluation/Returns Mean                               979.071\n",
      "evaluation/Returns Std                                 12.6107\n",
      "evaluation/Returns Max                                992.837\n",
      "evaluation/Returns Min                                949.564\n",
      "evaluation/Actions Mean                                 0.00327114\n",
      "evaluation/Actions Std                                  0.0733422\n",
      "evaluation/Actions Max                                  0.407874\n",
      "evaluation/Actions Min                                 -0.487708\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            979.071\n",
      "evaluation/env_infos/final/reward_forward Mean         -6.77487e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.057e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.40175e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.14398e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.01497\n",
      "evaluation/env_infos/initial/reward_forward Std         0.132025\n",
      "evaluation/env_infos/initial/reward_forward Max         0.201566\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.261742\n",
      "evaluation/env_infos/reward_forward Mean                0.00287019\n",
      "evaluation/env_infos/reward_forward Std                 0.0544374\n",
      "evaluation/env_infos/reward_forward Max                 1.72858\n",
      "evaluation/env_infos/reward_forward Min                -1.47873\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0213163\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.01334\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00754313\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0520678\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.249072\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0405253\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.169668\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.323095\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0215591\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0157837\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00576373\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.323095\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.46607e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.46826e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.40175e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.76372e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.129224\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.274746\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.708563\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.332493\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0022559\n",
      "evaluation/env_infos/torso_velocity Std                 0.0605166\n",
      "evaluation/env_infos/torso_velocity Max                 1.72858\n",
      "evaluation/env_infos/torso_velocity Min                -1.79092\n",
      "time/data storing (s)                                   0.0218411\n",
      "time/evaluation sampling (s)                           48.0623\n",
      "time/exploration sampling (s)                           2.49041\n",
      "time/logging (s)                                        0.281872\n",
      "time/saving (s)                                         0.0264946\n",
      "time/training (s)                                       4.83069\n",
      "time/epoch (s)                                         55.7136\n",
      "time/total (s)                                       6097.8\n",
      "Epoch                                                 103\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:53:02.351846 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 104 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 106000\n",
      "trainer/QF1 Loss                                        0.149872\n",
      "trainer/QF2 Loss                                        0.195054\n",
      "trainer/Policy Loss                                   -25.6592\n",
      "trainer/Q1 Predictions Mean                            33.4959\n",
      "trainer/Q1 Predictions Std                              1.91894\n",
      "trainer/Q1 Predictions Max                             35.2527\n",
      "trainer/Q1 Predictions Min                             23.4914\n",
      "trainer/Q2 Predictions Mean                            33.53\n",
      "trainer/Q2 Predictions Std                              1.90761\n",
      "trainer/Q2 Predictions Max                             36.9188\n",
      "trainer/Q2 Predictions Min                             23.6172\n",
      "trainer/Q Targets Mean                                 33.5521\n",
      "trainer/Q Targets Std                                   1.88911\n",
      "trainer/Q Targets Max                                  36.1341\n",
      "trainer/Q Targets Min                                  21.6052\n",
      "trainer/Log Pis Mean                                    7.92755\n",
      "trainer/Log Pis Std                                     2.48305\n",
      "trainer/Log Pis Max                                    19.755\n",
      "trainer/Log Pis Min                                     2.08377\n",
      "trainer/Policy mu Mean                                  0.00783509\n",
      "trainer/Policy mu Std                                   0.181839\n",
      "trainer/Policy mu Max                                   3.04343\n",
      "trainer/Policy mu Min                                  -2.80341\n",
      "trainer/Policy log std Mean                            -2.36867\n",
      "trainer/Policy log std Std                              0.197601\n",
      "trainer/Policy log std Max                             -0.969024\n",
      "trainer/Policy log std Min                             -3.50613\n",
      "trainer/Alpha                                           0.00711889\n",
      "trainer/Alpha Loss                                     -0.358151\n",
      "exploration/num steps total                        106000\n",
      "exploration/num paths total                           158\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.950329\n",
      "exploration/Rewards Std                                 0.102121\n",
      "exploration/Rewards Max                                 1.70575\n",
      "exploration/Rewards Min                                 0.560132\n",
      "exploration/Returns Mean                              950.329\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               950.329\n",
      "exploration/Returns Min                               950.329\n",
      "exploration/Actions Mean                                0.0124563\n",
      "exploration/Actions Std                                 0.133748\n",
      "exploration/Actions Max                                 0.525212\n",
      "exploration/Actions Min                                -0.519279\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           950.329\n",
      "exploration/env_infos/final/reward_forward Mean        -0.104096\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.104096\n",
      "exploration/env_infos/final/reward_forward Min         -0.104096\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.026145\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.026145\n",
      "exploration/env_infos/initial/reward_forward Min       -0.026145\n",
      "exploration/env_infos/reward_forward Mean              -0.0246532\n",
      "exploration/env_infos/reward_forward Std                0.232486\n",
      "exploration/env_infos/reward_forward Max                1.04881\n",
      "exploration/env_infos/reward_forward Min               -1.11237\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0859429\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0859429\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0859429\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0724594\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0724594\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0724594\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0721744\n",
      "exploration/env_infos/reward_ctrl Std                   0.043019\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00643456\n",
      "exploration/env_infos/reward_ctrl Min                  -0.439868\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0452575\n",
      "exploration/env_infos/final/torso_velocity Std          0.041735\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0118058\n",
      "exploration/env_infos/final/torso_velocity Min         -0.104096\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.133237\n",
      "exploration/env_infos/initial/torso_velocity Std        0.210888\n",
      "exploration/env_infos/initial/torso_velocity Max        0.431237\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.026145\n",
      "exploration/env_infos/torso_velocity Mean              -0.00577258\n",
      "exploration/env_infos/torso_velocity Std                0.225795\n",
      "exploration/env_infos/torso_velocity Max                1.05017\n",
      "exploration/env_infos/torso_velocity Min               -1.16116\n",
      "evaluation/num steps total                              2.625e+06\n",
      "evaluation/num paths total                           2625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.988657\n",
      "evaluation/Rewards Std                                  0.0417935\n",
      "evaluation/Rewards Max                                  2.29653\n",
      "evaluation/Rewards Min                                  0.0949416\n",
      "evaluation/Returns Mean                               988.657\n",
      "evaluation/Returns Std                                 11.5626\n",
      "evaluation/Returns Max                               1000.87\n",
      "evaluation/Returns Min                                941.33\n",
      "evaluation/Actions Mean                                 0.00865199\n",
      "evaluation/Actions Std                                  0.0579278\n",
      "evaluation/Actions Max                                  0.656025\n",
      "evaluation/Actions Min                                 -0.676129\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            988.657\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.27136e-05\n",
      "evaluation/env_infos/final/reward_forward Std           9.91692e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000149431\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000476903\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.012729\n",
      "evaluation/env_infos/initial/reward_forward Std         0.112113\n",
      "evaluation/env_infos/initial/reward_forward Max         0.289098\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.202997\n",
      "evaluation/env_infos/reward_forward Mean                0.00261911\n",
      "evaluation/env_infos/reward_forward Std                 0.0923225\n",
      "evaluation/env_infos/reward_forward Max                 1.75826\n",
      "evaluation/env_infos/reward_forward Min                -1.44796\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0134892\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0118778\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00500648\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0574309\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.167727\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0464141\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0772703\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.258935\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0137219\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0248822\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00196715\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.905058\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -7.92613e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           8.2124e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000149431\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000491104\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.169344\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.253949\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.74247\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.239703\n",
      "evaluation/env_infos/torso_velocity Mean                0.000126811\n",
      "evaluation/env_infos/torso_velocity Std                 0.0910323\n",
      "evaluation/env_infos/torso_velocity Max                 1.77896\n",
      "evaluation/env_infos/torso_velocity Min                -2.10269\n",
      "time/data storing (s)                                   0.0182811\n",
      "time/evaluation sampling (s)                           52.807\n",
      "time/exploration sampling (s)                           2.70945\n",
      "time/logging (s)                                        0.308264\n",
      "time/saving (s)                                         0.0323238\n",
      "time/training (s)                                       5.88453\n",
      "time/epoch (s)                                         61.7599\n",
      "time/total (s)                                       6160.44\n",
      "Epoch                                                 104\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:54:02.686338 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 105 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 107000\n",
      "trainer/QF1 Loss                                        0.141059\n",
      "trainer/QF2 Loss                                        0.0697823\n",
      "trainer/Policy Loss                                   -25.9286\n",
      "trainer/Q1 Predictions Mean                            33.783\n",
      "trainer/Q1 Predictions Std                              1.47391\n",
      "trainer/Q1 Predictions Max                             35.103\n",
      "trainer/Q1 Predictions Min                             22.5391\n",
      "trainer/Q2 Predictions Mean                            33.9614\n",
      "trainer/Q2 Predictions Std                              1.35398\n",
      "trainer/Q2 Predictions Max                             35.5063\n",
      "trainer/Q2 Predictions Min                             23.5637\n",
      "trainer/Q Targets Mean                                 34.0069\n",
      "trainer/Q Targets Std                                   1.39521\n",
      "trainer/Q Targets Max                                  35.6002\n",
      "trainer/Q Targets Min                                  23.5599\n",
      "trainer/Log Pis Mean                                    7.94379\n",
      "trainer/Log Pis Std                                     2.12366\n",
      "trainer/Log Pis Max                                    15.6818\n",
      "trainer/Log Pis Min                                    -0.170332\n",
      "trainer/Policy mu Mean                                 -0.0147542\n",
      "trainer/Policy mu Std                                   0.176109\n",
      "trainer/Policy mu Max                                   1.66603\n",
      "trainer/Policy mu Min                                  -2.31431\n",
      "trainer/Policy log std Mean                            -2.36746\n",
      "trainer/Policy log std Std                              0.191126\n",
      "trainer/Policy log std Max                             -1.0032\n",
      "trainer/Policy log std Min                             -3.18151\n",
      "trainer/Alpha                                           0.00695892\n",
      "trainer/Alpha Loss                                     -0.279218\n",
      "exploration/num steps total                        107000\n",
      "exploration/num paths total                           159\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.961603\n",
      "exploration/Rewards Std                                 0.10412\n",
      "exploration/Rewards Max                                 1.91367\n",
      "exploration/Rewards Min                                 0.782933\n",
      "exploration/Returns Mean                              961.603\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               961.603\n",
      "exploration/Returns Min                               961.603\n",
      "exploration/Actions Mean                               -0.0150223\n",
      "exploration/Actions Std                                 0.123749\n",
      "exploration/Actions Max                                 0.409742\n",
      "exploration/Actions Min                                -0.570061\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           961.603\n",
      "exploration/env_infos/final/reward_forward Mean        -0.238944\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.238944\n",
      "exploration/env_infos/final/reward_forward Min         -0.238944\n",
      "exploration/env_infos/initial/reward_forward Mean       0.136652\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.136652\n",
      "exploration/env_infos/initial/reward_forward Min        0.136652\n",
      "exploration/env_infos/reward_forward Mean               0.0172995\n",
      "exploration/env_infos/reward_forward Std                0.230221\n",
      "exploration/env_infos/reward_forward Max                0.967337\n",
      "exploration/env_infos/reward_forward Min               -0.738584\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0334111\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0334111\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0334111\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.178389\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.178389\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.178389\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0621578\n",
      "exploration/env_infos/reward_ctrl Std                   0.0331988\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00656209\n",
      "exploration/env_infos/reward_ctrl Min                  -0.241696\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00948721\n",
      "exploration/env_infos/final/torso_velocity Std          0.245737\n",
      "exploration/env_infos/final/torso_velocity Max          0.344159\n",
      "exploration/env_infos/final/torso_velocity Min         -0.238944\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.263662\n",
      "exploration/env_infos/initial/torso_velocity Std        0.218339\n",
      "exploration/env_infos/initial/torso_velocity Max        0.570908\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0834269\n",
      "exploration/env_infos/torso_velocity Mean               6.71408e-05\n",
      "exploration/env_infos/torso_velocity Std                0.245019\n",
      "exploration/env_infos/torso_velocity Max                1.01492\n",
      "exploration/env_infos/torso_velocity Min               -1.62658\n",
      "evaluation/num steps total                              2.65e+06\n",
      "evaluation/num paths total                           2650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.979126\n",
      "evaluation/Rewards Std                                  0.0279731\n",
      "evaluation/Rewards Max                                  2.16894\n",
      "evaluation/Rewards Min                                  0.192451\n",
      "evaluation/Returns Mean                               979.126\n",
      "evaluation/Returns Std                                  8.86566\n",
      "evaluation/Returns Max                                992.009\n",
      "evaluation/Returns Min                                958.792\n",
      "evaluation/Actions Mean                                -0.00474084\n",
      "evaluation/Actions Std                                  0.0735897\n",
      "evaluation/Actions Max                                  0.8041\n",
      "evaluation/Actions Min                                 -0.734165\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            979.126\n",
      "evaluation/env_infos/final/reward_forward Mean          5.55424e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.89765e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.17034e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.92293e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0303658\n",
      "evaluation/env_infos/initial/reward_forward Std         0.114485\n",
      "evaluation/env_infos/initial/reward_forward Max         0.237171\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.201592\n",
      "evaluation/env_infos/reward_forward Mean                0.0039377\n",
      "evaluation/env_infos/reward_forward Std                 0.0633313\n",
      "evaluation/env_infos/reward_forward Max                 1.53424\n",
      "evaluation/env_infos/reward_forward Min                -0.998384\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0207154\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00927733\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00725064\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0411259\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.116241\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0466296\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0459337\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.223049\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0217517\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0164041\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00222059\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.807549\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          6.02102e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.45466e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.30157e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.92293e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.162987\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.250211\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.651664\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.447965\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000202889\n",
      "evaluation/env_infos/torso_velocity Std                 0.0687443\n",
      "evaluation/env_infos/torso_velocity Max                 1.53424\n",
      "evaluation/env_infos/torso_velocity Min                -1.79062\n",
      "time/data storing (s)                                   0.0163677\n",
      "time/evaluation sampling (s)                           52.0139\n",
      "time/exploration sampling (s)                           2.34216\n",
      "time/logging (s)                                        0.285345\n",
      "time/saving (s)                                         0.0269054\n",
      "time/training (s)                                       4.64378\n",
      "time/epoch (s)                                         59.3285\n",
      "time/total (s)                                       6220.74\n",
      "Epoch                                                 105\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:54:58.648732 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 106 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 108000\n",
      "trainer/QF1 Loss                                        0.118387\n",
      "trainer/QF2 Loss                                        0.250267\n",
      "trainer/Policy Loss                                   -26.6471\n",
      "trainer/Q1 Predictions Mean                            33.9534\n",
      "trainer/Q1 Predictions Std                              2.64917\n",
      "trainer/Q1 Predictions Max                             35.515\n",
      "trainer/Q1 Predictions Min                             -1.24327\n",
      "trainer/Q2 Predictions Mean                            34.2446\n",
      "trainer/Q2 Predictions Std                              2.54204\n",
      "trainer/Q2 Predictions Max                             35.8218\n",
      "trainer/Q2 Predictions Min                              1.51027\n",
      "trainer/Q Targets Mean                                 34.0349\n",
      "trainer/Q Targets Std                                   2.54317\n",
      "trainer/Q Targets Max                                  36.1392\n",
      "trainer/Q Targets Min                                   0.346005\n",
      "trainer/Log Pis Mean                                    7.54893\n",
      "trainer/Log Pis Std                                     2.19866\n",
      "trainer/Log Pis Max                                    13.2086\n",
      "trainer/Log Pis Min                                     0.83351\n",
      "trainer/Policy mu Mean                                 -0.0045587\n",
      "trainer/Policy mu Std                                   0.167544\n",
      "trainer/Policy mu Max                                   1.36683\n",
      "trainer/Policy mu Min                                  -1.74084\n",
      "trainer/Policy log std Mean                            -2.32481\n",
      "trainer/Policy log std Std                              0.223152\n",
      "trainer/Policy log std Max                             -0.658588\n",
      "trainer/Policy log std Min                             -3.03549\n",
      "trainer/Alpha                                           0.00749261\n",
      "trainer/Alpha Loss                                     -2.20637\n",
      "exploration/num steps total                        108000\n",
      "exploration/num paths total                           160\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.938791\n",
      "exploration/Rewards Std                                 0.0295829\n",
      "exploration/Rewards Max                                 1.05518\n",
      "exploration/Rewards Min                                 0.692594\n",
      "exploration/Returns Mean                              938.791\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               938.791\n",
      "exploration/Returns Min                               938.791\n",
      "exploration/Actions Mean                                0.0106744\n",
      "exploration/Actions Std                                 0.124303\n",
      "exploration/Actions Max                                 0.50259\n",
      "exploration/Actions Min                                -0.446364\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           938.791\n",
      "exploration/env_infos/final/reward_forward Mean         0.00023511\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00023511\n",
      "exploration/env_infos/final/reward_forward Min          0.00023511\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.132159\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.132159\n",
      "exploration/env_infos/initial/reward_forward Min       -0.132159\n",
      "exploration/env_infos/reward_forward Mean              -0.00309647\n",
      "exploration/env_infos/reward_forward Std                0.0950272\n",
      "exploration/env_infos/reward_forward Max                1.34068\n",
      "exploration/env_infos/reward_forward Min               -0.607538\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0342701\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0342701\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0342701\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.266014\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.266014\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.266014\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0622607\n",
      "exploration/env_infos/reward_ctrl Std                   0.0286607\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00737527\n",
      "exploration/env_infos/reward_ctrl Min                  -0.307406\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.000746447\n",
      "exploration/env_infos/final/torso_velocity Std          0.00224296\n",
      "exploration/env_infos/final/torso_velocity Max          0.00371323\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00170901\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0703154\n",
      "exploration/env_infos/initial/torso_velocity Std        0.277432\n",
      "exploration/env_infos/initial/torso_velocity Max        0.462595\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.132159\n",
      "exploration/env_infos/torso_velocity Mean              -0.00623852\n",
      "exploration/env_infos/torso_velocity Std                0.0972773\n",
      "exploration/env_infos/torso_velocity Max                1.34068\n",
      "exploration/env_infos/torso_velocity Min               -1.15161\n",
      "evaluation/num steps total                              2.675e+06\n",
      "evaluation/num paths total                           2675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.947161\n",
      "evaluation/Rewards Std                                  0.0346126\n",
      "evaluation/Rewards Max                                  2.45122\n",
      "evaluation/Rewards Min                                  0.548916\n",
      "evaluation/Returns Mean                               947.161\n",
      "evaluation/Returns Std                                 20.2654\n",
      "evaluation/Returns Max                                969.976\n",
      "evaluation/Returns Min                                897.581\n",
      "evaluation/Actions Mean                                -0.00292965\n",
      "evaluation/Actions Std                                  0.11583\n",
      "evaluation/Actions Max                                  0.551495\n",
      "evaluation/Actions Min                                 -0.553534\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            947.161\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.59981e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.44811e-07\n",
      "evaluation/env_infos/final/reward_forward Max           1.16721e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.98819e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0280868\n",
      "evaluation/env_infos/initial/reward_forward Std         0.127516\n",
      "evaluation/env_infos/initial/reward_forward Max         0.189408\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.382411\n",
      "evaluation/env_infos/reward_forward Mean               -0.00116725\n",
      "evaluation/env_infos/reward_forward Std                 0.0536442\n",
      "evaluation/env_infos/reward_forward Max                 1.56308\n",
      "evaluation/env_infos/reward_forward Min                -1.2445\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0533195\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0210358\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0287626\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.103076\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.272816\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0739264\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.137914\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.451084\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.053701\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0228295\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0104045\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.451084\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -5.95491e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.5268e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           6.16357e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.98819e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.116392\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.265673\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.591606\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.382411\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00266905\n",
      "evaluation/env_infos/torso_velocity Std                 0.0579752\n",
      "evaluation/env_infos/torso_velocity Max                 1.56308\n",
      "evaluation/env_infos/torso_velocity Min                -1.78514\n",
      "time/data storing (s)                                   0.0169162\n",
      "time/evaluation sampling (s)                           47.5511\n",
      "time/exploration sampling (s)                           2.32265\n",
      "time/logging (s)                                        0.280067\n",
      "time/saving (s)                                         0.0281811\n",
      "time/training (s)                                       4.87702\n",
      "time/epoch (s)                                         55.076\n",
      "time/total (s)                                       6276.7\n",
      "Epoch                                                 106\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:55:56.119624 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 107 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 109000\n",
      "trainer/QF1 Loss                                        0.135495\n",
      "trainer/QF2 Loss                                        0.178974\n",
      "trainer/Policy Loss                                   -26.6031\n",
      "trainer/Q1 Predictions Mean                            34.435\n",
      "trainer/Q1 Predictions Std                              1.358\n",
      "trainer/Q1 Predictions Max                             38.1708\n",
      "trainer/Q1 Predictions Min                             25.9621\n",
      "trainer/Q2 Predictions Mean                            34.3418\n",
      "trainer/Q2 Predictions Std                              1.42081\n",
      "trainer/Q2 Predictions Max                             36.3411\n",
      "trainer/Q2 Predictions Min                             24.1025\n",
      "trainer/Q Targets Mean                                 34.5029\n",
      "trainer/Q Targets Std                                   1.32716\n",
      "trainer/Q Targets Max                                  36.3824\n",
      "trainer/Q Targets Min                                  25.3584\n",
      "trainer/Log Pis Mean                                    7.82556\n",
      "trainer/Log Pis Std                                     2.71398\n",
      "trainer/Log Pis Max                                    26.4203\n",
      "trainer/Log Pis Min                                    -0.493141\n",
      "trainer/Policy mu Mean                                  0.0656833\n",
      "trainer/Policy mu Std                                   0.192339\n",
      "trainer/Policy mu Max                                   2.8312\n",
      "trainer/Policy mu Min                                  -2.74862\n",
      "trainer/Policy log std Mean                            -2.37136\n",
      "trainer/Policy log std Std                              0.212458\n",
      "trainer/Policy log std Max                             -0.428822\n",
      "trainer/Policy log std Min                             -4.38059\n",
      "trainer/Alpha                                           0.00812574\n",
      "trainer/Alpha Loss                                     -0.839806\n",
      "exploration/num steps total                        109000\n",
      "exploration/num paths total                           161\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.916728\n",
      "exploration/Rewards Std                                 0.0589275\n",
      "exploration/Rewards Max                                 1.4455\n",
      "exploration/Rewards Min                                 0.658678\n",
      "exploration/Returns Mean                              916.728\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               916.728\n",
      "exploration/Returns Min                               916.728\n",
      "exploration/Actions Mean                                0.0801731\n",
      "exploration/Actions Std                                 0.127365\n",
      "exploration/Actions Max                                 0.566488\n",
      "exploration/Actions Min                                -0.430446\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           916.728\n",
      "exploration/env_infos/final/reward_forward Mean         0.115502\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.115502\n",
      "exploration/env_infos/final/reward_forward Min          0.115502\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.117817\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.117817\n",
      "exploration/env_infos/initial/reward_forward Min       -0.117817\n",
      "exploration/env_infos/reward_forward Mean              -0.0283456\n",
      "exploration/env_infos/reward_forward Std                0.179703\n",
      "exploration/env_infos/reward_forward Max                1.48103\n",
      "exploration/env_infos/reward_forward Min               -0.595549\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.101877\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.101877\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.101877\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.226423\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.226423\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.226423\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0905985\n",
      "exploration/env_infos/reward_ctrl Std                   0.0436096\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00895018\n",
      "exploration/env_infos/reward_ctrl Min                  -0.341322\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0237726\n",
      "exploration/env_infos/final/torso_velocity Std          0.0758676\n",
      "exploration/env_infos/final/torso_velocity Max          0.115502\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0702908\n",
      "exploration/env_infos/initial/torso_velocity Mean      -0.0283397\n",
      "exploration/env_infos/initial/torso_velocity Std        0.206529\n",
      "exploration/env_infos/initial/torso_velocity Max        0.257183\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.224385\n",
      "exploration/env_infos/torso_velocity Mean              -0.0116468\n",
      "exploration/env_infos/torso_velocity Std                0.175716\n",
      "exploration/env_infos/torso_velocity Max                1.48103\n",
      "exploration/env_infos/torso_velocity Min               -1.22515\n",
      "evaluation/num steps total                              2.7e+06\n",
      "evaluation/num paths total                           2700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.946059\n",
      "evaluation/Rewards Std                                  0.0348629\n",
      "evaluation/Rewards Max                                  2.1496\n",
      "evaluation/Rewards Min                                  0.467099\n",
      "evaluation/Returns Mean                               946.059\n",
      "evaluation/Returns Std                                 18.0255\n",
      "evaluation/Returns Max                                972.876\n",
      "evaluation/Returns Min                                907.979\n",
      "evaluation/Actions Mean                                 0.0752401\n",
      "evaluation/Actions Std                                  0.0904058\n",
      "evaluation/Actions Max                                  0.706271\n",
      "evaluation/Actions Min                                 -0.549957\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            946.059\n",
      "evaluation/env_infos/final/reward_forward Mean          1.38236e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.87658e-07\n",
      "evaluation/env_infos/final/reward_forward Max           1.24415e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -1.11702e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0228454\n",
      "evaluation/env_infos/initial/reward_forward Std         0.153348\n",
      "evaluation/env_infos/initial/reward_forward Max         0.435382\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.247984\n",
      "evaluation/env_infos/reward_forward Mean               -0.000290522\n",
      "evaluation/env_infos/reward_forward Std                 0.0499127\n",
      "evaluation/env_infos/reward_forward Max                 1.36714\n",
      "evaluation/env_infos/reward_forward Min                -1.15548\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0546727\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0176859\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0324365\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0927778\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.271589\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0744281\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.149356\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.455448\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0553371\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0217169\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0156688\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.532901\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          9.8882e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           4.88579e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.61911e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.11702e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.134598\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.253738\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.726855\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.286852\n",
      "evaluation/env_infos/torso_velocity Mean                2.63656e-05\n",
      "evaluation/env_infos/torso_velocity Std                 0.056094\n",
      "evaluation/env_infos/torso_velocity Max                 1.36714\n",
      "evaluation/env_infos/torso_velocity Min                -1.92046\n",
      "time/data storing (s)                                   0.0167705\n",
      "time/evaluation sampling (s)                           47.6916\n",
      "time/exploration sampling (s)                           2.25177\n",
      "time/logging (s)                                        0.29217\n",
      "time/saving (s)                                         0.0279259\n",
      "time/training (s)                                       6.29411\n",
      "time/epoch (s)                                         56.5744\n",
      "time/total (s)                                       6334.18\n",
      "Epoch                                                 107\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:56:55.424844 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 108 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 110000\n",
      "trainer/QF1 Loss                                        0.223556\n",
      "trainer/QF2 Loss                                        0.276698\n",
      "trainer/Policy Loss                                   -27.2549\n",
      "trainer/Q1 Predictions Mean                            34.7745\n",
      "trainer/Q1 Predictions Std                              1.3407\n",
      "trainer/Q1 Predictions Max                             36.3542\n",
      "trainer/Q1 Predictions Min                             23.0181\n",
      "trainer/Q2 Predictions Mean                            34.9024\n",
      "trainer/Q2 Predictions Std                              1.39653\n",
      "trainer/Q2 Predictions Max                             36.5313\n",
      "trainer/Q2 Predictions Min                             24.3569\n",
      "trainer/Q Targets Mean                                 34.6582\n",
      "trainer/Q Targets Std                                   1.3613\n",
      "trainer/Q Targets Max                                  36.664\n",
      "trainer/Q Targets Min                                  24.652\n",
      "trainer/Log Pis Mean                                    7.64021\n",
      "trainer/Log Pis Std                                     2.20466\n",
      "trainer/Log Pis Max                                    14.9068\n",
      "trainer/Log Pis Min                                     1.19942\n",
      "trainer/Policy mu Mean                                 -0.0905391\n",
      "trainer/Policy mu Std                                   0.183196\n",
      "trainer/Policy mu Max                                   0.980973\n",
      "trainer/Policy mu Min                                  -1.18077\n",
      "trainer/Policy log std Mean                            -2.32148\n",
      "trainer/Policy log std Std                              0.183134\n",
      "trainer/Policy log std Max                             -1.81929\n",
      "trainer/Policy log std Min                             -3.24037\n",
      "trainer/Alpha                                           0.00804834\n",
      "trainer/Alpha Loss                                     -1.7347\n",
      "exploration/num steps total                        110000\n",
      "exploration/num paths total                           162\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.770964\n",
      "exploration/Rewards Std                                 0.0764334\n",
      "exploration/Rewards Max                                 0.955166\n",
      "exploration/Rewards Min                                 0.433784\n",
      "exploration/Returns Mean                              770.964\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               770.964\n",
      "exploration/Returns Min                               770.964\n",
      "exploration/Actions Mean                               -0.123125\n",
      "exploration/Actions Std                                 0.205559\n",
      "exploration/Actions Max                                 0.490652\n",
      "exploration/Actions Min                                -0.681224\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           770.964\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0044359\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0044359\n",
      "exploration/env_infos/final/reward_forward Min         -0.0044359\n",
      "exploration/env_infos/initial/reward_forward Mean       0.119994\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.119994\n",
      "exploration/env_infos/initial/reward_forward Min        0.119994\n",
      "exploration/env_infos/reward_forward Mean              -0.00583955\n",
      "exploration/env_infos/reward_forward Std                0.0652723\n",
      "exploration/env_infos/reward_forward Max                0.279199\n",
      "exploration/env_infos/reward_forward Min               -0.429106\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.239172\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.239172\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.239172\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.302703\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.302703\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.302703\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.229658\n",
      "exploration/env_infos/reward_ctrl Std                   0.076096\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0448341\n",
      "exploration/env_infos/reward_ctrl Min                  -0.566216\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00484569\n",
      "exploration/env_infos/final/torso_velocity Std          0.00786946\n",
      "exploration/env_infos/final/torso_velocity Max          0.00458096\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0146821\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.185529\n",
      "exploration/env_infos/initial/torso_velocity Std        0.201591\n",
      "exploration/env_infos/initial/torso_velocity Max        0.458582\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0219891\n",
      "exploration/env_infos/torso_velocity Mean              -0.00591915\n",
      "exploration/env_infos/torso_velocity Std                0.140043\n",
      "exploration/env_infos/torso_velocity Max                1.01745\n",
      "exploration/env_infos/torso_velocity Min               -1.20599\n",
      "evaluation/num steps total                              2.725e+06\n",
      "evaluation/num paths total                           2725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.917055\n",
      "evaluation/Rewards Std                                  0.0614517\n",
      "evaluation/Rewards Max                                  2.43691\n",
      "evaluation/Rewards Min                                  0.254213\n",
      "evaluation/Returns Mean                               917.055\n",
      "evaluation/Returns Std                                 54.0695\n",
      "evaluation/Returns Max                                969.976\n",
      "evaluation/Returns Min                                769.432\n",
      "evaluation/Actions Mean                                -0.077148\n",
      "evaluation/Actions Std                                  0.122026\n",
      "evaluation/Actions Max                                  0.477196\n",
      "evaluation/Actions Min                                 -0.75488\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            917.055\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.34378e-05\n",
      "evaluation/env_infos/final/reward_forward Std           6.56388e-05\n",
      "evaluation/env_infos/final/reward_forward Max           3.29831e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000334998\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00576422\n",
      "evaluation/env_infos/initial/reward_forward Std         0.118883\n",
      "evaluation/env_infos/initial/reward_forward Max         0.223226\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.224462\n",
      "evaluation/env_infos/reward_forward Mean               -0.00137982\n",
      "evaluation/env_infos/reward_forward Std                 0.0524158\n",
      "evaluation/env_infos/reward_forward Max                 1.53398\n",
      "evaluation/env_infos/reward_forward Min                -1.33743\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0810154\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0549433\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0257132\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.231729\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.257943\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.070268\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.134658\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.362361\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0833688\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0593383\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0068368\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.745787\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.82066e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           6.87793e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000390893\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000334998\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.139292\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.26101\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.670599\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.325325\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00353434\n",
      "evaluation/env_infos/torso_velocity Std                 0.0626129\n",
      "evaluation/env_infos/torso_velocity Max                 1.53398\n",
      "evaluation/env_infos/torso_velocity Min                -2.09934\n",
      "time/data storing (s)                                   0.0173536\n",
      "time/evaluation sampling (s)                           49.8699\n",
      "time/exploration sampling (s)                           2.05773\n",
      "time/logging (s)                                        0.368386\n",
      "time/saving (s)                                         0.0285624\n",
      "time/training (s)                                       6.09108\n",
      "time/epoch (s)                                         58.433\n",
      "time/total (s)                                       6393.56\n",
      "Epoch                                                 108\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:57:57.086998 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 109 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 111000\n",
      "trainer/QF1 Loss                                        0.0825335\n",
      "trainer/QF2 Loss                                        0.126778\n",
      "trainer/Policy Loss                                   -26.5396\n",
      "trainer/Q1 Predictions Mean                            34.894\n",
      "trainer/Q1 Predictions Std                              1.65852\n",
      "trainer/Q1 Predictions Max                             37.5281\n",
      "trainer/Q1 Predictions Min                             24.2108\n",
      "trainer/Q2 Predictions Mean                            34.8072\n",
      "trainer/Q2 Predictions Std                              1.6632\n",
      "trainer/Q2 Predictions Max                             36.9903\n",
      "trainer/Q2 Predictions Min                             23.6152\n",
      "trainer/Q Targets Mean                                 34.8884\n",
      "trainer/Q Targets Std                                   1.60718\n",
      "trainer/Q Targets Max                                  38.0744\n",
      "trainer/Q Targets Min                                  24.9092\n",
      "trainer/Log Pis Mean                                    8.41037\n",
      "trainer/Log Pis Std                                     2.26514\n",
      "trainer/Log Pis Max                                    18.433\n",
      "trainer/Log Pis Min                                     1.27998\n",
      "trainer/Policy mu Mean                                 -0.00912664\n",
      "trainer/Policy mu Std                                   0.191533\n",
      "trainer/Policy mu Max                                   2.19036\n",
      "trainer/Policy mu Min                                  -2.25761\n",
      "trainer/Policy log std Mean                            -2.41928\n",
      "trainer/Policy log std Std                              0.246727\n",
      "trainer/Policy log std Max                             -1.63827\n",
      "trainer/Policy log std Min                             -3.28691\n",
      "trainer/Alpha                                           0.00822608\n",
      "trainer/Alpha Loss                                      1.97072\n",
      "exploration/num steps total                        111000\n",
      "exploration/num paths total                           163\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.916129\n",
      "exploration/Rewards Std                                 0.062381\n",
      "exploration/Rewards Max                                 1.40544\n",
      "exploration/Rewards Min                                 0.586848\n",
      "exploration/Returns Mean                              916.129\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               916.129\n",
      "exploration/Returns Min                               916.129\n",
      "exploration/Actions Mean                               -0.00357588\n",
      "exploration/Actions Std                                 0.153872\n",
      "exploration/Actions Max                                 0.522545\n",
      "exploration/Actions Min                                -0.753025\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           916.129\n",
      "exploration/env_infos/final/reward_forward Mean         0.0782786\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0782786\n",
      "exploration/env_infos/final/reward_forward Min          0.0782786\n",
      "exploration/env_infos/initial/reward_forward Mean       0.333271\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.333271\n",
      "exploration/env_infos/initial/reward_forward Min        0.333271\n",
      "exploration/env_infos/reward_forward Mean              -0.0202363\n",
      "exploration/env_infos/reward_forward Std                0.170064\n",
      "exploration/env_infos/reward_forward Max                0.540136\n",
      "exploration/env_infos/reward_forward Min               -0.763446\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0620706\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0620706\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0620706\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.227637\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.227637\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.227637\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0947572\n",
      "exploration/env_infos/reward_ctrl Std                   0.0426763\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00895444\n",
      "exploration/env_infos/reward_ctrl Min                  -0.413152\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0540053\n",
      "exploration/env_infos/final/torso_velocity Std          0.0210964\n",
      "exploration/env_infos/final/torso_velocity Max          0.0782786\n",
      "exploration/env_infos/final/torso_velocity Min          0.0268455\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.266049\n",
      "exploration/env_infos/initial/torso_velocity Std        0.282911\n",
      "exploration/env_infos/initial/torso_velocity Max        0.574006\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.109131\n",
      "exploration/env_infos/torso_velocity Mean              -0.0104858\n",
      "exploration/env_infos/torso_velocity Std                0.14834\n",
      "exploration/env_infos/torso_velocity Max                0.574006\n",
      "exploration/env_infos/torso_velocity Min               -1.59033\n",
      "evaluation/num steps total                              2.75e+06\n",
      "evaluation/num paths total                           2750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.90485\n",
      "evaluation/Rewards Std                                  0.0481565\n",
      "evaluation/Rewards Max                                  2.71439\n",
      "evaluation/Rewards Min                                  0.525866\n",
      "evaluation/Returns Mean                               904.85\n",
      "evaluation/Returns Std                                 37.7969\n",
      "evaluation/Returns Max                                957.849\n",
      "evaluation/Returns Min                                776.942\n",
      "evaluation/Actions Mean                                -0.000791908\n",
      "evaluation/Actions Std                                  0.154967\n",
      "evaluation/Actions Max                                  0.583778\n",
      "evaluation/Actions Min                                 -0.749491\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            904.85\n",
      "evaluation/env_infos/final/reward_forward Mean          3.14096e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.91638e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.84619e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.26089e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0476432\n",
      "evaluation/env_infos/initial/reward_forward Std         0.139994\n",
      "evaluation/env_infos/initial/reward_forward Max         0.277375\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.225081\n",
      "evaluation/env_infos/reward_forward Mean               -0.000289784\n",
      "evaluation/env_infos/reward_forward Std                 0.0532927\n",
      "evaluation/env_infos/reward_forward Max                 1.63421\n",
      "evaluation/env_infos/reward_forward Min                -1.56464\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0956465\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0382562\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0417318\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.222557\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.243556\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0492083\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.150306\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.369307\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0960611\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0395656\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0143868\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.474134\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.4844e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.93082e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.84619e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.53945e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.146193\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.261925\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.714268\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.322506\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00247961\n",
      "evaluation/env_infos/torso_velocity Std                 0.0616798\n",
      "evaluation/env_infos/torso_velocity Max                 1.63421\n",
      "evaluation/env_infos/torso_velocity Min                -1.84223\n",
      "time/data storing (s)                                   0.0215548\n",
      "time/evaluation sampling (s)                           51.5804\n",
      "time/exploration sampling (s)                           2.67782\n",
      "time/logging (s)                                        0.298758\n",
      "time/saving (s)                                         0.0403079\n",
      "time/training (s)                                       5.92544\n",
      "time/epoch (s)                                         60.5443\n",
      "time/total (s)                                       6455.14\n",
      "Epoch                                                 109\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:58:50.084968 PDT | [gher-antdirectionnewsparse-SAC-300e-1000s-disc0.99-horizon1000_2021_05_25_19_10_24_0000--s-10] Epoch 110 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 112000\n",
      "trainer/QF1 Loss                                        0.0941519\n",
      "trainer/QF2 Loss                                        0.112629\n",
      "trainer/Policy Loss                                   -26.8648\n",
      "trainer/Q1 Predictions Mean                            35.3536\n",
      "trainer/Q1 Predictions Std                              1.46111\n",
      "trainer/Q1 Predictions Max                             36.6386\n",
      "trainer/Q1 Predictions Min                             25.1822\n",
      "trainer/Q2 Predictions Mean                            35.4746\n",
      "trainer/Q2 Predictions Std                              1.42395\n",
      "trainer/Q2 Predictions Max                             36.7861\n",
      "trainer/Q2 Predictions Min                             25.2549\n",
      "trainer/Q Targets Mean                                 35.2628\n",
      "trainer/Q Targets Std                                   1.45558\n",
      "trainer/Q Targets Max                                  36.6357\n",
      "trainer/Q Targets Min                                  24.1762\n",
      "trainer/Log Pis Mean                                    8.5831\n",
      "trainer/Log Pis Std                                     2.02871\n",
      "trainer/Log Pis Max                                    18.0862\n",
      "trainer/Log Pis Min                                     1.72579\n",
      "trainer/Policy mu Mean                                 -0.0151401\n",
      "trainer/Policy mu Std                                   0.179771\n",
      "trainer/Policy mu Max                                   1.20677\n",
      "trainer/Policy mu Min                                  -1.69028\n",
      "trainer/Policy log std Mean                            -2.43986\n",
      "trainer/Policy log std Std                              0.209301\n",
      "trainer/Policy log std Max                             -1.63098\n",
      "trainer/Policy log std Min                             -3.25312\n",
      "trainer/Alpha                                           0.00694337\n",
      "trainer/Alpha Loss                                      2.89896\n",
      "exploration/num steps total                        112000\n",
      "exploration/num paths total                           164\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.852561\n",
      "exploration/Rewards Std                                 0.0753414\n",
      "exploration/Rewards Max                                 1.05013\n",
      "exploration/Rewards Min                                 0.445737\n",
      "exploration/Returns Mean                              852.561\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               852.561\n",
      "exploration/Returns Min                               852.561\n",
      "exploration/Actions Mean                               -0.0332698\n",
      "exploration/Actions Std                                 0.190132\n",
      "exploration/Actions Max                                 0.579431\n",
      "exploration/Actions Min                                -0.584847\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           852.561\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00624527\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.00624527\n",
      "exploration/env_infos/final/reward_forward Min         -0.00624527\n",
      "exploration/env_infos/initial/reward_forward Mean       0.193956\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.193956\n",
      "exploration/env_infos/initial/reward_forward Min        0.193956\n",
      "exploration/env_infos/reward_forward Mean               0.00977132\n",
      "exploration/env_infos/reward_forward Std                0.101444\n",
      "exploration/env_infos/reward_forward Max                1.05602\n",
      "exploration/env_infos/reward_forward Min               -0.317258\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.122169\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.122169\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.122169\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.155596\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.155596\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.155596\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.149029\n",
      "exploration/env_infos/reward_ctrl Std                   0.0740828\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0196043\n",
      "exploration/env_infos/reward_ctrl Min                  -0.554263\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.000800756\n",
      "exploration/env_infos/final/torso_velocity Std          0.0150367\n",
      "exploration/env_infos/final/torso_velocity Max          0.0216996\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0130521\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.268688\n",
      "exploration/env_infos/initial/torso_velocity Std        0.0743193\n",
      "exploration/env_infos/initial/torso_velocity Max        0.370057\n",
      "exploration/env_infos/initial/torso_velocity Min        0.193956\n",
      "exploration/env_infos/torso_velocity Mean               0.0116104\n",
      "exploration/env_infos/torso_velocity Std                0.135174\n",
      "exploration/env_infos/torso_velocity Max                1.05602\n",
      "exploration/env_infos/torso_velocity Min               -0.823739\n",
      "evaluation/num steps total                              2.775e+06\n",
      "evaluation/num paths total                           2775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.883047\n",
      "evaluation/Rewards Std                                  0.0348016\n",
      "evaluation/Rewards Max                                  2.185\n",
      "evaluation/Rewards Min                                  0.243071\n",
      "evaluation/Returns Mean                               883.047\n",
      "evaluation/Returns Std                                 19.382\n",
      "evaluation/Returns Max                                917.709\n",
      "evaluation/Returns Min                                841.727\n",
      "evaluation/Actions Mean                                -0.0276799\n",
      "evaluation/Actions Std                                  0.169163\n",
      "evaluation/Actions Max                                  0.763287\n",
      "evaluation/Actions Min                                 -0.684985\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            883.047\n",
      "evaluation/env_infos/final/reward_forward Mean         -4.0979e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.91121e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.72142e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.70167e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0861945\n",
      "evaluation/env_infos/initial/reward_forward Std         0.141931\n",
      "evaluation/env_infos/initial/reward_forward Max         0.441079\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.160609\n",
      "evaluation/env_infos/reward_forward Mean                0.00523856\n",
      "evaluation/env_infos/reward_forward Std                 0.0672359\n",
      "evaluation/env_infos/reward_forward Max                 1.60583\n",
      "evaluation/env_infos/reward_forward Min                -1.02242\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.116767\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0200578\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.081425\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.159047\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.243839\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0636435\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.167805\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.395614\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.117529\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0280005\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00648178\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.756929\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -2.98215e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.8385e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           5.78997e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.70167e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.181524\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.252378\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.824764\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.295559\n",
      "evaluation/env_infos/torso_velocity Mean                0.000884913\n",
      "evaluation/env_infos/torso_velocity Std                 0.0637949\n",
      "evaluation/env_infos/torso_velocity Max                 1.60583\n",
      "evaluation/env_infos/torso_velocity Min                -2.03796\n",
      "time/data storing (s)                                   0.0150691\n",
      "time/evaluation sampling (s)                           45.3898\n",
      "time/exploration sampling (s)                           1.98374\n",
      "time/logging (s)                                        0.282964\n",
      "time/saving (s)                                         0.0292126\n",
      "time/training (s)                                       4.21087\n",
      "time/epoch (s)                                         51.9117\n",
      "time/total (s)                                       6508.12\n",
      "Epoch                                                 110\n",
      "-------------------------------------------------  ----------------\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 300 --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 300 --relabel --n_sampled_latents 1 --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 300 --relabel --n_sampled_latents 100 --use_advantages --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 300 --relabel --n_sampled_latents 100 --use_advantages --env antdirectionnewsparse --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736924a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_no_r = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000/gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10/progress.csv')\n",
    "# df_rr   = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000/gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10/progress.csv')\n",
    "# df_air  = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000/gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10/progress.csv')\n",
    "# df_ar   = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000/gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_14_49_22_0000--s-10/progress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'evaluation/Average Returns'\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_no_r[name], label=\"No Relabeling\")\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_rr[name], label=\"Random Relabeling\")\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_air[name], label=\"AIR\")\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_ar[name], label=\"AR\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d76f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
